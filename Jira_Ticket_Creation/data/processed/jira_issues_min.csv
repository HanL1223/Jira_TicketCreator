issue_key,project_key,project_name,issue_type,status,priority,created,updated,labels,summary,description,acceptance_criteria,comments
CSCI-770,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Dec/25 9:51 AM,18/Dec/25 9:32 AM,,Appriss worksheet response,"h2. Overview

This task involves reviewing comments related to Appriss data. The goal is to identify and address any issues or concerns raised in the comments. This review is crucial for maintaining data quality and ensuring accurate reporting, ultimately improving the reliability of Appriss data for decision-making.

h2. Steps

# Access the Appriss comment review dashboard.
# Filter comments by date range (last 7 days).
# Read each comment carefully, noting any issues, concerns, or questions.
# Investigate any reported data discrepancies or errors.
# Document findings and resolutions for each comment.
# Escalate any unresolved issues to the appropriate team.
# Update the comment status to ""Resolved"" or ""In Progress"" as appropriate.

h2. Acceptance Criteria

* [ ] All comments from the last 7 days have been reviewed.
* [ ] Each comment has a documented resolution or action plan.
* [ ] All identified data discrepancies have been investigated and addressed.
* [ ] The Appriss comment review dashboard is updated with the current status of each comment.
* [ ] Any escalated issues have been assigned to the appropriate team.","Given, When, Then","Response is now shared with Appriss 

@user @user 

Worksheet link: [https://trebu.sharepoint.com/:x:/r/sites/ProjectManagement/_layouts/15/guestaccess.aspx?e=4%3Atz1jyA&at=9&share=Eb28Xewu_llMqK3p2YLR2KwBt7ctofD_Dodo4-rm-SmXZQ|https://trebu.sharepoint.com/:x:/r/sites/ProjectManagement/_layouts/15/guestaccess.aspx?e=4%3Atz1jyA&at=9&share=Eb28Xewu_llMqK3p2YLR2KwBt7ctofD_Dodo4-rm-SmXZQ]

Hey @user - I believe there are more than 1 question that might need respond?

Just to confirm we have addressed all of them and would you share a screenshot so we can close this one?

Hi @user @user @user , All questions raised is now answered."
CSCI-769,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Dec/25 9:47 AM,22/Dec/25 8:34 AM,,Work breakdown for RF tactical access,"h2. Overview

This card outlines the work required to establish RF tactical access. Providing RF tactical access will enable secure and reliable communication in environments where traditional network infrastructure is unavailable. This is crucial for maintaining operational effectiveness and situational awareness in critical scenarios.

h2. Steps

# Define the specific requirements for RF tactical access, including frequency bands, data rates, and security protocols.
# Identify and procure the necessary hardware and software components, such as radios, antennas, and encryption modules.
# Configure and integrate the RF equipment to establish a functional tactical network.
# Conduct thorough testing of the RF tactical access solution to ensure performance and reliability under various conditions.
# Develop comprehensive documentation and training materials for users of the RF tactical access system.
# Deploy the RF tactical access solution in the designated operational environment.
# Provide ongoing support and maintenance for the RF tactical access system.

h2. Acceptance Criteria

* [ ] The RF tactical access solution meets the defined requirements for frequency bands, data rates, and security protocols.
* [ ] The RF equipment is properly configured and integrated, allowing for seamless communication.
* [ ] The RF tactical access solution demonstrates reliable performance under simulated operational conditions.
* [ ] Comprehensive documentation and training materials are available for users.
* [ ] The RF tactical access solution is successfully deployed in the designated operational environment.
* [ ] A support and maintenance plan is in place for the RF tactical access system.","Given, When, Then","*The solution will be:*

* Create data share between Sigma & CW SF
* Create a dedicated warehouse for Rachel‚Äôs team on CW SF
* Her warehouse can only access the 5 x agreed Sigma datasets
* Everything will be automated (CI/CD) ‚Äì means alignment to security and quick turnaround for future data sharing requests from any team

¬†

*Cost:*

* For Rachel‚Äôs warehouse, start with extra small, apply a cap amount of our choosing
* If above doesn‚Äôt suffice then we would dial up accordingly

¬†

*Timelines:*

* Est delivery date is +*26*++^*th*^+ +*Feb*+ (we aim to finish by +*18*++^*th*^+ +*Feb*++)+
* Detailed plan below (basically 3 x weeks effort between Jag & Harry)
* We estimate this work to start in Feb 2026 because this is when we expect to have production environment ready"
CSCI-750,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,10/Dec/25 10:02 AM,18/Dec/25 11:14 AM,,Appriss query refine,"h2. Overview

The existing Appriss Datashare ELT query requires refinement to improve accuracy, maintainability, and alignment with the current data product standards. This work ensures the ELT logic is consistent with the latest Appriss data-share requirements, resolves gaps identified during QA, and prepares the pipeline for downstream reporting dependencies.

h2. Steps

# Review the current Appriss Datashare ELT SQL and document all existing joins, filters, transformations, and business rules.
# Identify inconsistencies, redundant logic, missing filters, or misaligned business definitions based on updated Appriss documentation and stakeholder feedback.
# Update and refine the SQL ELT logic to reflect the correct business rules, ensuring alignment with Bronze ‚Üí Silver conventions and naming standards.
# Validate refined output against expected Appriss samples (e.g., benchmark extracts, QA logs, historical outputs).
# Optimise query performance where appropriate (index use, partition pruning, CTE structure, join ordering).
# Peer review with Data Engineering and validate with Appriss SME before promoting the updated query.
# Update documentation, lineage, and add comments within the SQL code for ongoing maintainability.

h2. Deliverables

* Refined and validated Appriss Datashare ELT SQL query.
* Comparison document outlining logic changes vs previous version.
* Updated data lineage / documentation in Confluence & Alation.
* Peer-review sign-off and test results.

h2. Acceptance Criteria

* All business rules in the refined ELT logic align with current Appriss data share specifications.
* Output datasets match expected Appriss benchmarks with no material discrepancies.
* SQL code passes peer review and conforms to internal coding standards.
* Performance is equal to or better than the existing ELT job.
* Documentation and lineage links are updated and published.","Given, When, Then",
CSCI-749,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Dec/25 10:22 AM,12/Dec/25 2:43 PM,,Appriss audit data release,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Started working on this

Hi @user ,

¬†

Please find attached the query for AUDIT data. This would be a new table for Audit information. Please let me know when you create the table in Snowflake and load data. I will then create the Benchmark query and share it with you.

Main query and Benchmark query, both are now shared with Mike and data is also populated in Snowflake."
CSCI-748,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Dec/25 10:21 AM,12/Dec/25 2:43 PM,,investigate benchmark transaction issue for benchmark validation data for Appriss,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","I did the investigation from my end. It looks like a legitimate transaction to me. Below is the findings

For : 35358290519654432

Check For Invoice with ID 35358290519654432 Legitimate Invoice 
Check for Electronic Payment Payment Approved, Invoice not voided 
Check Item Level Data Legitimate Invoice 
Check for Audit Data with Abandoned Sales No records of abandoned Sales (SaleactivityID =27) Audit records shows it is legitimate invoice

Checked at Snowflake end as well

For: 35357668968628352

Check For Invoice with ID 35358290519654432 Legitimate Invoice 
Check for Electronic Payment Payment Approved, Invoice not voided ( positive pay) 
Check Item Level Data Legitimate Invoice All line items has got positive qty sold 
Check for Audit Data with Abandoned Sales No records of abandoned Sales (SaleactivityID =27) Audit records shows it is legitimate invoice

Checked at Snowflake end as well

It appears to be a legitimate ."
CSCI-743,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Dec/25 11:37 AM,16/Dec/25 9:55 AM,,Cognos || implementation and review,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",The offshore and DB team is now briefed about the Solution development. Task created for DB Team *REQ0165663 /¬† RITM0181203*
CSCI-742,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Dec/25 11:33 AM,23/Dec/25 9:47 AM,,Setting up DBT Cloud,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-741,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,04/Dec/25 12:40 PM,05/Dec/25 11:42 AM,,PAM- Cognos Onboarding Questionnaire,"h3. Context

* To help answer [PAM - Cognos Cube Onboarding Questionaire.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/SecurityOperations/_layouts/15/Doc.aspx?sourcedoc=%7B2D64A06A-1AD2-4C17-8246-3845EE9EA80E%7D&file=PAM%20-%20Cognos%20Cube%20Onboarding%20Questionaire.xlsx&wdLOR=c3791FC8F-67AC-4008-A97F-2C072FB52811&action=default&mobileredirect=true&isSPOFile=1&xsdata=MDV8MDJ8fGE5ZmQ2YzIxMzM0ODQ5NmQ3MGQ3MDhkZTMyYzliNjA2fDhiYzAyODBlYjNhZDQzOGFiYThhMmNmMDBkMTlhZjRhfDB8MHw2MzkwMDQwMzkwNTY4Nzk4MzF8VW5rbm93bnxWR1ZoYlhOVFpXTjFjbWwwZVZObGNuWnBZMlY4ZXlKRFFTSTZJbFJsWVcxelgwRlVVRk5sY25acFkyVmZVMUJQVEU5R0lpd2lWaUk2SWpBdU1DNHdNREF3SWl3aVVDSTZJbGRwYmpNeUlpd2lRVTRpT2lKUGRHaGxjaUlzSWxkVUlqb3hNWDA9fDF8TDJOb1lYUnpMekU1T2pFd09UaGpNbUl3TFdJM1pHVXROR1JsWkMwNE9XRmhMV1U0Tnpkak1qazJPR013WWw4ME9HUTNNVGxtTkMweVkyWTRMVFE0TnpRdE9XWTJOUzA0TXpNeE5HRmlNV1E1T1RKQWRXNXhMbWRpYkM1emNHRmpaWE12YldWemMyRm5aWE12TVRjMk5EZ3dOekV3TkRnME1BPT18NTJhOTVhMDgwNTAwNGNmOWY1ZTIwOGRlMzJjOWI2MDV8Y2ZjNjliOThlMjE0NGY1ZGJlNTI1NWJhMzBlZTU1YzM%3D&sdata=ejA2eFdMY3YxRW1TS0VuT1E5Vmo3M2NUcXZvOExESEI0SnVYQXJ4d210VT0%3D&ovuser=1dec29b4-9a5d-41fc-9c75-cdb2cb9c7fe6%2CHan.Li%40sigmahealthcare.com.au]

h3. Acceptance criteria

* All question answer and sent back to security team

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-740,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Nov/25 9:44 AM,15/Dec/25 9:21 AM,,Cognos || performance improvement investigation,"Detail to be fill in

h3. *Context*

Users are intermittently unable to see application content in Cognos. IBM‚Äôs initial investigation indicates a correlation with JVM memory pressure; JVM size has been increased as recommended.
Further internal analysis shows the issue occurs mainly on Mondays and Tuesdays and coincides with heavy execution of the _Buyer Category Performance Drill-Through Report_, which queries large-volume data across multiple tables.
This report appears to be a significant contributor to JVM load and may be impacting overall Cognos stability.

----

h3. *Acceptance Criteria*

* Performance investigation completed within estimation.
* Root cause confirmed and documented with evidence (logs, query analysis, JVM metrics).
* Proposed optimisation approach reduces report execution time and/or JVM load.

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Started working on this. The objective is to analyze the problem and come up with a solution approach. This task to continue in next sprint.

Thanks @user, Just fyi will move this exact card to next sprint so feel free to add note here as needed

Analysis complete. Creating queries for optimization. 2 queries need to be created for AUS and NZL

Query for AUS is now complete

Queries for NZL is also prepared now."
CSCI-739,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,27/Nov/25 10:45 AM,05/Dec/25 9:40 AM,,Appriss Tactical || Generate Updated Benchmark Dataset with New Logic Applied,"h3. Context

* Create 7* dataset for each Appriss generated script with
* selected 10 stores
** locations:
*** B004: Chadstone Amcal (Customer data enabled)
*** B005: Chadstone SC CWH
*** B057: Hurstville CWH
*** B222: Eastwood CWH
*** B509: Watergardens TC
*** B532: Sydney Central CWH
*** B859: Melbourne Amcal (Customer data enabled)
* Extraction Period: April 28, 2025 to May 16, 2025

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","@user Please note the extraction period is now Extraction Period: April 28, 2025 to May 16, 2025 per Frank

FYI @user

Thanks @user for the update. Will proceed accordingly. @user , We need to reload data for the said date range for all tables.

Picking this one now. Work in Progress"
CSCI-738,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,27/Nov/25 10:43 AM,03/Dec/25 10:03 AM,,Appriss Tactical || Join Transactions with Audit Logs,"h3. Context

* Design and implement a Snowflake dataset/view that joins transaction datasets with audit log data, applying the correct mapping logic and exposing required columns for Appriss Retail to validate QA issues.
* Add audit data to ardm_tender simliar to ardm_header
* Add audit data to ardm_event simliar to ardm_header

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* Combined dataset contains all columns requested by Appriss.

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Update as of now

# Add Audit Log data for Voided transactions into ARDM_EVENT.-- Completed.
# Add PLU information in Product Reference Data( New Request came today) -- Completed.
# Add Audit log data for voided transactions into ARDM_TENDER ¬†-- In Progress
ETA for 1,2 & 3 -> Tuesday, 2nd Dec

Hi All,

Please find below the updates as of now:

¬†

# Add Audit Log data for Voided transactions into ARDM_EVENT.-- Completed.
# Add PLU information in Product Reference Data( New Request came 1^st^ Dec) -- Completed.
# Add Audit log data for voided transactions into ARDM_TENDER ¬†-- Completed"
CSCI-737,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,26/Nov/25 10:08 AM,05/Dec/25 12:00 PM,APPRISS_Tactical_Solution,Appriss Tactical || Creating View for extraction script,"Create a set of Snowflake views to support the Appriss Tactical extraction process.
These views will sit on top of the raw source tables ingested via ADF, apply all required SQL transformation logic, and expose clean, consumable outputs for datashare.

*Deliverables*

* *Finalised Snowflake views* representing the curated, transformed outputs required for Appriss data share.","Given, When, Then",
CSCI-736,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,26/Nov/25 8:25 AM,02/Dec/25 9:51 AM,NLP_BI,NLP BI || Datashare establishment,"h3. *Objective*

h3. 
Establish Snowflake Data Share from CW Snowflake to Sigma for key Supply Chain datasets. The share must enable read-only access for Sigma stakeholders and ensure controlled, auditable data exchange with no manual intervention beyond initial setup. Target tables currently sit in EDP_DEV.STG_SOURCE_SUPPLYCHAIN (CW Bronze Layer):

* *DIM_PRODUCT*
* *DIM_STORE*
* *FCT_SALES*
* *FCT_STOCK*

Access to be provided to:

* [Girish.Bhatta@sigmahealthcare.com.au|mailto:Girish.Bhatta@sigmahealthcare.com.au]
* [Dunyin.Gu@sigmahealthcare.com.au|mailto:Dunyin.Gu@sigmahealthcare.com.au]
* [Phoebe.Song@sigmahealthcare.com.au|mailto:Phoebe.Song@sigmahealthcare.com.au]

----

h3. *Acceptance criteria*

* Secure Share created in CW Snowflake
* Tables added: DIM_PRODUCT, DIM_STORE, FCT_SALES, FCT_STOCK for mentioned users

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-735,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Nov/25 11:11 AM,10/Dec/25 9:55 AM,,Power BI SSO to Snowflake,"h3. Context

Enable and configure Microsoft Power BI to connect to Snowflake using single sign-on (SSO) via Microsoft Entra ID and External OAuth. This setup allows users to access Snowflake data without requiring an on-premises Power BI Gateway. The configuration includes security integration creation in Snowflake, network policy validation, role mapping, and troubleshooting of common SSO issues.

h3. Acceptance criteria

* Power BI security integration created and verified in Snowflake.
* Users can connect to Snowflake via Power BI with Microsoft Entra ID SSO.
* Verification of access using login history queries.

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Hi @user can you pls help us create a SSO integration for PowerBI in Snowflake using AccountAdmin privilege: 

Instruction in this doc 

[+Power BI SSO to Snowflake | Snowflake Documentation+|https://url.au.m.mimecastprotect.com/s/C3TBCBNq3QhxWxoqT6h9C2R4Yj?domain=docs.snowflake.com] 

If you have any question, please reach out to Jess and Eugene, 

@user @user @user

Hey @user,

Would you able to work with @user on this one after Appriss?

Thanks

performance issue to be review"
CSCI-734,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Nov/25 11:07 AM,25/Nov/25 1:28 PM,,Connect to Snowflake in the Power BI Service,"*Description:*
Configure the Power BI service to connect to Snowflake, optionally enabling Microsoft Entra ID single sign-on (SSO). This includes configuring administrative settings across Snowflake, Power BI, and Azure, and updating semantic models to support SSO or basic authentication.

*Acceptance Criteria:*

* Power BI Admin portal configured to send Microsoft Entra tokens to Snowflake.
* Semantic models updated to use Microsoft Entra ID credentials for SSO where required.
* Verification of connectivity and authentication tested successfully i","Given, When, Then",
CSCI-733,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Nov/25 9:25 AM,05/Dec/25 11:41 AM,NLP_BI,NLP BI || Retail Knowledge Transfer,"h3. Context

* Knowledge Transfer on retail data 

h3. Deliverables

Below information are provided:

# *Store IDs* for all MyChemist locations now converted to Amcal.
# *Mapping rules* between CW products and IQVIA products.
# CWG exclusive product (done)

h3. 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","@user - Can you review and confirm if any additional information is needed or not?

Thanks @user. I have a follow-up question about the Amcal Branch ID file and will connect with you when you are available tomorrow.

@user Could you pls. review the CWH_IQVIA_PRODUCT_MAPPING file and check if the CWH product can be mapped to the product in the Dim_Product_Retail_Master_VW using IQVIA ID. Pls. let us know if any qs. Thanks.

Required information and data is shared with Data Science Team

Hi @user , Could you please confirm that you have got all info and data that you needed from CWH team for now? I can then close this ticket.

@user yes we‚Äôve got all the datasets for now. Just to let you know that we might come back to you later as the business users start using the application and raise some important questions not covered by the current datasets. Thanks for your help."
CSCI-732,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Nov/25 9:24 AM,05/Dec/25 3:02 PM,NLP_BI,NLP BI || Snowflake Table review/refresh,"*Objective*

Review data quality issue identified in meeting regarding missing values.

Design and implement automated refresh processes for CW Retail Bronze tables, enabling continuous ingestion of the latest data. Implement delta or change-data-capture logic where applicable to ensure efficient updates and maintain historical accuracy.

Target tables:

* *DIM_PRODUCT*
* *DIM_STORE*
* *FCT_SALES*
* *FCT_STOCK*

All tables must be refreshed reliably and be made query-ready for downstream consumption via datashare

Acceptance Criteria

* Automated refresh pipelines for DIM_PRODUCT, DIM_STORE, FCT_SALES, FCT_STOCK
* Delta/CDC logic implemented where applicable
* Scheduling and monitoring enabled
* Validation evidence (row count checks, sample reconciliations)","Given, When, Then","Data Refreshed until 26/11/2025. Now reviewing with Amit. Will continue this process for a few days. Once we are happy with the stats. I will create the PR to publish the schedule.

duplicates identified in the data and noticed the data is getting deleted in the source table. Need to implement a logic to accommodate the deletions.

PR created and merge changes are rebased with master and waiting for approval.

Changes are now published. Job will run every day at 12:30pm"
CSCI-731,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,24/Nov/25 11:22 PM,26/Nov/25 10:38 AM,,Placeholder - New Table extraction from source to bronze,"h3. Context

* Placeholder card to extract any new table identify as needed for LLM project

h3. 

h3. 

h3. 

h3. 

h3.","Base on identified table needed to support LLM project

* Tables are extracted from source and ingested into CW SF Bronze
* Historical Data loaded",No additional table needed as of this sprint
CSCI-728,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Nov/25 12:49 PM,05/Dec/25 9:32 AM,,Error Investigation || SF SSO to PBI via PrivateLink,"h3. Context

* An investigation is required into a Power BI Desktop ‚Üí Snowflake SSO authentication issue occurring when connecting via PrivateLink. Microsoft Support has requested further validation steps and clarification on the configuration.

h3. Objective

* Analyse the SSO authentication error encountered in Power BI Desktop when connecting to Snowflake through PrivateLink and follow suggestion as per MS support
* Prepare required details or logs for Microsoft Support if escalation is needed.

h3. Acceptance criteria

* Technical findings on the root cause (where identifiable).
* Updated configuration steps or corrections if required.
* PBI is able to connect to SF when Private link turned on

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Based on my investigation, we need also to add PowerBI and Snowflake SSO Integration:

* [+Connect to Snowflake with Power BI - Power BI | Microsoft Learn+|https://learn.microsoft.com/en-us/power-bi/connect-data/service-connect-snowflake]
* [+Power BI SSO to Snowflake | Snowflake Documentation+|https://docs.snowflake.com/en/user-guide/oauth-powerbi]

Separate base on task

* [+Connect to Snowflake with Power BI - Power BI | Microsoft Learn+|https://learn.microsoft.com/en-us/power-bi/connect-data/service-connect-snowflake] ‚Äì CSCI-734

[+Power BI SSO to Snowflake | Snowflake Documentation+|https://docs.snowflake.com/en/user-guide/oauth-powerbi] +-+ CSCI-735"
CSCI-716,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Nov/25 12:35 PM,22/Dec/25 4:38 PM,,Source to Target Mapping || FactWarehouseInventoryHistory,Source to target mapping for FactWarehouseInventoryHistory- filling out sheet as per Source-to-target Fact,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** Table
*** Column
*** Derived? Yes or no","Bring card up to sprint 14 @user - fyi

[Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=G9oYh5&nav=MTVfe0RGM0EyMDAyLUFBMEQtNDc3MS05QzUzLTIwMzMwMjJEMDMyOH0]

@user Please review

As discussed with Bhavya, removed WarehouseLocation attribute from model (this cannot be done where the source is from AX - WarehouseLocation will sit on the FactWarehouseInventoryLocation-Intra/History models instead).

Renamed StockCostPrice to AverageRealCost.
Added AverageNetworkCost and AverageStoreCost (based on advice from Bhavya).
Removed StockAmount (this is removed from the platinum model, but will be added into the gold layer)."
CSCI-715,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Nov/25 12:33 PM,22/Dec/25 4:38 PM,,Source to Target Mapping || FactWarehouseInventoryIntra,Source to target mapping for FactWarehouseInventoryIntra- filling out sheet as per Source-to-target Fact,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** Table
*** Column
*** Derived? Yes or no","[Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=KeEcaO&nav=MTVfezhBRDI5QzE1LTY0RTctNEJBMi1CNzNGLUZEQjlCMkMyMTc2MX0]

@user Please review

As discussed with Bhavya, removed WarehouseLocation attribute from model (this cannot be done where the source is from AX - WarehouseLocation will sit on the FactWarehouseInventoryLocation-Intra/History models instead).

Renamed StockCostPrice to AverageRealCost.
Added AverageNetworkCost and AverageStoreCost (based on advice from Bhavya).
Removed StockAmount (this is removed from the platinum model, but will be added into the gold layer)."
CSCI-713,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Nov/25 12:32 PM,22/Dec/25 12:18 PM,,Source to Target Mapping || FactStoreInventoryHistory,Source to target mapping for FactStoreInventoryHistory- filling out sheet as per Source-to-target Fact,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** Table
*** Column
*** Derived? Yes or no","Made some amendments (based on new changes made to FactStoreInventoryIntra):

* StockCostPrice been renamed to AverageRealCost
* StockOnHandValue, InboundValue and OutboundValue all removed (these will be added to the gold layer product instead)
* AverageNetworkCost and AverageStoreCost added"
CSCI-712,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Nov/25 12:31 PM,22/Dec/25 12:09 PM,,Source to Target Mapping || FactStoreInventoryIntra,Source to target mapping for FactStoreInventoryIntra- filling out sheet as per Source-to-target Fact,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** Table
*** Column
*** Derived? Yes or no","@user - Can you pls review this one? thx

Made some amendments (based on new changes made to model):

* StockCostPrice been renamed to AverageRealCost
* StockOnHandValue, InboundValue and OutboundValue all removed (these will be added to the gold layer product instead)
* AverageNetworkCost and AverageStoreCost all added
** Copied over column values based on note Bhavya left regarding Opening/Closing SOH Network/Store Average Amount"
CSCI-707,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,20/Nov/25 8:21 PM,25/Nov/25 5:20 PM,,Appriss Tactical || Data Mapping Queries for Voided Transactions & Total Amount Fields - ARDM_HEADER,"h3. *Context*

Amit raised a data-mapping query regarding the fields *Total Amount*, *Calculated Total*, and *Original Total* in the _ARDM_HEADER_ table, noting that all three currently have the same mapping. Clarification is required from the Appriss team on the intended logic and definitions.
Additionally, review and updates are required for the mapping of *voided transactions*, based on customer feedback and sample data to be provided.

h3. *Objective*

Confirm the correct data-mapping rules for total amount fields and finalise the approach for handling voided transactions within the ARDM schema.

h3. *Deliverables*

* Confirmed definitions and mapping rules for Total Amount, Calculated Total, and Original Total.
* Updated mapping rules for voided invoices including required columns.
* Revised ARDM data mapping document or schema definition.
* Summary of decisions and rationale shared with the team.","* Mapping rules for total amount fields are clarified and approved.
* Final approach for voided transactions is documented and aligns with customer guidance.","Analysis done! Found a solution to combine two separate data sets into one. Will continue working on this in the next Sprint

Work in Progress. Planning to finish by tomorrow.

ARDM_HEADER is now complete, and queries are shared with @user"
CSCI-702,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,20/Nov/25 9:34 AM,21/Nov/25 11:58 AM,,Knowledge Transfer - Chloe & Ashu,"*Context*
Knowledge transfer session between *Chloe* and *Ashu* to ensure continuity of work and coverage of Chloe‚Äôs tasks during *26 Nov ‚Äì 5 Dec*, and any additional support as needed.

*Objective*
Ensure Ashu has full visibility and understanding of Chloe‚Äôs responsibilities, workflows, and in-flight tasks so that work can continue smoothly during the specified period.

*Steps*

* Chloe to walk through current tasks, priorities, outstanding actions

*Deliverables*

* Handover notes / documentation created where available.","Given, When, Then",
CSCI-698,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Nov/25 1:38 PM,20/Nov/25 9:22 AM,,Review procedure to get the data type from source system for MySQL,"h3. Context

* Test the snowflake procedure to get the data types of the columns from the source system for MYSQL.

h3. Objective

* Load the data for each table and auto generate the view with the source system datatypes 

h3. Acceptance criteria

* Load tables from Manhattan Active and generate views with column data types from same as the source system. 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-697,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,17/Nov/25 11:12 PM,18/Nov/25 3:30 PM,,"Alation || ""Configuring Power BI Tenant Settings for Alation API Service""","h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-695,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Nov/25 9:06 AM,21/Nov/25 12:05 PM,,ADF Linked Service Clean up,"h3. Context

* Rename the linked services according to the naming standards.

h3. Acceptance criteria

* All linked services should follow the same naming standards.

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Linked services cannot be renamed. I will document the details of the source system and linked services and delete the unused the linked services.

@user

Details documented in the operational document. Linked service deleted and waiting for PR post call with Eugene.

This done. PR created and will be approved on 24/11 just to avoid an issue over the weekend."
CSCI-694,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Nov/25 9:06 AM,18/Nov/25 1:14 PM,,Snowflake || Post-Migration Schemas Clean up,"h3. Context

* Perform the clean-up of the dev environments post migration to Bronze Schema. 

h3. Acceptance criteria

* Remove all unused/ migrated stg_ schemas from the EDP_DEV database 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Below schemas has been deleted. We have 30 days to undrop these with snowflake time travel feature.

EDP_DEV.STG_AXLINK
EDP_DEV.STG_CWMGTSTOREINVOICES
EDP_DEV.STG_GENERAL_REFERENCE
EDP_DEV.STG_ILS
EDP_DEV.STG_MANHATTAN_ACTIVE
EDP_DEV.STG_MANHATTAN_ACTIVE_DEFAULT_DCINVENTORY
EDP_DEV.STG_SCAX2012
EDP_DEV.STG_SKU
EDP_DEV.STG_SPSWHSPURCHASE
EDP_DEV.STG_STOCKDB
EDP_DEV.STG_TRANSACTION_STORAGE
EDP_DEV.STG_TRNS
EDP_DEV.TEST
EDP_DEV.STG_TRNS"
CSCI-693,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,12/Nov/25 9:03 AM,22/Dec/25 8:26 AM,,Update CI configuration for snowflake-infra,Update the CI configuration files to reference the new snowflake-infra folder location and ensure all pipelines work as expected.,,
CSCI-692,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,12/Nov/25 9:03 AM,22/Dec/25 8:26 AM,,Move base directory to base-infra folder,Relocate the entire base directory within the repository to a new folder named base-infra as part of the repo reorganization.,,
CSCI-691,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,12/Nov/25 9:03 AM,22/Dec/25 8:26 AM,,Update CI configuration for base-infra,Modify the continuous integration (CI) configuration files to ensure they reference the new base-infra folder location correctly.,,
CSCI-690,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,12/Nov/25 9:03 AM,22/Dec/25 8:26 AM,,Copy edp-snowflake-infrastructure to snowflake-infra folder,Copy the contents of the edp-snowflake-infrastructure repository into a new folder named snowflake-infra within the consolidated repository.,,
CSCI-689,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,12/Nov/25 9:03 AM,13/Nov/25 7:52 AM,,Rename edp-infrastructure repository to edp-infra-provision,Rename the existing edp-infrastructure repository to edp-infra-provision to reflect the new consolidated structure.,,edp-infrastructure renamed to [edp-infra-provision - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infra-provision]
CSCI-688,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Nov/25 8:41 AM,14/Nov/25 3:05 PM,,Investigate Null Value in Table,"h3. *Context*

An issue has been identified in the {{TransactionStorage}} database, {{BranchOrderItems}} table ‚Äî where all records appear to contain *null values*. This table is part of the *bulk upload automation* process, and the data issue may impact downstream integrations and reporting.

h3. *Scope*

* analysis on {{TransactionStorage.BranchOrderItems}}.
* Review of bulk upload automation flow related to this table.","* Root cause analysis done and issue identified
* Remediation (if applicable).
* Confirmation of table is loaded successfully after fix","Issue was due to the Parquet file headers are case sensitive. It is not fixed, and the data is loaded successfully. I would be spending some more time to check what happens to the procedure when there are spaces in the columns name. cc-@user"
CSCI-685,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:26 AM,05/Dec/25 5:16 PM,,EDP Infrastructure || Repo Configuration - Repo for ALTIDA CI/CD,"Implement full CI/CD and IaC for the ALTIDA framework in a dedicated isolated Git repository, so that ALTIDA is fully automated (no click-ops), minimally coupled to the core data platform, and can be cleanly removed and replaced in the future without refactoring other repos or shared infrastructure.","*Repository & Isolation:*

* ALTIDA has its own Git repo containing all ALTIDA code, config, IaC and pipeline definitions.
* No ALTIDA code/config/IaC lives in any core platform or shared infrastructure repo. Other repos can reference ALTIDA only via documented loosely coupled integration points (eg. queues, file locations, APIs).
* Disabling ALTIDA pipelines and archiving the ALTIDA repo does not require code changes in other repos beyond removing those integration references.

*IaC and no Click-Ops:*

* All Snowflake and cloud resources required by ALTIDA are defined and managed only via IaC in ALTIDA repo (no portal or Snowflake UI changes).
* A new environment (eg. DEV/SIT/UAT) can be provisioned from empty to full working ALTIDA by running the documented CI/CD pipeline, with no manual steps.

*CI/CD Configuration:*

* ALTIDA has dedicated CI/CD pipelines in its repo for build, test/validation and deploy to each environment.
* Environment differences are handled via parameters and secrets (eg. Key Vault) not by branching code or hand-editing config in the UI.

*Security & Access:*

* ALTIDA uses its own service accounts and roles (Snowflake, cloud) defined in IaC with least privilege, limited to the layers/resources it needs (Landing/Bronze) and not to curated/core platform components
* No shared ‚Äúgod‚Äù roles or service accounts exists between ALTIDA and other platform components. Secrets are stored only in approved secret stores, not in code or pipeline definitions.

*Replaceability/Decommissioning:*

* The ALTIDA repo includes a short, tested decommission procedure describing how to disable ALTIDA, remove its resources and revoke its identities without impacting the rest of the platform.
* There are no hard dependencies on ALTIDA internals in other repos (no imports of ALTIDA libraries or assumptions about ALTIDA-specific structures). Integration contracts are documented so a future replacement data ingestion solution can plug into the same interfaces.","Hi @user please add the Acceptance Criteria when you are ready.

@user can you please rename the edp-snowflake repo to edp-etl-atilda-legacy. This repo will be used for ALTIDA CI/CD purpose. 

FYI @user

@user @user this is renamed to: {{edp-etl-atilda-legacy}}

@user thank you!"
CSCI-684,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:25 AM,24/Nov/25 2:02 PM,,EDP Infrastructure || IaC Repo Configuration - edp-visualize-publish-dashboards,,"Given, When, Then",
CSCI-683,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:25 AM,24/Nov/25 1:59 PM,,EDP Infrastructure || IaC Repo Configuration - edp-model-publish-semantic,,"Given, When, Then",see development tracking at: [User Story 213682 Rename Semantic Model Repository|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213682]
CSCI-682,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:25 AM,18/Dec/25 10:56 AM,,EDP Infrastructure || IaC Repo Configuration - edp-transform-build-warehouse,"edp-transform-build-warehouse

This is repo naming reserved for dbt and transformation workflow.","Given, When, Then","see development tracking at: [User Story 213686 Rename Snowflake Repository|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213686]

we need a new repo for this @user. This is intended for dbt transformation workflow I believe.

@user - Do you need new card for rename {{edp-snowflak}} to {{edp-etl-atilda-legacy¬†¬†}}? Per Chloe edp-transform-build-warehouse is reserve for DBT?

@user this is already done. we need a ticket eventually for the dbt repo.

Hi @user @user this is repo naming reserved for dbt and transformation workflow."
CSCI-681,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:25 AM,25/Nov/25 10:35 AM,,EDP Infrastructure || IaC Repo Configuration - edp-ingest-orchestrate-pipelines,Rename [edp-data-factory|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/branches] to edp-ingest-orchestrate-pipelines,"Given, When, Then","@user @user @user I will be renaming the repo [edp-data-factory|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/branches] to edp-ingest-orchestrate-pipelines can you please confirm that you have checked in pending work in your branches?

Once confirmed, I will rename and notify you, then you can check out your branches from the new repository name, contents are the same, there should be no changes in functionality.

@user @user @user just waiting for your confirmation to move ahead with this.

@user All good from my side! @user @user @user can you please confirm if you have pending changes in ADF that need to be committed to your feature branch before the renaming action?

All good from my side as well @user @user

Confirmed by everyone @user Please proceed!

Hi @user , please proceed üôÇ

this is completed: [User Story 213677 Rename Data Factory Repository|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213677]

Eugene rectified the issue with security on the publish branch. Detail of the fixes is at [User Story 213677 Rename Data Factory Repository|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213677]"
CSCI-678,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:20 AM,16/Dec/25 3:32 PM,,EDP Infrastructure || IaC Repo Configuration - edp-infra-provision,"Combine edp-infrastructure and edp-snowflake-infrastructure into a single repo.

Steps:

# clone edp-infrastructure to edp-infrastructure-backup
# rename edp-infrastructure to edp-infra-provision
# move the whole base directory to base-infra
# update base-infra ci to reflect new folder location
# copy edp-snowflake-infrastructure to snowflake-infra
# update snowflake-infra ci to reflect new folder location","* Both source repositories are merged into {{edp-infra-provision}}.
* {{edp-infrastructure}} and {{edp-snowflake-infrastructure}} modules retain separate Terraform state files and CI/CD pipelines.
* Folder structure clearly separates Azure and Snowflake components (e.g., {{/azure}} and {{/snowflake}}).
* All pipeline definitions are updated to reflect new repository paths but maintain independent workflows.
* Backend configurations (state files, key vaults, storage accounts) remain unchanged.
* All contributors are notified pre- and post-migration.
* Documentation and pipeline badges updated to reflect new repository location.
* Validation builds for both areas succeed after migration.","current repo cloned to [edp-infrastructure-backup - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infrastructure-backup?version=GBmaster]

see development tracking: [User Story 213659 Merge and Rename Infrastructure Repositories|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213659]

@user"
CSCI-677,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:19 AM,16/Dec/25 3:54 PM,,EDP Infrastructure || IaC Repo Structures - Implementation Plan,"h3. Context

* Rename repositories to be aligned with implementation across different organizations and geographical locations.

h3. Objective

* Rename current repositories for clarity as to which organizations and geographical locations they are applicable to.

h3. Steps¬†

# Notify developers of the pending change in repository name
# Update the repository name
# Update CI/CD pipelines to use the new repository name if needed
# Validate that CI/CD pipelines work
# Developers to update their environments (VS Code, Data Factory Studio, Workspaces) to use the new repository name

h3. Acceptance criteria

* renamed repositories aligned with target organizations and geographical location

h3. Assumptions - (Optional)

* naming conventions for naming repositories, including the organization and geographical location.

h3. 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","@user i created a whole feature in Azure DevOps for the repo restructuring:

[Feature 213658 Align EDP Repository Naming with Enterprise Standards|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213658]

if you want, you can update this and related tasks to match what is in there."
CSCI-675,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:14 AM,21/Nov/25 1:51 PM,APPRISS_Tactical_Solution,Appriss Tactical || Share Audit Data with APPRISS,,"Given, When, Then","Hey @user - what is the expected outcome of ‚Äúsharing audit data‚Äù? 

Is to review and address some of the issue raised by appriss?"
CSCI-674,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:13 AM,21/Nov/25 1:47 PM,APPRISS_Tactical_Solution,Appriss Tactical || APPRISS Transform query ARDM Ref Location,,"Given, When, Then",Query Build is complete and is shared with the Engineering team
CSCI-673,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:13 AM,21/Nov/25 5:07 PM,APPRISS_Tactical_Solution,Appriss Tactical || APPRISS Transform query ARDM Ref Product,"h3. Context

Develop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.

*Objects to Develop:*

* ORDER_HEADER - done
* EVENT 
* ITEM
* TENDER
* REFERENCE_CUSTOMER
* REFERENCE_PRODUCT
* REFERENCE_LOCATION
* Placeholder - SUMMARY TABLE

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* Transformation script from source to generate APPRISS output

h3. Acceptance criteria

* SQL transformation models are created and committed for all eight objects listed.
* Each transform query follows the standard CTE and naming convention structure.
* Data types and key fields are consistent across all models.
* Transformations run successfully with no data loss or schema mismatches.
* QA validation confirms accurate mapping and record counts

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",Query prepared and shared with Mike
CSCI-672,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:13 AM,20/Nov/25 10:16 AM,APPRISS_Tactical_Solution,Appriss Tactical || APPRISS Transform query ARDM Item Discount,,"Given, When, Then",[^ARDM_ITEM_DISCOUNT.txt]
CSCI-670,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:12 AM,08/Dec/25 11:40 AM,APPRISS_Tactical_Solution,Appriss Tactical || APPRISS Transform query ARDM Tender,"h3. Context

Develop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.

*Objects to Develop:*

* ORDER_HEADER - done
* EVENT 
* ITEM
* TENDER
* REFERENCE_CUSTOMER
* REFERENCE_PRODUCT
* REFERENCE_LOCATION
* Placeholder - SUMMARY TABLE

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* Transformation script from source to generate APPRISS output

h3. Acceptance criteria

* SQL transformation models are created and committed for all eight objects listed.
* Each transform query follows the standard CTE and naming convention structure.
* Data types and key fields are consistent across all models.
* Transformations run successfully with no data loss or schema mismatches.
* QA validation confirms accurate mapping and record counts

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","+*ARDM_TENDER*+

¬†

Query build is complete based on my understanding and documentation available. I have got¬† few question for Business Team/APPRISS

¬†

# Amount : if electronic payment exists then electronic payment else sum of other payments
# Type: One invoice will have only one type : direction order will be as below

CASH, CREDIT, CHEQUE, ACCOUNT

# Card Expiry is defaulted to 12/99 . Is that correct? We don‚Äôt have card expiry date captured anyway.
# Cash back is mapped to Cashout Amount . Is it correct? What is the definition?
# Authorization code¬† is not correctly mapped Electronic Ref is hashed card key. It should be mapped to Approved code. changed in my query
# Tender data will be null for any payment other that electronic payment.
# Gift Balance is always null. It should be mapped with Gift Balance in Tender Data. Changed accordingly.
# 'card_number' hard coded as 'TOKENIZED' , it should be electronicref which is hashed Key of actual card number. Changed accordingly.
# Bin Number was wrongly mapped . Now it is mapped to Leading 9 characters of the hashed Card number
# electronic_receipt is mapped blank . It should be the actual electronic receipt? Mapped to Electronic receipt. Please let me know in case it needs to be blank¬†

Sample Data"
CSCI-669,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:11 AM,21/Nov/25 1:47 PM,APPRISS_Tactical_Solution,Appriss Tactical || APPRISS Transform query ARDM Item,"h3. Context

Develop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.

*Objects to Develop:*

* ORDER_HEADER 
* EVENT 
* ITEM
* TENDER
* CUSTOMER
* REFERENCE_CUSTOMER
* REFERENCE_PRODUCT
* REFERENCE_LOCATION
* SUMMARY TABLE

h3. Deliverables

* Transformation script from source to generate APPRISS output

h3. Acceptance criteria

* SQL transformation models are created and committed for all eight objects listed.
* Each transform query follows the standard CTE and naming convention structure.
* Data types and key fields are consistent across all models.
* Transformations run successfully with no data loss or schema mismatches.
* QA validation confirms accurate mapping and record counts

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","+*ARDM_ITEM*+

Query build is complete based on my understanding and documentation available. I have got¬† few question for Business Team/APPRISS

¬†

# Order id¬† & ID ¬†Same mapping. They both mapped to Invoice Id Is this expected?
# Definition of Department: Is it Product Group? Product Category is highest level, Product Group is one level down
# TRANSACTION_CLASS : Need definition , did not have any info in document
# Supplier ID¬† : Should it not be Manufacturer/ Main Supplier . Supplier ID is internal to CWH and checked that we are not sharing any supplier name in any file. So supplier ID might not be of any use
# We found some differences between APRISS Document and APPRISS Schema which was actually shared with APPRISS team. As per discussion with James, we are taking APPRISS Schema as bible and creating data points based on that.

¬†

Sample Data:"
CSCI-668,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:10 AM,25/Nov/25 5:20 PM,APPRISS_Tactical_Solution,Appriss Tactical || APPRISS Transform query ARDM Event,"h3. Context

Develop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.

*Objects to Develop:*

* ORDER_HEADER 
* EVENT 
* ITEM
* TENDER
* CUSTOMER
* REFERENCE_CUSTOMER
* REFERENCE_PRODUCT
* REFERENCE_LOCATION
* SUMMARY TABLE

h3. Deliverables

* Transformation script from source to generate APPRISS output

h3. Acceptance criteria

* SQL transformation models are created and committed for all eight objects listed.
* Each transform query follows the standard CTE and naming convention structure.
* Data types and key fields are consistent across all models.
* Transformations run successfully with no data loss or schema mismatches.
* QA validation confirms accurate mapping and record counts

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","@user - Is Event done but just pending business clarification to finalise?

[^ARDM_EVENT.txt]

FYI @user @user

Hi @user Han, yes, the Event is complete, pending business clarifications. Something is not looking right in the existing attachment. Reuploaded the query and shared it with Mike as well."
CSCI-667,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:05 AM,20/Nov/25 8:25 PM,APPRISS_Tactical_Solution,Appriss Tactical || Create New Schema for Tactical Solution,,"Given, When, Then",Using existing datashare from POC
CSCI-666,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:05 AM,26/Nov/25 9:53 AM,APPRISS_Tactical_Solution,Appriss Tactical || Once Off Historical Data Load,"h3. Context

Pending on [https://sigmahealthcare.atlassian.net/browse/CSCI-626|https://sigmahealthcare.atlassian.net/browse/CSCI-626]

To load historical data parquet

h3. Acceptance criteria

* Historical data showing for each table

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",Pending finalised script
CSCI-665,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:04 AM,21/Nov/25 1:44 PM,APPRISS_Tactical_Solution,Appriss Tactical Data Movement || Benchmark,,"Given, When, Then",
CSCI-664,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:04 AM,21/Nov/25 9:59 AM,APPRISS_Tactical_Solution,Appriss Tactical Data Movement || Ref Product,,"Given, When, Then",
CSCI-663,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:04 AM,21/Nov/25 9:59 AM,APPRISS_Tactical_Solution,Appriss Tactical Data Movement || Ref Location,,"Given, When, Then",
CSCI-662,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:04 AM,21/Nov/25 9:59 AM,APPRISS_Tactical_Solution,Appriss Tactical Data Movement || ARDM Item Discount,,"Given, When, Then",
CSCI-661,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:04 AM,21/Nov/25 9:59 AM,APPRISS_Tactical_Solution,Appriss Tactical Data Movement || ARDM Item,,"Given, When, Then",
CSCI-660,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:03 AM,21/Nov/25 9:59 AM,APPRISS_Tactical_Solution,Appriss Tactical Data Movement || ARDM Tender,,"Given, When, Then",
CSCI-659,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:03 AM,21/Nov/25 9:59 AM,APPRISS_Tactical_Solution,Appriss Tactical Data Movement || ARDM Event,,"Given, When, Then",
CSCI-658,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Nov/25 11:02 AM,21/Nov/25 9:59 AM,APPRISS_Tactical_Solution,Appriss Tactical Data Movement || ARDM Header,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-657,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,07/Nov/25 2:15 PM,17/Nov/25 8:36 AM,,PR - Bulk load pipeline,"h3. Context

* Create PR for the Bulk upload pipeline.

* 

h3. Object -

* Eugene to confirm what object to configure to push the changes for global parameters

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-656,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,07/Nov/25 10:10 AM,18/Dec/25 9:22 AM,,EDP Infra || Infra & Security provision for SIT UAT PROD-Sprint 13,"h3. Objective

Provision secure environments with consistent configurations across SIT, UAT and PROD. Achieve full environment parity and compliance alignment.

h3. Steps¬†

# Replicate IaC templates for SIT, UAT, PROD
# Validate VNet, Private Link, RBAC and logging setup

h3. Deliverables

* SIT, UAT, PROD environments deployed via IaC
* Security approval sign-offs","* 100% parity across SIT, UAT, PROD
* Security posture matches PROD baseline.
* All environments deployed via IaC (no manual setup).","Discussed requirements with Anjali and Eugene.

Anjali to raise a request to SysOps team.

@user to work with @user to document the technical requirements for this stream of work.

FYI @user

@user Hey any update on this?

@user Will look the details of the tickets raised by Anjali once have access to see other‚Äôs tickets on Service-Now .

[^EDP REQUESTS HELP.docx]

@user I will keep this in review until Chloe comes back, just in case she is after something more."
CSCI-655,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,07/Nov/25 9:55 AM,17/Nov/25 8:45 AM,Ingestion,Stage to Bronze Migration - Sprint 12,"h3. Context

* This is to implement the strategic solution and update the imported tables to move to BRONZE schema from STG schema except Supply Chain tactical solution schema. 

h3. Objective

* Migrate tables to ‚ÄúBronze‚Äù schema and update ALTIDA DATA IN configurations and Azure pipelines to load data in ‚ÄúBronze‚Äù schema.

|*Database*¬†|*No of tables*|
|AXLINK|1|
|CWMgtStoreInvoices|5|
|General_Reference|2|
|ILS|13|
|SCAX2012|10|
|SKU|7|
|SpsWhsPurchase|6|
|StockDb|26|
|StoreOrders|2|
|TransactionStorage|11|

 

h3. Steps¬†

# Create the new schema ‚ÄúBronze‚Äù
# Clone all the tables & views to from all STG_<Source Name> to ‚ÄúBronze‚Äù Schema.
# Update ALTIDA configuration for all the tables in each data source 
# Test the pipeline is able to successfully load the data to tables 
# 

h3. Deliverables

* -Create New Schema-
* -Clone Objects -
* -Update ALTIDA configuration -
* -Update & Test ADF pipelines loading data in the ‚ÄúBronze‚Äù tables-
*","Complete the deliverables for each source. 

* -TransactionStorage -
* -StockDb-","Discussing with Mike about the table names standards now that all the tables will be in the same schema

Naming convention finalised and will resume the task. Have to start from the beginning.

Block by [https://sigmahealthcare.atlassian.net/browse/CSCI-688|https://sigmahealthcare.atlassian.net/browse/CSCI-688]

Completed except two pipelines were not run end to end due to the number of records. Need to check with Database team on the plan for those"
CSCI-653,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,06/Nov/25 8:49 AM,14/Nov/25 7:48 AM,,Appriss Tactical || Sync TransactionReturns PDB14 to PBI05,"h3. Context

* This task is part of the _Appriss Tactical Delivery_ initiative. 

h3. Objective

* The objective is to synchronize table called TransactionReturns from PDB14.TransactionStorage to PBI05.TransactionsArchive 

h3. Approach

* a scheduled job will run every 30 (it can be changed) to trigger ETL that perform a merge (Insert or Update) for the last two days of the data

h3. Acceptance criteria

* Synchronized Table from SQL3 ‚Üí PBI05
* Data validation","Given, When, Then","Objects created

Table | [TransactionReturns] in PBI05.Trasna..Arch..

SAJ | called APPRISS_Sync_DB14_ToBI05_TransactionReturns located IN D/PIS01

SSIS | Sync_DB14_ToBI05_TransactionReturns.dtsx

raising CR for deployment

@user @user @user 

A CR has been raised, once approved we will deploy it to prod 

[CHG0052911 | Change Request | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/change_request.do%3Fsys_id%3D0de4c50733c5b25047764f945d5c7b0f%26sysparm_view%3D%26sysparm_domain%3Dnull%26sysparm_domain_scope%3Dnull]

deployed 
next is to monitor and validate the data

will confirm the result tomw"
CSCI-652,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Nov/25 11:32 AM,21/Nov/25 5:07 PM,APPRISS_Tactical_Solution,APPRISS Issue Investigation - Issue 13,"*Objective*
Investigate and resolve the rounding discrepancy in *Invoice ID: 35358912688554000*, where the total amount in the header ($100.00) does not align with the sum of the line items (approximately $19.00). The tender record also reflects $100.00, suggesting a data integrity or rounding issue between header, line item, and tender tables.

*Steps*

# Confirm with stakeholders whether the total amount should always equal the sum of all line items (excluding rounding tolerances) in the secure database schema.
# Data Engineering team to validate if the discrepancy originates from:
#* Source system calculation or rounding logic, or
#* ETL transformation or aggregation logic in the current data pipeline.
# Identify all affected invoices exhibiting similar discrepancies.","*Acceptance Criteria*

* *Root Cause Identified:* Source of the discrepancy (source system vs ETL logic) is clearly documented.
* *Fix Implemented:* Data logic corrected and validated so that header totals reconcile with summed line items (within accepted rounding limits).
* *Validation Passed:* QA confirms that post-fix, total header and item-level values are consistent across test invoices.","AC Nov 4: CW to investigate

AC Nov 18: Total value received by staff from Customer : $100

Mode of Payment: Cash

Line Item Total: $15.98

Rounding: $.02

Total Line Item: $16

Change given to Customer: $84

Invoice Header Amount value and Invoice Line item amount sum may differ whenever there is a cash payment involved. In this invoice:35358912688554000 if someone pays cash as payment and gives $100 to the cashier for products worth $16. Then the Total Amount from the product line item would be $16, and Header Invoice would be $100. It does not take the change returned ($84 ) into account."
CSCI-641,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Nov/25 11:07 AM,18/Dec/25 11:12 AM,SupplyChainP1,Data Modelling || FactWarehouseInventoryHistory,"Draw physical star schema design (Platinum Layer) for the fact table, defining:

* Column names
* Column types
* Primary Keys
* Foreign Keys
* Granularity
* Unique constraints

Please produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]

Preliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] 

Preliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]","Given, When, Then","[https://lucid.app/lucidchart/ab68325d-d632-4ea7-b305-a8d40606bc5c/edit?invitationId=inv_dfe592d5-a572-47ea-870c-995eeec337a1&page=9ZadQqz2rMGN#|https://lucid.app/lucidchart/ab68325d-d632-4ea7-b305-a8d40606bc5c/edit?invitationId=inv_dfe592d5-a572-47ea-870c-995eeec337a1&page=9ZadQqz2rMGN#] 

To be reviewed by @user"
CSCI-640,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Nov/25 11:07 AM,02/Dec/25 8:55 AM,SupplyChainP1,Data Modelling || FactWarehouseInventoryIntra,"Draw physical star schema design (Platinum Layer) for the fact table, defining:

* Column names
* Column types
* Primary Keys
* Foreign Keys
* Granularity
* Unique constraints

Please produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]

Preliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] 

Preliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]","Given, When, Then",
CSCI-638,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Nov/25 11:07 AM,02/Dec/25 8:55 AM,SupplyChainP1,Data Modelling || FactStoreInventoryHistory,"Draw physical star schema design (Platinum Layer) for the fact table, defining:

* Column names
* Column types
* Primary Keys
* Foreign Keys
* Granularity
* Unique constraints

Please produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]

Preliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] 

Preliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]","Given, When, Then",
CSCI-637,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Nov/25 11:06 AM,25/Nov/25 4:59 PM,SupplyChainP1,Data Modelling || FactStoreInventoryIntra,"Draw physical star schema design (Platinum Layer) for the fact table, defining:

* Column names
* Column types
* Primary Keys
* Foreign Keys
* Granularity
* Unique constraints

Please produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]

Preliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] 

Preliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]","Given, When, Then",
CSCI-635,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Nov/25 11:03 AM,10/Dec/25 9:53 AM,APPRISSModelling,Data Modelling || FactSalesRetailElectronicPayment,"Draw physical star schema design (Platinum Layer) for the fact table, defining:

* Column names
* Column types
* Primary Keys
* Foreign Keys
* Granularity
* Unique constraints

Please produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]

Preliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] 

Preliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]","Given, When, Then",
CSCI-633,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Nov/25 10:59 AM,21/Nov/25 4:07 PM,SupplyChainP1,Data Modelling || DimWarehouseLocation,"Draw physical star schema design (Platinum Layer) for the fact table, defining:

* Column names
* Column types
* Primary Keys
* Foreign Keys
* Granularity
* Unique constraints

Please produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]

Preliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] 

Preliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]","Given, When, Then","Hi @user - please have a review of the below and let me know if you‚Äôre happy for me to add this to our Enterprise Data Model.

Thanks!

!CW Data Modelling.png|width=369,alt=""CW Data Modelling.png""!

Created model based off of current Supply Chain reporting + Chat GPT suggestions

@user Nice one, happy for you to update confluence.

Updated final version to include sample data for some of the descriptive columns (included columns that could be interpreted ambiguously)

!CW Data Modelling (1).png|width=414,alt=""CW Data Modelling (1).png""!"
CSCI-632,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Nov/25 11:35 AM,11/Nov/25 9:44 AM,,EDP Infra || Adding Static Application Security Scan to ADF CI pipeline,"h3. Objective

Integrate Static Application Security Testing into ADF CI pipeline to automatically scan ADF resource templates for security vulnerabilities and non-compliance before deployment.

----

h3. Steps

----

h3. Deliverables

* Updated CI/CD YAML pipeline with a dedicated SAST scan step.
* Configuration files for the SAST tool defining scan rules.
* Documentation on how to view and address SAST scan results.

----","* -The ADF CI pipeline successfully executes the static security scan on every run.-
* -The scan covers all critical ADF components.-
* -Fail build on Critical Vulnerabilities-
* -Security Report Published-","see code tracking task: [Task 212773 Add Static Security Scan to ADF CI Pipeline|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/212773]

PR submitted: [Pull request 23128: Implemented #212773 - Update ADF CI pipeline configuration for improved security scanning - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/a9d4deeb-ad55-465e-9425-438e29d991d5/pullrequest/23128]"
CSCI-631,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,31/Oct/25 9:58 AM,21/Nov/25 1:43 PM,,Create Naming Convention for Repos,"h3. Context

* We need a simple, durable Git repo naming standard for the Enterprise Data Platform (EDP) that works across clouds/regions without future renames. Use lowercase *kebab-case* and avoid embedding env/region/provider in names.

h3. Objective

* Adopt a task-oriented pattern and pair it with CI/CD parameters for multi-cloud/region deployments.
*Naming pattern:* {{edp-<capability>-<task>[-<asset>]}}
Examples: {{edp-infra-provision}}, {{edp-ingest-orchestrate-pipelines}}, {{edp-transform-build-warehouse}}, {{edp-model-publish-semantic}}, {{edp-visualize-publish-dashboards}}. (Lowercase, hyphens only.)

h3. Steps¬†

# 
# Publish standard and examples in Confluence; notify teams.
# Rename existing repos to the new pattern (no env/region/provider in slug). 
# Update pipelines to use a *matrix* for cloud/region (e.g., {{CLOUD=az|aws}}, {{REGION=au-ae|eu}}).
# Configure Terraform state backends per cloud/region: *azurerm* on Azure, *s3* on AWS. 
# (If using ADF) Deploy from *publish* artifacts/ARM templates, not raw JSON.

h3. Deliverables

* Documented naming standard with examples.
* Renamed repos aligned to {{edp-<capability>-<task>[-<asset>]}}.
* CI/CD updated to matrix by cloud/region; TF backends isolated per target; ADF using publish flow. 

h3. Assumptions - (Optional)

* 

h3.","* *Given* the standard, *when* repos are reviewed, *then* each name matches {{edp-<capability>-<task>[-<asset>]}} in lowercase kebab-case with no env/region/provider. 
* *Given* multi-cloud/region needs, *when* pipelines run, *then* jobs fan out via *matrix* and deploy successfully per target.
* *Given* Terraform usage, *when* state is initialized, *then* Azure uses *azurerm* and AWS uses *s3* backends with separate keys/workspaces per region/env.
* *Given* ADF deployments, *then* CI consumes *publish* templates/parameters to promote between environments.","edp-infra-provision
 Infrastructure IaC for cloud resources (VNets, KV, IR, etc.)
edp-infra-provision-snowflake
 Snowflake account objects (roles, warehouses, databases, grants) via IaC
edp-ingest-orchestrate-pipelines
 ADF today; room for AWS (Glue/Airflow) later, selected by CI vars
edp-transform-build-warehouse
 dbt project for transformations
edp-model-publish-semantic

edp-visualize-publish-dashboards
 Power BI reports as artifacts with CI-driven deployment

and the repo structure: 
/modules # shared, cloud-agnostic
/cloud/az/... # AU/NZ specifics
/cloud/aws/... # future AWS
/overlays/eu/ # EU-only policy/params shared globally (residency, keys, logs, PII rules) 
/overlays/<org>/eu # EU-only policy/params org(cwr,sigma) specific (residency, keys, logs, PII rules)

created a new confluence page for [https://sigmahealthcare.atlassian.net/wiki/x/AQDkZw|https://sigmahealthcare.atlassian.net/wiki/x/AQDkZw]"
CSCI-630,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,31/Oct/25 9:42 AM,07/Nov/25 10:10 AM,,Tech Requirement capture for Create Request for Azure Subscription SIT/ UAT,"Clone from [https://sigmahealthcare.atlassian.net/browse/CSCI-274|https://sigmahealthcare.atlassian.net/browse/CSCI-274] to capture effort in capturing tech requirement on this card

[^Technical Requirements- SIT & UAT.docx]

Create request for creation of SIT and UAT Azure Subscription for Data Platform with the following RBAC:

h4. 1. Subscription Provisioning

* Ensure each subscription includes:
** Proper subnets provisioned within their respective VNets.
** Associated Key Vaults, blob storage, and other foundational resources.

h4. 2. Access Management

* Azure AD user groups created for:
** Contributors
** Readers
** Any other roles required for environment access.
* Assign appropriate RBAC roles to these groups within the new subscriptions.

h4. 3. Firewall Configuration

* Firewall rules updates to:
** Allow SIT and UAT subnets access to on-premises resources.
** Enable outbound access from SIT and UAT to the internet.
** Specifically allow Self-hosted Integration Runtime (SHIR) outbound access for software installation.
** Existing patterns for the dev environment are followed in SIT/UAT

h4. 4. Routing & Patterns

* Apply existing firewall routing patterns used in other environments to maintain consistency.
* Ensure both on-prem and Azure firewall rules are updated accordingly.

Reference from prev raised tickets:

[+Feature 146776+|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/146776]{{: Azure Subscriptions}}

[+Task 148083+|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/148083]{{: Create Request for Dev Azure Subscription Creation- REQ0136676 created}}

[+Task 148097+|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/148097]{{: Create QA Azure Subscription}}

[+Task 148114+|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/148114]{{: Create Request to Configure Azure Firewall}}","h4. 1. *Subscription Provisioning*

* Ensure each subscription includes:
** Proper *subnets* provisioned within their respective *VNets*.
** Associated *Key Vaults*, *blob storage*, and other foundational resources.

h4. 2. *Access Management*

* *Azure AD user groups created* for:
** *Contributors*
** *Readers*
** Any other roles required for environment access.
* Assign appropriate *RBAC roles* to these groups within the new subscriptions.

h4. 3. *Firewall Configuration*

* *Firewall rules* updates to:
** Allow *SIT and UAT subnets* access to *on-premises resources*.
** Enable *outbound access* from SIT and UAT to the internet.
** Specifically allow *Self-hosted Integration Runtime (SHIR)* outbound access for software installation.

h4. 4. *Routing & Patterns*

* Apply existing *firewall routing patterns* used in other environments to maintain consistency.
* Ensure both *on-prem* and *Azure firewall rules* are updated accordingly.",
CSCI-629,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Oct/25 1:40 PM,07/Nov/25 2:15 PM,,Review Password Expiry Policy for all database accounts in DEV,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","@user - can you please confirm? thx

DBA confirmed password expiry is set for only user and requested them to remove that"
CSCI-628,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Oct/25 9:11 AM,18/Nov/25 4:21 AM,,EDP infra|| Update IaC to Apply Subnet Change for Snowflake SSO PrivateLink Access,"*Objective*
Incorporate the subnet configuration changes into the existing Infrastructure-as-Code (IaC) to support Snowflake SSO connectivity via PrivateLink. Ensure IaC reflects the updated subnet design validated during connectivity testing.

*Steps*

* Review the subnet configuration validated by Raveen and Chloe.
* Update the IaC modules/templates to include the new subnet configuration.
* Validate the PrivateLink access from Azure environment using the updated IaC.
* Commit and push the changes to the IaC repository in Azure DevOps.
* Notify stakeholders once updates are deployed and verified.

*Deliverables*

* Updated IaC configuration reflecting new subnet setup.
* Verified PrivateLink access for Snowflake SSO post-deployment.
* Version-controlled IaC changes in Azure DevOps repository.","*Acceptance Criteria*

* IaC successfully deploys infrastructure with updated subnet configuration.
* Snowflake SSO PrivateLink access verified post-deployment.
* No manual changes required outside IaC.
* Changes reviewed and approved by Chloe and Raveen.","@user just want to confirm, if you remove the setting, does the snowflake private endpoint not become accessible to the VDI browser?

Hey @user , yes, I think so. Especially in a SSO scenario

see [Task 212755 Update Snowflake subnet private endpoint policy|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/212755] for development.

@user can you please review the PR: [Pull request 23103: Implemented #212755 - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-snowflake-infrastructure/pullrequest/23103?_a=files]

cc: @user @user

@user to generate service account

Waiting for deployment approval from @user

I believe this change is done. Can you pls confirm @user 

FYI @user"
CSCI-627,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Oct/25 1:43 PM,07/Nov/25 7:53 PM,APPRISS_Tactical_Solution,Create APPRISS Transform query,"h3. Context

Develop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.

*Objects to Develop:*

* ORDER_HEADER - done
* EVENT 
* ITEM
* TENDER
* CUSTOMER
* REFERENCE_CUSTOMER
* REFERENCE_PRODUCT
* REFERENCE_LOCATION
* Placeholder - SUMMARY TABLE

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* Transformation script from source to generate APPRISS output

h3. Acceptance criteria

* SQL transformation models are created and committed for all eight objects listed.
* Each transform query follows the standard CTE and naming convention structure.
* Data types and key fields are consistent across all models.
* Transformations run successfully with no data loss or schema mismatches.
* QA validation confirms accurate mapping and record counts

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","+*ARDM_HEADER*+

Query build is complete based on my understanding and documentation available. I have got¬† few question for Business Team/APPRISS

¬†

# Order id¬† & order Number Same mapping. They both mapped to Invoice Id Is this expected?
# Definition of Total Amount :¬† Invoice Header Amount value and Invoice Line item amount sum may differ whenever there is a cash payment involved. An example would give you a better understanding. ( Invoice ID : 801125062148140) If someone pays cash as payment and gives $50 to cashier¬† for product worth $34.50 . Then Total Amount from product line item would be 34.50 and Header Invoice would be $50. It does not take change returned ($15.50 ) into account. Which field is expected? Please confirm from APPRISS
# Total Amount, Calculated Total, Original Total : They all have same mapping . Is there any definition shared by APPRISS team for the same?
# We found some differences between APRISS Document and APPRISS Schema which was actually shared with APPRISS team. As per discussion with James, we are taking APPRISS Schema as bible and creating data points based on that.

¬†

Sample Data:

+*ARDM_EVENT*+

¬†

Query Building is in Progress. As discussed during call, we have got a few questions and statements

¬†

# Manager id mapping is wrong. We do not have that mapping in transactions. In document, ManagerID is mapped to InForStaffID. This filed cpatures if any CWH staff purchases items from a store. It is not manager. Manager id will be populated as null.
# Voided Flag: need definition: Is it voided if the whole invoice is cancelled ? Or any item in that invoice is voided
# We found some differences between APRISS Document and APPRISS Schema which was actually shared with APPRISS team. As per discussion with James, we are taking APPRISS Schema as bible and creating data points based on that.

Closing this card as sprint closure, remaining queries to have individual cards in the next sprint"
CSCI-626,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Oct/25 1:41 PM,21/Nov/25 1:17 PM,,Create APPRISS historical file in Parquet,"h3. Context

To perform once off historical load of 8 weeks data for Appriss - will require

* All transform script ready in order to extract

h3. Acceptance criteria

Parquet files generated for all appriss data share tables

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","@user from me to action this i need a query From @user

Pending all extraction script

This is no longer require as historical data ingestion can be done via ADF pipeline"
CSCI-625,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Oct/25 1:40 PM,20/Nov/25 8:24 PM,,APPRISS historical load to Snowflake,"h3. Context

Pending on [https://sigmahealthcare.atlassian.net/browse/CSCI-626|https://sigmahealthcare.atlassian.net/browse/CSCI-626]

To load historical data parquet

h3. Acceptance criteria

* Historical data showing for each table

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-623,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Oct/25 1:31 PM,18/Nov/25 1:46 PM,,APRRISS || Service Account for PDB 14 to Azure Dev,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Ticket raised under RITM0178360

Request approved under RITM0178360

No longer require will use SSIS to populate needed table to PDB 14 which SF is currently accessible"
CSCI-622,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Oct/25 1:30 PM,15/Dec/25 3:40 PM,APPRISS_Tactical_Solution,APPRISS || Whitelisting for PDB 14 to Azure Dev,"h3. Context

* Need to whitelist on-prem server PDB14 to our SHIR (Self-Hosted Integration Runtime) so that we can ingest data into Snowflake (via ADF)

h3. Objective

* Need to whitelist on-prem server PDB14 to our SHIR (Self-Hosted Integration Runtime) so that we can ingest data into Snowflake (via ADF)
* This data is required for the APPRISS project

h3. Steps¬†

# Raise firewall request via ServiceNow (be sure to use the correct ticket type, otherwise it will get rejected) - [https://cwretail.service-now.com/sp?id=sc_cat_item&sys_id=4e233585db8f8050633287f43a961929|https://cwretail.service-now.com/sp?id=sc_cat_item&sys_id=4e233585db8f8050633287f43a961929] 
## If unsure how to complete the request, copy ticket *RITM0177824*
## Specify all details in the ticket request, including the server IP address to whitelist (on-prem SQL server IP) and our SHIR IP (*172.29.84.64/27 (SHIR - cwr-ase-edp-shir-dev-vmss)*)
# After ticket is raised it will go to the *Security* team - they will ask us lots of questions about what we‚Äôre trying to do (even though it is specified within the ticket)
# After Security approve the ticket, it will go to the *Network* team automatically
## Note: we need to ask the Service Desk team to create an additional task assigned to this ticket to the *Cloud* team - in the current Firewall ServiceNow ticketing flow, this does not get automatically created. We can only do this *after* Security have approved the ticket (see *RITM0177824* as an example)
# For the whitelisting to be fully completed, both the network and cloud team need to complete their tasks
# Once we receive confirmation that firewall listing has been done on both sides, we can test this connection in ADF
## Note: to do this we need a service account create that can login onto PDB14, however note this has already been done and credentials are stored in the Key Vault
### UserName: *M2PSCSQL03N2-Prod-SQL-LocalAccountName-PDB14-ReadOnlyReplica*
### Password: *M2PSCSQL03N2-Prod-SQL-LocalAccountPassword-PDB14-ReadOnlyReplica*

For any help on creating the ServiceNow ticket/following up on assignees, speak with @user as she has helped us out with creating many firewall/whitelisting requests.

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Ticket under - RITM0178308

Update as of 20251117

Update completed pending testing from team

@user to test

@user 

Testing failed - receiving a firewall error:

Need to confirm with Sam and DBA team that there‚Äôs no issues with the User Account that‚Äôs been provided

Ticket raise for IT Cloud Team - REQ0164316/RITM0179824

I‚Äôve raised a ticket with the DBA team to check the access to PDB14 for the service account user (ServAC_EDPADF_pSQL03). Noticed that when logged into SSMS this user doesn‚Äôt have the ability to query any tables on PDB14.
Ticket: *RITM0179858*

@user - security reviewing this one now."
CSCI-621,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Oct/25 11:52 AM,07/Nov/25 4:40 PM,Ingestion,Stage to Bronze Migration - Sprint 11,"h3. Context

* This is to implement the strategic solution and update the imported tables to move to BRONZE schema from STG schema except Supply Chain tactical solution schema. 

h3. Objective

* Migrate tables to ‚ÄúBronze‚Äù schema and update ALTIDA DATA IN configurations and Azure pipelines to load data in ‚ÄúBronze‚Äù schema.

|*Database*¬†|*No of tables*|
|AXLINK|1|
|CWMgtStoreInvoices|5|
|General_Reference|2|
|ILS|13|
|SCAX2012|10|
|SKU|7|
|SpsWhsPurchase|6|
|StockDb|26|
|StoreOrders|2|
|TransactionStorage|11|

 

h3. Steps¬†

# Create the new schema ‚ÄúBronze‚Äù
# Clone all the tables & views to from all STG_<Source Name> to ‚ÄúBronze‚Äù Schema.
# Update ALTIDA configuration for all the tables in each data source 
# Test the pipeline is able to successfully load the data to tables 
# Once all the pipelines are able to load data to all the table ingested. Drop the STG_<Source Name> Schemas 

h3. Deliverables

* Create New Schema
* Clone Objects 
* Update ALTIDA configuration 
* Update & Test ADF pipelines loading data in the ‚ÄúBronze‚Äù tables
* Clean-up of the Stg_<Source Name>

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Complete the deliverables for each source.

* -AXLINK-
* -ILS-
* -SKU-
* TransactionStorage
* -CWMgtStoreInvoices-
* -General_Reference-
* -SCAX2012-
* -SpsWhsPurchase-
* StockDb","Discussing with Mike about the table names standards now that all the tables will be in the same schema

Naming convention finalised and will resume the task. Have to start from the beginning."
CSCI-620,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Oct/25 11:52 AM,19/Nov/25 1:30 PM,,Snowflake Service Account for IaC,"This account is used by Terraform (via CI/CD pipeline) to:

* Authenticate into Snowflake (usually via *key pair auth*).
* Create, modify, and manage objects (databases, schemas, warehouses, roles, grants, integrations).
* Apply changes across multiple environments consistently.","||Area||Setting||
|*Service User*|SVC_IAC_SNOWFLAKE|
|*Primary Role*|{{ROLE_INFRA_ADMIN_SNOWFLAKE}}|
|*Parent Role*|{{SECURITYADMIN}}|
|*Auth*|RSA Key Pair|
|*Scope*|Database, Warehouse, Role, Integration, Grant|
|*Privileged Avoidance*|No ACCOUNTADMIN, no CREATE USER|
|*Pipeline Integration*|Via Azure DevOps using Key Vault secrets|","Confirmed naming convention with Mike:

* User: SVC_IAC_SNOWFLAKE
* Role: ROLE_INFRA_ADMIN_SNOWFLAKE

* Chloe to script out user and role in Snowflake
* @user to generate key pair auth using IaC Terraform. 
* Chloe/ Eugene to document to AKV manual key rotation manual process.

FYI

@user @user @user

@user since this is a common service account across the different environments, I will create the key pair in the Snowflake Infrastructure Key Vault.

cc: @user

Please see [User Story 212758 Provision Snowflake Service Account for IaC Deployments|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/212758] for code development tracking.

cc: @user @user @user

@user sure thanks Eugene

I have created the role *ROLE_INFRA_ADMIN_SNOWFLAKE* in Snowflake (steps 1 - 3). Pending steps 4 - 5 when key pair authentication created in key vault 

Below are the scripts

-- 1. Create the IAC service role and data warehouse
USE ROLE SECURITYADMIN;

CREATE ROLE IF NOT EXISTS ROLE_INFRA_ADMIN_SNOWFLAKE COMMENT = 'Role used for IaC deployments';
CREATE WAREHOUSE IF NOT EXISTS INFRA_ADMIN_WH COMMENT = 'WH used for IaC deployments';

-- 2. Grant necessary privileges
GRANT CREATE DATABASE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;
GRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;
GRANT CREATE INTEGRATION ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;
GRANT CREATE ROLE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;
GRANT CREATE SCHEMA TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;

-- 3. Allow escalation only when approved
GRANT ROLE ROLE_INFRA_ADMIN_SNOWFLAKE TO ROLE SECURITYADMIN;

-- 4. Create the service account user
CREATE USER IF NOT EXISTS SVC_IAC_SNOWFLAKE
 LOGIN_NAME = 'SVC_IAC_SNOWFLAKE'
 DISPLAY_NAME = 'SVC_IAC_SNOWFLAKE'
 DEFAULT_ROLE = ROLE_INFRA_ADMIN_SNOWFLAKE
 DEFAULT_WAREHOUSE = 'INFRA_ADMIN_WH'
 MUST_CHANGE_PASSWORD = FALSE
 DISABLED = FALSE
 RSA_PUBLIC_KEY = '<your-public-key>'
 COMMENT = 'Service account for Snowflake IaC deployments';

-- 5. Grant the role to the user
GRANT ROLE ROLE_INFRA_ADMIN_SNOWFLAKE TO USER SVC_IAC_SNOWFLAKE;

@user @user @user @user

@user pls advise what other account you want added to this, especially the one for dbt.

cc: @user @user

Le's leave the dbt for later.

@user can you please approve these releases:
[edp-snowflake-infrastructure-release|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_release?view=mine&_a=releases&definitionId=6]

I think this is approved and deployed? Let me know if otherwise , thx- @user"
CSCI-619,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,27/Oct/25 8:51 AM,07/Nov/25 1:54 PM,APPRISS_Tactical_Solution,APPRISS Issue Investigation - Issue 12,"*Objective*

Determine if the ""missing"" transaction number (like 22845953) on benchmark images is a necessary field for the final secure database schema, and if so, initiate the process to add it to the source data pipeline. 

*Steps*

* Confirm with stakeholders if the on-receipt transaction number is required for the secure environment.
* Dev Team to confirm if this transaction number exists in the raw source data.
* Add transaction data to source data for","*Acceptance Criteria:*

* Decision Made: A clear business decision on whether the transaction number is a required field is documented.
* Source Confirmed The team has confirmed if the missing transaction number is available in the existing raw source data.","Need more information on how this image is genarted and what is the definition of Transaction# 

In this example the invoice id is 35358913223065664

Origin TransactionID 22845953 is a ranmdom id generated at store end for one of the line items in this invoice"
CSCI-618,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,25/Oct/25 4:06 AM,22/Dec/25 8:27 AM,,rename edp-powebi-reports,,,
CSCI-617,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,25/Oct/25 4:06 AM,22/Dec/25 8:27 AM,,rename edp-snowflake,,,
CSCI-616,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,25/Oct/25 4:06 AM,22/Dec/25 8:27 AM,,rename edp-semantic-models,,,
CSCI-615,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,25/Oct/25 4:06 AM,22/Dec/25 8:27 AM,,revise repository naming coventions to include organization and geographical location,,,
CSCI-614,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,25/Oct/25 4:05 AM,22/Dec/25 8:27 AM,,rename edp-data-factory,,,
CSCI-613,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,25/Oct/25 4:05 AM,22/Dec/25 8:27 AM,,rename edp-snowflake-infrastructure,,,
CSCI-612,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,25/Oct/25 4:04 AM,22/Dec/25 8:27 AM,,rename edp-infrastructure,,,
CSCI-611,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,24/Oct/25 3:34 PM,28/Oct/25 9:53 AM,,MergeCo - REtail tactical reporting-Sprint 11,"h3. Context

* The mergeco report is a report that is used by Execs to track the health of the business. Vikesh would like a series of reports that will help with this so he can make the correct decisions on the business.

h3. Objective

* To help with the availability of data that is used for producing Mergeco reports
* Tables to ingest
** FCT_CUSTOMER_COUNT_HOURLY_AUS - Correct Data
** FCT_CUSTOMER_COUNT_HOURLY_NZL - Correct Data
** SubCategoryBudgetDaily - Request data as Parquet files empty

h3. Steps¬†

# Ingest Dim tables 
# Ingest Fact tables 
# Ingest Historical Data

h3. Deliverables

* 
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Create Extract Meta for delta & historical data ""}],""attrs"":{""localId"":""50df6b4c-d7a0-435c-badb-b1db13ed0f02"",""state"":""TODO""}},{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Create Pipelines ""}],""attrs"":{""localId"":""0be7bc70-2c7e-46d0-b93b-f03e7ee404ba"",""state"":""TODO""}},{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Load historical data for big tables""}],""attrs"":{""localId"":""8ccec245-dca7-4ae9-95a5-0d86a402aed5"",""state"":""TODO""}}],""attrs"":{""localId"":""3af7df84-c13f-4a98-9ae1-d189349ee0a2""}}
{adf}

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* Data available for Mergeco reporting","Given, When, Then",
CSCI-609,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,24/Oct/25 1:27 PM,31/Oct/25 9:44 AM,,Snow ticket tracker,"h3. Context

* Create spreadsheet to capture all active SNOW ticket for visibility

h3. 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","h3. Acceptance criteria

* Sheet created and access being shared","Hey @user ,

As discussed please start capture all active SNOW ticket for the project and share the access to @user and myself.

Thanks

Please see attachment- @user"
CSCI-608,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,24/Oct/25 11:16 AM,10/Nov/25 9:48 AM,,MergeCo Reporting || Report Presentation Update,"To obtain enhance requirement for report update.

h3. Acceptance criteria

* Understand enhancement need
* Report update
* UAT

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-607,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,24/Oct/25 11:07 AM,10/Dec/25 9:39 AM,,Merge Supply Chain Documentation || End to End PBI solution,"h3. Context

Create consolidated documentation for the MergeCo tactical Supply Chain reporting solution, covering the full end-to-end pipeline‚Äîfrom source ingestion to data modelling, semantic layer, metrics alignment, and PBI visualisation. 

h3. 

h3. Acceptance criteria

* *Finalised End-to-End Documentation* capturing the full MergeCo Tactical Supply Chain reporting solution (source ‚Üí pipeline ‚Üí model ‚Üí metrics ‚Üí Power BI report).

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",Wip report - [https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design]
CSCI-606,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,24/Oct/25 10:02 AM,17/Dec/25 11:23 AM,,EDP Infra || Repository Renaming,"h3. Context

* Rename repositories to be aligned with implementation across different organizations and geographical locations.

h3. Objective

* Rename current repositories for clarity as to which organizations and geographical locations they are applicable to.

h3. Steps¬†

# Notify developers of the pending change in repository name
# Update the repository name
# Update CI/CD pipelines to use the new repository name if needed
# Validate that CI/CD pipelines work
# Developers to update their environments (VS Code, Data Factory Studio, Workspaces) to use the new repository name

h3. Acceptance criteria

* renamed repositories aligned with target organizations and geographical location

h3. Assumptions - (Optional)

* naming conventions for naming repositories, including the organization and geographical location.

h3. 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","@user can you please advise the naming convention i should change these to? Thanks.

cc: @user @user

@user do we already have the naming conventions written in confluence?

@user [https://sigmahealthcare.atlassian.net/wiki/x/AQDkZw|https://sigmahealthcare.atlassian.net/wiki/x/AQDkZw]"
CSCI-605,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,24/Oct/25 9:36 AM,19/Nov/25 9:21 AM,,Service Account Creation for Manhattan Active Database- RITM0178021,"* *Short:*¬†{{servac_edp_5wv2qm398}}
* *Long:*¬†{{servac_edp_mysql_manhattan_active_prod}}

We need *servac_edp_5wv2qm398* created","Given, When, Then","The Cloud Team is trying to move most service accounts to Group Managed Service Account across the Org, hence need more details before they provision the service account- Meeting set up for today. 

cc- @user @user @user

Moving this to @user to discuss with supply chain on getting service account created in Manhattan

Access granted and able to connect to tables"
CSCI-604,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 11:21 PM,07/Nov/25 10:07 AM,APPRISS_Tactical_Solution,APPRISS Issue Investigation - Issue 11,"*Objective*

We are seeing a lot of transactions where the tax amount and discount amount are exactly the same. In fact there are over 100 000 transactions like this but only 25 000 where the values are different. Is this intended and in line with their data?

Example invoice ids are: 35358290519654400 & 35357668974658200

*Steps*

* Confirm if the on-receipt transaction number is required for the secure environment.
* confirm if this transaction number exists in the raw source data.
* analyse the source data to find any relevant flags or indicators.
* Create and link a new card for any necessary pipeline fix.","*Acceptance Criteria:*

* Identify issue root cause.
* Purpose fix where needed","Original Invoice is 35357668974658280 & 35358290519654432

Checked the data . There is no discount associated in this invoice as Base price and unitprice is same. 

The data at APPRISS end does not look good. Need to revisit the code for file generation for Discount.Looks like need a code fix"
CSCI-603,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 11:21 PM,05/Nov/25 11:52 AM,APPRISS_Tactical_Solution,APPRISS Issue Investigation - Issue 10,"*Issue Detail:*
*DO 7550 ‚Äî BM 05 GC Purchase - BM 19 GC Redemption*
Source data does not indicate gift card payments. For *invoice ID 35357668968628300*, the benchmark receipt shows a gift card payment, but source data records it as a credit card payment. Please confirm how gift card tenders can be identified and whether gift card numbers are available in the data.

*Steps*

The correct source field for identifying Gift Card tenders is defined and confirmed by DBA/Business .

The source for a valid Gift Card number for purchases/payments is identified and confirmed.

A new card is created and linked to implement schema and pipeline changes for Gift Cards.","*Acceptance Criteria:*

* Identify issue root cause.
* Purpose fix where needed","Original Invoice ID is 35357668968628352

Header info says: Fully paid electronically

Tender Info: Has one record for this invoice

Audit data says Electronic Type as Credit, and Electronic receipt says Visa Prepaid, which signifies gift card"
CSCI-602,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 11:21 PM,05/Nov/25 11:52 AM,APPRISS_Tactical_Solution,APPRISS Issue Investigation - Issue 9,"*Issue Detail:*
*DO 7535 ‚Äî Tender Card Number - Tender Token*
The field *tender.card_number* always shows the value {{'TOKENIZED'}}. Please explain why the actual card or token values are not visible and confirm if this is intentional masking or data issue.","*Acceptance Criteria:*
Identify issue root cause.","Card Number is hard coded 'TOKENIZED' in data transform script

We do not store full card number in our system. We have hashed carddetails .

In our current code it is hardcoded as 'Tokenized' .

It can be changed to Hashed Electronic Ref for better visibility of vcard from APPRISS end"
CSCI-601,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 11:21 PM,05/Nov/25 11:53 AM,APPRISS_Tactical_Solution,APPRISS Issue Investigation - Issue 8,"*Issue Detail:*
*DO 7530 ‚Äî Secure Order No vs Order ID*
Source invoice ID values are not unique. Example *35358913223065600* appears multiple times with different amounts. Please confirm whether these represent multiple transactions or data duplication, and clarify how to distinguish them if valid.

Steps

*Uniqueness Defined:* confirm with business if the identical {{order_id}} values with different amounts are one transaction or several.

*Key Identified:* Business to advise business rule to define Unique Keys and DBA to update query","*Acceptance Criteria:*

* Identify issue root cause
* New card to be create and linked for any fix needed","Original Invoice ID is 35358913223065664

Header info says: Full paid electronically

Tender Info: Has got one record for this invoice

 we have only one record in headewr for the said invoice ids. 

As thye screenshot shows , There are 6 line items under one header , 6 different products were bought in this invoice"
CSCI-600,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 11:20 PM,05/Nov/25 11:53 AM,APPRISS_Tactical_Solution,APPRISS Issue Investigation - Issue 7,"*Issue Detail:*
*DO 7517 ‚Äî Tender - Tender Type - Tender Card Type*
Tender details for *invoice ID 35357668968628300* do not match benchmark data (missing indication of gift card tender). Additionally, *invoice ID 35358291367034900* has no tender data despite a payment in the benchmark. Please verify tender source data completeness.

*Steps*

Business to provide the correct tender identification method for the Gift Card issue.

 locate and provide the missing tender data for the second invoice ID.

Create new technical card(s) to address any confirmed mapping errors or missing data ingestion.","*Acceptance Criteria:*
Identify issue root cause.","Original Invoice ID is 35357668968628352

Header info says: Full paid electronically

Tender Info: Has got one record for this invoice

Audit data says Elctronic Type as Credit and Electronic receipt says Visa Prepaid, which signifies giftcard/prepaid card"
CSCI-599,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 11:20 PM,05/Nov/25 11:52 AM,APPRISS_Tactical_Solution,APPRISS Issue Investigation - Issue 6,"*DO 7584 ‚Äî Benchmark 04 Tender - Tender Void?*
For order *35357668974658200*, benchmark data shows two tenders (one voided, one Mastercard), while Secure shows only one Mastercard tender. If the voided tender is expected in Secure, please identify where it can be found in the raw data.

*Acceptance Criteria:*
Identify issue root cause.","*Acceptance Criteria:*
Identify issue root cause.","AC Oct 30: Original Invoice ID is : 35357668974658280

Transactions Data Shows Only one item was sold

Electronic Data(Tender) shows 2 records used for payments and one was cancelled

Audit Data Reiterates the same. So data is tied.

Did APPRISS Team receive only one record? or two? for issue 21(DO-7524) , they had received two records. If APPRISS has received one record, need to check extraction code for Tender Data to check why a cancelled tender record was missed

Original Invoice ID is : 801125062148140

Transactions Data Shows total amount paid is $50 full in cash

Electronic Data(Tender) shows 0 records confirming payment type as cash

Did APPRISS Team receive only one record? or two? for issue 21(DO-7524) , they had received two records. Need to check extraction code for Tender Data to check why a cancelled tender record was missed"
CSCI-598,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 11:20 PM,05/Nov/25 11:52 AM,APPRISS_Tactical_Solution,APPRISS Issue Investigation - Issue 5,"*Issue Detail:*
*DO 7525 ‚Äî BM 11 and BM 12 - Cash Refund or Line Void*
Orders *35358291367034900* appear both as Cash Refund and Line Void in benchmarks. In Secure, Item Return Flag is populated but Item Voided Flag is not. Please confirm whether this should be treated as a Cash Refund or Item Void and clarify identification rules in source data.","*Acceptance Criteria:*
Identify issue root cause.","Correct Invoice ID is : 35358291367034992

Header confirms $2 cash Refund

No entry in Electronic Payments Data ( Tender) confirms it‚Äôs a cash Refund

Audit Data chekcs : Chec ks if any prevevious refund was issued, Check Original Invoice ID against which refund needs to be made (35357669285600948) then confimrs refund of $2 cash refund. added retunred quantity as 1. So all data is tied together and as per benchmarking"
CSCI-597,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 11:19 PM,05/Nov/25 11:52 AM,APPRISS_Tactical_Solution,APPRISS Issue Investigation - Issue 4,"*Issue Detail:*
*DO 7524 ‚Äî Benchmark 09 - Multi Payment*
For order *35357669071783700*, benchmark data shows two payment cards (Visa and Mastercard) each for $38.46, but Secure data shows only one Visa tender. Please verify if this order is truly a multi-payment transaction, and clarify the correct tender details.

*Acceptance Criteria:*
Identify issue root cause.","*Acceptance Criteria:*
Identify issue root cause.",
CSCI-596,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 11:19 PM,05/Nov/25 11:52 AM,APPRISS_Tactical_Solution,APPRISS Issue Investigation - Issue 3,"*Issue Detail:*
*DO 7451 ‚Äî BM 02 - Item - Item List Amount*
Benchmark Transaction *35358912688554000* has three items with discounts. Items show identical *amount* and *list_amount* values. The *list_amount* should be the price before discounts, while *amount* is after discounts. Please investigate why these values are the same.","*Acceptance Criteria:*
Identify issue root cause.","Looks like same invoice id mentioned as line 19 (Internal Ticket: DO-7480). Our records shows different items as shown in line 19.
Suspecting wrong invoice ids mentioned in spreadsheet.Could you please sahre the Branch id and Tinmestamp so that we will try to point the right invoice id from ur end"
CSCI-595,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 11:19 PM,05/Nov/25 11:52 AM,APPRISS_Tactical_Solution,APPRISS Issue Investigation - Issue 2,"*Issue Detail:*
*DO 7480 ‚Äî BM 02 - Item - Item Discount to 0.00 displayed as Item Void*
Please investigate whether the third item in benchmark transaction *35358912688554000* should be voided. The data flags this as void in the source data but the benchmark summaries do not.

*Acceptance Criteria:*
Identify issue root cause.","*Acceptance Criteria:*
Identify issue root cause.",35358912688554000 is a legit transactions. 3rd transactions with 2 cents as amount is fr rounding . Mychemid is always null for rounding transactions and transaction type is 4 for roundng transactions
CSCI-594,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 11:18 PM,05/Nov/25 11:52 AM,APPRISS_Tactical_Solution,APPRISS Issue Investigation - Issue 1,"h3. Context

*Issue ID:* DO-7437
*Issue Detail:*
Benchmark 01 - Header - Void Ticket Flg
Benchmark transaction *35358290519654400* is not flagged as voided, despite being listed as voided in the benchmark summary. Please investigate the source data.

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","*Acceptance Criteria:*
Identify issue root cause.",got messed up in CSV original invoice id is 35358290519654432
CSCI-593,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 4:57 PM,21/Nov/25 1:43 PM,,Create EDP Share,"h3. Context

* Create scaffolding for Snowflake data share

h3. Objective

* Create foundation for data sharing in SF

h3. Deliverables

* Create repo called ‚ÄúEDP_Publish‚Äù
* Create new database called ‚ÄúEDP_Publish‚Äù
* Create new schema inside EDP_Publish called ‚ÄúED_Appriss_Share‚Äù
* Create SF Share object
* Set up E2E CI/CD pipeline

h3. Acceptance criteria

* Data share functionality enabled in EDP","Given, When, Then",
CSCI-589,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Oct/25 10:09 PM,08/Dec/25 1:45 PM,,DimOrgAreaStore + DimOrgArea + BridgeOrgAreaManager + Employee,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.",TBC,Duplicate with CSCI-446
CSCI-580,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 9:20 AM,21/Nov/25 1:40 PM,APPRISS_Tactical_Solution,Move data from Source to APPRISS Schema,"h3. Context

Move data to new Appriss production schema

h3. Objective

Create automated pipeline to move data from Appriss source(on prem SQL) to Appriss SF database on a daily basis

h3. Steps¬†

h3. Deliverables

* Create new repo called ‚ÄúAppriss_Tactical‚Äù
* Create table/view objects for Apprisss
* Create ADF pipelines to load data from source to ‚ÄúAppriss_Tactical‚Äù","* Code to be deployed to DEV, SIT, UAT, PROD
* No changes/deployments made manually",
CSCI-578,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Oct/25 9:01 AM,08/Dec/25 11:40 AM,APPRISS_Tactical_Solution,APPRISS Tactical || APPRISS tactical solution design,"h3. Objective

* Prepare design document for APPRISS tactical solution

h3. Deliverables

* Solution design available in Confluence

h3. 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","h3. Acceptance criteria

* Solution Design Document uploaded to Confluence",
CSCI-575,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Oct/25 2:32 PM,05/Nov/25 9:48 AM,APPRISS_Tactical_Solution,EDP Infra||Set Up CI/CD pipeline for ADF,"h3. Objective

Implement CI/CD pipeline to deploy Azure Data Factory assets across all environments via Infrastructure as Code. Eliminate manual publishing of ADF pipelines and ensure consistent, versioned deployments.

h3. Steps¬†

* Configure Git integration with ADF.
* Add automated validation and publish to target environment.
* Apply service principal authentication.

h3. Deliverables

* CI/CD YAML pipeline for ADF.
* Infrastructure templates under source control.
* Environment-specific variable files.","* -100% ADF deployments automated via CI/CD.-
* -Environment parity verified via automated comparison.-
* -No production changes made manually.-","Hi @user let‚Äôs connect to review any remaining requirements.

Please find below the following ADF Pipelines:

* CI - [edp-data-factory-ci - Pipelines|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_apps/hub/ms.vss-build-web.ci-designer-hub?pipelineId=638&branch=master] (Using ci.yaml)
* 

* CD - [edp-data-factory-release - Pipelines|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_releaseDefinition?definitionId=5&_a=environments-editor-preview] (using classic Azure DevOps pipelines)
*

ToDo: 

* changes in the CD process as soon as new environments for SIT and UAT are created.
* further improvements on static security scans during CI (password checks, etc)

Acceptance criteria:

* ADF deployment across different environments are executed using the CD pipeline
* The release pipeline does a comparison with the currently deployed resources and adds/updates existing resources if needed and deletes resources that are not in the deployment package.
* Production changes can still be done manually depending on the rights of the users who have access to the data factory instance. Recommend that this is strictly limited and secured properly.

@user can you please create a separate task to enhance the ADF CI pipeline to add static security scans?

@user [https://sigmahealthcare.atlassian.net/browse/CSCI-632|https://sigmahealthcare.atlassian.net/browse/CSCI-632] 

please update steps, acceptance criteria and estimated effort"
CSCI-574,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Oct/25 2:02 PM,25/Nov/25 10:53 AM,,EDP Infra|| Prepare script for security permissions,"h3. Objective

Automate the creation and management of roles, grants, and access privileges in Snowflake. Ensure RBAC is applied consistently across all environments with no manual SQL changes.

h3. Steps¬†

* Create parameterized SQL or Terraform modules for RBAC.
* Define standard roles: Platform Engineer, Data Engineer, Data Analyst etc.
* Automate privilege assignment and auditing.

h3. Deliverables

* RBAC script/module in repo.","* -Security permissions set up according to this design: Snowflake Security - Enterprise Data Platform - Confluence-
* -Roles and grants created only via IaC.-
* -Re-runable idempotent scripts.-
* -No changes/deployments made manually-","* Working on the RBAC documentation:
** Object Roles Scripts
** Function Roles Scripts
** Assign Object Roles to Function Roles

Added Managed Access Schema strategy to enforce least privilege and enable IaC-driven future grants

*Problem Statement*

Our Snowflake Infrastructure-as-Code (IaC) setup, deployed via Azure DevOps, uses a service role ROLE_INFRA_ADMIN_SNOWFLAKE to create and manage objects (databases, schemas, RBAC grants) across environments: *DEV, SIT, UAT, PROD*.

However, there‚Äôs a challenge:

* Future grants on schemas and objects require either *MANAGE GRANTS* (a high-privilege, organisation-wide permission) or *schema ownership*.
* Granting *MANAGE GRANTS* to the IaC role violates least-privilege principles.
* We need a *fully automated IaC solution* with no manual steps, while maintaining strong access control.

¬†

*Proposed Solution: Managed Access Schema + Role Ownership Pattern*

* Use *Managed Access Schemas (MAS)* in all environments for consistent IaC behaviour and centralised grant control.
* *Design Summary*:
** Each schema is created as a Managed Access Schema, e.g.
CREATE SCHEMA EDP_${ENV}.SILVER WITH MANAGED ACCESS;
** Schema ownership is assigned to a dedicated role, e.g.
OPS_${ENV}
** The IaC service role (ROLE_INFRA_ADMIN_SNOWFLAKE) performs grants and object creation on behalf of the schema-owner role (via USE ROLE or delegated ownership).
** Developers in *DEV* can assume the schema-owner role for flexibility.
** In *SIT/UAT/PROD*, only the IaC pipeline holds this privilege ‚Äî ensuring RBAC consistency, least privilege, and full automation.

¬†

*Benefits*

|*Area*|*Outcome*|
|*Security*|No need for MANAGE GRANTS; privileges limited to schema level.|
|*Consistency*|Identical IaC templates across all environments.|
|*Governance*|Centralised privilege control via schema-owner roles.|
|*Automation*|All schema creation and grants handled by IaC pipeline, zero manual steps.|
|*Flexibility*|Developers retain freedom in DEV through delegated roles.|

*In short:*
Standardise on Managed Access Schemas across environments and assign schema ownership to environment-specific roles. The IaC role manages objects and future grants under these roles ‚Äî delivering *full automation*, *least privilege*, and *end-to-end consistency*.

{noformat}-- 1. Create the IAC service role and data warehouse
USE ROLE SECURITYADMIN;

CREATE ROLE IF NOT EXISTS ROLE_INFRA_ADMIN_SNOWFLAKE COMMENT = 'Role used for IaC deployments';
CREATE WAREHOUSE IF NOT EXISTS INFRA_ADMIN_WH COMMENT = 'WH used for IaC deployments';

-- 2. Grant necessary privileges
GRANT CREATE DATABASE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;
GRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;
GRANT CREATE INTEGRATION ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;
GRANT CREATE ROLE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;
GRANT CREATE SCHEMA TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;
GRANT MANAGE GRANTS ON DATABASE EDP_DEV TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;

-- 3. Create the service account user
CREATE USER IF NOT EXISTS SVC_IAC_SNOWFLAKE
 LOGIN_NAME = 'SVC_IAC_SNOWFLAKE'
 DISPLAY_NAME = 'SVC_IAC_SNOWFLAKE'
 DEFAULT_ROLE = ROLE_INFRA_ADMIN_SNOWFLAKE
 DEFAULT_WAREHOUSE = 'INFRA_ADMIN_WH'
 MUST_CHANGE_PASSWORD = FALSE
 DISABLED = FALSE
 RSA_PUBLIC_KEY = 'MIICIjANBgk..........AwEAAQ=='
 COMMENT = 'Service account for Snowflake IaC deployments';

-- 4. Grant the role to the user
GRANT ROLE ROLE_INFRA_ADMIN_SNOWFLAKE TO USER SVC_IAC_SNOWFLAKE;

-- 5. Allow escalation only when approved
GRANT ROLE ROLE_INFRA_ADMIN_SNOWFLAKE TO ROLE SECURITYADMIN;{noformat}"
CSCI-573,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Oct/25 2:00 PM,25/Nov/25 11:00 AM,,EDP Infra|| Create Snowflake Environment For IaC,"h3. Objective

Create and configure Snowflake environments (DEV, SIT, UAT, PROD) using IaC, ensuring consistent configuration, compliance and zero manual intervention. All Snowflake environments must be fully automated via IaC to guarantee consistency, traceability and easy redeployment. No manual configuration.

h3. Steps¬†

# Design Teraform or scripted templates for Snowflake (accounts, database, roles, warehouse, parameters).
# Define environment variables for DEV, SIT, UAT, PROD (naming, warehouse sizes, privileges)
# Execute initial deployment pipelines using Azure DevOps (GitHub Actions preferred if possible)
# Validate resource creation and connectivity to each environment
# Document E2E setup and add validation tests

h3. Deliverables

* Terraform module repository for Snowflake IaC
* Separate environment configuration files
* Successful deployment logs and state files
* IaC documentation and validation checklist","h3. Acceptance criteria

* All Snowflake environments (DEV, SIT, UAT, PROD) provisioned automatically via IaC
* Configuration parity validated between envrionments
* No manual setup or post deployment SQL required
* Terraform plan and apply steps run successfully in CI/CD pipeline
* Approved security and configuration review completed","I am working on the high level design for Infrastructures as Code for Snowflake. Once I have got the design I will discuss with Alan, Mike and Eugene.

Working on the repo structure, naming convention and branch strategy.

Here‚Äôs a quick wrap-up:

¬†We‚Äôll separate the ALTIDA framework into its own repository and set up a dedicated pipeline for deployment and maintenance.

[@Mike Kazemi|mailto:Mike.Kazemi@sigmahealthcare.com.au] will assist in confirming the naming conventions for the following repositories:

* Snowflake Azure Infrastructure
* Snowflake Environment Setup
* ALTIDA Framework
* Snowflake Application (potentially including a dbt component)

[@Eugene Paden|mailto:ra_ray_eugene.paden@mychemist.com.au] will update the repositories by country and functionality.

[@Mike Kazemi|mailto:Mike.Kazemi@sigmahealthcare.com.au] will review and confirm the naming for the creation of the Service Account for IaC. I‚Äôll send a separate email with the required details.

[@Xavier Varalda|mailto:xavier.varalda@sigmahealthcare.com.au],
We can proceed straightaway with the first three layers to ensure version control of deployment artefacts. The final layer Snowflake Application CI/CD will require more detailed design, as it involves our data model architecture and dbt components. We‚Äôve agreed to tackle the first three layers first, and then have an in-depth discussion around the Snowflake Application CI/CD layer at a later stage.

Service account to be created for Snowflake IaC with the appropriate permission. 

 [https://sigmahealthcare.atlassian.net/browse/CSCI-620|https://sigmahealthcare.atlassian.net/browse/CSCI-620]"
CSCI-571,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Oct/25 1:45 PM,16/Dec/25 8:48 AM,,EDP Infra || Developer Access/Network Policy,"h3. Objective

Define and implement network, access and connectivity policies for developers and service accounts. Ensure secure, controlled developer access and prevent accidental changes to production.

h3. Steps¬†

# Create network policies per environment
# Implement IP whitelisting, VPN or VNet Gateway
# Enforce read-only access in production
# Apply SSO/MFA via Entra ID

h3. Deliverables

* Network policies and role mapping
* Access review documentation
* Connectivity tests","* Developers have modification privileges to DEV and sandbox only
* SIT, UAT and PROD can be modified via service accounts only
* MFA and leas privilege enforced","I am able to SSO auth to our Snowflake private link using the jumpbox

Here are the next steps in sequence:¬†

# Confirm all developers can SSO to privatelink url: [+cw-au.privatelink.snowflakecomputing.com+|http://cw-au.privatelink.snowflakecomputing.com]?¬†Jess¬†Coyle¬†Eugene¬†Paden¬†Ashutosh¬†Arya¬†Mike¬†Kazemi
# We will add a network policy to block access to the public link: [+cw-au.snowflakecomputing.com+|http://cw-au.snowflakecomputing.com]
# We will remove the Snowflake local users that were created before the SSO/ SCIM provision. Only keep local users for break glass accounts (ie. Alan, Xavier, Mike)¬†

Timeline: over the next 3 weeks - within the month of November

Hi Ashu can you pls confirm your access to the VDI still on and off? can you pls follow up to get it resolved with the relevant party pls @user

@user Nah, no luck. Getting a new error. Please see attached. @user Could you take a look into this?

@user to check on the availability of CWR jumpbox. When can we have access to the new jumpbox that are specifically requested for EDP/ Snowflake developers?

SNOW raised to create a dedicated desktop pool for Data Team - RITM0179179

CWR jumpbox is now accessible by all developer

Issue

No VS code/Dbeaver in for dataops and can‚Äôt be installed as CWR is a jb for sysops and eucops

Neville has chase up on jb creation for dataops

Emailed Neville/John to install below software to Dataops jumpbox

additional software installation in progress

20251204 - installation still in progress

Done and pending testing from Jess & Mike"
CSCI-570,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Oct/25 1:37 PM,07/Nov/25 10:12 AM,,EDP Infra || Infra & Security provision for SIT UAT PROD,"h3. Objective

Provision secure environments with consistent configurations across SIT, UAT and PROD. Achieve full environment parity and compliance alignment.

h3. Steps¬†

# Replicate IaC templates for SIT, UAT, PROD
# Validate VNet, Private Link, RBAC and logging setup

h3. Deliverables

* SIT, UAT, PROD environments deployed via IaC
* Security approval sign-offs","* 100% parity across SIT, UAT, PROD
* Security posture matches PROD baseline.
* All environments deployed via IaC (no manual setup).","Discussed requirements with Anjali and Eugene.

Anjali to raise a request to SysOps team.

@user to work with @user to document the technical requirements for this stream of work.

FYI @user

@user Hey any update on this?

@user Will look the details of the tickets raised by Anjali once have access to see other‚Äôs tickets on Service-Now ."
CSCI-568,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Oct/25 2:26 PM,24/Oct/25 9:55 AM,,EDP Infra || Local User Account Creation,"h3. Context

* Creating local user account with SF account for break glass use

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","h3. Acceptance criteria

* Local account created with necessary access

h3.","Awaiting confirmation from Xavier and Alan

Local users have completed for all mentioned users. Permission assigned."
CSCI-567,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Oct/25 10:00 AM,24/Oct/25 11:03 AM,,MergeCo Reporting || Distribution Solution Monitor,"h3. Context

* To monitor Power automate distribution performance

h3. Objective

* Power automate working as expected

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","h3. Acceptance criteria

* MergeCo report distributed as expected

h3.",
CSCI-566,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Oct/25 9:39 AM,06/Nov/25 9:47 AM,,Firewall Whitelisting for new IP for SCAX2012 database server in ADF- RITM0177824,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Anjali talking to the security team to discuss

The whitelisting has been implemented. @user to test the connectivity."
CSCI-565,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,20/Oct/25 8:00 PM,03/Nov/25 9:31 AM,,CW Platform - Internal Data Team Onboarding,"Coordinate access and onboard Sigma admins (Alan, Mike, Emil) to CW platform ADF, Snowflake, Azure DevOps","* Access granted in ADF, Snowflake, DevOps
* Walk-through sessions
* QnA, follow-ups","Access has been granted in ADF, Snowflake, DevOps

Walk-through sessions scheduled and being delivered

Need Anjali help to update access level from Stakeholder to Basic. I currently don't have this access.

@user - please share the update in this card regarding devops access change, thanks

@user @user @user your organisational access level in Azure DevOps have been updated. Can you pls check and confirm you have access to Repos?

Access confirmed for Mike & myself, @user can you check and let us know? Thx

[https://dev.azure.com/MyChemist/Enterprise Data Platform Implementation/_git/edp-infrastructure|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infrastructure]"
CSCI-564,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,20/Oct/25 5:27 PM,21/Oct/25 10:00 AM,,MergeCo Reporting || Bug Issue for Network Drive File Ingestion,"h3. Context

* As of Thursday 16th Oct, the DFIO_SKU.csv file located on the CWVault Network Drive stopped refreshing each day. Found the issue related to the fact Supply Chain team decommissioned the Boomi task to populate this file daily
* This impacts the data source for the MergeCo Supply Chain Scorecard

h3. Objective

* Need to reinstate the DFIO_SKU.csv file to be generated on the CWVault network drive

h3. Steps¬†

# Contact Boomi team to startup network drive file extract again
# Create request with Boomi team
# Validate that the extract is now back online and refreshing daily

h3. Acceptance criteria

* The DFIO_SKU.csv file is auto-generated by the Boomi extract into the Network Drive
* The file is refreshed for at least 2 days consecutively","* -The DFIO_SKU.csv file is auto-generated by the Boomi extract into the Network Drive-
* -The file is refreshed for at least 2 days consecutively-",
CSCI-563,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,20/Oct/25 10:51 AM,21/Oct/25 9:47 AM,,Add Users to AD Groups-RITM0177612,"h3. Groups : 

Snowflake-CW-AU-PlatformEngineers
Azure-Permissions-dev-edp-aus-Contributor
Azure-Permissions-qa-edp-aus-Contributor
Azure-Permissions-prd-edp-aus-Contributor
Azure-Permissions-dev-edp-aus-UAA
Azure-Permissions-qa-edp-aus-UAA
Azure-Permissions-prd-edp-aus-UAA
Azure-Permissions-dev-edp-aus-Billing Reader
Azure-Permissions-qa-edp-aus-Billing Reader
Azure-Permissions-prd-edp-aus-Billing Reader
Azure-Permissions-dev-edp-aus-Reader
Azure-Permissions-qa-edp-aus-Reader
Azure-Permissions-prd-edp-aus-Reader
Azure-Permissions-dev-edp-aus-Owner
Azure-Permissions-qa-edp-aus-Owner
Azure-Permissions-prd-edp-aus-Owner

Users to Add : 

[sig_mike.kazemi@chemistwarehouse.com.au|mailto:sig_mike.kazemi@chemistwarehouse.com.au] & [sig_alan.yuen@chemistwarehouse.com.au|mailto:sig_alan.yuen@chemistwarehouse.com.au] & [sig_emil.julius@chemistwarehouse.com.au|mailto:sig_emil.julius@chemistwarehouse.com.au]

Group:
Azure-Permissions-dev-edp-aus-Reader
Azure-Permissions-qa-edp-aus-Reader
Azure-Permissions-prd-edp-aus-Reader

User to add: [sig_han.li@chemistwarehouse.com.au|mailto:sig_han.li@chemistwarehouse.com.au]

h3. Acceptance criteria

* Users are successfully added to the above AD Groups

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Users have been added to the above mentioned AD Groups. 

@user @user"
CSCI-562,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,17/Oct/25 9:51 AM,20/Oct/25 9:59 AM,,Infra || Enterprise Observability Tool Review,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.",TBC,
CSCI-561,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Oct/25 1:54 PM,24/Oct/25 2:02 PM,,Azure DevOps access - PR review and approval,"To transfer Review and Approval access to Alan, Mike, Emil, and Jagdish, as they are the designated platform owners from the Sigma side:

@user could you please update the Pull Request settings for all EDP projects in Azure DevOps to include the following Sigma accounts as required reviewers?

* Alan Yuen
* Mike Kazemi
* Emil Julius
* Jagdish Jha

At least one approval from any of the individuals listed above is required to merge code into the repositories.

@user @user @user @user @user @user","Given, When, Then","@user need clarification on the changes:

* <repository name> 
** PR Review
*** number of reviewers - ?
*** required reviewers - ?
*** optional reviewers - ?
** Release Pipeline
*** QA
**** Approvers - ?
**** Approval Mode - ? (Any Order, In Sequence, Any One User)
*** Prod
**** Approvers - ?
**** Approval Mode - ? (Any Order, In Sequence, Any One User)

we are working on the following repositories

* edp-infrastructure
* edp-snowflake-infrastructure
* edp-data-factory
* edp-snowflake
* edp-semantic-models
* edp-powerbi-reports

will start working on them soon as I have the detailed breakdown.

@user let me check with @user and @user on the process.

@user based on our meeting yesterday, can we pls have a tentative list for each repo?

cc: @user @user

Updated [edp-infrastructure|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infrastructure/branches]

* PR Approver

* Deployment to QA

* Deployment to Prod

cc: @user @user @user

Updated [edp-snowflake-infrastructure|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=4e016c43-1374-45eb-b26a-62da7bb94997]

* PR Approver

* Deploy to Prod

cc: @user @user @user

Updated [edp-data-factory|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=a9d4deeb-ad55-465e-9425-438e29d991d5]

* PR Approver

* Deployment to QA

* Deployment to Prod

cc; @user @user @user

Updated [edp-snowflake|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=35a748cb-2300-47fc-a80d-206021e6c335]

* PR Approver

* Deployment to SIT

* Deployment to UAT

* Deployment to Prod

@user @user @user

Updated [edp-semantic-models|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=f1342e50-703c-470f-91c7-fee3a7eff4f4&_a=policiesMid]

* PR Approvers

cc: @user @user @user

Updated [edp-powerbi-reports|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=adb2a221-e26d-407d-aa7b-b5972abbd947]

* PR Approvers

cc: @user @user @user"
CSCI-559,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Oct/25 10:28 AM,24/Oct/25 11:04 AM,,MergeCo Reporting || Distribution Solution Discovery,"h3. Context

* To investigate on solution for report distribution

h3. Objective

* Understand options for automating report distribution

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","h3. Acceptance criteria

* Document/understand what option is most suitable for MergeCo report

h3.","Have created a Power Automate flow to generate the MergeCo Supply Chain Scorecard.
The flow:

* Extracts the PBI report as a .pdf
* Validates 3 metrics
** Ensures the latest date in the Semantic Model matches the latest date based on a TODAY() date function
** Availability % is between 80% and 100%
** SOH is between $500M and $1B
* Emails the report to distribution list if validation is successful
* Emails a notification alert to the Data Eng team if validation fails

[https://make.powerautomate.com/environments/Default-8bc0280e-b3ad-438a-ba8a-2cf00d19af4a/flows/74c08131-3e07-477c-9442-7a88a2d67751/details|https://make.powerautomate.com/environments/Default-8bc0280e-b3ad-438a-ba8a-2cf00d19af4a/flows/74c08131-3e07-477c-9442-7a88a2d67751/details]"
CSCI-558,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Oct/25 10:19 AM,10/Nov/25 9:48 AM,,MergeCo Reporting || Logic Update,"To obtain enhance requirement for report update.

h3. Acceptance criteria

* Understand enhancement need
* Report update
* UAT

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-557,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Oct/25 10:03 AM,23/Oct/25 9:49 AM,,Manhattan MAWM Data Save access || Service Account Review,"h3. Context

* Manhattan MAWM Data Save access require new service account as current *username ""readonlyuser""* will be disable. We need to raise a request to have new CW service account email ID created so Manhattan team can assign access.
* Expanding the question to 1. 

h3. Acceptance criteria

* -Draw/Document/Determine +*all*+ the service accounts we will need for CW Snowflake solution upfront.-
* Mike to review the above artefact
* Once reviewed and we have a list of all the service accounts we will need, then we start raising and tracking the requests

h3. 

h3. 

h3. 

h3. 

h3.","* -Draw/Document/Determine +*all*+ the service accounts we will need for CW Snowflake solution upfront.-
* -Mike/Alan to review the above artefact-
* Once reviewed and we have a list of all the service accounts we will need, then we start raising and tracking the requests","Have provided information to Alan and team via email.

|*SOURCETYPE*|*DATABASENAME*|*SERVERNAME*|*Azure Keyvault User Name*|*Azure Keyvault Password Name*|*Environment*|
|SqlServer|AXLINK|192.168.29.206|TDB19-Dev-SQL-LocalAccountName|TDB19-Dev-SQL-LocalAccountPassword|Dev|
|SqlServer|CWMgtStoreInvoices|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|
|SqlServer|General_Reference|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|
|SqlServer|ILS|192.168.29.78|TDB15-Dev-SQL-LocalAccountName|TDB15-Dev-SQL-LocalAccountPassword|Dev|
|SqlServer|SCAX2012|192.168.29.206|TDB19-Dev-SQL-LocalAccountName|TDB19-Dev-SQL-LocalAccountPassword|Dev|
|SqlServer|SKU|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|
|SqlServer|SpsWhsPurchase|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|
|SqlServer|StockDb|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|
|SqlServer|TransactionStorage|192.168.29.104|TDB14-Dev-SQL-LocalAccountName|TDB14-Dev-SQL-LocalAccountPassword|Dev|
|SqlServer|BI_Presentation|192.168.42.82|PBI05-Prod-SQL-LocalAccountName|PBI05-Prod-SQL-LocalAccountPassword|Prod|
|SqlServer|SupplyChain|192.168.42.82|PBI05-Prod-SQL-LocalAccountName|PBI05-Prod-SQL-LocalAccountPassword|Prod|
|SqlServer|TransactionsArchive|192.168.42.82|PBI05-Prod-SQL-LocalAccountName|PBI05-Prod-SQL-LocalAccountPassword|Prod|
| | | | | | |
|Nework Drive|NetworkDrive_CWVault|192.168.42.235|*pending (servac_edp_tactical)*|pending|Prod|
|MySQL|default_dcinventory|172.19.232.11|*require a CW Service Account Email ID*|¬†|Prod|

¬†

* We haven‚Äôt yet had any EDP service account to access corporate systems (network drive, sharepoint etc.). We are waiting for the servac_edp_tactical to be created for MergeCo.
* The MySQL Manhattan MAWM (high-lighted red) is pending a request.

@user @user 
Can you please review and advise further

Hi @user @user I have documented the service accounts for source systems all in the section *5. Source System* of the design doc [CWR - EDP Solution Architecture - Detailed Design v.01.docx|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/05.%20Architecture/CWR%20-%20EDP%20Solution%20Architecture%20%20-%20Detailed%20Design%20v.01.docx?d=w2b1d2a9d799b4326b9cf52840fafa912&csf=1&web=1&e=X6HgI7]. Please review and advise if @user/@user can proceed to raise a request for a new service account ie.servac_edp_{name}.

@user I‚Äôve added a new page under *‚ÄúStandards & Principles‚Äù* outlining the naming conventions for Service Accounts. When you have a moment, please take a look and let me know if you have any questions. [Service Accounts - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1688174594/Service+Accounts] (Still under dev)

@user @user @user please review"
CSCI-555,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Oct/25 2:02 PM,05/Dec/25 11:32 AM,,Engagement with dbt and CW teams for dbt Cloud,"# Initial discussions with dbt and CW infrastructure teams for onboarding and setup
# Appropriate infrastructure tasks created and completed
# Dbt to Alation

h3. 

h3. 

h3.","Given, When, Then",
CSCI-544,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Oct/25 1:19 PM,28/Nov/25 9:47 AM,,Dbt Implementation Business case,"Create Business Case for Dbt Implementation in Data Platform

h3. 

h3. 

h3. 

h3.",Completed Dbt Implementation Business Case,
CSCI-543,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Oct/25 1:07 PM,19/Nov/25 11:26 AM,,Document what tasks are still required to completed Developer Access/Network Policy for ADF and SF,"h3. 

Document remaining required tasks for Developer Access/Network Policy completion

DEV ‚Üí SIT ‚Üí UAT ‚Üí PROD

h3. 

*Open related tickets*

h3. [https://sigmahealthcare.atlassian.net/browse/CSCI-525|https://sigmahealthcare.atlassian.net/browse/CSCI-525]

h3.","Given, When, Then",
CSCI-542,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Oct/25 1:06 PM,19/Nov/25 11:26 AM,,Placeholder,"Document remaining required tasks for Infrastructure and security setup to enable e2e environment deployment.

DEV ‚Üí SIT ‚Üí UAT ‚Üí PROD

* All environments (DEV, SIT, UAT, PROD)
* Any security aspects outstanding? What are they?
* Access and network Policies
* Infrastructure (IaC)
* CI/CD Pipelines
* Ingestion framework
* Dbt and transformation framework
* All requires On-Prem Whitelisting on both sides for PROD environment, these should be called out now and raised now as we know this can take time
* Completion of build of one Core Dim ‚Äì Product or Store

@user and @user to help define what is outstanding to meet there requirements.

*Related Open Tickets*

h3. 

h3.","Given, When, Then",
CSCI-541,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Oct/25 10:56 AM,17/Oct/25 9:10 AM,Design,Bulk loading Proc - Logging/Documentation of usage,"h3. Context

* For the end-to-end automation of the parquet based bulk uploads a procedure is created in the Snowflake CONTROL.PR_BULKLOAD_PARQUET(SOURCE_NAME VARCHAR,STAGE_TABLE_NAME VARCHAR). 

h3. Objective

* Add logging at each step for better user experience.

h3. Acceptance criteria

* Add a logging table 
* Add logic in procedure to log every step‚Äôs start end and end time.
* Document the logging

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",Logging implemented. Have to document the details of the process in the operational documentation.
CSCI-540,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Oct/25 8:54 AM,14/Oct/25 10:56 AM,,Parquet File load behaviour,"h3. Context

* While testing the performance the data ingestion from Parquet file was behaving unexpectedly.
* The load configuration was set to ingest data for all null columns which is not behaving as expected.

h3. Objective

* Understand the behaviour of parquet data ingestion.

h3. Acceptance criteria

* Understand & document the behaviour of parquet data ingestion.

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-539,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Oct/25 8:22 AM,15/Oct/25 1:53 PM,,MergeCo Retail Data Ingestion || Sprint 10,"h3. Context

initiate the process to convert the tables in to Parquet files for last 2 years of data for the fact tables mentioned here [PBI05 Objects for MergeCo Reporting.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BD5AD27C1-A51F-46E0-89EE-1B61847B2DAA%7D&file=PBI05%20Objects%20for%20MergeCo%20Reporting.xlsx&action=default&mobileredirect=true] (except FCT_STOCK_HISTORY)

h3. Objective

|Table Name|
|FCT_SALES|
|FCT_CUSTOMER_COUNT_HOURLY_AUS|
|SubCategoryBudgetDaily|
|FCT_SALES_NZL|
|FCT_CUSTOMER_COUNT_HOURLY_NZL|

h3. Acceptance criteria

* Files being present in the Azure Storage","Given, When, Then","SubCategoryBudgetDaily and FCT_CUSTOMER_COUNT_HOURLY_AUS 

have finished extracting 

They are currently being uploaded intot he blob storage.

New card to be created in sprint 10 if ingestion failed.

the rest of the tables have been extracted and is being transfered into the Blobstorage

@user , can you please create a split card or another card for the new sprint. Thanks

Hey @user 

[https://sigmahealthcare.atlassian.net/browse/CSCI-519|https://sigmahealthcare.atlassian.net/browse/CSCI-519] - This was the one for Sprint 9

[https://sigmahealthcare.atlassian.net/browse/CSCI-539?sourceType=mention|https://sigmahealthcare.atlassian.net/browse/CSCI-539?sourceType=mention] - Which is this card is split from above and assigned to Sprint 10 for any follow up you need

Thx

@user Thank you"
CSCI-538,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Oct/25 9:24 AM,14/Oct/25 9:33 AM,Design,Bulk upload automation - performance testing,"h3. Context

* The automation is working for smaller table (61M records). 

h3. Objective

* To test the performance of the pipeline with high number parquet file and records table. 

h3. 

h3. Acceptance criteria

* We have loaded over 600M records in under 1 hour manually the automation should give similar results if not better.

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",Performance test complete and took around 1 hour to load 450M records.
CSCI-537,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,10/Oct/25 5:15 PM,20/Oct/25 9:52 AM,,Creation of Service Account to access Network Drive- RITM0176342 Sprint 10,"h3. Context

* Creation of Service Account to access Network Drive- RITM0176342

h3. We need a Service Account creation to access the below Network Drive for Ingestion of supply chain data into Snowflake for group level reporting.

Will be retrieving data from this location (Folder path):
 \\cwvault\Everyone\Supply Chain\Administration\Report and Dashboard\ZZ_DataSource

h3. 

h3. 

h3.","Given, When, Then",
CSCI-536,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,10/Oct/25 5:14 PM,14/Oct/25 9:33 AM,,Creation of Service Account to access Network Drive- RITM0176342 Sprint 10,"h3. Context

* Creation of Service Account to access Network Drive- RITM0176342

h3. We need a Service Account creation to access the below Network Drive for Ingestion of supply chain data into Snowflake for group level reporting.

Will be retrieving data from this location (Folder path):
 \\cwvault\Everyone\Supply Chain\Administration\Report and Dashboard\ZZ_DataSource

h3. 

h3. 

h3.","Given, When, Then",
CSCI-535,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,10/Oct/25 5:13 PM,16/Oct/25 3:22 PM,,RITM0173487 - Implement Azure DNS Private Resolver - Sprint 10,"Currently worked on by Shagun A.

Implement Azure DNS Private Resolver to enable on-premises DNS resolution for existing Snowflake Private Link endpoint ([privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com]).

The Azure Private Link for Snowflake is already configured and operational. This DNS implementation will enable on-premises systems to resolve the private Snowflake endpoint, which is a prerequisite for subsequent firewall access configuration for Enterprise Data Platform and critical applications requiring Snowflake access.

Spoke to Network team and they suggested we create AD security group for snowflake private link access control and add the relevant users who need access to this group as the user laptop ip address is dynamic so this way we can control this via global protect and firewall.¬†

¬†

Current status:","* Azure DNS Private Resolver is deployed and configured
* On-premises DNS resolution is verified
* AD Security Group for Snowflake Access is created
* Firewall and GlobalProtect integration is validated
* Documentation is updated
* -Stakeholder sign-off-","New Change Request CHG0052343 has been scheduled for today 

[CHG0052343|https://cwretail.service-now.com/nav_to.do?uri=change_request.do?sys_id=61cc0f803360b25047764f945d5c7b23]

Planned start date: 13-10-2025 11:00:00 AM
Planned end date: 13-10-2025 12:00:00 PM
Implementor: Louis Allsop
Active Directory DNS - Configure Conditional Forwarding to Azure for Private Endpoint resolution ([http://snowflakecomputing.com|http://snowflakecomputing.com] )

Please use [https://sigmahealthcare.atlassian.net/browse/CSCI-294|https://sigmahealthcare.atlassian.net/browse/CSCI-294] for tracking

thanks"
CSCI-534,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,10/Oct/25 5:11 PM,20/Oct/25 9:51 AM,,Creation of CW accts for sigma personnel- RITM0177239- Sprint 10,"h3. Context

* Sigma personnel need CW accounts to access CW snowflake
** Mike Kazemi
** Alan Yuen
** Han Li
** Emil Julius

h3. Objective

* CW snowflake access for sigma

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* Creation of sig_firstname.lastname@chemistwarehouse.com.au
* Access to snowflake

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-533,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,10/Oct/25 5:09 PM,19/Oct/25 7:56 PM,,"Review Source-to-Target Map for DimSupplier, BridgeProductSupplier - Sprint 10","h3. Context

* There is a need to document the source-to-target mapping for the [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.

h3. Objective

* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.

h3. Steps

# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document
# For each column in Fact_Warehouse_Location_Packing, fill in:
#* Pres Column name
#* Data type
#* Description
#* Source details (Server, DB, Schema, Table, Column)
#* Indicate if derived (Yes/No)
# Review the completed mapping for accuracy and completeness.

h3. Deliverables

* A fully completed source-to-target mapping sheet for [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table , with all required fields filled as per the template.

h3. Assumptions (Optional)

* Access to all necessary source systems and metadata is available.
* The supplied mapping sheet template is up to date and reflects current requirements.

h3.","h3. 

* Columns as per the supplied sheet are filled:
** Pres Column name
** Data type
** Description
** Source (Server, DB, Schema, Table, Column)
** Derived? (Yes/No)",review done
CSCI-532,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,10/Oct/25 5:07 PM,24/Oct/25 11:08 AM,,MergeCo Conformed Reporting - Draft Solution Design - Sprint 10,"Create draft of the solution design for the Merge Co Conformed reporting.

Just needs to be a one-page document, outlining *how* we will build and ingest the data for this reporting.

Includes:

* What grain of data required for each metric
* How we will push data for each environment into a join environment
* Merging them into a common dimension",,[https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design]
CSCI-531,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,10/Oct/25 3:04 PM,24/Oct/25 2:01 PM,,Architecture walk-through Eugene/ Mike,"Eugene to walk Mike through Azure architecture, Infrastructure as Code (IaC), DevOps, and related components","Given, When, Then",
CSCI-526,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,10/Oct/25 10:06 AM,24/Oct/25 3:05 PM,,DBT Cloud -> SF network connection,"h3. Context

* Connect dbt Cloud to our Snowflake instance on Azure to enable secure, version-controlled data transformations, scheduled jobs, and CI for analytics. The setup will use least-privilege access, correct Azure region selection, and standardized dev/staging/prod environments.

h3. Objective

* Provision Snowflake user/role with least-privilege and a dedicated warehouse.
* Configure dbt Cloud connection: account identifier, Azure region, user/role, warehouse, database, schema; select approved auth (key pair or OAuth/SSO).
* Store credentials in dbt Cloud environment variables; avoid hardcoded secrets.
* Set up dev and deployment environments/jobs; map targets to separate schemas.
* Validate with dbt debug and a sample dbt run; confirm role, warehouse, and permissions.
* Document egress IPs (for allowlisting/PrivateLink), connection details, and handoff steps.

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* TBC

h3. @user - Can you ad detail - thx

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Following our introductory session with the dbt technical team

and CWR Cloud Engineer, Brent Robinson, dbt proposed two hosting options:

1) Australia (AU) without Azure Private Link

2) Europe with Azure Private Link

Neither option meets the Cloud team‚Äôs requirements. Brent, the Security team,

and dbt will continue collaborating to identify a compliant solution. Updates to follow.

Split to [https://sigmahealthcare.atlassian.net/browse/CSCI-610|https://sigmahealthcare.atlassian.net/browse/CSCI-610]"
CSCI-525,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Oct/25 10:23 AM,31/Oct/25 9:51 AM,,CLONE - Capture the requirements for creating the Jumpbox for the developers (data engineers),"Capture the requirements for creating the Jumpbox for the developers (data engineers).

*Key action items*

* -On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/] - On prem and Cloud Team- *RITM0173487 (for reference)*-

What‚Äôs needed is for the DNS forwarding from on-premise DNS servers, to the Azure privatelink DNS. See: [Resolve Azure and on-premises domains. | Microsoft Learn|https://learn.microsoft.com/en-us/azure/dns/private-resolver-hybrid-dns] -CHG0052044 (change request created)

* Collaborate with the EUC Team (CWR) to *create* Jumpbox
* Give the Entra (AD) users access to the Jumpbox 

|*Snowflake_CW_AU_DataEngineers*|DEV_ROLE_DATA_ENG|

* Configure firewalls (whitelisting) (On-Prem, Azure) as follows
* VDI / Jumpbox network ‚Üí Snowflake VNet *(servicenow tickets)*

* *How many Jumpboxes* required? *no. of users 10*
* Note: Ports to be allowed: 443, 80, 1433
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Configure VDI / Jumbox images to pre-install tools listed below: - The list has been reviewed and approved by EUC Team.""}],""attrs"":{""localId"":""a6a01a2f-604c-4540-b9f2-173ad2e43e57"",""state"":""DONE""}}],""attrs"":{""localId"":""0c33acf9-c722-4f42-8518-6b140130d8da""}}
{adf}
* *Git for Windows* + *Git Credential Manager (GCM)* (Azure DevOps/GitHub SSO)
* *Azure CLI* & *Azure PowerShell*
* *AzCopy* (for ADLS/BLOB moves within Azure)
* *VS Code* (primary IDE)
** Extensions: _Snowflake SQL Tools_ (Snowflake), _SQLTools_ (optional), _Python_, _Pylance_, _Jupyter_, _YAML_, _GitLens_
* *Snowflake CLIs/SDKs*
** *Snow CLI* (preferred over SnowSQL)
** _(Optional)_ *SnowSQL* legacy client (only if you still need it)
* *Drivers*
** *Snowflake ODBC* and *JDBC* drivers (for BI tools, dbt, notebooks)
* *Languages / Runtimes*
** *Python 3.11+* (via *Miniconda* or *pyenv-win*; standardize on conda envs)
** *Java 17* (LTS) for Snowpark Java/Scala
** *Node.js 20 LTS* (if you use Streamlit/Node helpers)
* *Snowpark & data tooling (per environment)*
** {{pipx}} and/or {{conda}}
** {{snowflake-snowpark-python}}, {{pandas}}, {{pyarrow}}, {{jupyterlab}}, {{ipykernel}}

* *Power BI Desktop* _(or Power BI Desktop ‚Äì Optimized for Fabric if that‚Äôs your org standard)_
* *Tabular Editor*
** TE2 (free) for basic edits or *TE3* (licensed) for advanced modeling/CI
* *DAX Studio* (performance tuning)
* *ALM Toolkit* (schema compare/deployment)
* *Power BI Report Builder* (if you produce paginated reports)
* *Azure Data Studio* (lightweight SQL + notebooks)
* *SSMS* (for on-prem SQL Server admin)
* *Azure Storage Explorer* (browsing ADLS/BLOB via private endpoints)
* *On-prem / misc drivers*
** Microsoft SQL Server ODBC, Oracle/ODBC (if used), PostgreSQL ODBC (if used)",* The data engineers are able to access Snowflake and other relevant applications via Jumpbox,"@user - please update on outcome of your discussion with the network architecture

fyi - @user @user

@user Moving this to sprint 10 as today is end of sprint and task haven‚Äôt start (last minute assignment)

DNS resolver CR has been successfully completed. Now the traffic is completely private, doesn't go over internet

cc- @user @user @user @user 

@user - can you please update us if there is any discussion/outcome from Network team yet.

@user apologies if it is already covered, are the firewall rules (on-prem, azure) also done to connect from VDI network to the Snowflake VNet ?

||Item||Status||Comment||
|Azure ‚Üí Snowflake (Private Link)|Completed|Private path verified by @user|
|DNS Private Resolver|Completed| |
|Jumpbox Creation|Pending|To be done by EUC team @user is working on this|
|On-prem / VDI ‚Üí Snowflake VNet firewall rules|Completed|Network Readiness|

cc- @user @user @user

@user Yes, this is done- VDI network to Snowflake Private Endpoint

Hey @user ,

Given Mike is looking after the jumpbox creation, can this ticket move to done or there is another pending task?

Thanks

Neville would like a dedicated jumpbox for our team instead of using the shared CWR jumpbox (currently used by Sysops, EUCops, Netops, and Secops).

@user , please create a project task to request EUCops to start building the dedicated jumpbox. Neville will guide them through the setup once the task is created.

He also prefers a single firewall request for the new jumpbox once network policies are in place. In the meantime, the team can continue using the existing CWR jumpbox.please make sure not to save huge files with in the jumppbox as it will fill up the back end file server.

Project created under PRJ0116183

Neville to confirm if below user can access existing CWR jumpbox, once confirmed, next steps is to create deliciated jumpbox for EDP 

Mike K -¬†

[sig_mike.kazemi@chemistwarehouse.com.au|mailto:sig_mike.kazemi@chemistwarehouse.com.au]

¬†

Han L -

¬†[sig_Han.Li@chemistwarehouse.com.au|mailto:sig_Han.Li@chemistwarehouse.com.au]

¬†

Chloe T -¬†

[chloe.tran@chemistwarehouse.com.au|mailto:chloe.tran@chemistwarehouse.com.au]

¬†

Ashu A -¬†

[ashutosh.arya@chemistwarehouse.com.au|mailto:ashutosh.arya@chemistwarehouse.com.au]

Eugene

Aswini

Emil"
CSCI-524,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Oct/25 9:35 AM,09/Oct/25 2:43 PM,,Manhattan Active (MySQL) Integration - (Work on MySQL specific Procedure ),"We are importing data from MYSQL, and we need to create a custom procedure to get the data type of the columns and keep the Snowflake objects in sync","Definition of done:

* The pipeline should execute successfully.
* Staging view should have source data types","The work is in progress. Procedure has been created but it is under review as we don‚Äôt have direct access to any MYSQL to debug the code.

Continue in the next sprint in the CSCI-136"
CSCI-523,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Oct/25 9:29 AM,07/Nov/25 2:19 PM,,Optimize the Bulk load pipeline - end to end integration,"h3. Context

* Bulk upload process involves a few manual steps. 

h3. Objective

* Optimize the bulk upload process to be as automated as well.

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Hey @user , please update then content for this one

Reviewed and pending PR"
CSCI-520,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Oct/25 8:52 AM,22/Oct/25 9:58 AM,,Creating Detailed documentation - Detailed Documentation of work - Sprint 10,"h3. Context

* Comprehensive documentation is required to centralise and standardise knowledge on data engineering assets, including ADF pipelines, Snowflake extract metadata, and related sources (e.g., SCAX2012). This will support onboarding, troubleshooting, and ongoing enhancements.

h3. Objective

* Summarise and document the architecture, processes, and metadata for ADF pipelines, Snowflake extract mechanisms, and SCAX2012, consolidating all sources and references in Confluence for easy access and governance.

h3. Steps

# Identify and catalogue all relevant ADF pipelines, Snowflake extract processes, and SCAX2012 assets.
# Gather existing documentation, code, and metadata from source systems and repositories.
# Summarise technical details, dependencies, and integration points for each asset.
# Document best practices, naming conventions, and standards (referencing existing Snowflake and ADF standards).
# Create or update Confluence pages with structured summaries, diagrams, and links to source artifacts.
# Review documentation with stakeholders for completeness and accuracy.
# Publish and communicate the documentation to the engineering and analytics teams.

h3. Deliverables

* Confluence summary pages for ADF pipelines, Snowflake extract metadata, and SCAX2012.
* Linked references to source code, metadata tables, and configuration files.
* Diagrams illustrating architecture and data flows.
* Documented best practices and standards for each component.

h3. Assumptions (Optional)

* All relevant source systems and repositories are accessible.
* Existing documentation is available and up to date for reference.
* Stakeholders are available for review and feedback.","* All ADF pipelines, Snowflake extract processes, and SCAX2012 assets are documented in Confluence.
* Documentation includes architecture diagrams, metadata summaries, and integration points.
* Best practices and naming conventions are clearly outlined.
* Stakeholder review is completed and feedback incorporated.
* Documentation is published and communicated to all relevant teams.","Document framework is ready and now working on details.

Need help from DBA team to complete doc

Hey @user ,

Can we ensure document is in centralised location? Is it confluence or sharepoint?

Thanks,

Harrison

@user attaching it and it is at in the MergeCo data team‚Äôs architecture folder. I will send it for @user review now. It is still an evolving document. [CWR - EDP Operational Details v0.1.docx|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BB4A4EEE0-67BB-4D2F-A05A-EE4929DAA15B%7D&file=CWR%20-%20EDP%20Operational%20Details%20v0.1.docx&action=default&mobileredirect=true]

@user 
@user if this is TBD next week with @user et al, i‚Äôll split this task for sprint 10 and then mark this task as done?
LMK once you guys have had this discussion."
CSCI-519,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,08/Oct/25 10:08 AM,14/Oct/25 8:22 AM,,MergeCo Retail Data Ingestion,"h3. Context

initiate the process to convert the tables in to Parquet files for last 2 years of data for the fact tables mentioned here [PBI05 Objects for MergeCo Reporting.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BD5AD27C1-A51F-46E0-89EE-1B61847B2DAA%7D&file=PBI05%20Objects%20for%20MergeCo%20Reporting.xlsx&action=default&mobileredirect=true] (except FCT_STOCK_HISTORY)

h3. Objective

|Table Name|
|FCT_SALES|
|FCT_CUSTOMER_COUNT_HOURLY_AUS|
|SubCategoryBudgetDaily|
|FCT_STOCK|
|FCT_STOCK_HISTORY|
|FCT_SALES_NZL|
|FCT_CUSTOMER_COUNT_HOURLY_NZL|
|FCT_STOCK_NZL|
|FCT_STOCK_HISTORY_NZL|

h3. Acceptance criteria

* Files being present in the Azure Storage","Given, When, Then","SubCategoryBudgetDaily and FCT_CUSTOMER_COUNT_HOURLY_AUS 

have finished extracting 

They are currently being uploaded intot he blob storage.

New card to be created in sprint 10 if ingestion failed."
CSCI-518,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,08/Oct/25 9:06 AM,10/Oct/25 3:00 PM,,ADF Pull Requests - source config CI/CD setup,To update ADF Pipelines to use Global Parameters,PRs approved and merged,"see [Task 209939 Update ADF Pipelines to use Global Parameters|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/209939]

Added powershell script to replace the dev datafactory json with the target environment datafactory json.

Updated Variable groups to add adf-identity:"
CSCI-517,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,08/Oct/25 8:23 AM,10/Oct/25 9:55 AM,,SF- Creating Extract Meta Insert Scripts for all Sources completed,"h3. Context

* The first step to ingest data in any environment is to populate metadata into extract meta table.

h3. Objective

* Come up with one insert script per source to be used in higher environment.

h3. Steps¬†

# 

h3. Deliverables

* One .sql file per source

h3. Assumptions - (Optional)

* Will generate the inserts from the existing entries present in test environment.

h3. Acceptance criteria

* Come up with a generic script that can produce these scripts
* Include these scripts against each source

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Hi All, I have started working on the script that will generate the inserts when we pass the source name.

Hey @user I want to discuss this card with you separately.

FYI @user"
CSCI-516,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,07/Oct/25 11:05 AM,10/Oct/25 5:11 PM,,Creation of CW accts for sigma personnel- RITM0177057,"h3. Context

* Sigma personnel need CW accounts to access CW snowflake
** Mike Kazemi
** Alan Yuen
** Han Li
** Emil Julius

h3. Objective

* CW snowflake access for sigma

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* Creation of sig_firstname.lastname@chemistwarehouse.com.au
* Access to snowflake

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Followed up with Edward Jeffries to get the approval

cc- @user @user @user

@user thank you

Cc @user

@user @user we‚Äôll need to escalate this

Issue split into:
|CSCI-534|Creation of CW accts for sigma personnel- RITM0177057 - Sprint 10 |

Approved by Eddy pending CW work 

To continue monitor in sprint 10"
CSCI-515,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,07/Oct/25 9:46 AM,07/Nov/25 2:15 PM,,Optimize the Bulk load pipeline,"h3. Context

* Bulk upload process involves a few manual steps. 

h3. Objective

* Optimize the bulk upload process to be as automated as well.

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","ADF part is done

Review scheduled for 8/10/2025

Proc Successfully loaded data in Snowflake table. Not integrating end to end process.

Issue split into:
|CSCI-523|Optimize the Bulk load pipeline - Sprint 10|

End to end tested for one table and process worked fine without any manual intervention other than specifying the parameters."
CSCI-514,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,07/Oct/25 9:08 AM,24/Oct/25 12:38 PM,,Review Source-to-Target Map for Fact_Warehouse_Location_Packing,Source to target mapping for Fact_Warehouse_Location_Packing - filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no","as per the last conversation with @user the following tables need to be put into a single table ‚ÄúFact_WH_Loc_Inventory_Adjustment‚Äú and the below tables will be split in the gold layer

|Fact_Warehouse_Location_Putaway|
|Fact_Warehouse_Location_Picking|
|Fact_Warehouse_Location_Replenishment|
|Fact_Warehouse_Location_Packing|"
CSCI-513,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,07/Oct/25 9:08 AM,24/Oct/25 12:38 PM,,Review Source-to-Target Map for Fact_Warehouse_Location_Replenishment,Source to target mapping for Fact_Warehouse_Location_Replenishment - filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no","as per the last conversation with @user the following tables need to be put into a single table ‚ÄúFact_WH_Loc_Inventory_Adjustment‚Äú and the below tables will be split in the gold layer

|Fact_Warehouse_Location_Putaway|
|Fact_Warehouse_Location_Picking|
|Fact_Warehouse_Location_Replenishment|
|Fact_Warehouse_Location_Packing|

if we pick up the values from transaction tables. 

||TRANSACTION_TYPE||WORK_TYPE||WORK_GROUP||
|320|Z06 Cage Manual Move|Replenishment|
|140|Z12 Gen Pool Replen|Replenishment|
|100|Z04 General Wave Replen|Replenishment|
|100|Z70 Gen Manual Replen|Replenishment|
|120|Z10 Cage Manual Replen|Replenishment|
|320|Z13 Manual Move|Replenishment|
|130|Z51 Wave Replen|Replenishment|
|130|Z02 General Wave Replen|Replenishment|
|120|Z13 Cage Manual Replen|Replenishment|
|330|Z77 Cage Pool Replen|Replenishment|
|320|Z05 General Wave Replen|Replenishment|
|130|Z07 Flamm Wave Replen|Replenishment|
|130|Z10 Cage Manual Replen|Replenishment|
|330|Fragrance Nested Putaway|Putaway|
|140|Z10 Gen Pool Replen|Replenishment|
|100|Z57¬† Pool Replen|Replenishment|
|220|Cage Case Pick|Order Pick|
|330|Z07 Flamm Manual Replen|Replenishment|
|100|Z06 Manual Move|Replenishment|
|120|Z07 Gen Manual Replen|Replenishment|
|330|Z70 Gen Wave Replen|Replenishment|
|130|Z13 Cage Manual Replen|Replenishment|
|140|Z20¬† Manual Move|Replenishment|
|140|Z57¬† Pool Replen|Replenishment|
|320|Z10 Gen Manual Replen|Replenishment|
|130|Z11 Manual Move|Replenishment|
|130|Z05 Gen Pool Replen|Replenishment|
|240|General Case Pick|Order Pick|
|330|Z01 Gen Pool Replen|Replenishment|
|330|Z22 Manual Move|Replenishment|
|120|Fragrance Putaway|Putaway|
|320|Z70 Gen Wave Replen|Replenishment|
|140|Z77¬† Wave Replen|Replenishment|
|120|Z04 Gen Pool Replen|Replenishment|
|330|Z10 Gen Manual Replen|Replenishment|
|330|Z03 General Wave Replen|Replenishment|
|240|Z51¬† Manual Replen|Replenishment|
|130|Z06 Frag Pool Replen|Replenishment|
|140|Z05 General Manual Replen|Replenishment|
|130|Z51¬† WCS Manual Replen|NULL|
|320|Z51¬† WCS Manual Replen|Replenishment|
|130|Z07 Mezz201 Manual Replen|Replenishment|
|330|Cage Putaway|Putaway|
|330|Z06 Cage Manual Move|Replenishment|
|140|General Tote Pick|Order Pick|
|100|Z01 General Wave Replen|Replenishment|
|320|Z77¬† Cage Manual Move|Replenishment|
|100|Z21¬† Manual Replen|Replenishment|
|130|Z66¬† Wave Replen|Replenishment|
|140|Z99 Manual Move|Replenishment|
|240|Frag Tote Pick|Order Pick|
|330|Z13 Manual Move|Replenishment|
|140|Z07 Cage Manual Replen|Replenishment|
|100|Z70 Gen Pool Replen|Replenishment|
|140|Z01 General Manual Replen|Replenishment|
|120|Flamm Case Pick|Order Pick|
|330|Z11 Cage Manual Replen|Replenishment|
|320|Z07 Flamm Manual Replen|Replenishment|
|140|Z07 Manual Move|Replenishment|
|140|Z75¬† Manual Move|Replenishment|
|140|Z99 Gen Wave Replen|Replenishment|
|140|Z66¬† Pool Replen|Replenishment|
|140|Z06 General Wave Replen|Replenishment|
|140|Z02 General Manual Replen|Replenishment|
|120|Z04 Manual Move|Replenishment|
|100|Z06 Gen Pool Replen|Replenishment|
|140|Z51¬† WCS Manual Move|NULL|
|140|Z09 Gen Manual Replen|Replenishment|
|320|Z08 Gen Pool Replen|Replenishment|
|330|Z03 General Manual Replen|Replenishment|
|100|Z77¬† Manual Move|Replenishment|
|320|Z11 Cage Pool Replen|Replenishment|
|100|Z75¬† Manual Move|Replenishment|
|140|Z75¬† Pool Replen|Replenishment|
|140|Z20¬† Manual Replen|Replenishment|
|100|Z70 Manual Move|Replenishment|
|140|Z09 Gen Wave Replen|Replenishment|
|330|Z57¬† Manual Move|Replenishment|
|140|Z75¬† Manual Replen|Replenishment|
|120|Ethical Case Pick|Order Pick|
|320|Z02 General Manual Replen|Replenishment|
|330|Z10 Gen Wave Replen|Replenishment|
|330|Z06 Frag Manual Replen|Replenishment|
|130|Z08 Gen Manual Replen|Replenishment|
|100|Z02 Gen Pool Replen|Replenishment|
|320|Z77 Cage Pool Replen|Replenishment|
|330|Cage Case Pick|Order Pick|
|320|Z57¬† Manual Move|Replenishment|
|240|Z04 General Wave Replen|Replenishment|
|320|Z99 Manual Move|Replenishment|
|130|Z99¬† Gen Pool Replen|Replenishment|
|330|Z57¬† Pool Replen|Replenishment|
|130|Z07 Flamm Pool Replen|Replenishment|
|120|Z51¬† Manual Move|Replenishment|
|330|Z08 Gen Pool Replen|Replenishment|
|130|General Nested Putaway|Putaway|
|140|Z57¬† Manual Replen|Replenishment|
|140|Z10 Gen Manual Replen|Replenishment|
|330|Z12 Gen Pool Replen|Replenishment|
|140|Z13 Manual Move|Replenishment|
|120|Z06 Frag Manual Move|Replenishment|
|120|Z10 Cage Pool Replen|Replenishment|
|130|Z11 Cage Pool Replen|Replenishment|
|140|Z77¬† Cage Manual Move|Replenishment|
|330|Z06 General Wave Replen|Replenishment|
|140|Z07 Cage Wave Replen|Replenishment|
|140|Z07 Flamm Manual Move|Replenishment|
|460|Loading|Dock Management|
|120|Z04 General Manual Replen|Replenishment|
|320|Z08 Ethical Manual Replen|Replenishment|
|100|Z20¬† Pool Replen|Replenishment|
|120|Z04 General Wave Replen|Replenishment|
|260|Receipt|NULL|
|130|Z12 Gen Wave Replen|Replenishment|
|130|Z04 General Manual Replen|Replenishment|
|220|Frag Case Pick|Order Pick|
|320|Z07 Gen Wave Replen|Replenishment|
|320|General Tote Pick|Order Pick|
|130|Z06 Cage Manual Replen|Replenishment|
|130|Z66¬† Manual Move|Replenishment|
|120|Z06 Cage Manual Replen|Replenishment|
|330|Z51¬† WCS Manual Replen|Replenishment|
|320|General Putaway|Putaway|
|320|Z57¬† Pool Replen|Replenishment|
|330|Loading|Dock Management|
|120|Z09 Ethical Pool Replen|Replenishment|
|330|Z70 Gen Manual Replen|Replenishment|
|140|Z06 Cage Manual Move|Replenishment|
|330|Z05 General Wave Replen|Replenishment|
|130|Z07 Gen Manual Replen|Replenishment|
|120|Z09 Cage Manual Replen|Replenishment|
|320|Z07 Manual Move|Replenishment|
|130|Z11 Cage Manual Replen|Replenishment|
|140|Z10 Gen Wave Replen|Replenishment|
|130|Z12 Gen Manual Replen|Replenishment|
|140|Z08 Gen Wave Replen|Replenishment|
|320|Z03 General Wave Replen|Replenishment|
|320|Z20¬† Manual Move|Replenishment|
|140|Cage Case Pick|Order Pick|
|120|Z77¬† Manual Replen|Replenishment|
|320|Z21¬† Manual Move|Replenishment|
|130|Z57¬† Manual Move|Replenishment|
|330|Z03 Gen Pool Replen|Replenishment|
|80|NULL|NULL|
|320|Z11 Manual Move|Replenishment|
|370|NULL|NULL|
|320|Z22 Manual Move|Replenishment|
|130|Fragrance Nested Putaway|Putaway|
|130|Z06 Frag Manual Move|Replenishment|
|240|Ethical Tote Pick|Order Pick|
|330|Z75¬† Pool Replen|Replenishment|
|120|Z12 Manual Move|Replenishment|
|120|Ethical Nested Putaway|Putaway|
|120|Z07 Gen Pool Replen|Replenishment|
|320|Z06 Frag Manual Replen|Replenishment|
|100|Z21¬† Manual Move|Replenishment|
|240|Z09 Gen Manual Replen|Replenishment|
|130|Z12 Manual Move|Replenishment|
|320|Flamm Tote Pick|Order Pick|
|320|Z66¬† Pool Replen|Replenishment|
|140|Ethical Putaway|Putaway|
|120|Z05 Gen Pool Replen|Replenishment|
|320|Z05 General Manual Replen|Replenishment|
|120|Z08 Gen Manual Replen|Replenishment|
|120|Z99 Gen Manual Replen|Replenishment|
|330|Z10 Cage Manual Replen|Replenishment|
|130|Z20¬† Wave Replen|Replenishment|
|330|Z01 Manual Move|Replenishment|
|120|Z51 Wave Replen|Replenishment|
|130|Z77 Cage Pool Replen|Replenishment|
|320|Z10 Gen Pool Replen|Replenishment|
|320|Z20¬† Manual Replen|Replenishment|
|330|General Manual Replen|Replenishment|
|320|Z02 General Wave Replen|Replenishment|
|140|Z02 Manual Move|Replenishment|
|240|Standard Tote Pick|Order Pick|
|330|Z01 General Manual Replen|Replenishment|
|130|Z08 Ethical Pool Replen|Replenishment|
|320|Z07 Flamm Manual Move|Replenishment|
|130|Z07 Mezz201 Pool Replen|Replenishment|
|140|Z07 Gen Manual Replen|Replenishment|
|330|Z08 Gen Manual Replen|Replenishment|
|410|Dock Management|NULL|
|460|Staging|Packing|
|120|Z02 Manual Move|Replenishment|
|100|Z57¬† Manual Move|Replenishment|
|330|Z07 Gen Wave Replen|Replenishment|
|320|Z10 Manual Move|Replenishment|
|130|Cage Putaway|Putaway|
|320|Z51 Wave Replen|Replenishment|
|140|Flammable Putaway|Putaway|
|100|Z08 Gen Pool Replen|Replenishment|
|330|Z66¬† Wave Replen|Replenishment|
|100|Z10 Gen Manual Replen|Replenishment|
|130|Z77¬† Manual Replen|Replenishment|
|130|Z07 Flamm Manual Replen|Replenishment|
|320|Z06 Manual Move|Replenishment|
|330|Z02 General Wave Replen|Replenishment|
|140|Z10 Cage Pool Replen|Replenishment|
|140|Z12 Gen Manual Replen|Replenishment|
|140|Z04 General Wave Replen|Replenishment|
|320|Z21¬† Manual Replen|Replenishment|
|120|Z51¬† Manual Replen|Replenishment|
|320|Z13 Cage Pool Replen|Replenishment|
|130|Z51¬† WCS Manual Move|Replenishment|
|330|Z12 Cage Manual Replen|Replenishment|
|330|Z51¬† Gen Pool Replen|Replenishment|
|330|Z06 Frag Pool Replen|Replenishment|
|330|Z05 Gen Pool Replen|Replenishment|
|320|Z66¬† Manual Replen|Replenishment|
|120|Cage Case Pick|Order Pick|
|240|Frag Case Pick|Order Pick|
|130|Z01 Gen Pool Replen|Replenishment|
|140|Z66¬† Manual Move|Replenishment|
|330|Z09 Gen Pool Replen|Replenishment|
|330|Z77¬† Cage Manual Replen|Replenishment|
|320|Z04 General Wave Replen|Replenishment|
|320|Activity Count|Cycle Counting|
|120|Z75¬† Pool Replen|Replenishment|
|120|Z75¬† Manual Replen|Replenishment|
|140|Z07 Gen Pool Replen|Replenishment|
|320|Z09 Gen Pool Replen|Replenishment|
|130|Z04 Gen Pool Replen|Replenishment|
|330|Z99 Gen Manual Replen|Replenishment|
|330|Z20¬† Pool Replen|Replenishment|
|140|Z07 Mezz201 Pool Replen|Replenishment|
|330|Z51 Wave Replen|Replenishment|
|320|Z70 Gen Manual Replen|Replenishment|
|140|Z09 Ethical Pool Replen|Replenishment|
|130|Z10 Cage Wave Replen|Replenishment|
|140|Z11 Cage Wave Replen|Replenishment|
|130|Flammable Putaway|Putaway|
|130|Z70 Gen Wave Replen|Replenishment|
|130|Z10 Gen Wave Replen|Replenishment|
|240|Z02 General Manual Replen|Replenishment|
|450|NULL|NULL|
|330|Z13 Cage Wave Replen|Replenishment|
|330|Flamm Case Pick|Order Pick|
|140|Z21¬† Manual Move|Replenishment|
|320|Z77¬† Pool Replen|Replenishment|
|100|Z02 General Manual Replen|Replenishment|
|140|Z09 Ethical Manual Replen|Replenishment|
|320|Z09 Manual Move|Replenishment|
|320|Z01 General Wave Replen|Replenishment|
|130|Cage Case Pick|Order Pick|
|130|Manual Move|Replenishment|
|320|Z70 Manual Move|Replenishment|
|330|Z12 Cage Pool Replen|Replenishment|
|140|Z06 Cage Manual Replen|Replenishment|
|320|Z06 General Manual Replen|Replenishment|
|320|Z12 Manual Move|Replenishment|
|140|Z07 Cage Pool Replen|Replenishment|
|320|Z77¬† Manual Move|Replenishment|
|330|Z03 Manual Move|Replenishment|
|140|Z07 Flamm Pool Replen|Replenishment|
|130|Z09 Cage Pool Replen|Replenishment|
|130|Ethical Case Pick|Order Pick|
|330|General Wave Replen|Replenishment|
|140|Z75¬† Wave Replen|Replenishment|
|70|Packing|NULL|
|120|Z06 Cage Wave Replen|Replenishment|
|320|Z99 Gen Manual Replen|Replenishment|
|120|Z05 Manual Move|Replenishment|
|320|Z51¬† Manual Move|Replenishment|
|140|Z77¬† Manual Move|Replenishment|
|140|Cage Nested Putaway|Putaway|
|220|Ethical Case Pick|Order Pick|
|320|Cage Tote Pick|Order Pick|
|100|Z07 Cage Wave Replen|Replenishment|
|330|Ethical Nested Putaway|Putaway|
|140|Z22 General Manual Replen|Replenishment|
|140|Z08 Manual Move|Replenishment|
|130|Z07 Cage Manual Replen|Replenishment|
|130|Z07 Gen Wave Replen|Replenishment|
|140|Z06 Frag Wave Replen|Replenishment|
|120|General Nested Putaway|Putaway|
|360|NULL|NULL|
|330|Z11 Manual Move|Replenishment|
|130|Z09 Ethical Pool Replen|Replenishment|
|140|Z05 Manual Move|Replenishment|
|330|Staging|Packing|
|130|Z03 General Manual Replen|Replenishment|
|330|Z06 Frag Wave Replen|Replenishment|
|130|Z07 Cage Pool Replen|Replenishment|
|50|NULL|NULL|
|140|Z08 Ethical Manual Replen|Replenishment|
|120|Frag Tote Pick|Order Pick|
|100|Z03 Gen Pool Replen|Replenishment|
|100|Z12 Manual Move|Replenishment|
|90|NULL|NULL|
|330|Z77¬† Pool Replen|Replenishment|
|140|Z10 Manual Move|Replenishment|
|130|Z09 Ethical Manual Replen|Replenishment|
|290|Replenishment|NULL|
|130|Z05 General Manual Replen|Replenishment|
|320|Z08 Ethical Pool Replen|Replenishment|
|120|Z07 Gen Wave Replen|Replenishment|
|320|Z77¬† Wave Replen|Replenishment|
|330|Z07 Mezz201 Manual Replen|Replenishment|
|130|Z77¬† Cage Manual Replen|Replenishment|
|320|General Wave Replen|Replenishment|
|130|Z06 Cage Manual Move|Replenishment|
|140|General Putaway|Putaway|
|320|Cycle Counting|Cycle Counting|
|130|Z51¬† Gen Pool Replen|Replenishment|
|130|Ethical Tote Pick|Order Pick|
|120|Flamm Tote Pick|Order Pick|
|20|NULL|NULL|
|320|Z08 Gen Manual Replen|Replenishment|
|320|Z22 General Manual Replen|Replenishment|
|120|Z99 Gen Wave Replen|Replenishment|
|130|General Tote Pick|Order Pick|
|120|Z66¬† Pool Replen|Replenishment|
|320|Z75¬† Pool Replen|Replenishment|
|320|Z06 General Wave Replen|Replenishment|
|320|Z01 General Manual Replen|Replenishment|
|320|Z08 Gen Wave Replen|Replenishment|
|120|General Tote Pick|Order Pick|
|330|Z06 General Manual Replen|Replenishment|
|130|Fragrance Putaway|Putaway|
|140|Z06 Cage Wave Replen|Replenishment|
|130|Z05 Manual Move|Replenishment|
|140|Z01 Gen Pool Replen|Replenishment|
|140|Z99¬† Gen Pool Replen|Replenishment|
|130|Z03 Manual Move|Replenishment|
|330|Z10 Cage Wave Replen|Replenishment|
|320|Standard Tote Pick|Order Pick|
|330|Z06 Gen Pool Replen|Replenishment|
|330|Z07 Cage Manual Replen|Replenishment|
|320|Z07 Cage Pool Replen|Replenishment|
|330|Z10 Cage Pool Replen|Replenishment|
|330|Z57¬† Manual Replen|Replenishment|
|120|Z05 General Manual Replen|Replenishment|
|120|Z11 Cage Wave Replen|Replenishment|
|340|NULL|NULL|
|130|Z08 Ethical Manual Replen|Replenishment|
|120|Z02 General Manual Replen|Replenishment|
|320|Z06 Gen Pool Replen|Replenishment|
|140|Z08 Ethical Manual Move|Replenishment|
|140|Z11 Cage Manual Replen|Replenishment|
|130|Z66¬† Manual Replen|Replenishment|
|240|Ethical Case Pick|Order Pick|
|120|Z06 Manual Move|Replenishment|
|140|Z99 Gen Manual Replen|Replenishment|
|320|Z06 Cage Manual Replen|Replenishment|
|100|Z05 General Wave Replen|Replenishment|
|120|Z11 Manual Move|Replenishment|
|130|General Putaway|Putaway|
|140|Z70 Gen Manual Replen|Replenishment|
|140|Z22 Manual Move|Replenishment|
|330|Z77¬† Cage Manual Move|Replenishment|
|120|Z77¬† Manual Move|Replenishment|
|120|Z09 Ethical Manual Replen|Replenishment|
|120|Z21¬† Manual Move|Replenishment|
|120|Z20¬† Manual Move|Replenishment|
|130|Z20¬† Manual Move|Replenishment|
|140|Z12 Cage Pool Replen|Replenishment|
|330|Z09 Manual Move|Replenishment|
|130|Z06 Frag Manual Replen|Replenishment|
|210|Packing|Packing|
|320|Z01 Manual Move|Replenishment|
|100|Z07 Gen Manual Replen|Replenishment|
|330|Z08 Gen Wave Replen|Replenishment|
|320|Z09 Cage Wave Replen|Replenishment|
|330|Z08 Manual Move|Replenishment|
|130|Z22 Manual Move|Replenishment|
|320|Z04 General Manual Replen|Replenishment|
|330|Z07 Manual Move|Replenishment|
|130|Z99 Manual Move|Replenishment|
|320|Z11 Cage Manual Replen|Replenishment|
|130|Z12 Cage Manual Replen|Replenishment|
|130|Z02 Manual Move|Replenishment|
|140|Z57¬† Manual Move|Replenishment|
|320|Z07 Gen Pool Replen|Replenishment|
|320|Z75¬† Wave Replen|Replenishment|
|120|General Case Pick|Order Pick|
|120|Loading|Dock Management|
|320|Z02 Gen Pool Replen|Replenishment|
|130|Z20¬† Manual Replen|Replenishment|
|330|Flammable Putaway|Putaway|
|100|Z01 General Manual Replen|Replenishment|
|120|Z08 Manual Move|Replenishment|
|220|General Case Pick|Order Pick|
|140|Z77 Cage Pool Replen|Replenishment|
|130|Z06 General Manual Replen|Replenishment|
|100|Z07 Gen Pool Replen|Replenishment|
|100|Z01 Manual Move|Replenishment|
|120|Z01 Gen Pool Replen|Replenishment|
|140|Z20¬† Wave Replen|Replenishment|
|100|Z22 General Manual Replen|Replenishment|
|120|Z22 Manual Move|Replenishment|
|130|Cage Nested Putaway|Putaway|
|120|Z06 General Manual Replen|Replenishment|
|130|Z08 Gen Wave Replen|Replenishment|
|330|Activity Count|Cycle Counting|
|330|Z13 Cage Manual Replen|Replenishment|
|120|Z03 Manual Move|Replenishment|
|100|Z08 Gen Manual Replen|Replenishment|
|330|Fragrance Putaway|Putaway|
|120|Z66¬† Manual Replen|Replenishment|
|320|Staging|Packing|
|130|Z09 Gen Pool Replen|Replenishment|
|140|Z21¬† Manual Replen|Replenishment|
|120|Z10 Manual Move|Replenishment|
|190|NULL|NULL|
|120|Manual Move|Replenishment|
|320|Z09 Gen Manual Replen|Replenishment|
|140|Z13 Cage Pool Replen|Replenishment|
|330|Z05 General Manual Replen|Replenishment|
|130|Z06 Gen Pool Replen|Replenishment|
|220|Cage Tote Pick|Order Pick|
|120|Z20¬† Manual Replen|Replenishment|
|130|Z08 Manual Move|Replenishment|
|120|Cage Putaway|Putaway|
|330|Z07 Cage Wave Replen|Replenishment|
|330|Z07 Flamm Wave Replen|Replenishment|
|330|Ethical Tote Pick|Order Pick|
|130|Z07 Flamm Manual Move|Replenishment|
|100|Z12 Gen Pool Replen|Replenishment|
|330|Z05 Manual Move|Replenishment|
|270|Packing|Packing|
|120|Z06 Cage Pool Replen|Replenishment|
|130|Z10 Gen Pool Replen|Replenishment|
|320|Z09 Ethical Pool Replen|Replenishment|
|130|Z06 Cage Pool Replen|Replenishment|
|320|Ethical Nested Putaway|Putaway|
|140|Z09 Cage Manual Replen|Replenishment|
|40|NULL|NULL|
|330|Z51¬† WCS Manual Move|Replenishment|
|140|Z01 General Wave Replen|Replenishment|
|320|General Manual Replen|Replenishment|
|320|Z06 Frag Manual Move|Replenishment|
|140|Z66¬† Wave Replen|Replenishment|
|140|Ethical Case Pick|Order Pick|
|330|Z75¬† Manual Move|Replenishment|
|140|Z70 Manual Move|Replenishment|
|320|Z09 Gen Wave Replen|Replenishment|
|100|Z04 General Manual Replen|Replenishment|
|140|Z12 Gen Wave Replen|Replenishment|
|320|Z75¬† Manual Move|Replenishment|
|120|Z02 General Wave Replen|Replenishment|
|130|Z21¬† Manual Move|Replenishment|
|120|Z07 Manual Move|Replenishment|
|140|Z51¬† Manual Replen|Replenishment|
|320|Z66¬† Manual Move|Replenishment|
|140|Fragrance Nested Putaway|Putaway|
|320|Z07 Mezz201 Pool Replen|Replenishment|
|320|Fragrance Nested Putaway|Putaway|
|140|Z06 Manual Move|Replenishment|
|320|Z03 General Manual Replen|Replenishment|
|140|Z03 General Manual Replen|Replenishment|
|330|Z09 Gen Manual Replen|Replenishment|
|130|Z57¬† Manual Replen|Replenishment|
|120|Z07 Flamm Manual Replen|Replenishment|
|130|Z10 Gen Manual Replen|Replenishment|
|320|Z07 Cage Wave Replen|Replenishment|
|320|Ethical Tote Pick|Order Pick|
|130|Z20¬† Pool Replen|Replenishment|
|320|General Nested Putaway|Putaway|
|330|Z04 General Wave Replen|Replenishment|
|330|Z09 Cage Pool Replen|Replenishment|
|120|Z07 Cage Manual Replen|Replenishment|
|330|Z09 Ethical Manual Replen|Replenishment|
|120|Z06 Cage Manual Move|Replenishment|
|140|Z04 Gen Pool Replen|Replenishment|
|120|Z77¬† Pool Replen|Replenishment|
|130|Z13 Cage Wave Replen|Replenishment|
|330|Cage Tote Pick|Order Pick|
|320|Z03 Gen Pool Replen|Replenishment|
|240|Z04 General Manual Replen|Replenishment|
|320|Z12 Gen Manual Replen|Replenishment|
|100|Z75¬† Pool Replen|Replenishment|
|220|Frag Tote Pick|Order Pick|
|320|Z75¬† Manual Replen|Replenishment|
|130|Z07 Manual Move|Replenishment|
|320|Z09 Cage Pool Replen|Replenishment|
|320|Z20¬† Wave Replen|Replenishment|
|120|Ethical Putaway|Putaway|
|45|NULL|NULL|
|130|Z04 Manual Move|Replenishment|
|140|Z08 Gen Pool Replen|Replenishment|
|140|Z51¬† Manual Move|Replenishment|
|320|Z07 Flamm Pool Replen|Replenishment|
|320|Z06 Frag Wave Replen|Replenishment|
|240|Z01 General Manual Replen|Replenishment|
|140|Z77¬† Manual Replen|Replenishment|
|140|Z10 Cage Manual Replen|Replenishment|
|220|Packing|Packing|
|130|Z13 Manual Move|Replenishment|
|140|Z11 Cage Pool Replen|Replenishment|
|130|Ethical Putaway|Putaway|
|330|Standard Tote Pick|Order Pick|
|320|Z70 Gen Pool Replen|Replenishment|
|120|Z99 Manual Move|Replenishment|
|320|Z10 Cage Pool Replen|Replenishment|
|100|Z75¬† Manual Replen|Replenishment|
|330|Z06 Cage Pool Replen|Replenishment|
|320|Z12 Gen Pool Replen|Replenishment|
|100|Z03 General Manual Replen|Replenishment|
|100|Z12 Gen Manual Replen|Replenishment|
|130|Z12 Cage Pool Replen|Replenishment|
|220|NULL|NULL|
|330|Z66¬† Manual Replen|Replenishment|
|130|Z03 General Wave Replen|Replenishment|
|330|Z66¬† Manual Move|Replenishment|
|100|Z01 Gen Pool Replen|Replenishment|
|330|Frag Tote Pick|Order Pick|
|130|Z09 Cage Wave Replen|Replenishment|
|120|Z11 Cage Manual Replen|Replenishment|
|320|Z51¬† Manual Replen|Replenishment|
|120|Cage Tote Pick|Order Pick|
|120|Z08 Ethical Manual Replen|Replenishment|
|120|Z06 Gen Pool Replen|Replenishment|
|130|Cage Tote Pick|Order Pick|
|330|Z07 Flamm Manual Move|Replenishment|
|140|Z09 Cage Pool Replen|Replenishment|
|140|Z06 Frag Manual Move|Replenishment|
|330|Z07 Mezz201 Pool Replen|Replenishment|
|370|General Tote Pick|NULL|
|240|Cage Case Pick|Order Pick|
|320|Z07 Flamm Wave Replen|Replenishment|
|320|Z10 Gen Wave Replen|Replenishment|
|130|Z05 General Wave Replen|Replenishment|
|320|Flammable Putaway|Putaway|
|130|Z07 Cage Wave Replen|Replenishment|
|240|Z03 General Manual Replen|Replenishment|
|330|Z77¬† Manual Move|Replenishment|
|120|Standard Tote Pick|Order Pick|
|330|Z21¬† Manual Move|Replenishment|
|330|Z12 Gen Manual Replen|Replenishment|
|330|Z21¬† Manual Replen|Replenishment|
|330|Z01 General Wave Replen|Replenishment|
|120|Z11 Cage Pool Replen|Replenishment|
|100|Z03 Manual Move|Replenishment|
|140|General Manual Replen|Replenishment|
|140|Z07 Gen Wave Replen|Replenishment|
|320|Z04 Manual Move|Replenishment|
|330|Cage Nested Putaway|Putaway|
|165|NULL|NULL|
|140|Z77¬† Cage Manual Replen|Replenishment|
|120|Z70 Gen Manual Replen|Replenishment|
|320|Z06 Frag Pool Replen|Replenishment|
|330|Z09 Gen Wave Replen|Replenishment|
|130|Z75¬† Manual Move|Replenishment|
|330|General Case Pick|Order Pick|
|320|Z09 Cage Manual Replen|Replenishment|
|130|Z70 Gen Pool Replen|Replenishment|
|330|Z11 Cage Wave Replen|Replenishment|
|140|Z06 Frag Manual Replen|Replenishment|
|140|Z12 Manual Move|Replenishment|
|330|Z04 Manual Move|Replenishment|
|320|Z66¬† Wave Replen|Replenishment|
|330|Z99 Gen Wave Replen|Replenishment|
|330|Z66¬† Pool Replen|Replenishment|
|330|Z06 Manual Move|Replenishment|
|120|Z10 Gen Manual Replen|Replenishment|
|100|Z05 Gen Pool Replen|Replenishment|
|120|Z57¬† Manual Move|Replenishment|
|320|Z11 Cage Wave Replen|Replenishment|
|320|Z51¬† WCS Manual Move|Replenishment|
|330|Z02 General Manual Replen|Replenishment|
|140|Z03 Gen Pool Replen|Replenishment|
|120|General Putaway|Putaway|
|320|Z13 Cage Manual Replen|Replenishment|
|320|Z99 Gen Wave Replen|Replenishment|
|130|Z04 General Wave Replen|Replenishment|
|130|Z09 Gen Manual Replen|Replenishment|
|130|Z10 Manual Move|Replenishment|
|240|Z03 General Wave Replen|Replenishment|
|140|Z51 Wave Replen|Replenishment|
|100|Z22 Manual Move|Replenishment|
|100|Z02 Manual Move|Replenishment|
|140|Z03 General Wave Replen|Replenishment|
|330|Z75¬† Wave Replen|Replenishment|
|130|Z77¬† Wave Replen|Replenishment|
|330|Z07 Flamm Pool Replen|Replenishment|
|330|Z51¬† Manual Replen|Replenishment|
|130|Z08 Ethical Manual Move|Replenishment|
|140|Z51¬† Gen Pool Replen|Replenishment|
|120|Z70 Gen Pool Replen|Replenishment|
|330|Z20¬† Wave Replen|Replenishment|
|100|Z77¬† Manual Replen|Replenishment|
|330|Z75¬† Manual Replen|Replenishment|
|45|Cycle Counting|Cycle Counting|
|140|Z12 Cage Manual Replen|Replenishment|
|330|Z07 Gen Pool Replen|Replenishment|
|140|Z11 Manual Move|Replenishment|
|140|General Nested Putaway|Putaway|
|120|Z08 Gen Pool Replen|Replenishment|
|330|Z70 Manual Move|Replenishment|
|330|Z10 Gen Pool Replen|Replenishment|
|140|Z51¬† WCS Manual Move|Replenishment|
|330|Z20¬† Manual Replen|Replenishment|
|330|Z02 Gen Pool Replen|Replenishment|
|120|Frag Case Pick|Order Pick|
|140|Z03 Manual Move|Replenishment|
|330|Z12 Manual Move|Replenishment|
|130|Z01 Manual Move|Replenishment|
|140|Z70 Gen Pool Replen|Replenishment|
|130|Z03 Gen Pool Replen|Replenishment|
|330|Ethical Case Pick|Order Pick|
|140|Z77¬† Pool Replen|Replenishment|
|130|Z01 General Manual Replen|Replenishment|
|130|Z75¬† Manual Replen|Replenishment|
|320|Z08 Ethical Manual Move|Replenishment|
|130|Z70 Manual Move|Replenishment|
|320|Z13 Cage Wave Replen|Replenishment|
|100|Z10 Gen Pool Replen|Replenishment|
|60|NULL|NULL|
|100|Z05 Manual Move|Replenishment|
|100|Z05 General Manual Replen|Replenishment|
|240|Cage Tote Pick|Order Pick|
|100|Z66¬† Manual Replen|Replenishment|
|240|Flamm Tote Pick|Order Pick|
|100|Z02 General Wave Replen|Replenishment|
|120|Z77¬† Cage Manual Move|Replenishment|
|330|Z08 Ethical Pool Replen|Replenishment|
|100|Z07 Manual Move|Replenishment|
|330|Z04 General Manual Replen|Replenishment|
|130|Z06 Cage Wave Replen|Replenishment|
|130|Z51¬† WCS Manual Replen|Replenishment|
|200|Shipment|NULL|
|100|Z20¬† Manual Replen|Replenishment|
|140|Z04 Manual Move|Replenishment|
|320|Z09 Ethical Manual Replen|Replenishment|
|120|Z09 Gen Wave Replen|Replenishment|
|130|Z70 Gen Manual Replen|Replenishment|
|320|General Case Pick|Order Pick|
|330|General Nested Putaway|Putaway|
|130|Z12 Gen Pool Replen|Replenishment|
|100|Z03 General Wave Replen|Replenishment|
|320|Cage Putaway|Putaway|
|140|Fragrance Putaway|Putaway|
|320|Frag Tote Pick|Order Pick|
|130|Z06 Frag Wave Replen|Replenishment|
|130|General Wave Replen|Replenishment|
|140|Z06 Frag Pool Replen|Replenishment|
|130|Z99 Gen Manual Replen|Replenishment|
|120|General Wave Replen|Replenishment|
|320|Ethical Case Pick|Order Pick|
|130|Z51¬† Manual Replen|Replenishment|
|120|Z09 Cage Pool Replen|Replenishment|
|330|Z09 Cage Manual Replen|Replenishment|
|320|Z20¬† Pool Replen|Replenishment|
|330|Z10 Manual Move|Replenishment|
|130|Z22 General Manual Replen|Replenishment|
|330|Z07 Gen Manual Replen|Replenishment|
|100|Z07 Gen Wave Replen|Replenishment|
|330|Z77¬† Manual Replen|Replenishment|
|130|General Manual Replen|Replenishment|
|140|Z13 Cage Manual Replen|Replenishment|
|140|Z07 Mezz201 Manual Replen|Replenishment|
|320|Z12 Cage Pool Replen|Replenishment|
|110|Packing|Packing|
|100|Z77¬† Pool Replen|Replenishment|
|330|Z11 Cage Pool Replen|Replenishment|
|320|Z51¬† Gen Pool Replen|Replenishment|
|130|Ethical Nested Putaway|Putaway|
|120|Z03 Gen Pool Replen|Replenishment|
|120|Z01 General Manual Replen|Replenishment|
|320|Z03 Manual Move|Replenishment|
|290|Receipt|NULL|
|140|Ethical Tote Pick|Order Pick|
|140|Z07 Flamm Wave Replen|Replenishment|
|240|General Tote Pick|Order Pick|
|120|Z01 Manual Move|Replenishment|
|130|Z75¬† Pool Replen|Replenishment|
|120|Z22 General Manual Replen|Replenishment|
|100|Z07 Cage Manual Replen|Replenishment|
|320|Z12 Cage Manual Replen|Replenishment|
|100|Z06 General Manual Replen|Replenishment|
|170|NULL|NULL|
|320|Z10 Cage Wave Replen|Replenishment|
|330|Manual Move|Replenishment|
|150|NULL|NULL|
|320|Z77¬† Cage Manual Replen|Replenishment|
|330|Z13 Cage Pool Replen|Replenishment|
|260|NULL|NULL|
|130|Z51¬† WCS Manual Move|NULL|
|130|Z06 General Wave Replen|Replenishment|
|140|Z09 Manual Move|Replenishment|
|330|Z51¬† Manual Move|Replenishment|
|100|Z66¬† Pool Replen|Replenishment|
|120|Staging|Packing|
|320|Z06 Cage Pool Replen|Replenishment|
|130|Z09 Cage Manual Replen|Replenishment|
|120|Z12 Gen Pool Replen|Replenishment|
|140|Z09 Cage Wave Replen|Replenishment|
|320|Z04 Gen Pool Replen|Replenishment|
|320|Z05 Manual Move|Replenishment|
|130|Z57¬† Pool Replen|Replenishment|
|140|Z05 General Wave Replen|Replenishment|
|200|NULL|NULL|
|320|Z57¬† Manual Replen|Replenishment|
|140|Cage Tote Pick|Order Pick|
|330|Z12 Gen Wave Replen|Replenishment|
|120|Z66¬† Manual Move|Replenishment|
|320|Z12 Gen Wave Replen|Replenishment|
|140|Z66¬† Manual Replen|Replenishment|
|140|Z02 Gen Pool Replen|Replenishment|
|330|Z06 Frag Manual Move|Replenishment|
|100|Z08 Manual Move|Replenishment|
|330|Z04 Gen Pool Replen|Replenishment|
|330|Z09 Ethical Pool Replen|Replenishment|
|320|Cage Nested Putaway|Putaway|
|140|Z09 Gen Pool Replen|Replenishment|
|140|Z06 Gen Pool Replen|Replenishment|
|330|Cycle Counting|Cycle Counting|
|130|Z08 Gen Pool Replen|Replenishment|
|330|Ethical Putaway|Putaway|
|180|NULL|NULL|
|140|Z05 Gen Pool Replen|Replenishment|
|130|Z01 General Wave Replen|Replenishment|
|130|Z99 Gen Wave Replen|Replenishment|
|130|Z51¬† Manual Move|Replenishment|
|330|Z08 Ethical Manual Move|Replenishment|
|120|Z09 Manual Move|Replenishment|
|330|Z99 Manual Move|Replenishment|
|120|Z01 General Wave Replen|Replenishment|
|140|Z08 Ethical Pool Replen|Replenishment|
|120|Z03 General Manual Replen|Replenishment|
|130|Z10 Cage Pool Replen|Replenishment|
|240|General Wave Replen|Replenishment|
|330|Z09 Cage Wave Replen|Replenishment|
|320|Z99¬† Gen Pool Replen|Replenishment|
|100|Z04 Manual Move|Replenishment|
|140|Z08 Gen Manual Replen|Replenishment|
|130|Z77¬† Manual Move|Replenishment|
|130|Z09 Manual Move|Replenishment|
|120|Z75¬† Manual Move|Replenishment|
|140|Z02 General Wave Replen|Replenishment|
|140|Z51¬† WCS Manual Replen|NULL|
|320|Z06 Cage Wave Replen|Replenishment|
|330|Flamm Tote Pick|Order Pick|
|330|General Tote Pick|Order Pick|
|120|Z21¬† Manual Replen|Replenishment|
|330|General Putaway|Putaway|
|320|Fragrance Putaway|Putaway|
|320|Z07 Cage Manual Replen|Replenishment|
|100|Z08 Gen Wave Replen|Replenishment|
|410|NULL|NULL|
|320|Loading|Dock Management|
|130|Z75¬† Wave Replen|Replenishment|
|320|Manual Move|Replenishment|
|130|Z02 Gen Pool Replen|Replenishment|
|130|Z77¬† Pool Replen|Replenishment|
|130|Z77¬† Cage Manual Move|Replenishment|
|120|Z10 Gen Pool Replen|Replenishment|
|130|Z11 Cage Wave Replen|Replenishment|
|320|Z77¬† Manual Replen|Replenishment|
|130|Z66¬† Pool Replen|Replenishment|
|120|Z12 Gen Manual Replen|Replenishment|
|140|Z51¬† WCS Manual Replen|Replenishment|
|320|Frag Case Pick|Order Pick|
|320|Flamm Case Pick|Order Pick|
|320|Z07 Mezz201 Manual Replen|Replenishment|
|120|Z09 Gen Manual Replen|Replenishment|
|130|Z09 Gen Wave Replen|Replenishment|
|120|Z70 Manual Move|Replenishment|
|140|Z10 Cage Wave Replen|Replenishment|
|330|Z02 Manual Move|Replenishment|
|120|Z02 Gen Pool Replen|Replenishment|
|330|Z70 Gen Pool Replen|Replenishment|
|140|General Wave Replen|Replenishment|
|330|Z77¬† Wave Replen|Replenishment|
|130|Z21¬† Manual Replen|Replenishment|
|130|Z13 Cage Pool Replen|Replenishment|
|140|Z06 Cage Pool Replen|Replenishment|
|120|Cage Nested Putaway|Putaway|
|140|Z20¬† Pool Replen|Replenishment|
|320|Z08 Manual Move|Replenishment|
|320|Z10 Cage Manual Replen|Replenishment|
|140|Z13 Cage Wave Replen|Replenishment|
|100|Z57¬† Manual Replen|Replenishment|
|330|Z06 Cage Wave Replen|Replenishment|
|320|Z07 Gen Manual Replen|Replenishment|
|330|Z06 Cage Manual Replen|Replenishment|
|330|Z99¬† Gen Pool Replen|Replenishment|
|330|Z08 Ethical Manual Replen|Replenishment|
|120|Z57¬† Pool Replen|Replenishment|
|330|Z20¬† Manual Move|Replenishment|
|120|Ethical Tote Pick|Order Pick|
|140|Manual Move|Replenishment|
|140|Z70 Gen Wave Replen|Replenishment|
|130|Z07 Gen Pool Replen|Replenishment|
|330|Z22 General Manual Replen|Replenishment|
|140|Z04 General Manual Replen|Replenishment|
|320|Z01 Gen Pool Replen|Replenishment|
|140|Z01 Manual Move|Replenishment|
|140|Z07 Flamm Manual Replen|Replenishment|
|140|Z06 General Manual Replen|Replenishment|
|130|Z06 Manual Move|Replenishment|
|330|Frag Case Pick|Order Pick|
|140|Cage Putaway|Putaway|
|100|Z04 Gen Pool Replen|Replenishment|
|320|Z02 Manual Move|Replenishment|
|320|Ethical Putaway|Putaway|
|130|Z02 General Manual Replen|Replenishment|
|330|Z07 Cage Pool Replen|Replenishment|
|220|Standard Tote Pick|Order Pick|
|220|General Tote Pick|Order Pick|
|320|Cage Case Pick|Order Pick|
|320|Z05 Gen Pool Replen|Replenishment|
|140|Ethical Nested Putaway|Putaway|
|240|Z99 Gen Manual Replen|Replenishment|

|The Diffrent Transaction Type will help us differciante between the type of transactions for it¬†|
|By filtering out the¬† '-scale%'¬† location we will remove the the duplicated processes¬†|"
CSCI-511,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,03/Oct/25 5:40 PM,14/Oct/25 10:00 AM,,MergeCo Supply Chain Reporting - Create Availability Historical Table from Network Drive File,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-510,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,03/Oct/25 5:40 PM,16/Oct/25 9:52 AM,,MergeCo Supply Chain Reporting - Create ADF Pipeline to refresh Availability Network Drive file daily,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-509,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,03/Oct/25 5:39 PM,09/Oct/25 10:06 AM,,MergeCo Supply Chain Reporting - Schedule ADF Pipelines to refresh daily,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",Pipelines are created - just awaiting on approval for the PR by @user
CSCI-507,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,03/Oct/25 5:34 PM,07/Oct/25 9:59 AM,,MergeCo Supply Chain Reporting - Re-Engineer YTD Availability Metric,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-506,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,02/Oct/25 9:56 AM,25/Oct/25 4:13 AM,,CI/CD documentation - document CI/CD repos in documentation,"Azure DevOps Linked tickets:

* [Feature 209127 Document Environment IAC|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/209127]
* [User Story 210948 Create ADF Documentation|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/210948]

h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Can you please share doc link thx

Documents created:

* Infrastructure
** [Architecture - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5277/Architecture]
** [Configuration - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5279/Configuration]
** all docs in source control: [docs - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infrastructure?version=GBmaster&path=/docs]
* Snowflake Infrastructure
** [Architecture - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5265/Architecture]
** [Configuration - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5267/Configuration]
** all docs in source control: [docs - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-snowflake-infrastructure?path=/docs]

Documents for edp-data-factory repository are created for review: [docs - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory?version=GBfeature/210950-create-adf-architecture-documentation&path=/docs]

* README
* Architecture
* Configuration
* CI
* Deployment
* Security
* Developer Guide
* Troubleshooting

PR created for ed-data-factory documentation at; [Pull request 22988: Created documentation for edp-adf - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22988?path=%2Fdocs%2Farchitecture.md]

@user @user can you please review?

Thanks.

@user @user @user can you please revie this PR [Pull request 22988: Created documentation for edp-adf - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22988]. Thanks.

cc: @user

Hi Eugene, I have scanned through the docs. My only comment is: at the moment we are referencing QA but we will need to go back and add/ update details on SIT/UAT.

@user once we have the tickets for the additional environments, we need to update the docs accordingly as soon as they are configured."
CSCI-504,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Oct/25 11:38 AM,05/Nov/25 3:51 PM,,SCAX2012 test server availability,"The SCAX2012 DB on TDB19b is not available anymore. Adeel has set up access on the TDB16MB test server. The IP of this server is 172.21.10.33. 

The username and password are the same as the old server. 

Next steps: 

* Confirm the same tables and data history on the new server
* To turn off the trigger on SCAX2012 in ADF
* To get the new server whitelisted

h3. 

h3. 

h3.","Given, When, Then","Adeel has confirmed that there are the same tables and data history on the new server. Existing trigger in ADF was also removed.

Now we need to raise a request to get the new servers whitelisted and update ADF linked service to point to it.

@user will help take over from here.

Request has been sent to Anjali to create whitelisting of IPs from ADF. Once done we will check the ingestion process running with the new server."
CSCI-503,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Oct/25 8:34 AM,08/Oct/25 11:35 AM,,"ADF- Generic Dataset, Linked Service POC & Raise PR","h3. Context

* As per Review comments on ADF PR, Eugene suggested to do POC for generic pipeline to take care of all ingestion instead of creating different pipelines for different sources.

h3. Objective

* To come up with a generic pipeline strategy to take care of majority of ingestion.

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* -Create Parameterized LinkedService-
* -Create Parameterized DataSets-
* -Create Generic ADF Pipeline for SQL Server-
* Create Generic ADF Pipeline for MY SQL
* Work On to raise first clean PR to push these generic components

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","@user 
Can you note what @user has got you to try and then provide your approach?

Can update this in description

Initial PR created

Hi @user / @user ,

I have implemented the review comments provided by @user .

I also need to test the concurrent load testing on the shared pipeline. Looks like to do that we first need to publish the pipeline & triggers into master first so that the triggers can be tested to run parallelly.

Hi @user ,

The PR is merged into Master. We are able to trigger multiple instances of the pipeline for various sources at the same time. I will try to do say 5 concurrent loads at the same time to test how it goes and post the results here today.

@user , @user .. FYI as discussed"
CSCI-502,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Sep/25 2:55 PM,10/Oct/25 5:15 PM,,Creation of Service Account to access Network Drive- RITM0176342,"h3. Context

* Creation of Service Account to access Network Drive- RITM0176342

h3. We need a Service Account creation to access the below Network Drive for Ingestion of supply chain data into Snowflake for group level reporting.

Will be retrieving data from this location (Folder path):
 \\cwvault\Everyone\Supply Chain\Administration\Report and Dashboard\ZZ_DataSource

h3. 

h3. 

h3.","Given, When, Then","assigned to Ops teams.

IT SysOps Team is looking into this, I followed up today.

cc- @user @user @user @user

Reached out to a SysOps team member to help as it hadn‚Äôt been picked by anyone from their team.

cc- @user @user @user

Pending ServiceNow respond - WIP

currently WIP cc @user @user

Issue split into:
|CSCI-536|Creation of Service Account to access Network Drive- RITM0176342 Sprint 10|

To continue monitor in sprint 10

Issue split into:
|CSCI-537|Creation of Service Account to access Network Drive- RITM0176342 Sprint 10|"
CSCI-501,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Sep/25 1:13 PM,09/Oct/25 2:05 AM,,ADF Pull Requests - Review and Approve,"Eugene to review and approve the below PRs:

¬†

#1 Ashu: [https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22791|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22791] 

#2 Aswini: [https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22802|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22802] 

#3 Jess: [https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22748|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22748]",PRs approved and merged,"Eugene has provided feedback to the team. 

@user @user are exploring an option to create a generalised data pipelines where we can parameterise the Datasets.

Issue split into:
|CSCI-518|ADF Pull Requests - source config CI/CD setup|"
CSCI-500,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Sep/25 2:03 PM,30/Sep/25 8:52 AM,,ADF Naming standard documentation,Documentation of ADF naming standard.,* Create a Wiki page in azure DevOps,"@user to review it. [Azure Data Factory - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5259/Azure-Data-Factory]

@user Awesome! Thanks Ashu!"
CSCI-499,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Sep/25 9:45 AM,18/Dec/25 11:09 AM,,Snowflake - PowerBI network policy,"h3. Context

* To configure network policies for PowerBI to only allow access from trusted IP addresses

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Hi @user we need to add a network policy to the Snowflake PowerBI user being used for MergeCo reports. Can you pls help me confirm if the PowerBI is accessing Snowflake using the privatelink? and can you provide the IP addresses for this user?

Many thanks!

Chloe

FYI

@user @user @user @user

@user Hi Chloe,

Yes currently the PBI user is connecting with the privatelink ([cw-au.privatelink.snowflakecomputing.com|http://cw-au.privatelink.snowflakecomputing.com]). See below details of the current setup:

We are connecting via the *EDP_ANALYSIS_DEV* warehouse, and using the *S_POWERBI_DEV* user account (basic authentication).

The current VNet Gateway we are connecting with is *cwr-ase-edp-dev-vdgw*:

The virtual network this sits on is *cwr-ase-nonprod-dpdev-vnet*:

And the IP Address under the Subnet for Fabric is *172.29.84.96/27*.

As confirmed by Eugene the IP range below is fixed for Vnet Data Gateway in Dev environment. @user @user @user

We do not need a Snowflake Network Policy when we are using PrivateLink-only.

Once we enable the PrivateLink-only mode via the below scripts:

{noformat}SELECT SYSTEM$ENFORCE_PRIVATELINK_ACCESS_ONLY();{noformat}

Snowflake enforces:

* All traffic must come through Azure PrivateLink
* Public endpoint ‚Üí fully blocked
* TLS termination happens only on the PrivateLink endpoint
* Source traffic will always appear as your private VNet IPs (10.x.x.x)

This alone gives you a network boundary.

So even without any Snowflake Network Policy: 

* Public access is impossible
* Internet access is impossible
* Malicious external traffic cannot reach your account
* Only PrivateLink-connected Azure VNets can reach Snowflake

@user @user @user @user @user @user

I am looking to enable the Snowflake Privatelink-Only mode however @user tested again this morning and is still having issues with connecting to Snowflake's privatelink via Power BI Desktop. Suggesting delaying enabling privatelink-only mode until we get this resolved. 

Pending updates from Raveen and Eugene on MS response for the above issue.

Enabled the Snowflake Privatelink-Only mode today after confirming SSO authentication from PowerBI to Snowflake successful."
CSCI-496,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Sep/25 8:22 AM,10/Oct/25 5:07 PM,,MergeCo Conformed Reporting - Draft Solution Design - Sprint 9,"Create draft of the solution design for the Merge Co Conformed reporting.

Just needs to be a one-page document, outlining *how* we will build and ingest the data for this reporting.

Includes:

* What grain of data required for each metric
* How we will push data for each environment into a join environment
* Merging them into a common dimension",,"Have started populating page:

[https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design]

Issue split into:
|CSCI-532|MergeCo Conformed Reporting - Draft Solution Design - Sprint 10|

To be continue in sprint 10"
CSCI-495,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Sep/25 8:18 AM,06/Oct/25 5:30 PM,,Access Manhattan Active- Whitelisting- RITM0176041 - sprint 9,"h3. Context

This is for Snowflake to access Manhattan Active. They have already been able to action the whitelisting on their end, but we need to enable whitelisting on our firewall also.
We will be querying their database and ingesting into Snowflake via ADF (same pattern as our network drive). Manhattan won't be accessing anything our end - we just need to enable our firewall so that we can query their DB

h3. More Details:

# Port is 3306 (default)¬†
# We are wanting to enable the firewall rule for Azure (so our SHIR - which is hosted in Azure can access Manhattan)
# The data flow is: Manhattan Active --> SHIR + ADF --> Blob Storage --> Snowflake
#* We use ADF as the ETL tool to move data from Manhattan to Blob Storage, and the SHIR is the machine which powers this
# The requirement is to have the ability to move data *from Manhattan* *to our Azure* Environment - that's what we need enabled
#* From here, we then use ADF + SHIR to move the data to Snowflake
# SHIR is on an Azure VM Scale Set - cwr-ase-edp-shir-dev-vmss. It is all in the cloud and is sitting in our private network
# Warehouse Management/Stock Data - no PII

h3. 

h3. 

h3. 

h3.","Given, When, Then","assigned to Cloud team - Brent R - @user

@user - working on this (whitelisting) - ETA today.

Pretty sure this is done üôÇ 
 @user

This has been whitelisted, Team confirmed via testing (screenshot below)

cc- @user @user @user @user

@user I‚Äôll put it as in-review"
CSCI-494,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Sep/25 8:16 AM,30/Sep/25 9:48 AM,,Follow up on - RITM0175340- Azure Blob storage access from office - sprint 9,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Hey @user - i‚Äôll at least need a copy-paste from the original RITM ticket to here. 
Thanks.

with @user @user to test.

The ticket number is RITM0175340 as mentioned in the header of this ticket.

cc- @user"
CSCI-493,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Sep/25 8:09 AM,06/Oct/25 2:08 PM,,Handover From Adeel to next person from DB team - finalisation,"Confluence doc to cover:

* -where things are up to-
** -Modelling-
** -STTM-
* -Parquet extract:-
** -Extract-
** -Upload-
* What we need from this project
* Catchup session",,"@user I‚Äôve uploaded the handover docs in Mergco folder

[MergeCo Data Team - Adeel Handover - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F01%2E%20SC%20Interim%20Data%20Platform%2F03%2E%20Test%20and%20Deploy%2FAdeel%20Handover&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx]

handover with sam complete"
CSCI-492,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Sep/25 8:08 AM,10/Oct/25 3:44 PM,,Review Source-to-Target Map for Fact_Warehouse_Location_Inbound_Transactions - sprint 9,Source to target mapping for Fact_Warehouse_Location_Inbound_Transactions - filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",review complete. But we might have to revist this as we progress to clearly get the business logic in place
CSCI-490,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Sep/25 8:07 AM,10/Oct/25 10:25 AM,,Create Source-to-Target Map for DimBatch,"h3. Context

* There is a need to document the source-to-target mapping for the Dim_Batch table as part of the CW Cloud Data Platform Interim Solution project.

h3. Objective

* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.

h3. Steps

# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document
# For each column in Fact_Warehouse_Location_Packing, fill in:
#* Pres Column name
#* Data type
#* Description
#* Source details (Server, DB, Schema, Table, Column)
#* Indicate if derived (Yes/No)
# Review the completed mapping for accuracy and completeness.

h3. Deliverables

* A fully completed source-to-target mapping sheet for Dim_Batch table, with all required fields filled as per the template.

h3. Assumptions (Optional)

* Access to all necessary source systems and metadata is available.
* The supplied mapping sheet template is up to date and reflects current requirements.

h3.","* Columns as per the supplied sheet are filled:
** Pres Column name
** Data type
** Description
** Source (Server, DB, Schema, Table, Column)
** Derived? (Yes/No)","found the source table 
@user FYI, this mapping is only for on Prem ILS
location: PDB15.ILS.dbo.LOCATION_INVENTORY

{noformat}SELECT Top 100 li.BATCH, li.BATCH_EXPIRY_DATE,li.item, li.warehouse, li.LOCATION, li.RECEIVED_DATE,ON_HAND_QTY,AGING_DATE,INTERNAL_LOCATION_INV,*
from ILS.dbo.item i (nolock)
left join ILS.dbo.LOCATION_INVENTORY li (nolock) on li.item=i.item AND LI.COMPANY =I.COMPANY
where i.COMPANY = 'CW01' --Excluding EPH
AND i.ITEM_CATEGORY10 ='Y' --Batch and Expiry
AND li.LOCATION_TEMPLATE !='MISC'

and batch = 'ZL01Y'

Order by 1 desc {noformat}"
CSCI-489,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 10:53 AM,29/Sep/25 8:18 AM,,Access Manhattan Active- Whitelisting- RITM0176041,"h3. Context

This is for Snowflake to access Manhattan Active. They have already been able to action the whitelisting on their end, but we need to enable whitelisting on our firewall also.
We will be querying their database and ingesting into Snowflake via ADF (same pattern as our network drive). Manhattan won't be accessing anything our end - we just need to enable our firewall so that we can query their DB

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-488,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 10:06 AM,10/Oct/25 9:49 AM,,Capture the requirements for creating the Jumpbox for the developers (data engineers),"Capture the requirements for creating the Jumpbox for the developers (data engineers).

*Key action items*

* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/] - On prem and Cloud Team- *RITM0173487 (for reference)*

What‚Äôs needed is for the DNS forwarding from on-premise DNS servers, to the Azure privatelink DNS. See: [Resolve Azure and on-premises domains. | Microsoft Learn|https://learn.microsoft.com/en-us/azure/dns/private-resolver-hybrid-dns] -CHG0052044 (change request created)

* Collaborate with the EUC Team (CWR) to *create* Jumpbox
* Give the Entra (AD) users access to the Jumpbox 

|*Snowflake_CW_AU_DataEngineers*|DEV_ROLE_DATA_ENG|

* Configure firewalls (whitelisting) (On-Prem, Azure) as follows
* VDI / Jumpbox network ‚Üí Snowflake VNet *(servicenow tickets)*

* *how many jumpboxes* we need? *no. of users 8*
* Note: Ports to be allowed: 443, 80, 1433
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Configure VDI / Jumbox images to pre-install tools listed below: - The list has been reviewed and approved by EUC Team.""}],""attrs"":{""localId"":""a6a01a2f-604c-4540-b9f2-173ad2e43e57"",""state"":""DONE""}}],""attrs"":{""localId"":""0c33acf9-c722-4f42-8518-6b140130d8da""}}
{adf}
* *Git for Windows* + *Git Credential Manager (GCM)* (Azure DevOps/GitHub SSO)
* *Azure CLI* & *Azure PowerShell*
* *AzCopy* (for ADLS/BLOB moves within Azure)
* *VS Code* (primary IDE)
** Extensions: _Snowflake SQL Tools_ (Snowflake), _SQLTools_ (optional), _Python_, _Pylance_, _Jupyter_, _YAML_, _GitLens_
* *Snowflake CLIs/SDKs*
** *Snow CLI* (preferred over SnowSQL)
** _(Optional)_ *SnowSQL* legacy client (only if you still need it)
* *Drivers*
** *Snowflake ODBC* and *JDBC* drivers (for BI tools, dbt, notebooks)
* *Languages / Runtimes*
** *Python 3.11+* (via *Miniconda* or *pyenv-win*; standardize on conda envs)
** *Java 17* (LTS) for Snowpark Java/Scala
** *Node.js 20 LTS* (if you use Streamlit/Node helpers)
* *Snowpark & data tooling (per environment)*
** {{pipx}} and/or {{conda}}
** {{snowflake-snowpark-python}}, {{pandas}}, {{pyarrow}}, {{jupyterlab}}, {{ipykernel}}

* *Power BI Desktop* _(or Power BI Desktop ‚Äì Optimized for Fabric if that‚Äôs your org standard)_
* *Tabular Editor*
** TE2 (free) for basic edits or *TE3* (licensed) for advanced modeling/CI
* *DAX Studio* (performance tuning)
* *ALM Toolkit* (schema compare/deployment)
* *Power BI Report Builder* (if you produce paginated reports)
* *Azure Data Studio* (lightweight SQL + notebooks)
* *SSMS* (for on-prem SQL Server admin)
* *Azure Storage Explorer* (browsing ADLS/BLOB via private endpoints)
* *On-prem / misc drivers*
** Microsoft SQL Server ODBC, Oracle/ODBC (if used), PostgreSQL ODBC (if used)",* The data engineers are able to access Snowflake and other relevant applications via Jumpbox,"The change request has been raised for DNS forwarding from on-premise DNS servers to the Azure Privatelink DNS.

[CHG0052044|https://cwretail.service-now.com/nav_to.do?uri=sysapproval_approver.do?sys_id=97c987ff33c83e9047764f945d5c7bde]

If we implement this change, people will be locked out of snowflake and rely on VDI. We are awaiting Harrison‚Äôs and Alan‚Äôs architecture review as mentioned by Frank to proceed with this request.

cc- @user @user @user

@user to follow up - will create card"
CSCI-487,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 9:58 AM,03/Oct/25 9:55 AM,,MergeCo Conformed Reporting - Draft Dashboard Testing,"Once dashboard requirements are set and confirmed by the business, need to create a dashboard wireframe.

The wireframe will outline how the dashboard will look, what functionality will be available (eg: what filters available and any other interactive features).

Wireframe will also include definition of metrics within the dashboard and any cadences around dashboard refresh, and email subscriptions.",,
CSCI-486,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 9:57 AM,02/Oct/25 9:59 AM,,MergeCo Conformed Reporting - Draft Dashboard Development,"Once dashboard requirements are set and confirmed by the business, need to create a dashboard wireframe.

The wireframe will outline how the dashboard will look, what functionality will be available (eg: what filters available and any other interactive features).

Wireframe will also include definition of metrics within the dashboard and any cadences around dashboard refresh, and email subscriptions.",,Initial doc for wireframe done
CSCI-485,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 9:35 AM,10/Oct/25 5:06 PM,,Handover - PO's manual.,"h3. Context

* As @user is leaving D&A, he will be writing steps to ensure next person is well equipped with context/objective of what the role is about, what‚Äôs the steps to get there, and what are the deliverables.

h3. Objective

* To create a clear, empowering, and energising handover for the next Product owenr

h3. Steps¬†

# Create coverage map for handover
# Create induction checklist
# Create instruction manual for
## Scrum
## Prioritisation
## What a day/Week loooks like
## What happens if someone‚Äôs away
## 

h3. Deliverables

* PO‚Äôs manual - confluence doc

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* Aligned with @user / @user 
* Doc accessible via confluence/equivalent

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-484,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 9:26 AM,19/Oct/25 7:57 PM,,"Review Source-to-Target Map for Dim_Party, BridgeStorePartyRole","h3. Context

* There is a need to document the source-to-target mapping for the Dim_Party table and BridgeStorePartyRole table as part of the CW Cloud Data Platform Interim Solution project.

h3. Objective

* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.

h3. Steps

# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document
# For each column in Fact_Warehouse_Location_Packing, fill in:
#* Pres Column name
#* Data type
#* Description
#* Source details (Server, DB, Schema, Table, Column)
#* Indicate if derived (Yes/No)
# Review the completed mapping for accuracy and completeness.

h3. Deliverables

* A fully completed source-to-target mapping sheet for Dim_Party table and BridgeStorePartyRole table, with all required fields filled as per the template.

h3. Assumptions (Optional)

* Access to all necessary source systems and metadata is available.
* The supplied mapping sheet template is up to date and reflects current requirements.

h3.","* Columns as per the supplied sheet are filled:
** Pres Column name
** Data type
** Description
** Source (Server, DB, Schema, Table, Column)
** Derived? (Yes/No)",review done
CSCI-483,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 9:25 AM,19/Oct/25 7:56 PM,,"Review Source-to-Target Map for DimBuyer, BridgeProductBuyer","h3. Context

* There is a need to document the source-to-target mapping for the Dim_Buyer table and BridgeProductBuyer table as part of the CW Cloud Data Platform Interim Solution project.

h3. Objective

* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.

h3. Steps

# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document
# For each column in Fact_Warehouse_Location_Packing, fill in:
#* Pres Column name
#* Data type
#* Description
#* Source details (Server, DB, Schema, Table, Column)
#* Indicate if derived (Yes/No)
# Review the completed mapping for accuracy and completeness.

h3. Deliverables

* A fully completed source-to-target mapping sheet for Dim_Buyer table and BridgeProductBuyer table, with all required fields filled as per the template.

h3. Assumptions (Optional)

* Access to all necessary source systems and metadata is available.
* The supplied mapping sheet template is up to date and reflects current requirements.","* Columns as per the supplied sheet are filled:
** Pres Column name
** Data type
** Description
** Source (Server, DB, Schema, Table, Column)
** Derived? (Yes/No)",review done
CSCI-481,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 9:22 AM,10/Oct/25 10:25 AM,,Review Source-to-Target Map for DimBatch,"h3. Context

* There is a need to document the source-to-target mapping for the Dim_Batch table as part of the CW Cloud Data Platform Interim Solution project.

h3. Objective

* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.

h3. Steps

# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document
# For each column in Fact_Warehouse_Location_Packing, fill in:
#* Pres Column name
#* Data type
#* Description
#* Source details (Server, DB, Schema, Table, Column)
#* Indicate if derived (Yes/No)
# Review the completed mapping for accuracy and completeness.

h3. Deliverables

* A fully completed source-to-target mapping sheet for Dim_Batch table, with all required fields filled as per the template.

h3. Assumptions (Optional)

* Access to all necessary source systems and metadata is available.
* The supplied mapping sheet template is up to date and reflects current requirements.

h3.","* Columns as per the supplied sheet are filled:
** Pres Column name
** Data type
** Description
** Source (Server, DB, Schema, Table, Column)
** Derived? (Yes/No)",
CSCI-480,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 9:21 AM,10/Oct/25 10:25 AM,,"Review Source-to-Target Map for DimManufacturer, BridgeProductManufacturer","h3. Context

* There is a need to document the source-to-target mapping for the [Dim_Manufacturer|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer] table and [BridgeProductManufacturer |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.

h3. Objective

* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.

h3. Steps

# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document
# For each column in Fact_Warehouse_Location_Packing, fill in:
#* Pres Column name
#* Data type
#* Description
#* Source details (Server, DB, Schema, Table, Column)
#* Indicate if derived (Yes/No)
# Review the completed mapping for accuracy and completeness.

h3. Deliverables

* A fully completed source-to-target mapping sheet for [Dim_Manufacturer|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer] table and [BridgeProductManufacturer |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table, with all required fields filled as per the template.

h3. Assumptions (Optional)

* Access to all necessary source systems and metadata is available.
* The supplied mapping sheet template is up to date and reflects current requirements.

h3.","h3. Acceptance criteria

* Columns as per the supplied sheet are filled:
** Pres Column name
** Data type
** Description
** Source (Server, DB, Schema, Table, Column)
** Derived? (Yes/No)",The script had to be changed to include isManufacture Flag.
CSCI-479,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 9:20 AM,19/Oct/25 7:55 PM,,Review Source-to-Target Map for Fact_Warehouse_Location_Space_Utilisation,Source to target mapping for Fact_Warehouse_Location_Space_Utilisation - filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",review done
CSCI-478,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 9:20 AM,19/Oct/25 7:55 PM,,Review Source-to-Target Map for Fact_Warehouse_Location_Cycle_Count,Source to target mapping for Fact_Warehouse_Location_Cycle_Count- filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no","Review Done

But there is a duplicate Sheet called Fact_warehouse_cycle_count i think we can delete this one as this doesnt have enough details. I have coloured the tab red. once @user confirms i will delete it."
CSCI-477,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 9:18 AM,24/Oct/25 4:36 PM,,MergeCo - REtail tactical reporting,"h3. Context

* The mergeco report is a report that is used by Execs to track the health of the business. Vikesh would like a series of reports that will help with this so he can make the correct decisions on the business.

h3. Objective

* To help with the availability of data that is used for producing Mergeco reports
* Tables to ingest

|DIM_COUNTRY|
|DIM_STORE -Done|
|DIM_LIKE_FOR_LIKE -Done|
|DATE - Done|
|DIM_PRODUCT -Done|
|DIM_STORE_NZL -Done|
|DIM_PRODUCT_NZL - Done|
|FCT_SALES|
|FCT_CUSTOMER_COUNT_HOURLY_AUS - Done|
|SubCategoryBudgetDaily -|
|FCT_STOCK - Done|
|FCT_STOCK_HISTORY|
|FCT_SALES_NZL|
|FCT_CUSTOMER_COUNT_HOURLY_NZL -Done|
|FCT_STOCK_NZL -DOne|
|FCT_STOCK_HISTORY_NZL|

|| || || ||
| | | |
| | | |

h3. Steps¬†

# Ingest Dim tables 
# Ingest Fact tables 
# Ingest Historical Data

h3. Deliverables

* 
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Create Extract Meta for delta & historical data ""}],""attrs"":{""localId"":""50df6b4c-d7a0-435c-badb-b1db13ed0f02"",""state"":""TODO""}},{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Create Pipelines ""}],""attrs"":{""localId"":""0be7bc70-2c7e-46d0-b93b-f03e7ee404ba"",""state"":""TODO""}},{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Load historical data for big tables""}],""attrs"":{""localId"":""8ccec245-dca7-4ae9-95a5-0d86a402aed5"",""state"":""TODO""}}],""attrs"":{""localId"":""3af7df84-c13f-4a98-9ae1-d189349ee0a2""}}
{adf}

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* Data available for Mergeco reporting

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Dim Ingestion completed

Historical Load in progress. It got hampered due to Space is column names! The is now identified and rectified.

Data loaded. Pending for SubCategoryBudgetDaily"
CSCI-476,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 9:18 AM,14/Oct/25 1:30 PM,,MergeCo - Supply chain tactical report,"h3. Context

* The mergeco report is a report that is used by Execs to track the health of the business. Vikesh would like a series of reports that will help with this so he can make the correct decisions on the business.

h3. Objective

* To help with the availability of data that is used for producing Mergeco reports

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* Data available for Mergeco reporting

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Importing the template pipeline to ADF and test the import.

Hey @user ,

I believe this is duplicate of work @user has already completed, can we make sure we validate that and if so then not spend effort on this.

I believe duplicate of [https://sigmahealthcare.atlassian.net/browse/CSCI-511|https://sigmahealthcare.atlassian.net/browse/CSCI-511] ?

cc @user

@user you are right. @user Please close /remove this."
CSCI-475,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 8:43 AM,09/Oct/25 9:02 AM,,AA - Creating Detailed documentation - Detailed Documentation of work - Sprint 9,"h3. Context

* Comprehensive documentation is required to centralise and standardise knowledge on data engineering assets, including ADF pipelines, Snowflake extract metadata, and related sources (e.g., SCAX2012). This will support onboarding, troubleshooting, and ongoing enhancements.

h3. Objective

* Summarise and document the architecture, processes, and metadata for ADF pipelines, Snowflake extract mechanisms, and SCAX2012, consolidating all sources and references in Confluence for easy access and governance.

h3. Steps

# Identify and catalogue all relevant ADF pipelines, Snowflake extract processes, and SCAX2012 assets.
# Gather existing documentation, code, and metadata from source systems and repositories.
# Summarise technical details, dependencies, and integration points for each asset.
# Document best practices, naming conventions, and standards (referencing existing Snowflake and ADF standards).
# Create or update Confluence pages with structured summaries, diagrams, and links to source artifacts.
# Review documentation with stakeholders for completeness and accuracy.
# Publish and communicate the documentation to the engineering and analytics teams.

h3. Deliverables

* Confluence summary pages for ADF pipelines, Snowflake extract metadata, and SCAX2012.
* Linked references to source code, metadata tables, and configuration files.
* Diagrams illustrating architecture and data flows.
* Documented best practices and standards for each component.

h3. Assumptions (Optional)

* All relevant source systems and repositories are accessible.
* Existing documentation is available and up to date for reference.
* Stakeholders are available for review and feedback.","* All ADF pipelines, Snowflake extract processes, and SCAX2012 assets are documented in Confluence.
* Documentation includes architecture diagrams, metadata summaries, and integration points.
* Best practices and naming conventions are clearly outlined.
* Stakeholder review is completed and feedback incorporated.
* Documentation is published and communicated to all relevant teams.","Document framework is ready and now working on details.

Need help from DBA team to complete doc

@user 
@user if this is TBD next week with @user et al, i‚Äôll split this task for sprint 10 and then mark this task as done?
LMK once you guys have had this discussion.

Hey @user ,

Can we ensure document is in centralised location? Is it confluence or sharepoint?

Thanks,

Harrison

@user attaching it and it is at in the MergeCo data team‚Äôs architecture folder. I will send it for @user review now. It is still an evolving document. [CWR - EDP Operational Details v0.1.docx|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BB4A4EEE0-67BB-4D2F-A05A-EE4929DAA15B%7D&file=CWR%20-%20EDP%20Operational%20Details%20v0.1.docx&action=default&mobileredirect=true]"
CSCI-472,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 8:41 AM,09/Oct/25 9:39 AM,,"Remaining ingestion - (APPRISS, MergeCo Retail)","h3. Context

* The ingestion of data from APPRISS and MergeCo Retail is pending completion. These integrations are critical to ensure comprehensive data coverage and enable downstream analytics and reporting.

h3. Objective

* Complete the end-to-end ingestion processes for APPRISS and MergeCo Retail, ensuring data is reliably extracted, transformed, loaded, and validated within the enterprise data platform.

h3. Steps

# Confirm data source access and connectivity for APPRISS and MergeCo Retail.
# Gather and review source data structures, formats, and update frequencies.
# Design and document ingestion pipelines, including extraction, transformation, and loading logic.
# Implement pipelines using standard naming conventions and best practices *1*.
# Populate and maintain extract/load metadata tables for orchestration and monitoring.
# Conduct system, integration, and performance testing of ingestion processes *2* *3*.
# Validate ingested data with business stakeholders and resolve discrepancies.
# Finalise documentation and update Confluence with process details and data dictionaries.
# Transition pipelines to production and monitor initial loads.

h3. Deliverables

* Fully operational ingestion pipelines for APPRISS and MergeCo Retail.
* Updated extract/load metadata and monitoring dashboards.
* Technical documentation and data dictionaries in Confluence.
* Test results and sign-off from stakeholders.

h3. Assumptions (Optional)

* Source systems for APPRISS and MergeCo Retail are accessible and stable.
* Required credentials and permissions are available.
* Business stakeholders are available for validation and sign-off.","* Data from APPRISS and MergeCo Retail is ingested and available in the enterprise data platform.
* Ingestion pipelines follow established naming conventions and best practices.
* Metadata and monitoring are in place for all new ingestion processes.
* Successful completion of system, integration, and performance testing.
* Documentation is complete and published in Confluence.
* Stakeholder validation and sign-off are obtained.",cc @user
CSCI-471,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 8:33 AM,10/Oct/25 10:31 AM,,AP - Creating Detailed documentation - Detailed Documentation of work,"h3. Context

* Comprehensive documentation is required to centralise and standardise knowledge on data engineering assets, including ADF pipelines, Snowflake extract metadata, and related sources (e.g., SCAX2012). This will support onboarding, troubleshooting, and ongoing enhancements.

h3. Objective

* Summarise and document the architecture, processes, and metadata for ADF pipelines, Snowflake extract mechanisms, and SCAX2012, consolidating all sources and references in Confluence for easy access and governance.

h3. Steps

# Identify and catalogue all relevant ADF pipelines, Snowflake extract processes, and SCAX2012 assets.
# Gather existing documentation, code, and metadata from source systems and repositories.
# Summarise technical details, dependencies, and integration points for each asset.
# Document best practices, naming conventions, and standards (referencing existing Snowflake and ADF standards).
# Create or update Confluence pages with structured summaries, diagrams, and links to source artifacts.
# Review documentation with stakeholders for completeness and accuracy.
# Publish and communicate the documentation to the engineering and analytics teams.

h3. Deliverables

* Confluence summary pages for ADF pipelines, Snowflake extract metadata, and SCAX2012.
* Linked references to source code, metadata tables, and configuration files.
* Diagrams illustrating architecture and data flows.
* Documented best practices and standards for each component.

h3. Assumptions (Optional)

* All relevant source systems and repositories are accessible.
* Existing documentation is available and up to date for reference.
* Stakeholders are available for review and feedback.","* All ADF pipelines, Snowflake extract processes, and SCAX2012 assets are documented in Confluence.
* Documentation includes architecture diagrams, metadata summaries, and integration points.
* Best practices and naming conventions are clearly outlined.
* Stakeholder review is completed and feedback incorporated.
* Documentation is published and communicated to all relevant teams.",
CSCI-470,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Sep/25 8:19 AM,10/Oct/25 10:31 AM,,AP - Creating Documentation (Summary and scaffolding),"h3. Context

* Comprehensive documentation is required to centralise and standardise knowledge on data engineering assets, including ADF pipelines, Snowflake extract metadata, and related sources (e.g., SCAX2012). This will support onboarding, troubleshooting, and ongoing enhancements.

h3. Objective

* Summarise and document the architecture, processes, and metadata for ADF pipelines, Snowflake extract mechanisms, and SCAX2012, consolidating all sources and references in Confluence for easy access and governance.

h3. Steps

# Identify and catalogue all relevant ADF pipelines, Snowflake extract processes, and SCAX2012 assets.
# Gather existing documentation, code, and metadata from source systems and repositories.
# Summarise technical details, dependencies, and integration points for each asset.
# Document best practices, naming conventions, and standards (referencing existing Snowflake and ADF standards).
# Create or update Confluence pages with structured summaries, diagrams, and links to source artifacts.
# Review documentation with stakeholders for completeness and accuracy.
# Publish and communicate the documentation to the engineering and analytics teams.

h3. Deliverables

* Confluence summary pages for ADF pipelines, Snowflake extract metadata, and SCAX2012.
* Linked references to source code, metadata tables, and configuration files.
* Diagrams illustrating architecture and data flows.
* Documented best practices and standards for each component.

h3. Assumptions (Optional)

* All relevant source systems and repositories are accessible.
* Existing documentation is available and up to date for reference.
* Stakeholders are available for review and feedback.

h3. Acceptance criteria

* [ ] All ADF pipelines, Snowflake extract processes, and SCAX2012 assets are documented in Confluence.
* [ ] Documentation includes architecture diagrams, metadata summaries, and integration points.
* [ ] Best practices and naming conventions are clearly outlined.
* [ ] Stakeholder review is completed and feedback incorporated.
* [ ] Documentation is published and communicated to all relevant teams.","Given, When, Then",
CSCI-468,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,24/Sep/25 11:27 AM,10/Oct/25 5:09 PM,,"Review Source-to-Target Map for DimSupplier, BridgeProductSupplier","h3. Context

* There is a need to document the source-to-target mapping for the [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.

h3. Objective

* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.

h3. Steps

# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document
# For each column in Fact_Warehouse_Location_Packing, fill in:
#* Pres Column name
#* Data type
#* Description
#* Source details (Server, DB, Schema, Table, Column)
#* Indicate if derived (Yes/No)
# Review the completed mapping for accuracy and completeness.

h3. Deliverables

* A fully completed source-to-target mapping sheet for [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table , with all required fields filled as per the template.

h3. Assumptions (Optional)

* Access to all necessary source systems and metadata is available.
* The supplied mapping sheet template is up to date and reflects current requirements.

h3.","h3. 

* Columns as per the supplied sheet are filled:
** Pres Column name
** Data type
** Description
** Source (Server, DB, Schema, Table, Column)
** Derived? (Yes/No)",To resume in sprint 10 if needed
CSCI-467,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,24/Sep/25 10:28 AM,25/Sep/25 3:47 PM,,Data Model Review,"Fact tables review:

Initial review for the fact tables built by Amit. These facts are built in line with the strategic solution but at this point will be used for the APPRISS delivery. 

* -Validated all the underlying objects are ingested.-
* -Confirm the joins-
* -Confirm the calculations-

Below are the fact tables:

* FCT_Sales_Retail
* FCT_Sales_Retail_Electronic_Pay
* FCT_Transactions_Audit_History

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Review complete. We scheduled a call for today with Amit.

We identified a missing table from the ingestion table list"
CSCI-466,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,24/Sep/25 10:21 AM,25/Sep/25 2:09 PM,,Review catalogue ingestion data for Retail,"h3. Context

* The project aims to ensure that the correct and complete set of retail catalogue data is ingested into Snowflake to support downstream retail data modelling, analytics, and reporting. This includes aligning source data structures, applying data quality checks, and following Snowflake development standards for consistency and performance.

h3. Objective

* To validate and document the ingestion process for retail catalogue data into Snowflake, ensuring data completeness, accuracy, and readiness for use in retail data models and Power BI reporting.

h3. Steps

* TBC @user 

h3. Deliverables

* TBC 

h3. Assumptions - (Optional)

* All required source systems are accessible and up-to-date.
* Existing Snowflake environments follow established development and security standards.
* Stakeholders are available for clarification and validation.

h3.","* [ ] All relevant catalogue data sources are identified and documented.
* [ ] Data ingestion pipelines are reviewed and validated for completeness and accuracy.
* [ ] Data mapping and transformation logic is documented and aligns with Snowflake standards.
* [ ] Data quality checks are performed and results are satisfactory.
* [ ] Any issues or gaps are documented with recommendations.
* [ ] Stakeholder sign-off confirming data readiness for retail modelling and reporting.",
CSCI-465,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,24/Sep/25 9:23 AM,10/Oct/25 9:55 AM,,APPRISS Documentation : Progress so far & going forward,"h2. Summary

Come up with a documentation to track progress on APPRISS and approach going forward.

h2. Context

Most tables required for APPRISS have already been ingested. This issue focuses on ensuring that we are capturing all what we have done so far APPRISS ( like sharing of sample data share from on prem, ingesting all necessary tables in snowflake EDP) and how are we going to move forward. 

h2. Other information

N/A

Databases covered:

* transactionstorage
* stockdb
* general_reference","We need to create documentation for APPRISS

* -Added Information on sample data share-
* -Queries used to create sample data share added-
* Other documentation links available are provided
* Current Progress of data Ingestion into EDP
* Progress on any additional table ingestion required
* Future data share solution from snowflake (through Query/tables/views etc)","cc @user

Hi All, 

This is link to the currently in progress Confluence documentation for APPRISS:

[APPRISS Documentation: Progress so far & going forward - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1654292481/APPRISS+Documentation+Progress+so+far+going+forward]

FYI.. @user , @user , @user , @user

Hi All,

I have added most of the details in the confluence. Will try to add any further links/recordings of meetings we had in past as well to mark this complete.

@user - is this something you can validate whether we have capture what‚Äôs needed here? -[+APPRISS Documentation: Progress so far & going forward - Enterprise Data Platform - Confluence+|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1654292481/APPRISS+Documentation+Progress+so+far+going+forward]

FYI @user 

Thanks

Han"
CSCI-464,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Sep/25 4:21 PM,10/Oct/25 10:12 AM,,EDP - Detailed Work Breakdown Plan,"h3. Context

* Based on the EDP Solution Design Doc [https://sigmahealthcare.atlassian.net/browse/CSCI-415|https://sigmahealthcare.atlassian.net/browse/CSCI-415] , Chloe to work out the Work Breakdown Plan with Alan and Harrison

h3. Objective

* To produce an work breakdown plan that can be executed with coordination

h3. Steps¬†

# Write work breakdown plan document

h3. Deliverables

* Work breakdown plan in confluence

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* Reviewed by Alan
* Available for access in confluence

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","Working with Alan and Mike on the Detailed Work Breakdown Plan. 

* Interim Solution - Done - Pending Review
* Interim Solution (with dbt)
* Future State Solution

Reviewed and responded to Alan‚Äôs architecture doco on Confluence [https://sigmahealthcare.atlassian.net/wiki/x/BYBAYw|https://sigmahealthcare.atlassian.net/wiki/x/BYBAYw]"
CSCI-463,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Sep/25 4:00 PM,09/Oct/25 9:39 AM,,contributing to Detail design documentation,"h3. Context

* to help @user with detail design doc

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-462,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Sep/25 3:40 PM,03/Oct/25 9:49 AM,,Enable SSO login to snowflake - SCIM,"Consdering the options between 

* MS Entra ID
* Okta

CK to have a follow-up call with Brent from Cloud to firm up the plan for SSO implementation

* -book next tuesday to follow up-

Meeting completed on Tues:

Implementation of Microsoft Entra ID SSO for Snowflake access across the entire data platform, enabling unified authentication for all user personas and tools including PowerBI, Tabular Editor 3, Azure Data Factory, DBT, and direct Snowflake access.","* --booking made for tuesday meeting--
* --To follow-up from the results of the call on Tuesday--
* --Decision made between MS Entra ID vs Okta- ‚úÖ *MS Entra ID Selected*-

* -Meeting with Brent from Cloud team scheduled- ‚úÖ *Completed*
* *Entra ID SSO configured and functional for Snowflake*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Work with Raveendran from our cloud team ""},{""type"":""mention"",""attrs"":{""id"":""557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84"",""accessLevel"":"""",""localId"":""5a778d84-3fcf-46a6-8eed-a75d321d3ac5""}},{""type"":""text"",""text"":"" and ""},{""type"":""mention"",""attrs"":{""id"":""712020:a3bb398f-73df-472d-a226-f67ed86b49ee"",""accessLevel"":"""",""localId"":""542bb25d-3701-49bc-be49-7b5f9c0a5bea""}},{""type"":""text"",""text"":"" ""}],""attrs"":{""localId"":""0a6a01ee-0890-4b42-99a3-17e68a53d8bb"",""state"":""DONE""}}],""attrs"":{""localId"":""340fd36c-fd9c-494d-93b5-65b83210118a""}}
{adf}
* *Service Principal OAuth setup for PowerBI Service scheduled refresh*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Work with Raveendran from our cloud team ""},{""type"":""mention"",""attrs"":{""id"":""557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84"",""accessLevel"":"""",""localId"":""29da3589-921e-4bd8-9020-7c2027ba319a""}},{""type"":""text"",""text"":"" and ""},{""type"":""mention"",""attrs"":{""id"":""712020:a3bb398f-73df-472d-a226-f67ed86b49ee"",""accessLevel"":"""",""localId"":""7cfbbe27-380e-4949-a2c5-ab2e425ff05a""}},{""type"":""text"",""text"":"" ""}],""attrs"":{""localId"":""7d2f681f-168c-4a83-8bc1-c56634d2d967"",""state"":""TODO""}}],""attrs"":{""localId"":""340fd36c-fd9c-494d-93b5-65b83210118a""}}
{adf}
* *Network security implemented (VNet Gateway)*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":"" ""},{""type"":""mention"",""attrs"":{""id"":""557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84"",""accessLevel"":"""",""localId"":""4d11860e-95ff-49b1-bcd3-89eb13c3af94""}},{""type"":""text"",""text"":"" - as part of the SSO can we also collaborate on this""}],""attrs"":{""localId"":""736cca4b-dae9-4882-9b4e-de06b4790373"",""state"":""TODO""}}],""attrs"":{""localId"":""819123db-3a96-41ca-8543-00cb537f3b64""}}
{adf}
* *All user personas can authenticate successfully*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Testing post entra ID sso dependencies completed""}],""attrs"":{""localId"":""c9b6f8b8-7224-4cd6-9ba4-1b8c34b4d47d"",""state"":""TODO""}}],""attrs"":{""localId"":""db194595-b904-41c9-8026-19c645d2aaac""}}
{adf}
* *Conditional access policies and MFA enforced*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Testing post entra ID sso dependencies completed""}],""attrs"":{""localId"":""b541fd8a-1751-4451-945b-4cd78a6a3069"",""state"":""TODO""}}],""attrs"":{""localId"":""db194595-b904-41c9-8026-19c645d2aaac""}}
{adf}
* *Production rollout completed*",@user Will be working with @user tomorrow
CSCI-461,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Sep/25 3:20 PM,27/Nov/25 2:38 PM,,Manhattan Active (MySQL) Integration part 2,"DATA IN for critical identified objects from Manhattan Active (MySQL) db

* confirm with RFoong decision for data lake extraction rather than API
* Whitelinsting
* Credential needed for GCP - login and testing connection
* Create pipeline

@user","Definition of done:

* Extract meta created
* Linked services created
* Pipeline run end to end","To resume on unfinish parts for CSCI-136

Initial work is done. We need to additional work with the business to identify the load pattern and might be additional tables."
CSCI-460,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Sep/25 11:03 AM,03/Oct/25 2:36 PM,,Manhattan Active - Discovery,"Identify how we can connect to the Manhattan Active MY SQL Database

# We have MYSQL login, but it we are unable to connect with Dbeaver
# Supply chain team is connecting the database using Cogonos to get table details.","|Table|Record count|
|default_dcinventory.DCI_INVENTORY|37,311¬†|
|default_dcinventory.DCI_LOCATION|19,775¬†|
|default_dcinventory.DCI_LOCATION_CAPACITY_USAGE|17,388¬†|
|default_dcinventory.DCI_LOCATION_ITEM_ASSIGNMENT|15,076¬†|
|default_dcorder.DCO_ORDER_FAILURE|50,736¬†|
|default_dcorder.DCO_ORDER_FAILURE_DETAIL|267,805¬†|
|default_dcorder.DCO_ORDER_PLAN_RUN_STRATEGY|12,338¬†|
|default_dcorder.DCO_PLANNING_RUN_STATUS¬†|¬†8¬†|
|default_item_master.ITE_ITEM|68,546¬†|
|default_item_master.ITE_ITEM_PACKAGE|119,436¬†|
|default_pickpack.PPK_OLPN|807,382¬†|

|| || || ||
| | | |
| | | |","Have sent email to Chandan from Boomi team to raise whitelist request for us with the Manhattan Active team

This is an ongoing process. CK helped us understand the current access method and Jess is communicating with Candan and we are waiting for the response from them.

Connection has been made. Continue with the finding the number of records of the table identified by Jess

Initial tables have been identified. Need to contact Supply change team another ticket will be created for that"
CSCI-459,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Sep/25 9:36 AM,23/Sep/25 9:36 AM,,"Review - DimWarehouse, DimWarehouseLocation, DimZone","h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then",
CSCI-458,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Sep/25 1:33 PM,10/Oct/25 5:06 PM,,Network Drive Whitelisting- RITM0176046,"h3. Context

* In order to ingest supply chain data into snowflake for group level reporting, network drive has to be whitelisted
* 

h3. Objective

* network drive to be whitelisted

h3. Steps¬†

# ServiceNow ticket RITM0176046 submitted (Screenshot below)

# track progress

h3. Deliverables

* network drive whitelisted

h3. Rules Implemented for the whitelisting:

RULE1

***************************************************************
Name:
VDI and AlbertSt To Azure_Snowflake Priv-Endpoints

¬†

Source:¬†
10.15.0.0/19‚ÄÉ‚ÄÉalbert_st_10.15.0.0_19_subnet_1
172.25.128.0-172.25.150.62‚ÄÉ‚ÄÉAR_m1_vdi_desktop_full_range
172.27.128.0-172.27.150.62‚ÄÉ‚ÄÉ

¬†

Destination:
172.29.100.4 (cwraseedp-snowflake-prd-pep)

¬†

¬†

Service‚ÄÉ‚ÄÉ443‚ÄÉ‚ÄÉTCP‚ÄÉ‚ÄÉ
***************************************************************

¬†

¬†

RULE 2

***************************************************************
Name: Snowflake to CW Vault

¬†

Source:
172.29.84.76 - cwr-ase-edp-shir-dev-vmss_0

¬†

Destination:
192.168.42.235 - H_m2phofil01_fileshare_srv

¬†

Service:¬†
SMB - TCP 445¬†
***************************************************************

h3. Acceptance criteria

* network drive accessible - \\cwvault\Everyone\Supply Chain\Administration\Report and Dashboard\ZZ_DataSource

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","@user-- will get ticket number if possible-
Thanks @user

Now asssigned to the network team - reaching out to Brent to work on this @user

Diafy is working on this task

Followed up with network team- Diafy will be implementing this today

cc-@user @user @user

@user thank you

Rule has been implemented: TESTING IS PENDING

RULE1

***************************************************************
Name:
VDI and AlbertSt To Azure_Snowflake Priv-Endpoints

¬†

Source:¬†
10.15.0.0/19‚ÄÉ‚ÄÉalbert_st_10.15.0.0_19_subnet_1
172.25.128.0-172.25.150.62‚ÄÉ‚ÄÉAR_m1_vdi_desktop_full_range
172.27.128.0-172.27.150.62‚ÄÉ‚ÄÉ

¬†

Destination:
172.29.100.4 (cwraseedp-snowflake-prd-pep)

¬†

¬†

Service‚ÄÉ‚ÄÉ443‚ÄÉ‚ÄÉTCP‚ÄÉ‚ÄÉ
***************************************************************

¬†

¬†

RULE 2

***************************************************************
Name: Snowflake to CW Vault

¬†

Source:
172.29.84.76 - cwr-ase-edp-shir-dev-vmss_0

¬†

Destination:
192.168.42.235 - H_m2phofil01_fileshare_srv

¬†

Service:¬†
SMB - TCP 445¬†
***************************************************************

cc- @user @user @user @user

@user - Please update testing result

@user Hi Han - as of today (8/10/25) the testing is still failing (error message received is ‚Äònetwork drive not found‚Äô)

Thanks Jess .

@user - can we forward the feedback to the team for further troubleshoot?

Thanks

This has been completed. Test result awaited from @user 

@user @user

Can confirm as of yesterday we can now connect ADF to the network drive successfully üôÇ 

This whitelisting task is now done."
CSCI-457,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Sep/25 9:53 AM,29/Sep/25 8:15 AM,,Network Drive Whitelisting- Cloud/Network Team- RITM0176046,"h3. +*Context:*+ 

*Location of Network Drive is:*¬†

\\cwvault\Everyone\Supply Chain\Administration\Report and Dashboard\ZZ_DataSource\DFIO_SCI_ZZ\

h3. +*Steps:*+

We're going to need:

* Our Azure Subscription to be whitelisted to access the file drive and,
* an account created to use to connect to the file drive

h3. Objective

* 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","@user awaiting for a design (diagram) from @user 

cc- @user @user

@user has already given design 
will loop @user in

RITM0175995 created@user @user @user @user

Firewall ticket for whitelisting Snowflake SHIR from Manhattan will be raised after meeting with Brent on 25/09/2025 at 10:30 am@user @user @user @user

[https://sigmahealthcare.atlassian.net/browse/CSCI-458|https://sigmahealthcare.atlassian.net/browse/CSCI-458] 

ticket has been split to here."
CSCI-455,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Sep/25 1:19 PM,22/Sep/25 9:49 AM,Ingestion,Data Ingestion - PBI05,"Import data from PBI05 for MergeCo tactical reporting.

* Train Jess on ALTIDA
** -Initial Session-
** Follow up Session
* -Identify the list of tables we want to import-
*","Given, When, Then","Had initial session with Jess and 10 tables has been identified in the first list.

@userCan you list the tables here thanks"
CSCI-451,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,17/Sep/25 2:24 PM,25/Sep/25 9:21 AM,,"Create Source-to-Target Map for DimSupplier, BridgeProductSupplier","h3. Context

* There is a need to document the source-to-target mapping for the [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.

h3. Objective

* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.

h3. Steps

# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document
# For each column in Fact_Warehouse_Location_Packing, fill in:
#* Pres Column name
#* Data type
#* Description
#* Source details (Server, DB, Schema, Table, Column)
#* Indicate if derived (Yes/No)
# Review the completed mapping for accuracy and completeness.

h3. Deliverables

* A fully completed source-to-target mapping sheet for [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table , with all required fields filled as per the template.

h3. Assumptions (Optional)

* Access to all necessary source systems and metadata is available.
* The supplied mapping sheet template is up to date and reflects current requirements.

h3.","h3. 

* Columns as per the supplied sheet are filled:
** Pres Column name
** Data type
** Description
** Source (Server, DB, Schema, Table, Column)
** Derived? (Yes/No)","@user Completed the mapping for DimSupplier, BridgeProductSupplier,
[https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=xXfNPJ&nav=MTVfezMwMERDQ0Q5LUI2QkQtNDlFRC1BNTZBLTFGQzNGMzZFQjY1M30|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=xXfNPJ&nav=MTVfezMwMERDQ0Q5LUI2QkQtNDlFRC1BNTZBLTFGQzNGMzZFQjY1M30]

CC:- @user

Issue split into:
|CSCI-468|Review Source-to-Target Map for DimSupplier, BridgeProductSupplier|"
CSCI-450,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,17/Sep/25 2:24 PM,25/Sep/25 9:21 AM,,"Create Source-to-Target Map for DimManufacturer, BridgeProductManufacturer","h3. Context

* There is a need to document the source-to-target mapping for the [Dim_Manufacturer|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer] table and [BridgeProductManufacturer |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.

h3. Objective

* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.

h3. Steps

# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document
# For each column in Fact_Warehouse_Location_Packing, fill in:
#* Pres Column name
#* Data type
#* Description
#* Source details (Server, DB, Schema, Table, Column)
#* Indicate if derived (Yes/No)
# Review the completed mapping for accuracy and completeness.

h3. Deliverables

* A fully completed source-to-target mapping sheet for [Dim_Manufacturer|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer] table and [BridgeProductManufacturer |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table, with all required fields filled as per the template.

h3. Assumptions (Optional)

* Access to all necessary source systems and metadata is available.
* The supplied mapping sheet template is up to date and reflects current requirements.

h3.","h3. Acceptance criteria

* Columns as per the supplied sheet are filled:
** Pres Column name
** Data type
** Description
** Source (Server, DB, Schema, Table, Column)
** Derived? (Yes/No)",
CSCI-449,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,17/Sep/25 2:23 PM,29/Sep/25 8:10 AM,,Create Source-to-Target Map for DimBatch,"h3. Context

* There is a need to document the source-to-target mapping for the Dim_Batch table as part of the CW Cloud Data Platform Interim Solution project.

h3. Objective

* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.

h3. Steps

# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document
# For each column in Fact_Warehouse_Location_Packing, fill in:
#* Pres Column name
#* Data type
#* Description
#* Source details (Server, DB, Schema, Table, Column)
#* Indicate if derived (Yes/No)
# Review the completed mapping for accuracy and completeness.

h3. Deliverables

* A fully completed source-to-target mapping sheet for Dim_Batch table, with all required fields filled as per the template.

h3. Assumptions (Optional)

* Access to all necessary source systems and metadata is available.
* The supplied mapping sheet template is up to date and reflects current requirements.

h3.","* Columns as per the supplied sheet are filled:
** Pres Column name
** Data type
** Description
** Source (Server, DB, Schema, Table, Column)
** Derived? (Yes/No)",
CSCI-448,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,17/Sep/25 2:22 PM,29/Sep/25 8:04 AM,,"Create Source-to-Target Map for DimBuyer, BridgeProductBuyer","h3. Context

* There is a need to document the source-to-target mapping for the Dim_Buyer table and BridgeProductBuyer table as part of the CW Cloud Data Platform Interim Solution project.

h3. Objective

* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.

h3. Steps

# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document
# For each column in Fact_Warehouse_Location_Packing, fill in:
#* Pres Column name
#* Data type
#* Description
#* Source details (Server, DB, Schema, Table, Column)
#* Indicate if derived (Yes/No)
# Review the completed mapping for accuracy and completeness.

h3. Deliverables

* A fully completed source-to-target mapping sheet for Dim_Buyer table and BridgeProductBuyer table, with all required fields filled as per the template.

h3. Assumptions (Optional)

* Access to all necessary source systems and metadata is available.
* The supplied mapping sheet template is up to date and reflects current requirements.","* Columns as per the supplied sheet are filled:
** Pres Column name
** Data type
** Description
** Source (Server, DB, Schema, Table, Column)
** Derived? (Yes/No)",
CSCI-447,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,17/Sep/25 2:19 PM,29/Sep/25 8:04 AM,,"Create Source-to-Target Map for Dim_Party, BridgeStorePartyRole","h3. Context

* There is a need to document the source-to-target mapping for the Dim_Party table and BridgeStorePartyRole table as part of the CW Cloud Data Platform Interim Solution project.

h3. Objective

* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.

h3. Steps

# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document
# For each column in Fact_Warehouse_Location_Packing, fill in:
#* Pres Column name
#* Data type
#* Description
#* Source details (Server, DB, Schema, Table, Column)
#* Indicate if derived (Yes/No)
# Review the completed mapping for accuracy and completeness.

h3. Deliverables

* A fully completed source-to-target mapping sheet for Dim_Party table and BridgeStorePartyRole table, with all required fields filled as per the template.

h3. Assumptions (Optional)

* Access to all necessary source systems and metadata is available.
* The supplied mapping sheet template is up to date and reflects current requirements.

h3.","* Columns as per the supplied sheet are filled:
** Pres Column name
** Data type
** Description
** Source (Server, DB, Schema, Table, Column)
** Derived? (Yes/No)","@user Its completed, I‚Äôve moved it to review"
CSCI-443,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Sep/25 3:45 PM,17/Sep/25 12:58 PM,,CW BAU Tasks,"* Needed to investigate issue with the PBI On-prem gateway
* SC Team recently receiving a ‚Äúgateway datasource‚Äù error on a few of their SM‚Äôs intermittently:
** 

h3. 

h3.","Given, When, Then",
CSCI-442,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Sep/25 1:05 PM,29/Sep/25 8:17 AM,,Follow up on - RITM0175340- Azure Blob storage access from office,"h3. Context

* 

h3. Objective

* 

h3. Steps¬†

# 

h3. Deliverables

* 

h3. Assumptions - (Optional)

* 

h3. Acceptance criteria

* 

h3. 

h3. 

h3. 

h3. 

h3. 

h3.","Given, When, Then","In touch with Systems and Cloud team to get this sorted.

As Per @user the systems team has advised to let them know when @user and him are in the head office so that they can run somethings is the background to check the connections.

cc- @user

The firewall rules are now in place from IT side for Aswini and Ashutosh to access the *BLOB Storage*. They will test this week when they are in office.¬†

cc- @user @user @user

Issue split into:
|CSCI-494|Follow up on - RITM0175340- Azure Blob storage access from office - sprint 9 |"
CSCI-441,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Sep/25 9:04 AM,19/Sep/25 9:47 AM,,Create the prod access for PDB14 and PDB19 reader servers,"h3. Create the SQL authenticated logins for PDB14 and PDB19 readonly servers and grant access

h3. 

h3. 

h3. 

h3.","* Create required SQL logins
* Create SQL user for SCAX2012 and TransactionStorage databases.
* Grant SELECT access on tables used in the data platform
* Add secrets to Key Vault","@user Created the SQL users and added the passwords to the DEV key vault. These databases need to be accessed with application intent set to read-only

cc: @user

aswini to test connection to prod 

need:
- whitelisting

PHil to split task"
CSCI-440,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 6:32 PM,23/Sep/25 9:52 AM,,MergeCo Conformed Reporting - Identify Source for Availability Metrics - part 2,"Identify which specific tables/views to use for Merge Co reporting from a CW domain for the below availability metrics:

* Availability %
* TOS %
** Temporarily Out of Stock
* MCS %
** Manufacturer Can‚Äôt Supply
* SOH $
** Stock on Hand
* DOI
** Days of Inventory - within CW also known as DIH (Days In Hand)

Ideal grain is at the: 

per day, per DC, per product level

Need to work out:

* Which tables best to extract from
* If tables can‚Äôt be identified, why? Is it not available at all? Or is there an issue with the grain of data? Or access to data?",,"||*Metric*||*Current SC Source*||*Proposed Tactic Source*||*Detail*||
|Availability %|DFIO Extract
(stored on-prem network drive)|SKU? (PDB08)|\\cwvault\Everyone\Supply Chain\Administration\Report and Dashboard\ZZ_DataSource\DFIO_SCI_ZZ\|
|TOS%|DFIO Extract
(stored on-prem network drive)|SKU? (PDB08)|\\cwvault\Everyone\Supply Chain\Administration\Report and Dashboard\ZZ_DataSource\DFIO_SCI_ZZ\|
|MCS%|DFIO Extract
(stored on-prem network drive)|SKU? (PDB08)|\\cwvault\Everyone\Supply Chain\Administration\Report and Dashboard\ZZ_DataSource\DFIO_SCI_ZZ\|
|SOH $/Units|PBI05 AX Table|PBI05 AX Table|PBI05.SupplyChain.SCAX.DCsInventoryHistory.InventoryAmount|
|DOI|PBI05 AX Tables|PBI05 AX Tables|[BI_Presentation].[dbo].[AX_FCT_DC_STOCK].[STOCK_VALUE] && [BI_Presentation].[dbo].[AX_FCT_DC_SALES].[ExtendedCostExGST]|

Have collated all info - need to talk with @user re: what to do about the metrics where DFIO extracts are its source

AX_FCT_DC_STOCK - only displays grain for per DC, per product (does not distinguish per day, only displays current data)"
CSCI-439,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 6:32 PM,18/Sep/25 4:53 PM,,MergeCo Conformed Reporting - Pull Data from CW to SF - part 2,"Copy data from CW on-prem SQL Server (PBI05) to SF via ADF.

Grain:

* High-level: Per DC, Per Day",,"Had meeting with network team - they advised they have whitelisted ‚Äútheir side‚Äù of PBI05 - however we now require the cloud team to enable the whitelisting on their side within Azure.

@user to follow up

Cloud team performed whitelisting action on their end.

Ran test in ADF to test connection to PBI05 and test ran successful.

Once a table has been loaded into SF - will mark task as complete."
CSCI-438,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 6:28 PM,19/Sep/25 9:21 AM,,APPRISS - next steps on data modelling - For Alan/Harrison to review,"This card is for Alan/Harrison to review Amit‚Äôs work

* -Identify business process for APPRISS-

{quote}Identify source table(s) for APPRISS - partially done (review)

Create structure for FActs and Dims{quote}

{quote}

* Finalizing identification of business processes relevant to APPRISS.
* Reviewing and confirming source tables for APPRISS data ingestion.
* Designing and documenting the structure for Fact and Dimension tables to support robust analytics and reporting.
This task aims to ensure a clear, scalable, and auditable data model foundation for APPRISS integration within the CW Cloud Data Platform Interim Solution.

{quote}","* -Business Process Identification-
* -Source Table Confirmation-
* -Review Initial Data Extract Queries-
* - Dimension Table Design-
* -Fact Tables Design-
* -Sample Data Validation-
* -Documentation-
* Stakeholder Review
* -Readiness for Next Steps-",
CSCI-437,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 6:27 PM,19/Sep/25 9:21 AM,,Silver model for facts - review for Alan/Harrison,"This card is for Alan/Harrison to review Amit‚Äôs work

{quote}Develop a Silver data model for Facts, transforming and integrating data from the Bronze layer to provide a cleansed, structured, and business-ready dataset. {quote}

{quote}The Silver model should address data quality issues, apply necessary business logic, and ensure consistency for downstream analytics and reporting.{quote}","* -Silver (Kimball Medallion model) for Facts to be constructed.-
* -Diagram in tool i.e. [http://draw.io|http://draw.io] -
* peer reviewed
* Documentation
* Stakeholder Sign-off",
CSCI-436,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 6:21 PM,25/Sep/25 9:21 AM,,Create Source-to-Target Map for Fact_Warehouse_Location_Space_Utilisation - part 2,Source to target mapping for Fact_Warehouse_Location_Space_Utilisation - filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",
CSCI-435,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 6:21 PM,25/Sep/25 9:21 AM,,Create Source-to-Target Map for Fact_Warehouse_Location_Cycle_Count - part 2,Source to target mapping for Fact_Warehouse_Location_Cycle_Count- filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",
CSCI-434,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 6:18 PM,23/Sep/25 9:31 AM,,Review Source-to-Target Map for Dim_Product - Alan - part 2,to have a discussion with you @user about this task.,* Finalise that the work on Source-to-Target Map for Dim_product is correct.,
CSCI-433,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 6:18 PM,23/Sep/25 9:30 AM,,Review Source-to-Target Map for Dim_Product_Pack (Previously Dim_Product_UOM),Review the mapping of DIM_PRODICT_UOM Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=67yqFK&nav=MTVfe0RDNzkwNDhDLTg0NDAtNEREOS05NkE3LUM2ODRFOTNBNjkzQX0],"* Finalize that the work on Source-to-Target Map for DIM_PRODICT_UOM is correct.
** Review the Source-to-Target Map for DIM_PRODICT_UOM against the current production environment.
** Update the mapping file to reflect any changes or corrections found.
** Peer review completed and signed off by Chloe/Harrison
** -Communicate any changes to relevant business stakeholders.-
** Communicate changes of mapping file to the Jira/standup for traceability.","@user @user - please rearrange spreadsheet as per Alan‚Äôs [data model|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#Silver.20]

LMK if there‚Äôs any issues cc @user

@user @user / @user best to split this to an additional card linked to this card."
CSCI-432,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 6:18 PM,23/Sep/25 9:30 AM,,Review Source-to-Target Map for Dim_Store - Alan part 2,Reviewing Source-to-Target Map for Dim_Store in Supply chain,* Finalise that the work on Source-to-Target Map for Dim_Store is correct.,"@user @user

add Dim_address, StoreHours, StoreHoursSpecial

cc @ @user"
CSCI-431,CSCI,CW Cloud Data Platform Interim Solution,Task,Duplicate,Medium,12/Sep/25 6:14 PM,22/Dec/25 8:27 AM,,Enable developer access to snowflake via Privatelink,"Enable developer access to snowflake via Privatelink - via browser 

User story:

* As a snowflake person (not service) user, I want to access snowflake via Privatelink so that it is accessible and secure (confirming to security requirements).

@user - Below are the key action items as identified by Eugene for us as next steps.

Details are [here|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5145/Developer-Access?anchor=**developer-access-to-snowflake-via-azure-privatelink**]

Can you please help us engage with Cloud team and raise a servicenow ticket.

cc @user 

*Key action items* 

* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com]
* Configure firewalls (On-Prem, Azure) as follows

* On-Prem Network for developers ‚Üí Snowflake VNet *(servnicenow ticket)*
* VDI / Jumpbox network ‚Üí Snowflake VNet *(servicenow tickets)*
** *how many jumpboxes* we need?
** *What are the things we need?*
* VPN Network for developers ‚Üí Snowflake VNet (servicenow ticket)
** QoS? - traffic prioritisation
** What tools are involved.
** CW - Palo Alto GlobalProtect.
* Note:
** Ports to be allowed: 443, 80, 1433
* Configure VDI / Jumbox images to pre-install tools defined above
* Create Development VMs in Snowflake Subscription","* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/]
* Access snowflake via [https://app.snowflake.com/cw/au/#/homepage|https://app.snowflake.com/cw/au/#/homepage] after infrastructure changes.
* Test cases
** Access on premises (office network)
** Access off premises
*** (fail path) access with no VPN
*** Access via GlobalProtect (CW)
*** Access via Sigma VPN
** Access via PBI
*","yes since @user has been onboarded, she will be working on the setting up the DE jumpbox, once this is set up, I already explained to her this is the next step. So this should be aligned to the list of tasks to what Anjali will be working on.

I believe she set up meeting for Monday and we can discuss this.

 @user @user@user

For clarity, approach

# Jumpbox will be created
# Create VMs
# VMs will have access to privatelink

@user this task is a duplicate of existing - [https://sigmahealthcare.atlassian.net/browse/CSCI-422|https://sigmahealthcare.atlassian.net/browse/CSCI-422] 

Can we please close this? To avoid confusion and duplication.

@user sure I‚Äùll close this one"
CSCI-430,CSCI,CW Cloud Data Platform Interim Solution,Task,Duplicate,Medium,12/Sep/25 6:13 PM,22/Dec/25 8:27 AM,,RITM0174658 - Aswini and Ashutosh access to CW BLOB storage,PHil to raise in Servicenow direct access to CW BLOB storage for Aswini,Jumpbox ‚Üí Blob Access availble for Aswini,"maybe duplicate ticket - will take down if it is so

Hi @user this is a duplicate ticket, there is already existing-RITM0175340 on which the Systems team is working on. You may please take this one down.

@user marked as duplicate"
CSCI-429,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 2:56 PM,18/Sep/25 9:56 AM,,PBI05 whitelisting Servicenow Ticket - MergeCo Conformed Reporting - Pull Data from CW to SF,"Copy data from CW on-prem SQL Server (PBI05) to SF via ADF.

Grain:

* High-level: Per DC, Per Day",,"Have scheduled a meeting with the CWR Network team to walk them through the Snowflake data flow (related to ticket _RITM0173895_) to ensure clarity on how the flow operates within our network.

The goal is to provide the context required for the Network team to review and approve the ticket so we can move forward.

cc- @user @user @user

The network team has whitelisted the firewall from their end. Awaiting cloud team to whitelist from Azure end.

The firewall rule has been applied from both, Cloud and On-prem side. And jess has tested this."
CSCI-428,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 2:51 PM,24/Sep/25 9:46 AM,,Documentation for transformation logic for fact tables for APPRISS,"Create mapping & transformation document for Silver Layer of Fact tables needed for Appriss Data Delivery 

[https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/02.%20Commercial%20Data%20Platform/Retail%20Data%20Model%20-%20Silver/Silver%20Fact%20Tables%20Mapping%20Info.xlsx?d=w8de2c221e0014e0da95ac90496078562&csf=1&web=1&e=nqkmvj|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/02.%20Commercial%20Data%20Platform/Retail%20Data%20Model%20-%20Silver/Silver%20Fact%20Tables%20Mapping%20Info.xlsx?d=w8de2c221e0014e0da95ac90496078562&csf=1&web=1&e=nqkmvj]","* -Document link-
* Engineering team should be able to populate silver layer fact tables based on Mapping document 

----

Columns:

|Column Name|Description|Source Table|Mapping|Logic If any|","Started with FCT_Sales_Retail. Work in progress

Two more to be done afterwards

FCT_Transactions_Audit_History
FCT_Sales_Retail_Electronic_Payments

Can we fill description and AC

Thanks

added them @user

@user can we have a bit more details e.g. what the AC entailis?
What are we expecting as a deliverable at the end of this task?

Thanks

I‚Äùll get you to place link to document here @user and go from there.

[Silver Fact Table Mapping info|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/02.%20Commercial%20Data%20Platform/Retail%20Data%20Model%20-%20Silver/Silver%20Fact%20Tables%20Mapping%20Info.xlsx?d=w8de2c221e0014e0da95ac90496078562&csf=1&web=1&e=nqkmvj] 
Hi @user and @user here is the document link .@user

Completed doc for 
 FCT_Sales_Retail
FCT_Sales_Retail_Electronic_Payment

Will complete FCT_Transactions_Audit_History by today

Completed FCT_Transactions_Audit_History mapping. This task is now completed"
CSCI-427,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 2:42 PM,25/Sep/25 11:51 AM,,Issue in parquet file load - part 2,Getting variable columns in Parquet file load for historical data for ILS / Apriss tables,,
CSCI-426,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 2:41 PM,18/Sep/25 3:37 PM,Ingestion,Manhattan Scale data ingestion - part 2,"Import the data for ILS tables

location_inventory

AR_Transaction_History

Transaction_History

* Create extract meta records
* Create & Run end to end pipelines in ADF",,"assigning to @user 

@user when you‚Äôre back, if you can let Aswini know what‚Äôs covered, that‚Äôll be great.

Thanks!

Hi @user ,

I have started on AR_TRANSACTION_HISTORY. I can still see the missing column issue. Let‚Äôs connect to discuss more about this."
CSCI-425,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 2:36 PM,29/Sep/25 8:10 AM,,Handover From Adeel to next person from DB team,"Confluence doc to cover:

* where things are up to
** Modelling
** STTM
* -Parquet extract:-
** -Extract-
** -Upload-
* What we need from this project",,"Added doco to extract parquet files

CC:- @user

Added steps to upload parquet extracts

Issue split into:
|CSCI-493|Handover From Adeel to next person from DB team - finalisation|"
CSCI-424,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 2:27 PM,29/Sep/25 8:10 AM,,Review Source-to-Target Map for Fact_Warehouse_Location_Inbound_Transactions,Source to target mapping for Fact_Warehouse_Location_Inbound_Transactions - filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",
CSCI-422,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 2:16 PM,29/Sep/25 8:14 AM,,Capture the requirements for creating the Jumpbox for the developers (data engineers),"Capture the requirements for creating the Jumpbox for the developers (data engineers).

*Key action items*

* -On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/] - On prem and Cloud Team- *RITM0173487 (for reference)*-

What‚Äôs needed is for the DNS forwarding from on-premise DNS servers, to the Azure privatelink DNS. See: [Resolve Azure and on-premises domains. | Microsoft Learn|https://learn.microsoft.com/en-us/azure/dns/private-resolver-hybrid-dns] IN PROGRESS- CR Created

* -Collaborate with the EUC Team (CWR) to *create* Jumpbox IN PROGRESS- Onboarding of resources-
* Give the Entra (AD) users access to the Jumpbox 

|*Snowflake_CW_AU_DataEngineers*|DEV_ROLE_DATA_ENG|

* Configure firewalls (whitelisting) (On-Prem, Azure) as follows
* VDI / Jumpbox network ‚Üí Snowflake VNet *(servicenow tickets)*

* *how many jumpboxes* we need? *no. of users 8*
* Note: Ports to be allowed: 443, 80, 1433
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Configure VDI / Jumbox images to pre-install tools listed below: - The list has been reviewed and approved by EUC Team.""}],""attrs"":{""localId"":""a6a01a2f-604c-4540-b9f2-173ad2e43e57"",""state"":""DONE""}}],""attrs"":{""localId"":""0c33acf9-c722-4f42-8518-6b140130d8da""}}
{adf}
* *Git for Windows* + *Git Credential Manager (GCM)* (Azure DevOps/GitHub SSO)
* *Azure CLI* & *Azure PowerShell*
* *AzCopy* (for ADLS/BLOB moves within Azure)
* *VS Code* (primary IDE)
** Extensions: _Snowflake SQL Tools_ (Snowflake), _SQLTools_ (optional), _Python_, _Pylance_, _Jupyter_, _YAML_, _GitLens_
* *Snowflake CLIs/SDKs*
** *Snow CLI* (preferred over SnowSQL)
** _(Optional)_ *SnowSQL* legacy client (only if you still need it)
* *Drivers*
** *Snowflake ODBC* and *JDBC* drivers (for BI tools, dbt, notebooks)
* *Languages / Runtimes*
** *Python 3.11+* (via *Miniconda* or *pyenv-win*; standardize on conda envs)
** *Java 17* (LTS) for Snowpark Java/Scala
** *Node.js 20 LTS* (if you use Streamlit/Node helpers)
* *Snowpark & data tooling (per environment)*
** {{pipx}} and/or {{conda}}
** {{snowflake-snowpark-python}}, {{pandas}}, {{pyarrow}}, {{jupyterlab}}, {{ipykernel}}

* *Power BI Desktop* _(or Power BI Desktop ‚Äì Optimized for Fabric if that‚Äôs your org standard)_
* *Tabular Editor*
** TE2 (free) for basic edits or *TE3* (licensed) for advanced modeling/CI
* *DAX Studio* (performance tuning)
* *ALM Toolkit* (schema compare/deployment)
* *Power BI Report Builder* (if you produce paginated reports)
* *Azure Data Studio* (lightweight SQL + notebooks)
* *SSMS* (for on-prem SQL Server admin)
* *Azure Storage Explorer* (browsing ADLS/BLOB via private endpoints)
* *On-prem / misc drivers*
** Microsoft SQL Server ODBC, Oracle/ODBC (if used), PostgreSQL ODBC (if used)",* The data engineers are able to access Snowflake and other relevant applications via Jumpbox,"Have scheduled a meeting with @user and @user to discuss the requirements for setting up the Jumpbox. 

cc- @user

Had a meeting with EUC Team and Network Team to discuss the requirements. Awaiting EUC Team resource to officially get allocated to the project to start the creation of Jumpbox.

Meeting Minutes as per today‚Äôs meeting 18/09/2025:

# [@Frank Perez|mailto:frank.perez@chemistwarehouse.com.au] will engage [@Jai Bagga|mailto:jai.bagga@chemistwarehouse.com.au] & IT Architecture Team (Neville) officially, to work on this requirement.
# [@Jai Bagga|mailto:jai.bagga@chemistwarehouse.com.au] will start working on creation of Jumpbox, we have a Teams channel to discuss everything related to this requirement once officially allocated.
# The team confirmed that number of VMs created will be *8* (depending on the number of users).
# The AD Groups that will require access are as follows:

|*Snowflake-CW-AU-Consultants*|DEV_ROLE_CONSULTANT|
|*Snowflake-CW-AU-DataAnalysts*|UAT_ROLE_ANALYST|
|*Snowflake-CW-AU-DataEngineers*|DEV_ROLE_DATA_ENG|
|*Snowflake-CW-AU-PlatformEngineers*|DEV_ROLE_PLATFORM_ENG|

¬†

# [@Diafy Gerardo|mailto:diafeked.gerardo@chemistwarehouse.com.au] from network team will be sharing the NIC ( Network Interface card) details to [@Jai Bagga|mailto:jai.bagga@chemistwarehouse.com.au]
# [@Anjali Verma|mailto:anjali.verma@chemistwarehouse.com.au] to work with [@Diafy Gerardo|mailto:diafeked.gerardo@chemistwarehouse.com.au] to create Service Now Ticket for the DNS resolution and Firewall Whitelisting. CC- [@Chloe Tran|mailto:chloe.tran@chemistwarehouse.com.au] [@Eugene Paden|mailto:eugene@raybiztech.com] [@Raveendran Kumaravelu|mailto:raveendran.kumaravel@chemistwarehouse.com.au]
# [@Jai Bagga|mailto:jai.bagga@chemistwarehouse.com.au] also confirmed and reviewed the tools/applications to be installed in the Jumpbox.

cc- @user @user @user

Dependency on getting the EUC Team officially onboarded. Frank is working on this already.

cc- @user @user @user @user @user

The change request has been raised for DNS forwarding from on-premise DNS servers to the Azure Privatelink DNS. 

We have change freeze this week, so this CR will be actioned next week.

@user @user @user @user

@user Project Task Created PRJTASK0192572. David Strah is aware and actively on it.

Issue split into:
|CSCI-488|Capture the requirements for creating the Jumpbox for the developers (data engineers)|"
CSCI-421,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Sep/25 9:22 AM,12/Sep/25 6:36 PM,,Issue in parquet file load,Getting variable columns in Parquet file load for historical data for ILS / Apriss tables,,
CSCI-420,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 6:31 PM,07/Oct/25 2:11 PM,,APPRISS Modelling/Transformation,"h2. Summary

We need to identify any additional objects required for APPRISS that have not yet been ingested into EDP Snowflake.

h2. Context

Most tables from the StockDb and TransactionStorage databases have already been ingested. This issue focuses on ensuring that all necessary objects for APPRISS are accounted for.

h2. Other information

N/A

Databases covered:

* transactionstorage
* stockdb
* general_reference","We need to set up pipelines for initial and delta loads for the following APPRISS tables

* Extract meta created
* Linked services created
* pipelines set up
* initial load tested
* delta load tested
* pipeline run from end to end","cc @user

Hi @user , 

Currently I am awaiting access to confluence documentation created by Amit. Thanks for raising the access request to get started.

Hi @user, @user 

We need the source to target transformation mapping document (which is worked by Amit) to get started. Also, Amit is currently creating the APPRISS data mart (subset of enterprise data model) if APPRISS needs to be prioritized.

 FYI.. @user , @user @user

Hey @user , @user ,

What is the actual purpose of this ticket.

My understanding is @user has completed data modelling work, is this correct? This ticket is to just review with Amit and NOT to do any actual DATA OUT build yeh?

Thanks,

Harrison

Hi @user,

Your understanding is correct. This is to do initial review and start putting initial questions. No build work at the moment. At the moment Amit has provided the mapping sheet for facts ( Fact Sales, electronic payments etc) but not the dimensions yet. Also, he is yet to provide APPRISS data model for us to get started.

FYI.. @user"
CSCI-419,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 6:28 PM,07/Oct/25 9:54 AM,,APPRISS Objects ingestion - Additional tables Ingestion Requirement,"h2. Summary

We need to identify any additional objects required for APPRISS that have not yet been ingested into EDP Snowflake.

h2. Context

Most tables from the StockDb and TransactionStorage databases have already been ingested. This issue focuses on ensuring that all necessary objects for APPRISS are accounted for.

h2. Other information

N/A

Databases covered:

* transactionstorage
* stockdb
* generalstorage","We need to set up pipelines for initial and delta loads for the following APPRISS tables

* -Extract meta created-
* -Linked services created-
* pipelines set up
* initial load tested
* delta load tested
* pipeline run from end to end","transactionstorage
stockdb
generalstorage

Hi @user ,

The historic data ingestion for below 2 tables are done through parquet files. However, There is an ongoing issue with ingestion framework where some of the columns getting missed if they contain null in first row. So, at the moment we have added the missing columns manually. Hence there is a need to test these columns getting populated properly when we manually add these in snowflake tables and views. 

|TDB14|TransactionStorage|TRANSACTIONS_ELECTRONICPAYMENTS|

|TDB14|TransactionStorage|TransactionAuditSaleActivityLogHistory|

@user following up with @user about details for additional tables today.

@user cc @user

Hi @user ,

As discussed yesterday, please provide the list of additional tables required after your review with Bhavya and team.

FYI. @user, @user

Hi @user,

I tested the end to end load for TRANSACTIONS_ELECTRONICPAYMENTS & TransactionAuditSaleActivityLogHistory after the issue of missing column got fixed. 

@user FYI.. There was an ongoing issue where in the historic data load, some of the columns were getting missed in snowflake tables as these columns were empty in the first row. Now the issue is resolved and hence the need for end to end testing.

@user , Hopefully the final remaining list of tables have been finalized for me to get started today.

@user - have these tables been finalised?

Thanks

Hi @user , @user

Amit had indicated that there are some more tables to be ingested while he was doing the mapping documents. However, he is yet to provide the details if any table is missing and needs to be ingested.

Hi @user,

I have received the additional table names from stockDb for ingestion from Amit. However, he advised us to double verify the server details with Bhavya/Adeel.

Hi @user / @user ,

Currently we are ingesting the stock DB data from following server. Amit asked us to verify if this is the right server as he he is not very sure if this is the right one or not. FYI, the below one was identified much earlier (Adeel is aware of). Please double confirm this information.

|Server|Database|Table|Total Row Count¬†|¬†Table Size(MB)¬†|¬†Table Size(GB)¬†|
|TDB08AX2012|StockDb|BranchInfoHierarchy|899|*|*|
|TDB08AX2012|StockDb|BuyByPg|275|*|*|
|TDB08AX2012|StockDb|Buyers|46|*|*|
|TDB08AX2012|StockDb|Catalogues|12,483|1|*|
|TDB08AX2012|StockDb|DimensionProductView|6|*|*|
|TDB08AX2012|StockDb|Dimensions|407,993|55|*|
|TDB08AX2012|StockDb|DimensionType|61|*|*|
|TDB08AX2012|StockDb|DimensionValues|12,807|1|*|
|TDB08AX2012|StockDb|LockieMainGroups|56|*|*|
|TDB08AX2012|StockDb|MultiBuy|18,066|1|*|
|TDB08AX2012|StockDb|MultiBuyTriggers|18,359|1|*|
|TDB08AX2012|StockDb|ProductGroup|168|¬†|¬†|
|TDB08AX2012|StockDb|ProductNetworkCosts|126,223|23|*|
|TDB08AX2012|StockDb|ProductNetworkCostsHistory|136,546,828|16,037|15|
|TDB08AX2012|StockDb|Products|196,344|103|*|
|TDB08AX2012|StockDb|ProductsDrugMatch|1,535|*|*|
|TDB08AX2012|StockDb|SOHAdjustmentTypes|22|¬†|¬†|
|TDB08AX2012|StockDb|SubGroup|275|*|*|
|TDB08AX2012|StockDb|Supplier|294,671|71|*|
|TDB08AX2012|StockDb|SupplierDetails|2,837|*|*|
|TDB08AX2012|StockDb|StoreStaffDetails|97,048| | |
|TDB08AX2012|StockDb|BranchInfoGlobal|760| | |
|TDB08AX2012|StockDb|CatalogueItems| | | |
|TDB08AX2012|StockDb|CatalogueMeta| | | |
|TDB08AX2012|StockDb|CatGroup| | | |
|TDB08AX2012|StockDb|CatalogueTypeDefinition| | | |

FYI .. @user

Hi @user/ @user,

We got confirmation from @user yesterday saying the server details are correct mentioned earlier in this ticket. So we will be adding these new tables from this server only.

 @user FYI.."
CSCI-418,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 6:25 PM,20/Oct/25 7:56 PM,,CWR Data Ingestion Architecture walk through,"Take Mike through the architecture in CWR (Snowflake, ADF, Altida)",,
CSCI-417,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 6:25 PM,23/Sep/25 9:48 AM,,AA - Detailed Design Documentation,,,"with @user at the moment.

[https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B2B1D2A9D-799B-4326-B9CF-52840FAFA912%7D&file=CWR%20-%20EDP%20Solution%20Architecture%20%20-%20Detailed%20Design%20v.01.docx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B2B1D2A9D-799B-4326-B9CF-52840FAFA912%7D&file=CWR%20-%20EDP%20Solution%20Architecture%20%20-%20Detailed%20Design%20v.01.docx&action=default&mobileredirect=true]"
CSCI-416,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 6:23 PM,25/Sep/25 9:49 AM,,EDP Architecture - Detailed Solution Design doco,"Provide a document of the EDP:

* High-level logical architecture (Interim Solution)
* High-level logical architecture (Future State Solution)
* Detailed design components of Azure, Snowflake across environments. 

*Agreed Timeline:*

* *Completed Detailed Design:* 19/09 (this Friday)
* *Alan Review + CT & HW Discussions:* 26/09 (end of next week)
* *Detailed Work Breakdown Plan:* 03/10",* -link to documentation-,"Walk-through meeting with Alan, Harrison and Xavier yesterday. We are largely aligned on the architecture and agreed on the timeline. 

Chloe to add the below information: 

* High level transition plan and effort estimation from DLA Transformation to dbt Data Tranformation.
* Design Snowflake‚Äôs Organisation and Account Structure for Chemist Warehouse International.
* Azure DevOps GIT Repos (how many repos and what are they?)
* Data Engineering best practices (incl. naming convention in Snowflake, ADF, Azure DevOps)
* Attach Altis DLA handbook.

The doc is being reviewed by Alan, Xavier and Harrison. 

Initial feedback by Xavier:

* To include Snowflake OpenFlow and Snowflake Intelligence as part of the platform solution design for Future State"
CSCI-415,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 6:23 PM,12/Sep/25 6:25 PM,,CT - Detailed Design Documentation,,,hey @user i think this would be the same as [https://sigmahealthcare.atlassian.net/browse/CSCI-148|https://sigmahealthcare.atlassian.net/browse/CSCI-148] but just wanted to double check
CSCI-414,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 6:10 PM,10/Oct/25 3:00 PM,,Infrastructures As Code in EDP - Walk-through and Documentation,"h2. Summary

Eugene will conduct a walk-through of the Infrastructure As Code (IAC) implementation with Alan. This is scheduled for Tuesday, 17th September. Documentation of the deployment process for Azure services and environments is also required.

h2. Context

The issue involves the implementation of Infrastructure As Code within the EDP. It includes a scheduled meeting for a detailed walk-through and the need for comprehensive documentation.

h2. Acceptance criteria

* Eugene to walk through IAC implementation with Alan on Tuesday, 17th September.
* Document the deployment of Azure services and environments.

h2. Other information

The documentation has been created at [https://dev.azure.com/MyChemist/Enterprise Data Platform Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5263/Snowflake-Subscription|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5263/Snowflake-Subscription]",Infrastructure As Code implementation is documented by Eugene and reviewed by Alan and Mike,"@user

@user i don‚Äôt have access to the Sharepoint. Also, i think it would be best to document the IAC in the code itself and create a separate readme file. Same readme file can be stored in Wiki.

linking this to [Feature 209127 Document Environment IAC|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/209127]

Thanks @user

----

h3. üîß *Architecture Overview*

* Snowflake deployed securely on Microsoft Azure using a hub-spoke network model.
* Supports Dev, UAT, and Prod environments with private connectivity and centralised monitoring.
* Designed for scalability, compliance, and high availability.

----

h3. üõ°Ô∏è *Security Principles*

* Zero Trust Network: All traffic via private endpoints.
* Network isolation with dedicated VNets and controlled peering.
* TLS 1.2+ encryption for data at rest and in transit.
* Managed identities for secure service-to-service authentication.

----

h3. ‚öôÔ∏è *High Availability & Governance*

* Geo-redundant storage (RAGRS) across regions.
* Hub-spoke design enables resilient failover.
* Resource tagging, management locks, and Azure Policy for governance.
* Centralised monitoring via Log Analytics at the management group level.

----

h3. üåê *Network Architecture*

* Hub subscription connects to Dev, UAT, Prod, and Snowflake via VNet peering.
* Snowflake subscription includes dedicated subnets for endpoints, storage, and VMs.
* Cross-subscription private endpoints enable secure access to Snowflake from all environments.

----

h3. üóÇÔ∏è *Subnet & Resource Group Design*

* Subnets allocated for Snowflake, storage, VMs, and future expansion.
* Separate resource groups for application, network, and security components.
* Key Vault integrated for secrets and certificates.

----

h3. üîÑ *Data Flow & Integration*

* Data ingested into environment-specific Data Lakes (Dev/UAT/Prod).
* Processed via Azure Data Factory and Self-hosted Integration Runtime (SHIR).
* Integrated into shared Snowflake warehouse for analytics and reporting.
* Exported to local storage for specific use cases.

----

h3. üîê *Network Security Controls*

* NSGs enforce strict traffic rules (allow trusted sources, deny all else).
* Private endpoints for Snowflake, storage, and Key Vault.
* Hub-managed private DNS zones with automatic registration and conditional forwarding.

----

h3. üë§ *Identity & Access Management*

* System- and user-assigned managed identities for secure access.
* RBAC roles assigned for contributors, network admins, and security teams.
* Key Vault access managed via identities and private endpoints.

----

h3. üì¶ *Storage Architecture*

* Local Data Lake Storage Gen2 for staging and exports.
* Environment-specific Data Lakes for ingestion and transformation.
* RAGRS replication and private endpoint-only access.
* Managed identities enable cross-subscription access to Snowflake.

----

h3. üìä *Monitoring & Observability*

* Azure Policy enforces diagnostic settings and Log Analytics across all resources.
* Monitors network flow logs, storage operations, Key Vault access, and Snowflake connectivity.
* Centralised logging reduces complexity and ensures compliance."
CSCI-413,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 6:09 PM,12/Sep/25 2:46 PM,,ExpressRoute Monitoring - (15 Sep - 29th Sep),Monitor ExpressRoute throughput utilisation,"# *Metric Visibility:*
#* ExpressRoute connection metrics (e.g., Ingress/Egress throughput) are visible in Azure Monitor or Network Insights.
#* Bandwidth usage can be correlated with ADF pipeline execution timelines.
# *Alerting Rules:*
#* Alerts are configured to trigger when bandwidth usage exceeds a defined threshold (e.g., 70% of provisioned bandwidth).
#* Alerts include pipeline metadata (e.g., pipeline name, run ID) when possible, to trace high-usage executions.
# *Log Collection:*
#* Network bandwidth logs (e.g., from Network Performance Monitor or Azure Network Watcher) are retained for a minimum of 30 days.
#* ADF pipeline run logs include integration runtime IPs or endpoints for cross-reference.
# *Validation Testing:*
#* At least one high-throughput ADF pipeline is executed to validate that bandwidth spikes are captured and logged as expected.
#* Test scenarios simulate concurrent pipeline runs to verify system responsiveness and alert thresholds.
# *Governance & Review:*
#* A monthly review is conducted to assess if bandwidth consumption patterns require scaling up/down of ExpressRoute or ADF optimization.
#* A weekly review to be initially done on until Oct 2025.",
CSCI-412,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 4:00 PM,24/Oct/25 1:02 PM,,Create User Guide for using ServiceNow to create tickets,Create user guide that will alllow staff to submit servicenow tickets,"* confluence page for Servicenow access
* Peer reviewed with @user","hi @user - I recommend to do this asap to avoid further communication issues amongst teams re: networking drive whitelisting re [https://sigmahealthcare.atlassian.net/browse/CSCI-458|https://sigmahealthcare.atlassian.net/browse/CSCI-458]

Hey @user 
Can you please prioritize this? I would suggest you note down the step when you are raising any new serviceNow ticket

Thanks"
CSCI-411,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 3:58 PM,12/Sep/25 2:24 PM,,Test task 1,test text,,
CSCI-409,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 11:51 AM,02/Oct/25 10:03 AM,,MergeCo Conformed Reporting - Draft Dashboard Wireframe,"Once dashboard requirements are set and confirmed by the business, need to create a dashboard wireframe.

The wireframe will outline how the dashboard will look, what functionality will be available (eg: what filters available and any other interactive features).

Wireframe will also include definition of metrics within the dashboard and any cadences around dashboard refresh, and email subscriptions.",,"@user for availability wireframe initially

Issue split into:
|CSCI-486|MergeCo Conformed Reporting - Draft Dashboard Development|
|CSCI-487|MergeCo Conformed Reporting - Draft Dashboard Testing|

Marked as done."
CSCI-408,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 11:49 AM,29/Sep/25 8:23 AM,,MergeCo Conformed Reporting - Draft Solution Design,"Create draft of the solution design for the Merge Co Conformed reporting.

Just needs to be a one-page document, outlining *how* we will build and ingest the data for this reporting.

Includes:

* What grain of data required for each metric
* How we will push data for each environment into a join environment
* Merging them into a common dimension",,
CSCI-407,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 11:45 AM,29/Sep/25 8:23 AM,,MergeCo Conformed Reporting - Identify Source for SC Operational Metrics,"Identify which specific tables/views to use for Merge Co reporting from a CW domain for the below Operational metrics:

* Order Qty
* Confirmed Qty
* Invoiced Qty
* ATP %
* DIF %
* Cust Impact %
* Availability Opportunity Qty/$
* DC Opportunity Qty
* DOT %

Ideal grain is at the: 

per day, per DC, per product level

Need to work out:

* Which tables best to extract from
* If tables can‚Äôt be identified, why? Is it not available at all? Or is there an issue with the grain of data? Or access to data?",,"|Sigma Metrics|CW Equivalent Available?|PBI Report|Notes|Table|Notes 2|
|Order Qty|Y*| |no clear definition of order qty - some dashboards (SPS KPIs) use PDB08 as definition, other use Scale as definition (KPI Dashboard)| | |
|Confirmed Qty|N*| |currently not defined by business| | |
|Invoiced Qty|Y*| |no clear definition of order qty - some dashboards (SPS KPIs) use AX as definition, other use Scale as definition (KPI Dashboard)| | |
|ATP %|N| | |-| |
|DIF %|Y*|DC KPI Dashboard|within CW known also as FillRate|[SupplyChain].[Scale].[shipment_detail].[TOTAL_QTY] (delivered Qty)
/
[SupplyChain].[Scale].[shipment_detail].[REQUESTED_QTY]|**only contains SCALE data - no active (therefore no QLD DC data) - current source references the MySQL DB|
|DIF %|Y*|Complicance & Governance|is based on supplier + PO information - assume this refers to inbound orders (not outbound)|Refers to PDB08 objects**| |
|DIF %|Y*|SPS KPIs|is based on supplier + PO information - assume this refers to inbound orders (not outbound)|Refers to PDB08 objects**| |
|Cust Impact|N| | |-| |
|Availability Opp $|N| | |-| |
|DC Opportunity Qty|N| | |-| |
|DOT %|Y*|DC KPI Dashboard|current definition different to Sigma (within 2 days CW vs 1 day Sigma)|On Time Qty = where diff between [ACTUAL_SHIP_DATE_TIME] and [SCHEDULED_SHIP_DATE] on table [SupplyChain].[Scale].[shipment_header] is <= 2 days (factors in excluding weekends)
/
[SupplyChain].[Scale].[shipment_detail].[TOTAL_QTY] (delivered Qty)|**only contains SCALE data - no active (therefore no QLD DC data) - current source references the MySQL DB|
|DOT %|Y*|Complicance & Governance|is based on supplier + PO information - assume this refers to inbound orders (not outbound)|Refers to PDB08 objects**| |
|DOT %|Y*|SPS KPIs|is based on supplier + PO information - assume this refers to inbound orders (not outbound)|Refers to PDB08 objects**| |

Issue split into:
|CSCI-497|MergeCo Conformed Reporting - Identify Source for SC Operational Metrics - Sprint 9|"
CSCI-406,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 11:42 AM,12/Sep/25 6:32 PM,,MergeCo Conformed Reporting - Identify Source for Availability Metrics,"Identify which specific tables/views to use for Merge Co reporting from a CW domain for the below availability metrics:

* Availability %
* TOS %
** Temporarily Out of Stock
* MCS %
** Manufacturer Can‚Äôt Supply
* SOH $
** Stock on Hand
* DOI
** Days of Inventory - within CW also known as DIH (Days In Hand)

Ideal grain is at the: 

per day, per DC, per product level

Need to work out:

* Which tables best to extract from
* If tables can‚Äôt be identified, why? Is it not available at all? Or is there an issue with the grain of data? Or access to data?",,
CSCI-405,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 11:38 AM,12/Sep/25 5:12 PM,,MergeCo Conformed Reporting - Update Notes to Tactical Reporting Slides,"After scoping what KPI metrics are available within the CW domain - need to add my notes to the ‚ÄúTactical Reporting‚Äù slides created by Harrison:

[https://sigmacompanylimited.sharepoint.com/:p:/r/sites/EnterpriseDataReportingPlatformproject/_layouts/15/doc2.aspx?sourcedoc=%7B11de107f-f7a5-4028-8bf6-c52457bec338%7D&action=edit&wdPreviousSession=7d624487-be28-0f92-a297-ea52b70d0d8a&previoussessionid=cd8ab3a8-4459-49b6-4d8b-e08e28bec6ea|https://sigmacompanylimited.sharepoint.com/:p:/r/sites/EnterpriseDataReportingPlatformproject/_layouts/15/doc2.aspx?sourcedoc=%7B11de107f-f7a5-4028-8bf6-c52457bec338%7D&action=edit&wdPreviousSession=7d624487-be28-0f92-a297-ea52b70d0d8a&previoussessionid=cd8ab3a8-4459-49b6-4d8b-e08e28bec6ea]",,
CSCI-404,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 9:50 AM,11/Sep/25 10:00 AM,,SSO/ SCIM - Snowflake AD Groups definition,To design and confirm the AD Groups for SSO/SCIM mapping with Snowflake purpose.,,"Confirmed with Alan/ Raveen the below AD Groups and mapping is to be implemented:

|*AD Groups*|*Default Role*|
|*Snowflake_CW_AU_Consultants*|DEV_ROLE_CONSULTANT|
|*Snowflake_CW_AU_BusinessUsers*|UAT_ROLE_BUSINESS_READONLY|
|*Snowflake_CW_AU_DataAnalysts*|UAT_ROLE_ANALYST|
|*Snowflake_CW_AU_DataEngineers*|DEV_ROLE_DATA_ENG|
|*Snowflake_CW_AU_PlatformEngineers*|DEV_ROLE_PLATFORM_ENG|
|*Snowflake_CW_AU_IT_Audit*|IT_AUDIT|"
CSCI-403,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 9:44 AM,11/Sep/25 9:58 AM,,Snowflake - Single vs Multiple Account design,Get alignment with Alan on having Single vs Multiple Account design for Snowflake environments,Decision to be made on which design to be implemented for EDP,
CSCI-402,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 9:27 AM,10/Oct/25 12:10 PM,,"Review Source-to-Target Map for Dim_DC part 2 - DimWarehouse DimWarehouseLocation, DimZone & BridgeWarehouseLocationZone",Review the mapping of DIM_DC Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=XQdnyO&nav=MTVfe0Q4MDZEQUY3LTJCQzUtNDRBMy1CNDEwLUNBMEE4RjVFQkYwNX0],"* Finalize that the work on Source-to-Target Map for DIM_DC is correct.
** Review the Source-to-Target Map for DIM_DC against the current production environment.
** Update the mapping file to reflect any changes or corrections found.
** Peer review completed and signed off by Chloe/Harrison
** Communicate changes of mapping file to the Jira/standup for traceability.","@user This has been broken down to: DimWarehouse, DimWarehouseLocation, DiimZone & BridgeWarehouseLocationZone. Need @user to review my model designs in Confluence.

@user would you like me to create this (create Source to target Mapping) card?

{quote}DimWarehouse, DimWarehouseLocation, DiimZone & BridgeWarehouseLocationZone.{quote}

@user Up to you, personally I would just leave it as one card with multiple tasks. But no preference either way for me.

@user OK i‚Äôll leave it, will just change title for now."
CSCI-401,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 9:26 AM,20/Nov/25 9:50 AM,,Review Source-to-Target Map for Dim_PO part 2,Review the mapping of DIM_PO Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=KnX8aA&nav=MTVfezA1OUMyOEU2LTI4QTQtNDY2Ny1CODMzLUIwQUM0NDVFODBDNH0],"* Finalize that the work on Source-to-Target Map for DIM_PO is correct.
** Review the Source-to-Target Map for DIM_PO against the current production environment.
** Update the mapping file to reflect any changes or corrections found.
** Peer review completed and signed off by Chloe/Harrison
** Communicate changes of mapping file to the Jira/standup for traceability.","@user I have changed this to FactPOLineItem, need @user to review my design in Confluence.

To Review after Meeting (20251014)

Dim PO is not required for our Supply Chain Data Model.
This Dim can be removed as part of phase 1 delivery - if there‚Äôs a specific requirement that requires a Dim PO in future we can look to reinstate this."
CSCI-400,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 9:26 AM,23/Sep/25 9:29 AM,,Review Source-to-Target Map for Dim_location part 2,Review the mapping of DIM_LOCATION Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=XQdnyO&nav=MTVfe0Q4MDZEQUY3LTJCQzUtNDRBMy1CNDEwLUNBMEE4RjVFQkYwNX0],"* Finalize that the work on Source-to-Target Map for DIM_LOCATIONis correct.
** Review the Source-to-Target Map for DIM_LOCATION against the current production environment.
** Update the mapping file to reflect any changes or corrections found.
** Peer review completed and signed off by Chloe/Harrison
** Communicate changes of mapping file to the Jira/standup for traceability.","Renamed to Dim_address
Added Dim_Country and Dim_currency

 @user @usercc @user"
CSCI-399,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 9:26 AM,10/Oct/25 12:10 PM,,Review Source-to-Target Map for Dim_Date part 2,to have a discussion with you @user about this task.,* Finalize that the work on Source-to-Target Map for DIM_DATE is correct.,
CSCI-398,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Sep/25 9:25 AM,29/Sep/25 9:12 AM,,Review Source-to-Target Map for Dim_Employee part 2,Review the mapping of DIM_Employee Dimension: DIM_Employee,"* Finalize that the work on Source-to-Target Map for DIM_Employee correct.
** Review the Source-to-Target Map for DIM_Employee against the current production environment.
** Update the mapping file to reflect any changes or corrections found.
** Peer review completed and signed off by Chloe/Harrison
** Communicate changes of mapping file to the Jira/standup for traceability.","@user to review cc @user

@user I have completed model design for this."
CSCI-394,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,09/Sep/25 3:20 PM,11/Sep/25 11:38 AM,,Summary of KPIs from businesses that are available on the platform.,Looking at KPIs and what‚Äôs already available in Snowflake,"To pick available KPIs available on Presentation Layer, and eventually structured in the Gold layer when in medallion structure.",
CSCI-391,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,08/Sep/25 8:49 AM,12/Sep/25 6:20 PM,,Extract the parquet for AR_Transaction_History,"*Description*

Extract the parquets files for these tables

* TDB15.ILS.dbo.AR_Transaction_History

h2.","* -Extract the parquet files-
* -File validation-
* -Upload it in Azure blob storage-
* -Post-upload verification-","@user @user 

Parquet files have been uploaded to Azure Storage.

[parquetfiles - Microsoft Azure|https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Fdc516e92-8716-44f9-b09c-fc5ca9cdd01a%2FresourceGroups%2Fcwr-ase-dpdev-rg%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Fcwraseedpdevst/path/parquetfiles/etag/%220x8DD4FE2D7009ECA%22/defaultId//publicAccessVal/None]"
CSCI-390,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,08/Sep/25 8:29 AM,12/Sep/25 6:13 PM,,RITM0174658 - - Ashutosh Jumpbox access to CW BLOB storage,Phil to raise servnicenow ticket for Ashutosh Jumpbox access to CW BLOB storage,Jumpbox ‚Üí Blob Access availble for Ashutosh,still in progress
CSCI-389,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,08/Sep/25 8:29 AM,12/Sep/25 6:13 PM,,RITM0174658 - Aswini access to CW BLOB storage,PHil to raise in Servicenow direct access to CW BLOB storage for Aswini,Jumpbox ‚Üí Blob Access availble for Aswini,"still in progress

Issue split into:
|CSCI-430|RITM0174658 - Aswini and Ashutosh access to CW BLOB storage|"
CSCI-388,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,02/Sep/25 12:41 PM,12/Sep/25 6:36 PM,Ingestion,Manhattan Scale data ingestion,"Import the data for ILS tables

location_inventory

AR_Transaction_History

Transaction_History

* Create extract meta records
* Create & Run end to end pipelines in ADF",,"Delta in progress. 

Waiting for @Adeel to get access to start historical upload.

Historical upload in progress

Issue split into:
|CSCI-426|Manhattan Scale data ingestion - part 2|"
CSCI-387,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,02/Sep/25 11:40 AM,12/Sep/25 6:20 PM,,Review Source-to-Target Map for Dim_PO,Review the mapping of DIM_PO Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=KnX8aA&nav=MTVfezA1OUMyOEU2LTI4QTQtNDY2Ny1CODMzLUIwQUM0NDVFODBDNH0],"* Finalize that the work on Source-to-Target Map for DIM_PO is correct.
** Review the Source-to-Target Map for DIM_PO against the current production environment.
** Update the mapping file to reflect any changes or corrections found.
** Peer review completed and signed off by Chloe/Harrison
** Communicate changes of mapping file to the Jira/standup for traceability.",
CSCI-386,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,02/Sep/25 11:40 AM,12/Sep/25 3:24 PM,,CWRetail EDP - Details Tech Design - 2/9 - 14/9,Work on Detailed Tech Design for EDP CWRetails .,,"Making Physical Diagram of Snowflake, ADF and combined"
CSCI-385,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,02/Sep/25 9:56 AM,12/Sep/25 6:32 PM,,Silver model for facts,"Develop a Silver data model for Facts, transforming and integrating data from the Bronze layer to provide a cleansed, structured, and business-ready dataset. 

The Silver model should address data quality issues, apply necessary business logic, and ensure consistency for downstream analytics and reporting.","* -Silver (Kimball Medallion model) for Facts to be constructed.-
* -Diagram in tool i.e. [http://draw.io|http://draw.io] -
* peer reviewed
* Documentation
* Stakeholder Sign-off","Facts covered:

* fact_sales_retail
* fact_sales_retail_electronic_payments
* Fact_sales_audit_log_history

Fct_Retail_Sales completed with Columns and definitions
FCT_Sales_Retail_Electronic_Payments completed with Columns and definitions

All Fact tables definitions are completed

FCT_Sales_Retail 

FCT_Sales_Retail_Electronic_Payments
FCT_Transactions_Audit_History

Need to review with Harrison and Alan

Structure is Updted in Merge Co Folder

[Business Matrix_Retail - Simplified.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Business%20Matrix_Retail%20-%20Simplified.xlsx?d=w0e262bf7be84407d8dbff2bb4d4c5e00&csf=1&web=1&e=dGjeuu]

Issue split into:
|CSCI-437|Silver model for facts - review for Alan|"
CSCI-384,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,02/Sep/25 9:56 AM,12/Sep/25 9:49 AM,,Silver model for Dims,"Develop a Silver data model for Dims, transforming and integrating data from the Bronze layer to provide a cleansed, structured, and business-ready dataset. 

The Silver model should address data quality issues, apply necessary business logic, and ensure consistency for downstream analytics and reporting.","* -Silver (Kimball Medallion model) for Dims to be constructed.-
* -Diagram in tool i.e. draw.io-
* -peer reviewed-
* Documentation
* Stakeholder Sign-off","Workshop for Silver Model for Product and Store is complete now. Agreed data model will be published on Confluence soon.

Store dim is complete and is available on Confluence
Product Dim final review complete. Suggestions to be incorporated and made available in Confluence

@user - to update on details on what dims are covered

Hey @user - do you have links to the [draw.io|http://draw.io] diagram?

Dim tables Silver Layer Structure is updated in Confluence
[https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model|https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model] . Tool used is Lucid Chart

Review completed with Alan. All good to go."
CSCI-383,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,02/Sep/25 8:42 AM,12/Sep/25 6:20 PM,,Extract parquet for AR_SHIPMENT_DETAIL table,"Extract the parquets files for these tables

* AR_SHIPMENT_DETAIL","* -Extract the parquet files-
* -File validation-
* -Upload it in Azure blob storage-
* -Post-upload verification-","Extracted the parquet for AR_shipment Detail, 

Validated the files, sizes and row count

Uploaded the parquet extracts in the blob storage

CC: @user @user"
CSCI-382,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Sep/25 9:50 AM,10/Sep/25 4:29 PM,,DAta ingestion - SPSWHSPurchase,"Ingest data in the below tables from the database SPSWHSPurchase.

|TDB08AX2012|SpsWhsPurchase|POInvoiceLine|781|
|TDB08AX2012|SpsWhsPurchase|POInvoiceTable|251|
|TDB08AX2012|SpsWhsPurchase|PurchReqLine|1,692|
|TDB08AX2012|SpsWhsPurchase|PurchReqTable|368|
|TDB08AX2012|SpsWhsPurchase|PurchStatus|83|
|TDB08AX2012|SpsWhsPurchase|VendorType|75|","* -Insert records in extract Meta-
* -Create linked services -
* -Configure and run end to end pipelines-",
CSCI-381,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Sep/25 2:30 AM,19/Nov/25 11:46 AM,,Implement Snowflake CD pipelines,Review output of [https://sigmahealthcare.atlassian.net/browse/CSCI-151|https://sigmahealthcare.atlassian.net/browse/CSCI-151] and [https://sigmahealthcare.atlassian.net/browse/CSCI-159|https://sigmahealthcare.atlassian.net/browse/CSCI-159],,"I‚Äôll reach out for what‚Äôs covered in part 3 @user

This is to pick up from what was left in late July. @user

Chloe and Eugene were working on the Release pipeline for Snowflake which follows the sequenced logic as fleshed out in the edp-snowflake-release.

Next step: Eugene to review and fix the PowerShell command scripts."
CSCI-380,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Sep/25 2:29 AM,12/Sep/25 6:25 PM,,ExpressRoute Monitoring - (1 Sep - 15th Sep),Monitor ExpressRoute throughput utilisation,"# *Metric Visibility:*
#* ExpressRoute connection metrics (e.g., Ingress/Egress throughput) are visible in Azure Monitor or Network Insights.
#* Bandwidth usage can be correlated with ADF pipeline execution timelines.
# *Alerting Rules:*
#* Alerts are configured to trigger when bandwidth usage exceeds a defined threshold (e.g., 70% of provisioned bandwidth).
#* Alerts include pipeline metadata (e.g., pipeline name, run ID) when possible, to trace high-usage executions.
# *Log Collection:*
#* Network bandwidth logs (e.g., from Network Performance Monitor or Azure Network Watcher) are retained for a minimum of 30 days.
#* ADF pipeline run logs include integration runtime IPs or endpoints for cross-reference.
# *Validation Testing:*
#* At least one high-throughput ADF pipeline is executed to validate that bandwidth spikes are captured and logged as expected.
#* Test scenarios simulate concurrent pipeline runs to verify system responsiveness and alert thresholds.
# *Governance & Review:*
#* A monthly review is conducted to assess if bandwidth consumption patterns require scaling up/down of ExpressRoute or ADF optimization.
#* A weekly review to be initially done on until Oct 2025.",
CSCI-379,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Sep/25 2:18 AM,12/Sep/25 6:20 PM,,Extract parquet for TransactionAuditSaleActivityLog - part 2,"Extract the parquets files for these tables 

* TransactionStorage.dbo.TransactionAuditSaleActivityLog
* TransactionStorage.dbo.TransactionAuditSaleActivityLogHistory
* TransactionStorage.dbo.Transactions_ElectronicPayments","* -Extract the parquet files-
* -File validation-
* -Upload it in Azure blob storage-
* -Post-upload verification-","Uploaded the parquet extracts in the blob storage

cc @user @user"
CSCI-378,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Sep/25 2:17 AM,12/Sep/25 6:20 PM,,Create Source-to-Target Map for Dim_PO - part 2,Source to target mapping for Dim_PO - filling out sheet as per Source-to-target dims,"* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no","Completed the DIM_PO mapping [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=U4d8Ue&nav=MTVfezA1OUMyOEU2LTI4QTQtNDY2Ny1CODMzLUIwQUM0NDVFODBDNH0]

Create review ticket
[https://sigmahealthcare.atlassian.net/browse/CSCI-387|https://sigmahealthcare.atlassian.net/browse/CSCI-387]"
CSCI-377,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Sep/25 2:14 AM,12/Sep/25 6:22 PM,,Discussion - how we should approach getting specs/info on MAWM visibility - part 2,"Discussion - how we should approach getting specs/info on MAWM visibility

This is mainly captured in sessions with Chandan about where data lives for Manhattan Scale and Manhattan active.","Discussion - how we should approach getting specs/info on MAWM visibility

* Gaining insight on Scale and Active structure from Chandan‚Äôs sessions
* Documentation about Scale and Active","Hey @user - would it be OK to note what‚Äôs being captured from Chandan‚Äôs sessions here?

Thank you

Issue split into:
|CSCI-423|Discussion - how we should approach getting specs/info on MAWM visibility - part 2|"
CSCI-376,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Sep/25 2:13 AM,12/Sep/25 8:55 AM,,Review Source-to-Target Map for Fact_Store_Inventory_Snapshot,Source to target mapping for Fact_Store_Inventory_Snapshot - filling out sheet as per Source-to-target dims,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",Hey @user is this something we get @user to review? (much like @user 's Dim tables)
CSCI-375,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Aug/25 3:13 PM,11/Sep/25 9:57 AM,,ILS - Manhattan Scale Integration - Ingestion of remaining 7 tables - review,"DATA IN for critical identified objects from Manhattan Scale db (ILS)

Implement data ingestion for critical identity objects from the on-premises Manhattan Scale (ILS) database into the cloud data platform. This includes configuring the necessary Azure Data Factory (ADF) components to support automated and reliable data flow.

Latest update;

* Ticket raised. REQ0156142
* Currently with the infrastructure cloud team - Brent - Cloud services manager.
* !image-20250729-063227 (1ed4c70f-781d-46cb-9ced-b8db1e7f0691).png|width=961,height=959,alt=""image-20250729-063227.png""!
Below are the list of tables to be ingested. Out of these the first one in the list i.e. Carrier is ingested in phase 1.

|ILS|Carrier|
|ILS|Functional_area_status_flow|
|ILS|Item|
|ILS|Location|
|ILS|Location_inventory|
|ILS|Routing_guide|
|ILS|Shipment_detail|
|ILS|Shipment_header|","Definition of done:

* -Extract meta created ()-
* -Linked services created-
* -Pipeline run end to end-
* Whitelisting of on-prem prod database server. (Done by @user )","Hi @user ,

As we discussed on Friday, the tables Shipment_detail & Shipment_header contains only 15 days worth of data in source. So if we intend to load more history we may have to ingest their historic tables presented below highlighted by @user .

# AR_SHIPMENT_DETAIL
# AR_SHIPMENT_HEADER

Please help to get this confirmed and I can add these in current sprint.

@user confirmed with @user - these should be included.

Will start with delta part only"
CSCI-374,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Aug/25 3:12 PM,12/Sep/25 6:26 PM,,Understanding current architecture - please fill out coverage,"As an engineer - I would like to receive the current in future state of architecture so that I know what type of work I need to create and where my work is going towards (i.e. a goal)

Key areas:

* DLA
* @user - I might need your help to fill the rest of this out

Covered areas as per 12/9:

# Got a fair idea of how the ingestion framework (ADF +Snowflake ) from various source systems to snowflake stage works.
# Became aware of the branching strategy that are being discussed
# CI/CD is in initial stage at the moment . Confident to get acquainted with this when the approach is finalized etc.","* -receive documentation about the current architecture - now reviewing-
** receive documentation about architecture we are working towards
* -Ask relevant questions about where we currently are at to fill knowledge gaps-
* -Able to apply professional knowledge and opinion on current work-
* Able to leverage architecture knowledge to create own Jira tasks with acceptance criteria levera","Can you fill the rest of this out @user

# Got a fair idea of how the ingestion framework (ADF +Snowflake ) from various source systems to snowflake stage works.
# Became aware of the branching strategy that are being discussed
# CI/CD is in initial stage at the moment . Confident to get acquainted with this when the approach is finalized etc.

If this ticket is to cover above point only, then I am fine to complete this one. In case we need a separate ticket going forward, we can very well raise to cover other aspects not listed here."
CSCI-373,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Aug/25 3:10 PM,11/Sep/25 9:56 AM,,General Reference-Integration - Ingestion of 2 tables - review,"DATA IN for critical identified objects from General Reference db

Below are the list of tables to be ingested. 

|General_Reference|AXVendorDetails_EDP|
|General_Reference|BranchInfoGlobal_EDP|","Definition of done:

* -Extract meta created ()-
* -Linked services created-
* -Pipeline run end to end-
* Whitelisting of on-prem prod database server. (Done by @user )",
CSCI-372,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Aug/25 3:10 PM,12/Sep/25 6:25 PM,,APPRISS Objects ingestion - remaining talbes ingestion,"h2. Summary

We need to identify any additional objects required for APPRISS that have not yet been ingested into EDP Snowflake.

h2. Context

Most tables from the StockDb and TransactionStorage databases have already been ingested. This issue focuses on ensuring that all necessary objects for APPRISS are accounted for.

h2. Other information

N/A","We need to set up pipelines for initial and delta loads for the following APPRISS tables

* -Extract meta created-
* -Linked services created-
* -pipelines set up-
* -initial load tested-
* delta load tested
* pipeline run from end to end","starting on delta ingestion. cc @user

need to catch up with Adeel about access

@user Diafy Gerardo was working on these. I was told they should have finished by yesterday. Needs confirmation

@user cc @user

@user : Could you please confirm if access to Blob is sorted?

cc @user , @user

@user / @user - would it be OK to get the servicenow ticket number for this?

Thanks

@user Created a project task for it for the team. PRJTASK0189032. Diafy is aware of it and told me it shouldn‚Äôt take more than a couple of hours.

@user [RITM0173947 | Requested Item | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/sc_req_item.do%3Fsys_id%3Daf06e80f3323e21047764f945d5c7b20%26sysparm_view%3D]

FYI ticket complete

Aswini to get blob storage access as per next dependency discovered.

Request for Jumpbox for data engineers to use to access BLOB storage created. REQ0159213

Hi @user ,@ @user,

Historic data ingestion for TRANSACTIONS_ELECTRONICPAYMENTS , TransactionAuditSaleActivityLogHistory into snowflake is in progress. I am facing some data/ column issues which I will debug and reach out to @user to see how we can handle this issue in framework . Hopefully we will ingest these 2 tables this sprint.

Issue split into:
|CSCI-419|APPRISS Objects ingestion - remaining talbes ingestion part 2|"
CSCI-371,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Aug/25 2:41 PM,16/Sep/25 9:56 AM,,APPRISS Sample datashare - review,"Data share - Data Generated by CK - Manual data created, now need to be uploaded.
The sample data is placed under below share point location by CK. These sample data needs to be placed/loaded in data share.",* -Data is uploaded.-,
CSCI-370,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Aug/25 2:34 PM,12/Sep/25 2:48 PM,,Create an SSO approach with SCIM + SAML - part 2,"This task involves implementing secure Single Sign-On (SSO) and automated user/group provisioning between *Microsoft Entra ID* and *Snowflake*, using *SAML 2.0* and *SCIM v2*, with all traffic routed through *Azure Private Link* to eliminate public exposure.

The configuration includes:

* Creating and configuring the Snowflake Enterprise App in Entra ID.
* Establishing SAML trust and metadata exchange between Entra ID and Snowflake.
* Enabling SCIM API integration in Snowflake.
* Setting up automatic provisioning in Entra ID.
* Validating SSO and SCIM flows.
* Applying security best practices and ongoing maintenance.","* The Entra ID Enterprise Application for Snowflake is created and configured with correct SAML attributes.
* SAML metadata is exchanged and validated between Entra ID and Snowflake.
* SCIM API integration is successfully created in Snowflake with OAuth credentials.
* Automatic provisioning is enabled in Entra ID and tested for users and groups.
* SSO login via Entra ID (both IdP-initiated and SP-initiated) is functional.
* All traffic is confirmed to route through Azure Private Link, with no public endpoints.
* Logging and monitoring are enabled in both Snowflake and Entra ID.
* Security controls such as MFA, RBAC, NSGs, and certificate rotation are implemented.
* Documentation and disaster recovery procedures are updated and tested.","Feel free to cut down the AC if you wish @user

Document available at: [SCIM+SAML - Snowflake and Entra ID SSO Configuration via Azure Private Link - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5170/SCIM-SAML-Snowflake-and-Entra-ID-SSO-Configuration-via-Azure-Private-Link]"
CSCI-369,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Aug/25 11:06 AM,10/Sep/25 4:38 AM,,VDI change in direction - notes,"* Size VDI
** CReate VM
* Install tools inside VDI - 

Expecatation - for people to log into VDI for work.",VDI should be accessible to access privatelink and blob storage,
CSCI-368,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Aug/25 10:37 AM,30/Aug/25 3:10 PM,,General Reference-Integration - Ingestion of 2 tables - development,"DATA IN for critical identified objects from General Reference db

Below are the list of tables to be ingested. 

|General_Reference|AXVendorDetails_EDP|
|General_Reference|BranchInfoGlobal_EDP|","Definition of done:

* -Extract meta created ()-
* -Linked services created-
* -Pipeline run end to end-
* Whitelisting of on-prem prod database server. (Done by @user )","Ingestion of these 2 tables are done in snowflake dev.

Issue split into:
|CSCI-373|General Reference-Integration - Ingestion of 2 tables - review|"
CSCI-367,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 2:58 PM,27/Aug/25 3:00 PM,,202734 - Share Snowflake metadata with Entra Admin,"Provide metadata to complete trust setup.

_Steps:_

* Copy {{SAML2_SNOWFLAKE_METADATA}} value
* Share securely with Entra ID admin",,
CSCI-366,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 2:58 PM,27/Aug/25 3:00 PM,,202732 - Validate Integration,"Ensure Snowflake accepted configuration.

_Steps:_

* Run {{DESC SECURITY INTEGRATION ENTRA_ID_SSO}}
* Confirm values (Issuer, SSO URL, Cert) are correct",,
CSCI-365,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 2:57 PM,27/Aug/25 3:00 PM,,202731 - Create SAML Security Integration,"Define SAML integration with Entra ID.

_Steps:_

* Connect to Snowflake via Snowsight/CLI
* Run {{CREATE OR REPLACE SECURITY INTEGRATION}} with Tenant ID, SSO URL, certificate",,
CSCI-354,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 2:32 PM,10/Oct/25 5:42 PM,,202750 - Enable Provisioning,"[devops task 202750|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202750]

_Steps:_

* Set provisioning *On*
* Monitor sync progress in logs",,
CSCI-353,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 2:32 PM,10/Oct/25 5:42 PM,,202749 - Configure attribute mappings,"[devops task 202749|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202749]

_Steps:_

* Map user attributes (UPN ‚Üí userName, mail ‚Üí email, etc.)
* Optionally configure groups",,
CSCI-352,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 2:31 PM,10/Oct/25 5:42 PM,,202748 - Test Connection,"[devops task 202748|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202748]

_Steps:_

* Click *Test Connection*
* Verify success via PrivateLink",,
CSCI-351,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 2:31 PM,10/Oct/25 5:42 PM,,202747 - Input SCIM endpoint & secret,"[devops task 202747|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202747]

_Steps:_

* Paste Snowflake SCIM endpoint
* Paste {{OAUTH_CLIENT_SECRET}}",,
CSCI-350,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 2:31 PM,10/Oct/25 5:42 PM,,202746 - Configure provisioning mode = Automatic,"[devops link 202746|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202746]

_Steps:_

* Open Snowflake Enterprise App ‚Üí *Provisioning*
* Select *Automatic*",,
CSCI-345,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 1:27 PM,02/Sep/25 4:00 AM,,203066 - Add DNS entries to privatelink.snowflakecomputing.com,,,
CSCI-344,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 1:26 PM,02/Sep/25 4:01 AM,,202774 - Add NSG outbound rule to service tag AzureActiveDirectory from snet-vm,,,
CSCI-343,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 1:26 PM,11/Sep/25 9:32 AM,,202739 - Troubleshoot errors if required,"Resolve SAML issues.

_Steps:_

* Check Entra Sign-in logs
* Validate DNS points to Private Endpoint
* Re-check certificate format",,Hi @user - just want to know where this task is up to. Thanks.
CSCI-342,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 1:26 PM,27/Aug/25 1:29 PM,,202738 - Test SP-initiated login,"Confirm login works via Snowflake portal.

_Steps:_

* Open PrivateLink Snowflake URL
* Select ‚ÄúSign in with SSO‚Äù
* Verify access granted",,
CSCI-341,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 1:26 PM,27/Aug/25 1:31 PM,,202737 - Test IdP-initiated login,"Confirm login works via MyApps.

_Steps:_

* Sign into MyApps portal
* Launch Snowflake app ‚Üí expect MFA + successful login",,
CSCI-340,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 1:26 PM,19/Sep/25 3:47 PM,,202736 - Raveen - Update Entra SAML Config with Snowflake metadata,"Finalize trust with Snowflake metadata.

_Steps:_

* Update Identifier/Reply URL with Snowflake metadata
* Save changes in Entra ID",,
CSCI-339,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 1:18 PM,03/Sep/25 9:54 AM,,"202733 - Document Tenant ID, App ID, SAML URLs","[202733|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202733]

Capture values for use in Snowflake config.

_Steps:_

* Note Tenant ID (GUID)
* Note Application (client) ID
* Store in shared secure documentation (e.g., OneNote, Key Vault)",,
CSCI-338,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 1:18 PM,03/Sep/25 9:53 AM,,TASK 202729 202729 Extract Entra ID Certificate and Metadata,"[202729|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202729]

Export required certificate for Snowflake.

_Steps:_

* Download *Base64 certificate*
* Copy *App Federation Metadata URL*",,
CSCI-337,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 1:18 PM,03/Sep/25 9:52 AM,,202728 Add SAML Claims,"[202728 |https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202728]

Map Entra user attributes to Snowflake claims.

_Steps:_

* Add claims for email, givenName, surname, and UPN
* Verify values match Snowflake requirements",,
CSCI-336,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 1:17 PM,03/Sep/25 9:51 AM,,202727 Configure Basic SAML Settings,"[202727|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202727]

Ensure SAML endpoints use PrivateLink.

_Steps:_

* Open the Snowflake Enterprise App ‚Üí *Single Sign-On*
* Configure Identifier, Reply URL, Sign-On URL, and Logout URL using {{https://cw-au.privatelink.snowflakecomputing.com}}",,
CSCI-335,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,27/Aug/25 1:17 PM,03/Sep/25 9:50 AM,,202726 Create Snowflake Enterprise Application,"[Devops|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202276]

Add Snowflake as an application in Entra ID.

_Steps:_

* Go to *Entra ID ‚Üí Enterprise Applications*
* Click *+ New Application* ‚Üí Search *Snowflake*
* Select and click *Create*",,
CSCI-334,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,27/Aug/25 1:14 PM,19/Nov/25 11:38 AM,,202757 - Apply Security Best Practices,"[devops task 202757|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202757]

_As a Security Engineer, I want to enforce security best practices so the integration remains compliant._","* Public endpoints disabled
* NSGs enforced
* Conditional Access & MFA applied
* Least privilege for SCIM account",
CSCI-333,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,27/Aug/25 1:12 PM,19/Nov/25 11:38 AM,,202751 - Test & Monitor Integration,"[devops task 202751|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202751]

_As a Security Engineer, I want to test and monitor SSO + SCIM so I can ensure reliability and compliance._","* Test user provisioned successfully
* Group sync validated
* Logs visible in Snowflake & Entra",
CSCI-332,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,27/Aug/25 1:12 PM,10/Oct/25 5:42 PM,,202745 - Raveen - Configure SCIM Provision in Entra ID,"_As an Azure Admin, I want to configure SCIM so that Entra ID provisions users/groups automatically._","[devops link 202745|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202745]

* Automatic provisioning enabled
* Test connection succeeds
* Attribute mappings configured
* Initial sync successful","@user - to confirm the new naming convention on AD groups.

@user cc @user for more details

Hi @user , @user ,

Kindly review, on the AD snowflake groups, we would like to propose these.

|*Current*|*Proposed*|
|DEV_PlatformEngineers|Snowflake-dev-PlatformEngineers|
|DEV_DataEngineers|Snowflake-dev-DataEngineers|
|DEV_Consultants|Snowflake-dev-Consultants|
|STEST_PlatformEngineers|Snowflake-stest-PlatformEngineers|
|STEST_DataEngineers|Snowflake-stest-DataEngineers|
|STEST_Consultants|Snowflake-stest-Consultants|
|QA_PlatformEngineers|Snowflake-qa-PlatformEngineers|
|QA_DataEngineers|Snowflake-qa-DataEngineers|
|QA_DataAnalysts|Snowflake-qa-DataAnalysts|
|QA_BusinessUsers|Snowflake-qa-BusinessUsers|
|PROD_PlatformEngineers|Snowflake-prod-PlatformEngineers|
|PROD_DataEngineers|Snowflake-prod-DataEngineers|
|PROD_Consultants|Snowflake-prod-Consultants|
|PROD_DataAnalysts|Snowflake-prod-DataAnalysts|
|PROD_IT_Audit|Snowflake-prod-IT-Audit|

If we need one for PowerBI, we can have similar group created and associate the member to that group. e.g

PowerBI-dev-Admin

PowerBI-dev-Developer

PowerBI-dev-Viewer

@user @user , @user

HI @user 

As Alan confirmed, we are progressing to provision these AD groups and associate members to it.

[^snowflake_ad_groups_members_V0.1.xlsx]

@user , @user

A [RITM0174818 |https://cwretail.service-now.com/nav_to.do?uri=sc_req_item.do?sys_id=96d0763a3337ae5047764f945d5c7b69]has been raised to provision the groups and associate members

@user , @user 

Task is progressing. We have the groups created in AD and it was sync to EntraId, member association is in progress

@user Thanks
I was wondering if there are any paper trails to this progress Ravee?
If not, no stress.

@user please note, SCIM implementation steps are [here|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5170/SCIM-SAML-Snowflake-and-Entra-ID-SSO-Configuration-via-Azure-Private-Link]

Creating of Groups/Members in AD/Endra are managed by SysOps/Cloud

Hi @user This task be closed now. We were able to sync successfully groups/members to snowflake

@user @user"
CSCI-331,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,27/Aug/25 1:09 PM,02/Oct/25 2:15 PM,,202740 - Enable SCIM API Integration in Snowflake,"[devops task 202740|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202740]

_As a Snowflake Admin, I want to enable SCIM API integration so Entra ID can provision users automatically._","* SCIM integration created
* OAuth credentials generated
* SCIM endpoint available",
CSCI-330,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,27/Aug/25 1:09 PM,10/Oct/25 10:12 AM,,Authenticate to Snowflake via Entra ID,"As a user, I want to authenticate to Snowflake via Entra ID so that I can use my corporate credentials for seamless sign-on.","* IdP-initiated login works via MyApps
* SP-initiated login works via PrivateLink
* MFA enforced
* DNS resolution works","SSO Authentication and SCIM sync are successfully configured.

Tested by Ashu (authentication) and Chloe (SCIM - AAD provisioned)"
CSCI-329,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,27/Aug/25 1:04 PM,27/Aug/25 2:57 PM,,202730 - Snowflake SAML Security Integration,"As a Snowflake Administrator, I want to configure SAML authentication so that users can log in using Entra ID credentials","* SAML2 security integration created in Snowflake
* Snowflake metadata generated and shared with Entra ID
* Test query confirms integration is enabled",
CSCI-328,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,27/Aug/25 1:03 PM,03/Sep/25 9:51 AM,,202725 - Raveen - Entra ID enterprise Application setup,"[202725|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202725]

As an Identity Administrator, I want to create and configure the Snowflake enterprise application in Entra ID so that the foundation for SSO is established","* Snowflake enterprise application is created from Azure AD gallery
* Basic SAML settings are configured with Private Link URLs
* SAML attributes are properly mapped for user claims
* Entra ID certificate and metadata are extracted",
CSCI-327,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,26/Aug/25 11:56 AM,12/Sep/25 6:20 PM,,Review Source-to-Target Map for Dim_Employee,Review the mapping of DIM_Employee Dimension: DIM_Employee,"* Finalize that the work on Source-to-Target Map for DIM_Employee correct.
** Review the Source-to-Target Map for DIM_Employee against the current production environment.
** Update the mapping file to reflect any changes or corrections found.
** Peer review completed and signed off by Chloe/Harrison
** Communicate changes of mapping file to the Jira/standup for traceability.",
CSCI-326,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Aug/25 12:16 PM,01/Sep/25 2:18 AM,,Extract parquet for TransactionAuditSaleActivityLog - part 1,"Extract the parquets files for these tables 

* TransactionStorage.dbo.TransactionAuditSaleActivityLog
* TransactionStorage.dbo.TransactionAuditSaleActivityLogHistory
* TransactionStorage.dbo.Transactions_ElectronicPayments","* Extract the parquet files
* File validation
* Upload it in Azure blob storage
* Post-upload verification
* Documentation","Created this task as @user has requested parquet extracts

cc:- @user

@user 

updated AC - LMK if that makes sense - can change back if needed

@user I‚Äôm currently blocked by this firewall error when attempting to access Blob Storage to upload the Parquet file.

@user do you have a servicenow ticket for this? - If so just screenshot here and I‚Äôll create a Jira ticket to establish parity.

cc @user

@user Raised the incident
[INC0474862 | Incident | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/incident.do%3Fsys_id%3D8103494e932b2a50f226b5058aba1078%26sysparm_stack%3D%26sysparm_view%3D]

Extracted the parquet files for TDB14.TransactionStorage.dbo.TransactionAuditSaleActivityLogHistory

Access to the blob storage is still blocked by the firewall, I cannot upload the files into the container for ingestion.

@user @user Need assistance from the network team to resolve this, can you help escalate? [INC0474862 | Incident | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/incident.do%3Fsys_id%3D8103494e932b2a50f226b5058aba1078%26sysparm_stack%3D%26sysparm_view%3D]

I see you already raised ticket. @user will follow up and talk to the cloud team on this. 

This is due to the changes to firewall where conditional forwarding for {{privatelink.dfs.core.windows.net}} to setup private connectivity from On-prem/VDI to our Azure Data Lake File System Gen2 Storage Accounts.¬†so since you guys are using to directly access the storage accounts manually then we must allow connections from on-prem/vdi to the storage accounts.

@user the Infra Cloud team has raised a request with the Information Security team to allow my IP address for blob storage access.

[RITM0173947 | Requested Item | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/sc_req_item.do%3Fsys_id%3Daf06e80f3323e21047764f945d5c7b20%26sysparm_stack%26sysparm_view]

Issue split into:
|CSCI-379|Extract parquet for TransactionAuditSaleActivityLog - part 2|"
CSCI-325,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Aug/25 9:50 AM,09/Oct/25 9:35 AM,,CT - Snowflake CI/CD process - Create PR,"* Extract Meta
* Schema DDL (tables + views)
* Create PR
* Review and merge
* Test end to end",,
CSCI-324,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Aug/25 9:49 AM,08/Oct/25 4:44 PM,,AA - Snowflake CI/CD process - Create PR,"* Extract Meta
* Schema DDL (tables + views)
* Create PR
* Review and merge
* Test end to end",,
CSCI-323,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Aug/25 9:45 AM,12/Sep/25 6:32 PM,,APPRISS - next steps on data modelling,"* -Identify business process for APPRISS-

Identify source table(s) for APPRISS - partially done (review)

Create structure for FActs and Dims

* Finalizing identification of business processes relevant to APPRISS.
* Reviewing and confirming source tables for APPRISS data ingestion.
* Designing and documenting the structure for Fact and Dimension tables to support robust analytics and reporting.
This task aims to ensure a clear, scalable, and auditable data model foundation for APPRISS integration within the CW Cloud Data Platform Interim Solution.","* -Business Process Identification-
* -Source Table Confirmation-
* -Review Initial Data Extract Queries-
* - Dimension Table Design-
* -Fact Tables Design-
* -Sample Data Validation-
* -Documentation-
* Stakeholder Review
* -Readiness for Next Steps-","Appriss Data points for Ingestion Purpose are identified. Received the queries for sample data that has been shared with APPRISS team. 

Will review the queries today.

Fact table structure is in progress.

Business Process Identification Complete

Source Table Identification Complete

Initial Query review Complete. Good from ingestion point of view.

Fact Table Structure is updated in MergeCo Folder 
[Business Matrix_Retail - Simplified.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Business%20Matrix_Retail%20-%20Simplified.xlsx?d=w0e262bf7be84407d8dbff2bb4d4c5e00&csf=1&web=1&e=dGjeuu]

Dim tables Silver Layer Structure is updated in Confluence
[https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model|https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model]

Dim tables Silver Layer Structure is updated in Confluence
[https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model|https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model]

Sample source data is validated

Source Data is identified. Target table structure is there now. So the next step would be to start with transformation logic documentation

Issue split into:
|CSCI-438|APPRISS - next steps on data modelling - alan to review|"
CSCI-321,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Aug/25 9:40 AM,09/Oct/25 9:36 AM,,ILS Manhattan Scale Integration : Whitelist Prod server,To create a request to whitelist the PROD server for ADF SHIR in Dev,,"I think this is blocked @user ?

cc @user 

Will raise servicenow request - near end of sprint 8

I will attend to this next week. Gotta focus on the Detailed Design doco @user

Chloe to catch up with Adeel on details"
CSCI-320,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Aug/25 9:30 AM,08/Dec/25 1:38 PM,,AP - Snowflake CI/CD process - Create PR,"* Extract Meta
* Schema DDL (tables + views)
* Create PR
* Review and merge
* Test end to end",,
CSCI-319,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Aug/25 9:28 AM,25/Sep/25 9:49 AM,,CT - ADF CI/CD process - Create PR,"* Deployment checklist
* Add trigger
* Create PR
* Get approval and Merger
* End to end testing
* Publish schedule",,"To walk through the ADF process to create a PR with Ashu, Aswini and Jess"
CSCI-318,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Aug/25 9:27 AM,25/Sep/25 11:51 AM,,AA - ADF CI/CD process - Create PR,"* Deployment checklist
* Add trigger
* Create PR
* Get approval and Merger
* End to end testing
* Publish schedule",,"ran this thru Rovo, is it OK to check @user ?

you can use this for future tickets as a scaffold

----

h3. Context

* The project requires implementing a CI/CD process for Azure Data Factory (ADF) pipelines as part of the CW Cloud Data Platform Interim Solution. This involves automating deployment steps, ensuring code quality, and enabling smooth release management through pull requests and approvals.

h3. Objective

* To establish and execute a robust CI/CD workflow for ADF, including deployment automation, code review, approval, and scheduling, ensuring reliable and efficient delivery of data platform changes.

h3. Steps

# Complete the deployment checklist for ADF CI/CD.
# Add the necessary trigger(s) to automate the pipeline.
# Create a Pull Request (PR) for the changes.
# Obtain approval and merge the PR.
# Conduct end-to-end testing of the deployment.
# Publish and schedule the pipeline as required.

h3. Deliverables

* A fully operational CI/CD process for ADF, including:
** Documented deployment checklist
** Configured triggers
** Merged and approved PR
** Evidence of successful end-to-end testing
** Published and scheduled ADF pipeline

h3. Assumptions (Optional)

* Required permissions are available for repository and ADF resources.
* Stakeholders are available for timely PR review and approval.
* Test environments are set up and accessible.

h3. Acceptance criteria

* Deployment checklist is completed and documented.
* Trigger(s) are added and verified.
* Pull Request is created, reviewed, approved, and merged.
* End-to-end testing is completed with no critical issues.
* Pipeline is published and scheduled as per requirements.

Created a PR, reviewing another branch that got created during the process."
CSCI-317,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Aug/25 9:26 AM,29/Sep/25 9:00 AM,,AP - ADF CI/CD process - Create PR,"* Deployment checklist
* Add trigger
* Create PR
* Get approval and Merger
* End to end testing
* Publish schedule",,
CSCI-316,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Aug/25 9:16 AM,29/Aug/25 4:47 PM,,ADF/Altis DLA - Snowflake pipeline showcase,To organise a session on ADF/Altis DLA - Snowflake pipeline showcase with the Sigma/CW Data Engineering team.,,
CSCI-315,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Aug/25 8:45 PM,29/Sep/25 9:40 AM,,SF-004 - Configure role-based access control framework,Implement Snowflake roles aligned to business; map to Entra ID groups.,Implement Snowflake roles aligned to business; map to Entra ID groups.,
CSCI-314,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Aug/25 8:44 PM,29/Sep/25 9:40 AM,,SF-003 - Set up user and group attribute mappings,Define mappings from Entra ID to Snowflake users/roles for automated provisioning.,Define mappings from Entra ID to Snowflake users/roles for automated provisioning.,
CSCI-313,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Aug/25 8:44 PM,29/Sep/25 9:40 AM,,SF-002 - Configure SCIM API integration,Automated user provisioning from Entra ID; lifecycle based on group membership.,Automated user provisioning from Entra ID; lifecycle based on group membership.,
CSCI-308,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Aug/25 8:27 PM,29/Sep/25 9:41 AM,,GRP-003 Access pattern role mapping strategy,"Define patterns and role assignments for on-prem, remote, specialist tools scenarios.","Define patterns and role assignments for on-prem, remote, specialist tools scenarios.",
CSCI-307,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Aug/25 8:26 PM,01/Oct/25 3:29 PM,,GRP-002 - Authorization framework documentation,"Map Entra ID groups to Snowflake roles, infra access, and tool permissions for auditability.","Map Entra ID groups to Snowflake roles, infra access, and tool permissions for auditability.",
CSCI-306,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Aug/25 8:26 PM,29/Sep/25 9:41 AM,,GRP-001 - Developer access group definitions and access matrix,Define Entra ID groups for¬†Snowflake.,Define Entra ID groups for¬†Snowflake.,
CSCI-304,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Aug/25 8:24 PM,09/Oct/25 9:42 AM,,AUTH-003 - Development tool authentication standardization,"Standardize auth configs across Snow CLI, VS Code, Tabular Editor, Power BI (externalbrowser).","Standardize auth configs across Snow CLI, VS Code, Tabular Editor, Power BI (externalbrowser).",
CSCI-302,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Aug/25 8:22 PM,30/Oct/25 9:50 AM,,End-to-end SSO integration testing across all access methods,"Validate externalbrowser SSO flow across on-prem, VPN, jumpboxes, Azure VMs before prod rollout.","Validate externalbrowser SSO flow across on-prem, VPN, jumpboxes, Azure VMs before prod rollout.","Pending other tasks - provision of jump box + Azure DNS Private Resolver

Chloe to catch up with Raveen to check on status

Logging to Snowflake from a VDI is unsuccessful after the Azure DNS Private Resolver was implemented. Raveen to schedule a call with Eugene, myself and Mike to troubleshoot and discuss next step.

Have reached out to Raveen. Awaiting for resolution updates

Task now unblocked üôÇ

Test done successfully with Raveen on login to Snowflake Privatelink via VDI. 

However default roles (provisioned via SCIM) have been wiped out. Raveen to check which settings have caused removal of default roles.

I‚Äôve tested the default role provision via SCIM. All is working well. Happy to close this ticket.

Next steps:

* @user to add a ticket for @user to verify and update subnet in the IaC. 
* @user to let us know about VDI/ Jumpbox access so we can all access Snowflake Privatelink."
CSCI-294,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Aug/25 7:10 PM,19/Nov/25 1:30 PM,,RITM0173487 - Implement Azure DNS Private Resolver,"Currently worked on by Shagun A.

Implement Azure DNS Private Resolver to enable on-premises DNS resolution for existing Snowflake Private Link endpoint ([privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com]).

The Azure Private Link for Snowflake is already configured and operational. This DNS implementation will enable on-premises systems to resolve the private Snowflake endpoint, which is a prerequisite for subsequent firewall access configuration for Enterprise Data Platform and critical applications requiring Snowflake access.

Spoke to Network team and they suggested we create AD security group for snowflake private link access control and add the relevant users who need access to this group as the user laptop ip address is dynamic so this way we can control this via global protect and firewall.¬†

¬†

Current status:","* -Azure DNS Private Resolver is deployed and configured-
* -On-premises DNS resolution is verified-
* -AD Security Group for Snowflake Access is created-
* -Firewall and GlobalProtect integration is validated-
* -Documentation is updated-
* -SSO working as expected-
* -Stakeholder sign-off-","Hi @user - LMK if this AC is correct as per box

currently in progress.

@user - to check with @user

Current status: in fulfilment 
Linked to task RITM0173487

Hi @user - just assigned this to you as I may be able to liaise with you what the status of this ticket is.

Checking on this @user

Hi @user , I tried to reach you to schedule a call on this to explain :)

Could you review attached spreadsheet please

[^RITM0173487_snowflake_connectivity_requirements.xlsx]

 @user, @user

@user what is this for?

Here are two Wiki‚Äôs that have dependencies on the privatelink integration:

* [SCIM+SAML - Snowflake and Entra ID SSO Configuration via Azure Private Link - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5170/SCIM-SAML-Snowflake-and-Entra-ID-SSO-Configuration-via-Azure-Private-Link]

* [Developer Access - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5145/Developer-Access]

Hey@user , are we saying this Jira card is no longer required? or are you talking about the spreadsheet?

we need to collect and consolidate our requests so the team can focus on specifics like VDI, on-prem network requirements, and so forth. Given that not everyone can access the wiki, and it contains quite a lot of information, we should pull together the specific details for them.

@user the private DNS resolver is already in place and configured in Azure ([privatelink.snowflakecomputing.com - Microsoft Azure|https://portal.azure.com/#@mychemist.com.au/resource/subscriptions/154716e9-0cb4-45c8-9fa7-dfea0954bbe1/resourceGroups/cwr-ase-platform-rg/providers/Microsoft.Network/privateDnsZones/privatelink.snowflakecomputing.com/recordsets]), what‚Äôs needed is for the DNS forwarding from on-premise DNS servers to the Azure privatelink DNS. See: [Resolve Azure and on-premises domains. | Microsoft Learn|https://learn.microsoft.com/en-us/azure/dns/private-resolver-hybrid-dns]

Thanks@user That‚Äôs exactly what I was trying to clarify. Since most of the configurations from RITM0173487 have already been completed. Had a chat with @user, we need to provide detailed information and specifics for the Network team. We can either repurpose the existing card or create a new one to track this outstanding work for Daify

CC @user @user

We have raised CR CHG0052044 for next week to configure Conditional Forwarder

Hi @user Raised Firewall request *RITM0176342* (pre-requisite task for Conditional Forwarding)

Hi @user , @user , @user ,

We are creating Firewall rule in Azure but can you review and confirm sources please? 

# CWR Jumpbox M1
# CWR Jumpbox M2
# M1 RBT Vendor Jumpbox
# M2 RBT Vendor Jumpbox

Just FYI

{noformat}¬† ¬† ¬† source_addresses = [
¬† ¬† ¬† ¬† ""172.25.130.0/24"", # CWR Jumpbox M1
¬† ¬† ¬† ¬† ""172.27.130.0/24"", # CWR Jumpbox M2
¬† ¬† ¬† ¬† ""172.25.150.0/26"", # M1 RBT Vendor Jumpbox
¬† ¬† ¬† ¬† ""172.27.150.0/26"", # M2 RBT Vendor Jumpbox
¬† ¬† ¬† ¬† ""10.15.0.0/19"", ¬† ¬†# Albert Street Head Office
¬† ¬† ¬† ]{noformat}

Please note, once we have our own VDI range, we will revisit and update the firewall rule accordingly. 

We should also enforce network policies on snowflake side to block public access.

@user cc @user @user @user

@user @user @user @user , please confirm. 

We can add or exclude VDI Groups later if required

@user / @user @user - do we know if these are the jumpboxes we need?
Just trying to get this over the line so you guys get access.

Hi @user ,

FYI..

@user @user , when we get a separate VDI group for our team, that new group will be part (subset) of this above wider range.

When we receive are VDI group, we will reconfigure the firewall rule to be specific to our group.

hi Raveen, happy to proceed. Thanks! @user

Thanks@user 

We have successfully implemented Azure Firewall rule. 

Raised new CR *CHG0052343* for Active Directory DNS - Configure Conditional Forwarding

Planned start date: 13-10-2025 11:00:00 AM
Planned end date: 13-10-2025 12:00:00 PM
Assigned to: Louis Allsop

Issue split into:
|CSCI-535|RITM0173487 - Implement Azure DNS Private Resolver - Sprint 10|

To continue in sprint 10

Hey @user - any reason this needs to be reopen but not using the split one in sprint 10? [https://sigmahealthcare.atlassian.net/browse/CSCI-535|https://sigmahealthcare.atlassian.net/browse/CSCI-535] 

Thanks

Han

Logging to Snowflake from a VDI is unsuccessful after the Azure DNS Private Resolver was implemented. Raveen to schedule a call with Eugene, myself and Mike to troubleshoot and discuss next step. @user

h3. 
*Raveendran Kumaravelu*

October 13, 2025 at 9:05 AM

New Change Request CHG0052343 has been scheduled for today

[+CHG0052343+|https://cwretail.service-now.com/nav_to.do?uri=change_request.do?sys_id=61cc0f803360b25047764f945d5c7b23]

Planned start date: 13-10-2025 11:00:00 AM
Planned end date: 13-10-2025 12:00:00 PM
Implementor: Louis Allsop
Active Directory DNS - Configure Conditional Forwarding to Azure for Private Endpoint resolution ([http://snowflakecomputing.com|http://snowflakecomputing.com] )

Changed was executed successfully and connectivity testing was all good. 

However, I have scheduled a meeting for Tuesday 21/10 with @user to test SSO 

CC: @user , @user , @user

Thx Raveendran Can I confirm this means all Acceptance Criteria are passed except for SSO?

@user Yes

@user thx can we ensure this is tracked moving forward so others know the progress at a glance.

@user Whom‚Äôs review is this pending now?

FYI @user 

thanks

@user I think this is good and we can close this ticket. Thanks @user"
CSCI-293,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Aug/25 3:41 PM,12/Sep/25 6:20 PM,,Review Source-to-Target Map for Dim_location,Review the mapping of DIM_LOCATION Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=XQdnyO&nav=MTVfe0Q4MDZEQUY3LTJCQzUtNDRBMy1CNDEwLUNBMEE4RjVFQkYwNX0],"* Finalize that the work on Source-to-Target Map for DIM_LOCATIONis correct.
** Review the Source-to-Target Map for DIM_LOCATION against the current production environment.
** Update the mapping file to reflect any changes or corrections found.
** Peer review completed and signed off by Chloe/Harrison
** Communicate changes of mapping file to the Jira/standup for traceability.",
CSCI-292,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Aug/25 1:41 PM,12/Sep/25 6:20 PM,,Review Source-to-Target Map for Dim_DC,Review the mapping of DIM_DC Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=XQdnyO&nav=MTVfe0Q4MDZEQUY3LTJCQzUtNDRBMy1CNDEwLUNBMEE4RjVFQkYwNX0],"* Finalize that the work on Source-to-Target Map for DIM_DC is correct.
** Review the Source-to-Target Map for DIM_DC against the current production environment.
** Update the mapping file to reflect any changes or corrections found.
** Peer review completed and signed off by Chloe/Harrison
** Communicate changes of mapping file to the Jira/standup for traceability.",
CSCI-291,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Aug/25 9:24 AM,12/Sep/25 6:20 PM,,Review Source-to-Target Map for Dim_Product_UOM,Review the mapping of DIM_PRODICT_UOM Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=67yqFK&nav=MTVfe0RDNzkwNDhDLTg0NDAtNEREOS05NkE3LUM2ODRFOTNBNjkzQX0],"* Finalize that the work on Source-to-Target Map for DIM_PRODICT_UOM is correct.
** Review the Source-to-Target Map for DIM_PRODICT_UOM against the current production environment.
** Update the mapping file to reflect any changes or corrections found.
** Peer review completed and signed off by Chloe/Harrison
** -Communicate any changes to relevant business stakeholders.-
** Communicate changes of mapping file to the Jira/standup for traceability.","Reviewed.
Thanks @user

@userupdated.

Please have a look and see if there are points to change

Issue split into:
|CSCI-433|Review Source-to-Target Map for Dim_Product_UOM part 2|"
CSCI-290,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,20/Aug/25 1:54 PM,22/Aug/25 8:50 AM,Integeration,Add NOLOCK for all the tables getting ingested in snowflake,Add NOLOCK for all the tables getting ingested in snowflake and test if the query is running as expected and data is getting loaded,"* -Data ingestion processes execute successfully without errors after adding NOLOCK.-
* -Query performance is not negatively impacted beyond acceptable thresholds after implementing NOLOCK.-
* -All changes are tested and validated in a non-production environment before deployment.-
* -Documentation is updated to reflect the use of NOLOCK in the ingestion process.-","@useryou can edit the AC where needed.

Hey @user 

Just want to confirm if docs are not required.

I can strike thru the text if that‚Äôs the case"
CSCI-289,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,20/Aug/25 9:48 AM,12/Sep/25 6:32 PM,,MergeCo Conformed Reporting - Pull Data from CW to SF,"Copy data from CW on-prem SQL Server (PBI05) to SF via ADF.

Grain:

* High-level: Per DC, Per Day",,"Spoke with CK about enabling PBI05 to be whitelisted with ADF.

CK has raised a request: *REQ0157732*

@user I‚Äôll find screenshot for this.

Followed up on ticket *REQ0157732* - ticket closed due to wrong ticket type.

CK has raised a new ticket as per instructions: *REQ0158466*

I wasn‚Äôt really too sure how much work was actually spent here @user before it got blocked - so I moved this task rather than splitting it.

@user 1-2 hours

@user

@user @user is it OK to reply back to this ticket? Thank you.

Had a meeting on 10/9/25 with Cloud Services team + members of the SC Snowflake project.

Cloud team are concerned that PBI05 is a ‚Äúproduction server‚Äù and our Azure Subscription is listed as ‚Äúdev‚Äù. Explained the reasoning behind this in great detail.

Cloud team have advised if the owner of PBI05 can provide their approval then they can proceed with this request. 

Spoke with Rachel Wan and she has approved this within the SNOW ticket - awaiting on the cloud team to action the request.

Issue split into:
|CSCI-429|MergeCo Conformed Reporting - Pull Data from CW to SF|

Issue split into:
|CSCI-439|MergeCo Conformed Reporting - Pull Data from CW to SF - part 2|"
CSCI-288,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,20/Aug/25 9:40 AM,08/Oct/25 11:15 AM,,MergeCo Conformed Reporting - Develop Outbound Performance Agg Fact,"* Outbound Performance
** ATP%
*** Available to Promise
**** Confirmed / Ordered
** DIF%
*** Invoiced / Confirmed
** Cust(Customer) Impact (% + $)
*** Invoiced / Ordered
** DOT%
*** Qty Units Dispatched in 24 hrs / Qty Units Dispatched","Develop facts from CW to create these KPIs

* ATP%
* DIF%
* Cust Impact%
* Cust Impact $
* DOT%","Have access to required tables. However data source lives in separate servers (PDB08 and PDB19B) - so cannot create a single view over different servers. Need to request DBA team to bring relevant data into a single server in order to merge and create a single object.

Message sent to Heshan (DBA team) to request we load data into PBI05 - so that I can consume in PBI + we can then pull into Snowflake

have raised a SNOW request to pull relevant tables into PBI05:

*REQ0158141*

I wasn‚Äôt really too sure how much work was actually spent here @user before it got blocked - so I moved this task rather than splitting it.

@user 3-4 days

@user Hi Phil,

I have been working on the following with this specific task:

* Performed Data Discovery of SCAX2012 and PDB08 to locate and find where this data exists in CW (2 story points)
* Developed SQL views of data (1 story point)
* Performed validation/sense checking of data (0.5 story points)
* Gathered all requirements of what data we need to ingest in PBI05 (0.5 story points)

Validated data loaded into TBI05 - have identified an aggregation issue in the Orders table with the DC Location ID (is causing duplicate values to appear).

Have identified the issue and raised a request with Bhavya to action.

Have updated current fact table to include current ordered/confirmed/invoiced logic.

@user *REQ0158141* paused (pending)"
CSCI-287,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,19/Aug/25 12:57 PM,12/Sep/25 6:20 PM,,Review Source-to-Target Map for Dim_Date,to have a discussion with you @user about this task.,* Finalize that the work on Source-to-Target Map for DIM_DATE is correct.,"linked.

 @user@user

Issue split into:
|CSCI-399|Review Source-to-Target Map for Dim_Date part 2|"
CSCI-286,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,19/Aug/25 11:00 AM,30/Aug/25 3:08 PM,,APPRISS Sample datashare - development,"Data share - Data Generated by CK - Manual data created, now need to be uploaded.
The sample data is placed under below share point location by CK. These sample data needs to be placed/loaded in data share.",* -Data is uploaded.-,"Will validate records. - perhaps test case?

ardm csv files are uploaded to datashare in tables. Also validated the some sample records between files and corresponding tables.

Issue split into:
|CSCI-371|APPRISS Sample datashare - part 2|"
CSCI-285,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Aug/25 2:37 PM,21/Aug/25 2:48 PM,,Update Source Table Glossary with APPRISS tables,"I need to find and update the Source Table Glossary with APPROSS related tables. The tasks include:

* Identifying the source systems
* Finding and recording the table sizes.
* Identifying key columns (primary keys).

TransactionStorage

* TRANSACTIONS_INVOICE (Transaction Headers)
* TRANSACTIONS (Line Items)
* TRANSACTIONS_ELECTRONICPAYMENTS (Payment Details)
* TransactionAuditSalesActivityLog and TransactionsAuditSalesActivityLogHistory (Audit Trail)
* TransactionsReturns
* TransactionTypes
* TransactionType_Invoice

¬†

StockDB

* Products
* Employee (Staff Details)
* Location (BranchInfoGlobal)","* Find the sources of these tables
* Find the table sizes
* Find key columns","@userI have created this ticket as there is a request from @user. Please make necessary changes if required

@user Added the Transaction and StockDb tables in data source spreadsheet

cc @user"
CSCI-284,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Aug/25 10:15 AM,20/Aug/25 9:43 AM,,follow up with data extraction to prod,,* access for chloe dev prod DB,
CSCI-283,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Aug/25 9:58 AM,01/Sep/25 2:23 AM,,Discussion - how we should approach getting specs/info on MAWM visibility - part 1,Discussion - how we should approach getting specs/info on MAWM visibility,Discussion - how we should approach getting specs/info on MAWM visibility,"hi @user im guessing MAWM means Manhattan warehouse management? so this would the sessions that we had with Chandan?

Hey@user sorry I didn‚Äôt get back to you on this.

Yep, MAWM is manhattan active - so Chandan‚Äôs sessions would be one of them - we‚Äôre trying to discover ways of finding out how to get this info.

Issue split into:
|CSCI-377|Discussion - how we should approach getting specs/info on MAWM visibility - part 2|"
CSCI-282,CSCI,CW Cloud Data Platform Interim Solution,Story,Done,Medium,18/Aug/25 8:56 AM,18/Aug/25 3:39 PM,Integeration,Data time Review in snowflake,Chloe has changed the datetime in snowflake to UTC time. We need to confirm everything is looking as expected.,,"@user - will add task in lieu of this story.

Note:
- Store transactions - local date time. -

 e.g. CWH perth - Local date and time (WST) CWH melb - AEST

The source time zone is intact in the snowflake and there is no impact of the changing the time zone of snowflake account.

Aswini is confirming my findings

Snowflake is not tempering the data in source system. So, the time zone changes of the snowflake accounts are snowflake specific. No impact on import process and unification of time zone should be handled at presentation layer."
CSCI-281,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Aug/25 4:39 PM,01/Sep/25 2:31 AM,,Review of CI/CD Pipeline - part 2,Review output of [https://sigmahealthcare.atlassian.net/browse/CSCI-151|https://sigmahealthcare.atlassian.net/browse/CSCI-151] and [https://sigmahealthcare.atlassian.net/browse/CSCI-159|https://sigmahealthcare.atlassian.net/browse/CSCI-159],,"Stretch Sprint 5 goal

I have reviewed the CD pipeline and am satisfied with the following:

* *Branching strategy:* Use a feature branch, merge to {{master}}, then deploy to each environment upon approval.
* *Deployment monitoring:* Check DevOps Releases for deployment status and logs.
* *Script execution:* {{Config_meta.sql}} will be sequenced and run within {{deployment_master_scripts}}.
* *Versioning:* The current mechanism functions but remains limited. We still require a defined process for version naming, review and auto-validation.

*Next step:* Chloe to define the desired version control process with detailed requirements.

hi Eugene, please review this PR and the Release's for Snowflake :

[https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-snowflake/pullrequest/22486|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-snowflake/pullrequest/22486] 

[https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_releaseDefinition?definitionId=7&_a=definition-tasks&environmentId=16|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_releaseDefinition?definitionId=7&_a=definition-tasks&environmentId=16]

High level approach for Snowflake CI/CD:

@userto review.

Issue split into:
|CSCI-381|Review of CI/CD Pipeline - part 3|"
CSCI-280,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Aug/25 4:37 PM,01/Sep/25 2:13 AM,,Source to target Facts - Fact_Store_Inventory - part 2,"This task involves mapping out target fact tables using the source-to-target sheet, with a focus on the key facts outlined in the acceptance criteria. The main fact tables to be addressed include:

* {{Fact_Store_Inventory_Intra}}
* {{Fact_Store_Inventory_Adjustment}}
* {{Fact_Store_Inventory_Snapshot}}

These tables are listed as Priority 1 and Priority 2 in the control table and are critical for meeting the acceptance criteria. The goal is to ensure accurate and complete mapping from source to target for these facts.","Priority 1 and Priority 2 Fact tables on control table will be mapped

* -Fact_Store_Inventory_Intra-
* -Fact_Store_Inventory_Adjustment-
* -Fact_Store_Inventory_Snapshot-","Note - not a daily snapshot - just movement

Going thru logic i.e. filtration.
Will take longer time

Fact_Store_Inventory_snapshot and Fact_Store_Inventory_Intra

ETL Logic onPrem. 

Step 1: 

Insert into Staging : 

{noformat}select rn=row_number() over (partition by DateStampDayOnly, MyCHemID, OriginBranchID order by DateStamp desc, RecordID desc)
 ,*
 ,1 as isPlaceholder
 from Fact_Store_IncrementalSOHChanges
 where DateStamp >= ""PreviousDate"" and DateStamp < ""CurrentDate""
 ) dt2
 where rn = 1 {noformat}

Insert into the new data from source which is only housing data changes 

{noformat}insert Into Staging 
select 
 RecordID, MychemID, AvgRealCost, SOH, DateStamp
 , coalesce(ChangeType, 26) as ChangeType -- set null ChangeType to SOH Import
 , coalesce(SOHChange, 0) as SOHChange
 , StaffID, RealCost, ExpectedSales, OriginBranchID, DateStampDayOnly
 , 0 as SOH_Last
 , 0 as ExpectedSales_Last
 , 0 as LineNumber
 , DateStamp as DateStampOriginal
 , 0 as isPlaceholder
 , hasExcessiveValue
from 
 Store_IncrementalSOHChanges
where 
 DateStamp >= ""currentDate"" and 
 DateStamp < ""nextDate"" and
 RecordID > ""maxRecordID"" {noformat}

Then It Self joins and deletes all records where isPlaceholder = 1 and has a record in the second join where the isPlaceholder = 0

{noformat}delete Staging 
from
(
 select nr.OriginBranchID, nr.MyCHemID
 from Staging nr
 where 
 nr.isPlaceholder = 0
 and nr.DateStamp >= @currentDate
 and nr.DateStamp < @nextDate 

) dt
where 
 Staging.isPlaceholder = 1
 and Staging.DateStamp >= ""currentDate""
 and Staging.DateStamp < ""nextDate""
 and Staging.OriginBranchID = dt.OriginBranchID
 and Staging.MychemID = dt.MychemID{noformat}

this ensures that we will have one record for every ChemID , for every Store fore everyday. 

The ETL for this table must be really thought through as it can get quite resource hungry.

split this task to part 2 for next sprint - please adjust story point as per req"
CSCI-279,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Aug/25 4:37 PM,27/Aug/25 9:41 AM,,Source to Target for Facts - Fact_Product_Network_Average_Cost_History - review,"Mapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]

main facts to focus on are as per acceptance criteria

@user -will split this task out so that fits in to sprint cadence- done

notes from @user 

{quote}StockDB.dbo.productnetworkcosts and StockDB.dbo.productnetworkcostshistory{quote}

Product Network Cost Standardised Network cost that needs to be applied across all the inventory tables.

CWR standardised Names, definitions and prices across all the teams. This needs to be carried into the Data platform as well. 

attached is the PDF with the names as definitions. 

[^Cost Prices in CWR Network_Finalised.pdf]","* Priority 1 and Priority 2 Fact tables on control table will be mapped
** Reviewing Fact_Product_Network_Average_Cost_History","Product Network Cost Standardised Network cost that needs to be applied across all the inventory tables.

on Prem there are 2 tables Product Network Cost and Product Network Cost history. 

one is today‚Äôs value and the other has a daily snapshot. we need to join on all the snapshot tables to this on the snapshot date to get the prices for the correct day. 

the grain is 1 record per product per day.

@user and @user 
Please review this ticket."
CSCI-278,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Aug/25 4:36 PM,27/Aug/25 9:41 AM,,Source to Target for Facts - Fact_Warehouse_Location_inventory - Review,"Mapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]

main facts to focus on are as per acceptance criteria

@user -will split this task out so that fits in to sprint cadence- done

notes from @user 

{quote}The Stock Fact_Warehouse_Inventory_Snapshot- the mapping is done based off the PBI05.supplychain.scax.dcsinventoryhistory

base tables: INVENTDIM and INVENTDIM. 
Things to note: 

* DataareaID- this defines the business entities. 
* read from the most current partition: This function gives the most current partition; SCAXLink.[dbo].[udf_GetPartition] (null)
* ensure Nolock is used for these reads.{quote}

{quote}

Fact_Warehouse_Inventory_Adjustment: Based of [BI_Presentation].[dbo].[WarehouseInventoryAdjustments]

E.g

{quote}","* Priority 1 and Priority 2 Fact tables on control table will be mapped
** -Reviewing Fact_Warehouse_Location_Inventory_Intra-
** -Reviewing Fact_Warehouse_Location_Inventory_Snapshot-
** Fact_Warehouse_Location_Inventory_Adjustment
** Cycle count
*","Hey @user ,

Who created the details here? Description and ticket Acceptance Criteria dont seem to marry up?

Thanks,

Harrison

Reviewing Fact_Warehouse_Location_Inventory_Snapshot: 

Table is based of ILS.dbo.LocationInventory

This is a live table out of the WH Management system.

Fact_Warehouse_Inventory_Adjustment:

is based on the SC schematic model. As this was not part of the pervious EDP project."
CSCI-277,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Aug/25 4:35 PM,27/Aug/25 9:42 AM,,Source to Target for Facts - DC Inventory - review,"Source to target mapping for Inventory history related Facts - filling out sheet that covers:

* Fact_DC_inventory_history
* Fact_DC_inventory_intra
* Fact_DC_Inventory_Location_History
* Fact_DC_Inventory_Adjustments","* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Inventory%20Source-to-target%20document%20-%20Facts.xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=uvHOeg&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no","This task will/should be split out - I can do this.

@user

need ot map the BI tables for DC

is approaching secondary mapping for DC

To verify ‚ÄòData type‚Äô whether it‚Äôs needed to be specified.
\ @user@user

* Fact_DC_inventory_intra

Harrison had entered a few more fields. SAFETYSTOCKQTY, DamagedQty, ¬†UnsellableQty and ExpiredQty.

I have done some research to to be able to get these information out of the ERP system. 
expired qty can possibly be found from INVENTBATCH table but when i checked the table it doesn't contain any useful information. 

Have started a conversation with CWR AX Consultants and jess to see if can get this data out. 

@user , we might want to think about this table. As i think we cant get a lot these fields out of the scale system."
CSCI-276,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Aug/25 11:08 AM,12/Sep/25 6:20 PM,,Review Source-to-Target Map for Dim_Product - Alan,to have a discussion with you @user about this task.,* Finalise that the work on Source-to-Target Map for Dim_product is correct.,"Sounds good @user , feel free to book a catch up some time next week

Thanks @user. I have booked a meeting for Monday

Issue split into:
|CSCI-434|Review Source-to-Target Map for Dim_Product - Alan - part 2|"
CSCI-275,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Aug/25 10:44 AM,12/Sep/25 6:20 PM,,Review Source-to-Target Map for Dim_Store - Alan,Reviewing Source-to-Target Map for Dim_Store in Supply chain,* Finalise that the work on Source-to-Target Map for Dim_Store is correct.,
CSCI-273,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Aug/25 9:01 AM,18/Aug/25 10:13 AM,,Create APPRISS Data Share,"Create APPRISS Data Share based on the following info:

Organization Name: OKB13541

Account Name: NSWAZ

Cloud Platform: AZURE

Edition: Business Critical",,"Datashare created and confirmed by Appriss

@user - will follow up with Harrison/Frank fir next steps

@user FYI

From @user 
Next steps:

* sample data present - will place in data share (produced by CK - need to place)
** CK also got info of where he got that info
* Creating production data push from our end

Will create separate ticket for @user to start.
cc @user / @user"
CSCI-272,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Aug/25 3:27 PM,02/Oct/25 2:13 PM,,VPN Network for developers to Snowflake VNet,"(servicenow ticket no - Sorry still not available) Visibility Ticket: On-Prem Network Access to Snowflake VNet Setup (Mirroring ServiceNow ticket)

@user @user @user 

Notes to consider

* QoS? - traffic prioritisation
* What tools are involved.
* CW - Palo Alto GlobalProtect compatibility
* Ports to be allowed: 443, 80, 1433","Test cases

* Access with no VPN
* Access via GlobalProtect (CW)
* Access via Sigma VPN
* Access via CW VPN","Caught up with CK - 

Finish off SSO first

Then get developer access

@user due to this, No servicenow ticket available yet. @user

@user 

Agreed with Security and Azure team to not use VPN, so we can remove this.

cc: @user"
CSCI-271,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Aug/25 3:23 PM,10/Oct/25 2:30 PM,,VDI / Jumpbox network to Snowflake VNet,"(servicenow ticket no - Sorry still not available) Visibility Ticket: VDI / Jumpbox network to Snowflake VNet Setup (Mirroring ServiceNow ticket)

@user @user @user 

*Points to consider*

* How many jump boxes we need?
** What are the components we need? (From CW IT)","Test cases

* Access with no VPN
* Access via GlobalProtect (CW)
* Access via Sigma VPN
* Access via CW VPN","Caught up with CK - 

Finish off SSO first

Then get developer access

@user due to this, No servicenow ticket available yet. @user

I‚Äôve been in discussion with Raveen to proceed with the firewall/ DNS resolution forwarding. 

Happy to continue with the firewall rules to enable access from VDI."
CSCI-270,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Aug/25 3:19 PM,02/Oct/25 2:13 PM,,On-Prem Network access to Snowflake Vnet setup,"(servicenow ticket no - Sorry still not available) Visibility Ticket: On-Prem Network Access to Snowflake VNet Setup (Mirroring ServiceNow ticket)

@user @user @user","* On premises network access to snowflake Vnet setup
* Connection tested","Caught up with CK - 

Finish off SSO first

Then get developer access

@user due to this, No servicenow ticket available yet. @user

@user 

Agreed with CK, Security and Azure that developer access will be VDI, so no need to do this. 

cc:@user"
CSCI-269,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Aug/25 3:06 PM,15/Aug/25 4:38 PM,,Source to Target for Facts - Fact_Product_Network_Average_Cost_History,"Mapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]

main facts to focus on are as per acceptance criteria

@user -will split this task out so that fits in to sprint cadence- done

notes from @user 

{quote}StockDB.dbo.productnetworkcosts and StockDB.dbo.productnetworkcostshistory{quote}

Product Network Cost Standardised Network cost that needs to be applied across all the inventory tables.

CWR standardised Names, definitions and prices across all the teams. This needs to be carried into the Data platform as well. 

attached is the PDF with the names as definitions. 

[^Cost Prices in CWR Network_Finalised.pdf]","* Priority 1 and Priority 2 Fact tables on control table will be mapped
** Reviewing Fact_Product_Network_Average_Cost_History","Product Network Cost Standardised Network cost that needs to be applied across all the inventory tables.

on Prem there are 2 tables Product Network Cost and Product Network Cost history. 

one is today‚Äôs value and the other has a daily snapshot. we need to join on all the snapshot tables to this on the snapshot date to get the prices for the correct day. 

the grain is 1 record per product per day."
CSCI-266,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Aug/25 7:46 AM,10/Oct/25 10:31 AM,,ILS - Manhattan Scale Integration - Ingestion of remaining 7 tables - development,"DATA IN for critical identified objects from Manhattan Scale db (ILS)

Implement data ingestion for critical identity objects from the on-premises Manhattan Scale (ILS) database into the cloud data platform. This includes configuring the necessary Azure Data Factory (ADF) components to support automated and reliable data flow.

Latest update;

* Ticket raised. REQ0156142
* Currently with the infrastructure cloud team - Brent - Cloud services manager.
* !image-20250729-063227 (1ed4c70f-781d-46cb-9ced-b8db1e7f0691).png|width=961,height=959,alt=""image-20250729-063227.png""!
Below are the list of tables to be ingested. Out of these the first one in the list i.e. Carrier is ingested in phase 1.

|ILS|Carrier|
|ILS|Functional_area_status_flow|
|ILS|Item|
|ILS|Location|
|ILS|Location_inventory|
|ILS|Routing_guide|
|ILS|Shipment_detail|
|ILS|Shipment_header|","Definition of done:

* -Extract meta created ()-
* -Linked services created-
* -Pipeline run end to end-
* Whitelisting of on-prem prod database server. (Done by @user )","ils db being added to firewall per new ticket from CK per Chloe request

Following up with @user on this

Have followed up with @user on this - have updated description - and will get further updates on this shortly.

Update:
- need to whiltelist this in Azure firewall.

* Ticket raised. REQ0156142
* Currently with the infrastructure cloud team - Brent.
*

Have caught up with @user CK|
This is tested and resolved. 
CK will update this.

Have updated acceptance criteria 

* whitelisting on prem server done by @user - TYSM!

@user can you follow this up

@user will do

@user @user I have split this into part 1 and part 2 so that if this doesn‚Äôt finish there will be work flowing to the next sprint.

Will work with @user on date/time

will be monitoring loads

Hi @user , As discussed please find below overview of data ingestion for ILS:

{adf:display=block}
{""type"":""table"",""attrs"":{""isNumberColumnEnabled"":false,""layout"":""center"",""localId"":""09c6f457-ef8a-4abc-b537-2088001bf9b1""},""content"":[{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""colwidth"":[212,106,159,159],""colspan"":4},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""ILS Ingestion into Snowflake""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""colwidth"":[212]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Table Name""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[106]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Row Count""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[159]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Copy to BLOB""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[159]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Total Pipeline run end to end""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""colwidth"":[212]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Carrier""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[106]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""1,551""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[159]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""1m 26s""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[159],""rowspan"":8},""content"":[{""type"":""paragraph""},{""type"":""paragraph""},{""type"":""paragraph""},{""type"":""paragraph""},{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":"" 9 mins""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""colwidth"":[212]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Functional_area_status_flow""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[106]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""25""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[159]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""1m 25s""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""colwidth"":[212]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Item""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[106]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""217,135""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[159]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""3m 19s""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""colwidth"":[212]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Location""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[106]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""208,815""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[159]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""1m 32s""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""colwidth"":[212]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Location_inventory""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[106]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""118,029""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[159]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""3m 13s""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""colwidth"":[212]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Routing_guide""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[106]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""38""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[159]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""1m 43s""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""colwidth"":[212]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Shipment_detail""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[106]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""103,025""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[159]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""3m 31s""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""colwidth"":[212]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Shipment_header""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[106]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""15,157""}]}]},{""type"":""tableCell"",""attrs"":{""colwidth"":[159]},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""1m 6s""}]}]}]}]}
{adf}

Next step: @userto contact CK to raise a ticket to whitelist the Prod Server.

Hi @user @user ,

Below 2 tables have got 15 days of history and not beyond that as data gets archived to other tables. 

|TDB15|ILS|Shipment_detail|
|TDB15|ILS|Shipment_header|

Do we need to consider the other 2 tables for ingestion where data gets archived from above tables? If yes, I am happy to consider in our next sprint. Currently these are not listed in our spreadsheet for ingestion.

[Data Source As-IS Details.xlsx (sharepoint.com)|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B3077E8A5-A280-43D1-BF3F-4A38BB4C4258%7D&file=Data%20Source%20As-IS%20Details.xlsx&action=default&mobileredirect=true]

|TDB15|ILS|AR_SHIPMENT_DETAIL|
|TDB15|ILS|AR_SHIPMENT_HEADER|

Issue split into:
|CSCI-375|ILS - Manhattan Scale Integration - Ingestion of remaining 7 tables - review|"
CSCI-264,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 3:41 PM,21/Aug/25 9:58 AM,,SKU objects ingestion - Initial and delta load,,"We need to set up pipelines for initial Delta loads for the following SKU Objects

|SKU|m_DestinationCode|
|SKU|m_DestinationType|
|SKU|m_LCP|
|SKU|SkuLcpActivityLog|
|SKU|SKUHeader|
|SKU|SKUdetails|
|SKU|SkuStoreExceptions|

* -Extract meta created-
* -Linked services created-
* -pipelines set up-
* -initial load tested-
* -delta load tested-
* -pipeline run from end to end-",seeking assistance from @user
CSCI-263,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 3:41 PM,26/Aug/25 12:53 PM,,CWMgtStoreInvoices objects ingestion,,"We need to set up pipelines for initial Delta loads for the following CWMgtStoreInvoices Objects

|CWMgtStoreInvoices|NonPharmXInvoiceHeader|
|CWMgtStoreInvoices|NonPharmxInvoiceItems|
|CWMgtStoreInvoices|PharmXInvoiceHeader3V0|
|CWMgtStoreInvoices|PharmxInvoiceItems3V0|
|CWMgtStoreInvoices|PharmXSupplier|

* -Extract meta created-
* -Linked services created-
* -pipelines set up-
* -initial load tested-
* -delta load tested-
* pipeline run from end to end","PharmXSupplier Skipped due to no records in production

Added one table. Starting historical load for big tables."
CSCI-262,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 3:37 PM,07/Oct/25 2:14 PM,,APPRISS Objects ingestion,"h2. Summary

We need to identify any additional objects required for APPRISS that have not yet been ingested into EDP Snowflake.

h2. Context

Most tables from the StockDb and TransactionStorage databases have already been ingested. This issue focuses on ensuring that all necessary objects for APPRISS are accounted for.

h2. Other information

N/A","We need to set up pipelines for initial and delta loads for the following APPRISS tables

TBA from [doc|https://mychemist-my.sharepoint.com/:w:/g/personal/frank_perez_chemistwarehouse_com_au/EehJD-B4GqdAv9Xehsr2r9ABiWtU--dVk-O07qbnzCP9vA?email=sig_phillip.yuen%40chemistwarehouse.com.au&e=bp6jUs] monday @user (apologies)

* -Extract meta created-
* -Linked services created-
* -pipelines set up-
* -initial load tested-
* -delta load tested-
* pipeline run from end to end","To follow up CK today

Thanks for bringing this into sprint @user

Will check with Harrison/Chloe today if we can get the data up. @user

To start with we have received below tables needed for APPRISS.

TransactionStorage.dbo.TRANSACTIONS_INVOICE
TransactionStorage.dbo.TRANSACTIONS
TransactionStorage.dbo.TRANSACTIONS_ELECTRONICPAYMENTS¬†
TransactionStorage.dbo.TransactionAuditSaleActivityLog
TransactionStorage.dbo.TransactionAuditSaleActivityLogHistory
TransactionStorage.dbo.TransactionReturns
TransactionStorage.dbo.TransactionType
TransactionStorage.dbo.TransactionType_Invoice

TDB08AX2012.StockDB.dbo.Products
TDB08AX2012.StockDB.dbo.StoreStaffDetails¬†
TDB08AX2012.GeneralReference.dbo.BranchInfoGlobal_EDP

Hi @user ,

Because below two tables needs historic Ingestion through Parquet and @user is still not able to access blob storage due to network policy changes , the ingestion of these two tables move into next sprint for which we need to create a separate ticket.

|TDB14|TransactionStorage|TRANSACTIONS_ELECTRONICPAYMENTS|

|TDB14|TransactionStorage|TransactionAuditSaleActivityLogHistory|

@user just confirmed wiht @user that we need 8 weeks.
best to create another ticket

@user Confirming, 8 weeks of transactional and audit history in initial data dump, followed by daily delta at T-2.

Issue split into:
|CSCI-372|APPRISS Objects ingestion - remaining talbes ingestion|"
CSCI-261,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 3:23 PM,29/Sep/25 8:10 AM,,Create Source-to-Target Map for Fact_Warehouse_Location_Outbound_Deliveries,Source to target mapping for Fact_Warehouse_Location_Outbound_Deliveries - filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",
CSCI-260,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 3:21 PM,12/Sep/25 6:21 PM,,Create Source-to-Target Map for Fact_Warehouse_Location_Space_Utilisation,Source to target mapping for Fact_Warehouse_Location_Space_Utilisation - filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",
CSCI-259,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 3:20 PM,16/Sep/25 10:29 AM,,Create Source-to-Target Map for Fact_Warehouse_Location_Cycle_Count,Source to target mapping for Fact_Warehouse_Location_Cycle_Count- filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no","Aasking for business process doc from Harrison - will follow up @user

Issue split into:
|CSCI-435|Create Source-to-Target Map for Fact_Warehouse_Location_Cycle_Count - part 2|"
CSCI-258,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 3:19 PM,12/Sep/25 6:22 PM,,Create Source-to-Target Map for Fact_Warehouse_Location_Inbound_Transactions,Source to target mapping for Fact_Warehouse_Location_Inbound_Transactions - filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",
CSCI-257,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 3:18 PM,07/Oct/25 9:08 AM,,Create Source-to-Target Map for Fact_Warehouse_Location_Packing,Source to target mapping for Fact_Warehouse_Location_Packing - filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no","Completed the mapping for Packing [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&action=default&mobileredirect=true&DefaultItemOpen=1&ct=1756953007481&wdOrigin=OFFICECOM-WEB.START.EDGEWORTH&cid=a737358a-69d2-4fb4-a238-a2d225802a7c&wdPreviousSessionSrc=HarmonyWeb&wdPreviousSession=363d545e-c6e7-4e45-a585-9d9cd0302ef7]

CC:- @user@user

Issue split into:
|CSCI-514|Review Source-to-Target Map for Fact_Warehouse_Location_Packing|"
CSCI-256,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 3:17 PM,07/Oct/25 9:08 AM,,Create Source-to-Target Map for Fact_Warehouse_Location_Replenishment,Source to target mapping for Fact_Warehouse_Location_Replenishment - filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",
CSCI-255,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 3:17 PM,12/Sep/25 6:20 PM,,Create Source-to-Target Map for Fact_Warehouse_Location_Picking,Source to target mapping for Fact_Warehouse_Location_Picking - filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",
CSCI-254,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 3:16 PM,12/Sep/25 6:20 PM,,Create Source-to-Target Map for Fact_Warehouse_Location_Putaway,Source to target mapping for Fact_Warehouse_Location_Putaway - filling out sheet as per Source-to-target facts,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",
CSCI-253,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 3:11 PM,01/Sep/25 2:13 AM,,Create Source-to-Target Map for Fact_Store_Inventory_Snapshot - Mapping,Source to target mapping for Fact_Store_Inventory_Snapshot - filling out sheet as per Source-to-target dims,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no","mapping has been completed

Issue split into:
|CSCI-376|Create Source-to-Target Map for Fact_Store_Inventory_Snapshot - Review|"
CSCI-252,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 3:09 PM,27/Aug/25 9:41 AM,,Create Source-to-Target Map for Fact_Store_Inventory_Intra,Source to target mapping for Fact_Store_Inventory_Intra - filling out sheet as per Source-to-target dims,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",The mapping has been completed
CSCI-250,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 2:53 PM,09/Sep/25 9:15 AM,,Create Source-to-Target Map for Fact_Store_inventory_adjustment,Source to target mapping for Fact_Store_Inventory_Adjustment - filling out sheet as per Source-to-target dims,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",the Store incremental is an movement table. which included all the type of movements. from sales to adjustments.
CSCI-248,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,13/Aug/25 9:38 AM,01/Sep/25 2:31 AM,,Create an SSO approach with SCIM + SAML,"This task involves implementing secure Single Sign-On (SSO) and automated user/group provisioning between *Microsoft Entra ID* and *Snowflake*, using *SAML 2.0* and *SCIM v2*, with all traffic routed through *Azure Private Link* to eliminate public exposure.

The configuration includes:

* Creating and configuring the Snowflake Enterprise App in Entra ID.
* Establishing SAML trust and metadata exchange between Entra ID and Snowflake.
* Enabling SCIM API integration in Snowflake.
* Setting up automatic provisioning in Entra ID.
* Validating SSO and SCIM flows.
* Applying security best practices and ongoing maintenance.","* The Entra ID Enterprise Application for Snowflake is created and configured with correct SAML attributes.
* SAML metadata is exchanged and validated between Entra ID and Snowflake.
* SCIM API integration is successfully created in Snowflake with OAuth credentials.
* Automatic provisioning is enabled in Entra ID and tested for users and groups.
* SSO login via Entra ID (both IdP-initiated and SP-initiated) is functional.
* All traffic is confirmed to route through Azure Private Link, with no public endpoints.
* Logging and monitoring are enabled in both Snowflake and Entra ID.
* Security controls such as MFA, RBAC, NSGs, and certificate rotation are implemented.
* Documentation and disaster recovery procedures are updated and tested.","Design is present - session to be held tomorrow

@user to reach out to @user @user later about filling this

Document available at: [SCIM+SAML - Snowflake and Entra ID SSO Configuration via Azure Private Link - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5170/SCIM-SAML-Snowflake-and-Entra-ID-SSO-Configuration-via-Azure-Private-Link]

Issue split into:
|CSCI-370|Create an SSO approach with SCIM + SAML part 2|"
CSCI-247,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/Aug/25 12:09 PM,13/Aug/25 9:49 AM,,Calculate daily data source sizes,"As a DBA, I want to collect the daily sizes of source data tables for TransactionStorage.","Get the data sizes of transactionStorage tables.

* *Data Collection*
** Retrieve the *daily size (in MB/GB)* of each table in the {{TransactionStorage}} schema.
** Include the following tables at minimum:
{{BranchOrderItems}}
{{BranchOrders}}
{{Store_IncrementalSOHChanges}}
{{Transactions}}
{{Transactions_Invoice}}
* *Row Count Metrics*
** Capture the *average number of rows added daily* for each table listed above.
** Use production environment data for accuracy.
* *Output Format*
** Present the data in a *tabular format* with columns:
{{TableName}}
{{AvgDailyRows}}
{{DailySizeMB}}
** Optionally include a timestamp or date for each data point.
* *Automation*
** The data collection process should be *automated to run daily*.
** Results should be stored or logged in a location accessible to the DBA team (e.g., shared folder, dashboard, or monitoring tool).
* *Validation*
** Ensure the script or process runs successfully for at least *3 consecutive days* without errors.
** Validate that the reported sizes and row counts match actual database metrics.","@user Average no of daily rows added in TransactionStorage tables in Prod and DEV

|TableName|PROD|DEV|
|BranchOrderItems|¬†¬†¬†¬†¬†¬† 569,871|¬†¬† 651,906|
|BranchOrders|¬†¬†¬†¬†¬†¬†¬†¬†¬† 22,503|¬†¬†¬†¬† 24,467|
|Store_IncrementalSOHChanges|¬†¬† 2,617,223|¬†¬†¬†¬† 14,557|
|Transactions|¬†¬†¬†¬†¬†¬† 195,153|¬†¬† 189,938|
|Transactions_Invoice|¬†¬†¬†¬†¬†¬† 504,750|¬†¬† 475,245|"
CSCI-246,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,08/Aug/25 3:08 PM,11/Aug/25 9:16 AM,,UPdating control table to have DIMs and status colour codes,"Enhance the control table by:

* Adding Dimension (DIM) references.
* Implementing status colour codes to visually represent the state of each DIM.

This update aims to improve traceability and provide quick visual cues for data status across the pipeline.","* -control table has DIMS-
* -Colour codes implemented-",cc @user
CSCI-245,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,06/Aug/25 4:31 PM,12/Sep/25 6:14 PM,,Enable developer access to snowflake via Privatelink,"Enable developer access to snowflake via Privatelink - via browser 

User story:

* As a snowflake person (not service) user, I want to access snowflake via Privatelink so that it is accessible and secure (confirming to security requirements).

@user - Below are the key action items as identified by Eugene for us as next steps.

Details are [here|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5145/Developer-Access?anchor=**developer-access-to-snowflake-via-azure-privatelink**]

Can you please help us engage with Cloud team and raise a servicenow ticket.

cc @user 

*Key action items* 

* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com]
* Configure firewalls (On-Prem, Azure) as follows

* On-Prem Network for developers ‚Üí Snowflake VNet *(servnicenow ticket)*
* VDI / Jumpbox network ‚Üí Snowflake VNet *(servicenow tickets)*
** *how many jumpboxes* we need?
** *What are the things we need?*
* VPN Network for developers ‚Üí Snowflake VNet (servicenow ticket)
** QoS? - traffic prioritisation
** What tools are involved.
** CW - Palo Alto GlobalProtect.
* Note:
** Ports to be allowed: 443, 80, 1433
* Configure VDI / Jumbox images to pre-install tools defined above
* Create Development VMs in Snowflake Subscription","* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/]
* Access snowflake via [https://app.snowflake.com/cw/au/#/homepage|https://app.snowflake.com/cw/au/#/homepage] after infrastructure changes.
* Test cases
** Access on premises (office network)
** Access off premises
*** (fail path) access with no VPN
*** Access via GlobalProtect (CW)
*** Access via Sigma VPN
** Access via PBI
*","@usercan you please check with CK where we are? As CK doesn't attend the stand-up, maybe this item should be under Eugene?

Update from CK - this is still in progress.

Hey @user - would you like me to create separate tickets for this to reflect the work required for this?
Or would you like to keep everything in this ticket?

I‚Äôll assign this ticket to you - let me know how you would like to structure/manage this ticket.

Thank you.

Caught up with @user

Updated ticket and indicated which parts need servicenow tickets

Updated AC with light test case.

cc @user @user

Issue split into:
|CSCI-431|Enable developer access to snowflake via Privatelink |"
CSCI-244,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,06/Aug/25 9:43 AM,11/Aug/25 8:56 AM,,Understanding current architecture,"As an engineer - I would like to receive the current in future state of architecture so that I know what type of work I need to create and where my work is going towards (i.e. a goal)

Key areas:

* DLA
* @user - I might need your help to fill the rest of this out","* -receive documentation about the current architecture-
** -receive documentation about architecture we are working towards-
* Ask relevant questions about where we currently are at to fill knowledge gaps
* Able to apply professional knowledge and opinion on current work
* Able to leverage architecture knowledge to create own Jira tasks with acceptance criteria levera","@useryou may be able to help on this
I think some parts of this we can already tick off

@user I‚Äôve ticked some of the above off. In saying that of course you can ask for further info where needed.

LMK if you require further assistance"
CSCI-243,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,06/Aug/25 9:34 AM,30/Aug/25 3:12 PM,,Understanding current architecture,"As an engineer - I would like to receive the current in future state of architecture so that I know what type of work I need to create and where my work is going towards (i.e. a goal)

Key areas:

* DLA
* @user - I might need your help to fill the rest of this out","* -receive documentation about the current architecture - now reviewing-
** receive documentation about architecture we are working towards
* Ask relevant questions about where we currently are at to fill knowledge gaps
* Able to apply professional knowledge and opinion on current work
* Able to leverage architecture knowledge to create own Jira tasks with acceptance criteria levera","@useryou may be able to help on this
I think some parts of this we can already tick off

Much of these we have covered yesterday in the office. @user pls reach out if you have any question. 

FYI @user

@user I think we can tick some of the above off. In saying that of course you can ask for further info where needed.

LMK if you require further assistance

Phil

Hi @user, Currently going through the Documentation from altis for DLA and walkthroughs provided by Chloe. Yesterday, got few questions clarified. Let‚Äôs keep this till today as we have adf walkthrough scheduled.

Issue split into:
|CSCI-374|Understanding current architecture - please fill out coverage |"
CSCI-242,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,06/Aug/25 9:28 AM,07/Aug/25 11:33 AM,,removing duplicates from CUSTINVOICETRANS,"I have created this ticket in lieu of data quality checks @user 

I may need more details - will ask during standup 6/8",* Duplicates are removed from CUSTINVOICETRANS table,"Getting the below SQL access error

SQL access control error:

{noformat}Insufficient privileges to operate on table 'CUSTINVOICETRANS'{noformat}

@user I am assign to OPS_DEV_RO read only role. Please grant delete permission for this table.

Duplicates removed from the CUSTINVOICETRANS Table.

There is no straight forward delete as this table does not have any unique column to identify the delete and snowflake does not support rownum, cte and qualify for deleting duplicates without unique column. Used the below workaround to achieve this.

Actions Taken: -

* Created CUSTINVOICETRANS_DuplicatesRemoved with distinct records.
* Renamed CUSTINVOICETRANS to CUSTINVOICETRANS_Backup.
* Renamed the new table to CUSTINVOICETRANS.

Scripts:-

Create table CUSTINVOICETRANS_DuplicatesRemoved as
SELECT ASSETBOOKID,ASSETID,BILLINGCODE,COMMISSAMOUNTCUR,COMMISSAMOUNTMST,COMMISSCALC,COUNTRYREGIONOFSHIPMENT,CREATEDBY,CREATEDDATETIME,CURRENCYCODE,CUSTINVOICELINEIDREF,CUSTOMERLINENUM,DATAAREAID,DEFAULTDIMENSION,DELIVERYPOSTALADDRESS,DELIVERYTYPE,DEL_CREATEDTIME,DISCAMOUNT,DISCPERCENT,DLVDATE,EINVOICEACCOUNTCODE,EXTERNALITEMID,INTERCOMPANYINVENTTRANSID,INTRASTATDISPATCHID,INTRASTATFULFILLMENTDATE_HU,INVENTDIMID,INVENTQTY,INVENTREFID,INVENTREFTRANSID,INVENTREFTYPE,INVENTTRANSID,INVOICEDATE,INVOICEID,ITEMCODEID,ITEMID,LEDGERDIMENSION,LINEAMOUNT,LINEAMOUNTMST,LINEAMOUNTTAX,LINEAMOUNTTAXMST,LINEDISC,LINEHEADER,LINENUM,LINEPERCENT,MCRDELIVERYNAME,MCRDLVMODE,MODIFIEDDATETIME,MULTILNDISC,MULTILNPERCENT,NAME,NGPCODESTABLE_FR,NUMBERSEQUENCEGROUP,OLAPCOSTVALUE,ORDERLINEREFERENCE_NO,ORIGCOUNTRYREGIONID,ORIGSALESID,ORIGSTATE,PARENTRECID,PARTDELIVERY,PARTITION,PDSCWQTY,PDSCWQTYPHYSICAL,PDSCWREMAIN,PORT,PRICEUNIT,QTY,QTYPHYSICAL,REASONREFRECID,RECID,RECVERSION,REMAIN,REMAINBEFORE,RETAILCATEGORY,RETURNARRIVALDATE,RETURNCLOSEDDATE,RETURNDISPOSITIONCODEID,REVERSECHARGEAPPLIES_UK,REVERSEDRECID,SALESCATEGORY,SALESGROUP,SALESID,SALESMARKUP,SALESPRICE,SALESUNIT,SOURCEDOCUMENTLINE,STATLINEAMOUNTMST,STATPROCID,STOCKEDPRODUCT,SUMLINEDISC,SUMLINEDISCMST,TAXAMOUNT,TAXAMOUNTMST,TAXAUTOGENERATED,TAXGROUP,TAXITEMGROUP,TAXWITHHOLDGROUP_TH,TAXWITHHOLDITEMGROUPHEADING_TH,TAXWRITECODE,TRANSACTIONCODE,TRANSPORT,WEIGHT,ETL_FILE_NAME,ETL_ROW_DELETED_FLAG,ETL_INSERT_AUDIT_KEY,ETL_UPDATE_AUDIT_KEY,ETL_ROW_EFFECTIVE_DATE,ETL_ROW_EXPIRY_DATE,ETL_ROW_CURRENT_FLAG,ETL_INSERT_DATETIME,ETL_UPDATE_DATETIME,ETL_INSERT_JOB_NAME,ETL_UPDATE_JOB_NAME
FROM (
 SELECT *,
 ROW_NUMBER() OVER (PARTITION BY RECID ORDER BY RECID) AS rn
 FROM CUSTINVOICETRANS
) AS check_dups
WHERE rn = 1;

ALTER TABLE EDP_DEV.STG_SCAX2012.CUSTINVOICETRANS RENAME TO EDP_DEV.STG_SCAX2012.CUSTINVOICETRANS_BACKUP070825

ALTER TABLE EDP_DEV.STG_SCAX2012.CUSTINVOICETRANS_DuplicatesRemoved RENAME TO EDP_DEV.STG_SCAX2012.CUSTINVOICETRANS"
CSCI-241,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Aug/25 5:17 PM,15/Aug/25 4:38 PM,,Source to target Facts - Fact_Store_Inventory,"This task involves mapping out target fact tables using the source-to-target sheet, with a focus on the key facts outlined in the acceptance criteria. The main fact tables to be addressed include:

* {{Fact_Store_Inventory_Intra}}
* {{Fact_Store_Inventory_Adjustment}}
* {{Fact_Store_Inventory_Snapshot}}

These tables are listed as Priority 1 and Priority 2 in the control table and are critical for meeting the acceptance criteria. The goal is to ensure accurate and complete mapping from source to target for these facts.","Priority 1 and Priority 2 Fact tables on control table will be mapped

* Fact_Store_Inventory_Intra
* Fact_Store_Inventory_Adjustment
* Fact_Store_Inventory_Snapshot","Note - not a daily snapshot - just movement

Going thru logic i.e. filtration.
Will take longer time

Fact_Store_Inventory_snapshot and Fact_Store_Inventory_Intra

ETL Logic onPrem. 

Step 1: 

Insert into Staging : 

{noformat}select rn=row_number() over (partition by DateStampDayOnly, MyCHemID, OriginBranchID order by DateStamp desc, RecordID desc)
 ,*
 ,1 as isPlaceholder
 from Fact_Store_IncrementalSOHChanges
 where DateStamp >= ""PreviousDate"" and DateStamp < ""CurrentDate""
 ) dt2
 where rn = 1 {noformat}

Insert into the new data from source which is only housing data changes 

{noformat}insert Into Staging 
select 
 RecordID, MychemID, AvgRealCost, SOH, DateStamp
 , coalesce(ChangeType, 26) as ChangeType -- set null ChangeType to SOH Import
 , coalesce(SOHChange, 0) as SOHChange
 , StaffID, RealCost, ExpectedSales, OriginBranchID, DateStampDayOnly
 , 0 as SOH_Last
 , 0 as ExpectedSales_Last
 , 0 as LineNumber
 , DateStamp as DateStampOriginal
 , 0 as isPlaceholder
 , hasExcessiveValue
from 
 Store_IncrementalSOHChanges
where 
 DateStamp >= ""currentDate"" and 
 DateStamp < ""nextDate"" and
 RecordID > ""maxRecordID"" {noformat}

Then It Self joins and deletes all records where isPlaceholder = 1 and has a record in the second join where the isPlaceholder = 0

{noformat}delete Staging 
from
(
 select nr.OriginBranchID, nr.MyCHemID
 from Staging nr
 where 
 nr.isPlaceholder = 0
 and nr.DateStamp >= @currentDate
 and nr.DateStamp < @nextDate 

) dt
where 
 Staging.isPlaceholder = 1
 and Staging.DateStamp >= ""currentDate""
 and Staging.DateStamp < ""nextDate""
 and Staging.OriginBranchID = dt.OriginBranchID
 and Staging.MychemID = dt.MychemID{noformat}

this ensures that we will have one record for every ChemID , for every Store fore everyday. 

The ETL for this table must be really thought through as it can get quite resource hungry."
CSCI-239,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Aug/25 2:24 PM,08/Oct/25 11:15 AM,,MergeCo Conformed Reporting - Test PBI Dashboard,,,
CSCI-238,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Aug/25 2:24 PM,08/Oct/25 11:15 AM,,MergeCo Conformed Reporting - Develop PBI Dashboard,,,"@user

Currently blocked as unable to proceed further in development.

Currently awaiting on access from [https://sigmahealthcare.atlassian.net/browse/CSCI-288|https://sigmahealthcare.atlassian.net/browse/CSCI-288] to be unblocked before continuing further.

I wasn‚Äôt really too sure how much work was actually spent here @user before it got blocked - so I moved this task rather than splitting it.

@user 1 day"
CSCI-237,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Aug/25 2:23 PM,08/Oct/25 11:15 AM,,MergeCo Conformed Reporting - Combine Facts and Dims from CW + Sigma,,,
CSCI-236,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Aug/25 2:23 PM,12/Sep/25 2:33 PM,,MergeCo Conformed Reporting - Pull Data from Sigma to CW SF Tenancy,"Copy over from Sigma the below KPIs:

* Inventory
** *SOH* (Units + $)
*** Stock on Hand
** *DOI*
*** Days of Inventory
**** Avg SOH $ / avg Daily Sales $ at Cost (last 12 weeks)
* Outbound Performance
** *ATP*%
*** Available to Promise
**** Confirmed / Ordered
** *DIF*%
*** Invoiced / Confirmed
** *Cust(Customer) Impact* (% + $)
*** Invoiced / Ordered
** *DOT*%
*** Qty Units Dispatched in 24 hrs / Qty Units Dispatched

Grain:

* High-level: Per DC, Per Day","need to build queries on sigma side for mergeco KPI reporting view.

# *Data Accuracy*:
#* Ensure all KPIs copied from Sigma are accurate and match the source data.
#* Verify that the values for SOH, DOI, ATP%, DIF%, Cust Impact, and DOT% are correctly reflected.
# *KPI Completeness*:
#* All attriubutes are exist to compute the KPIs:
#** Inventory: SOH (Units + $), DOI
#** Outbound Performance: ATP%, DIF%, Cust Impact (% + $), DOT%
# *Calculation Verification*:
#* Confirm that the calculations for Avg SOH $ and avg Daily Sales $ at Cost for DOI are performed correctly based on the last 12 weeks of data.
# *Documentation*:
#* Provide documentation outlining the data sources, calculations, and any assumptions made during the data pull process.
# *User Acceptance Testing (UAT)*:
#* Conduct UAT with end-users to ensure the report meets their needs and expectations.
#* Can also be done by cross check with existing KPI report","Per DC per day all attribute used to calculate the KPIs

@user - Try this

[^3PL Phase 2 Internal Report.pbix]

[^colorTheme.json]

Raw script

{noformat}SELECT kpi.""Date_Key"",
 dd.""Full Date"",
 dd.""Day Month Year (DD/MM/YYYY)"",
 ""Day Indicator"",
 ""Month Indicator"",
 ""Year Indicator"",
 kpi_attr.""Plant_Code"",
 kpi_attr.""Plant_Name"",
 -- SOH qty and $
 SUM(IFF(kpi_def.""System_Name"" = 'SOHDollar', kpi.""Value_Current_Numerator"", 0)) AS ""SOH_amt"",
 SUM(IFF(kpi_def.""System_Name"" = 'SOHQty', kpi.""Value_Current_Numerator"", 0)) AS ""SOH_Qty"",
 -- DOI TO REVIEW
 SUM(IFF(kpi_def.""System_Name"" = 'VPRSDollar', kpi.""Value_Current_Numerator"", 0)) as ""VPRS$"",
 --ATP%
 DIV0(SUM(IFF(kpi_def.""System_Name"" = 'ATP', kpi.""Value_Current_Numerator"", 0)),
 SUM(IFF(kpi_def.""System_Name"" = 'ATP', kpi.""Value_Current_Denominator"", 0))) AS ""ATP"",
 --DIF%
 DIV0(SUM(IFF(kpi_def.""System_Name"" = 'DIF', kpi.""Value_Current_Numerator"", 0)),
 SUM(IFF(kpi_def.""System_Name"" = 'DIF', kpi.""Value_Current_Denominator"", 0))) AS ""DIF"",
 --Cust Impact$,%
 DIV0(SUM(IFF(kpi_def.""System_Name"" = 'CustImpactPercentage', kpi.""Value_Current_Numerator"", 0)),
 SUM(IFF(kpi_def.""System_Name"" = 'CustImpactPercentage', kpi.""Value_Current_Denominator"",
 0))) AS ""CustImpact%"",
 SUM(IFF(kpi_def.""System_Name"" = 'CustImpactDollar', kpi.""Value_Current_Numerator"",
 0)) AS ""CustImpact$"",
 --DOT %
 DIV0(SUM(IFF(kpi_def.""System_Name"" = 'DOT', kpi.""Value_Current_Numerator"", 0)),
 SUM(IFF(kpi_def.""System_Name"" = 'DOT', kpi.""Value_Current_Denominator"", 0))) AS ""DOT%"",
--Others
 SUM(IFF(kpi_def.""System_Name"" = 'OrderQty', kpi.""Value_Current_Numerator"", 0)) AS ""OrderQty"",
 SUM(IFF(kpi_def.""System_Name"" = 'ConfirmedOrderQty', kpi.""Value_Current_Numerator"",
 0)) AS ""ConfirmedOrderQty"",
 SUM(IFF(kpi_def.""System_Name"" = 'InvoiceQty', kpi.""Value_Current_Numerator"",
 0)) AS ""InvoiceQty""
FROM EDP_PROD.PRES.""Fact_KPI_Summary"" kpi
 INNER JOIN
 pres.""Dim_KPI_Attribute"" kpi_attr ON kpi.""KPI_Attribute_Key"" = kpi_attr.""KPI_Attribute_Key""
 INNER JOIN
 pres.""Dim_KPI_Definition"" kpi_def ON kpi.""KPI_Definition_Key"" = kpi_def.""KPI_Definition_Key""
 INNER JOIN pres.""Dim_Date_VW"" dd on kpi.""Date_Key"" = dd.""Date Key""
WHERE 0 = 0
 AND kpi_attr.""Material_Group_1_Code"" NOT IN ('MED', 'UKN', 'N/A')
GROUP BY kpi.""Date_Key"", dd.""Full Date"",
 kpi_attr.""Plant_Code"",
 kpi_attr.""Plant_Code"", kpi_attr.""Plant_Name"", dd.""Day Month Year (DD/MM/YYYY)"", ""Day Indicator"",
 ""Month Indicator"", ""Year Indicator""

{noformat}

Next:

Review DOI logic & build test case

Test Cases - WIP (Will need to add more cases?)

# Figure to match current Sigma KPI report

{code:python}(
'1201',
'20250903',
[
 (""SOH_amt"",97867067.1,DecimalType()),
 (""OrderQty"",364891,IntegerType()),
 ('ConfirmedOrderQty',325137,IntegerType()),
 ('InvoiceQty',319391,IntegerType()),
 (""DIF%"",0.9823,DecimalType()),
 ( ""CustImpact$"",705547.99,DecimalType()),
 (""CustImpact%"",0.8753,DecimalType()),
 (""DOT%"",1.00,DecimalType())
]
)
]{code}

[^test_sigdatashare.py]

Hey @user ,

I ran the testing script and is align with sigma‚Äôs KPI report

The only attribute that is not directly pulled is DOI as it would be easier to calculate the 12 week avg in PBI ‚Üí i.e. SOH$/VPRS$(12 week avg)

Do you wanna give it a good and see that align what you need? Then i will adjust base on that

@user Hi Han!

As discussed this looks great üôÇ I will speak to Harrison about looking to see if we want to include the product level in this view. As well as getting him to enable datasharing of this to our CW SF tenancy.

Will mark this ticket as done üôÇ 

Cheers!

Just note for self

if we wanna bring in product grain need to

update {{DIM_KPI_ATTRIBUTE}} worker view ‚Üí Currently we have Dim_product info hence just need to add the column"
CSCI-235,CSCI,CW Cloud Data Platform Interim Solution,Story,Done,Medium,05/Aug/25 8:44 AM,20/Nov/25 8:44 AM,,Source-To-Target-Mapping for DIMS - Core Master Data,"h3. *Objective*

To establish a comprehensive Source-To-Target Mapping (STTM) for key Core Master Data dimensions that will be used in the Control Table and downstream data processes. This mapping will serve as a foundational reference for data ingestion, transformation, and reporting within the Supply Chain domain.

h3. *Description*

This task involves identifying and documenting the source and target structures for the following core Master Data dimensions:

*Core Master Data*

* Product
* DC/Warehouse/Plant
* Store
* Manhattan Warehouse Location (What Grain? The Bin?)
* Supplier
* Manufacturer
* Product-Plant (SKU)
* Date
* Batch (Product Batch)

The mapping should include:

* Source system and table details
* Target schema and table structure
* Transformation logic (if applicable)
* Grain and SCD (Slowly Changing Dimension) policies for each dimension ( Confirmed? @user )

* confirmed that SCD policy is needed for each dimension","* Mapping is completed and reviewed for all listed dimensions
* -[STTM document|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true] updated with Dims-
* Mapping Completed for these DIMs
** -Product (to be reviewed)-
** DC/Warehouse/Plant
** -Store (to be reviewed)-
** Manhattan Warehouse Location (What Grain? The Bin?)
** Supplier
** Manufacturer
** Product-Plant (SKU)
** Date
** Batch (Product Batch)","Hey @user , @user , @user ,

Can you review and suggest any other core Master Data elements we should include in this ticket for initial core master Data?

@user can you create sample DIM in STTM document and add all DIMs to Control Table?

Thanks,

Harrison

@user thanks Harrison, these look great üôÇ 

Only thing I think is missing is a Date dim üóìÔ∏è

@user 

haha Im an idiot, good point

Dim_Store and Dim_Product mapping is completed

@user @user Can you review it please

Will split this card to separate tickets and retire this card. 

Will link cards

Converted to story, linked all cards, and removed from backlog to reduce noise

@user @user @user"
CSCI-230,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,04/Aug/25 6:46 AM,14/Aug/25 11:37 AM,,PowerBI - Infrastructure /gateway setup - review,,"Checking if the below setup aligns to connection needs.

# *Connection Availability*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Two Power BI connections are available in Fabric:""}],""attrs"":{""localId"":""455a3c43-7f3a-4fa2-b526-d6e824179873"",""state"":""TODO""}}],""attrs"":{""localId"":""2c1368f5-d548-4845-bb95-7855192fb70c""}}
{adf}
#* {{SF_S_POWERBI_DEV}} (On-Premise Data Gateway)
#* {{SF_S_POWERBI_DEV_VGW}} (VNet Data Gateway)
# *Access Permissions*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""The Azure group ""},{""type"":""text"",""text"":""Azure-Permissions-dev-edp-aus-Contributor"",""marks"":[{""type"":""code""}]},{""type"":""text"",""text"":"" has access to both connections.""}],""attrs"":{""localId"":""eb67f980-c57b-4fb1-95ad-8d98f3bc1b73"",""state"":""TODO""}}],""attrs"":{""localId"":""3f813f8b-704e-46e0-b564-8e9b0cab0748""}}
{adf}
# *Compute Management for On-Premise Gateway*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""When using ""},{""type"":""text"",""text"":""SF_S_POWERBI_DEV"",""marks"":[{""type"":""code""}]},{""type"":""text"",""text"":"", the VM ""},{""type"":""text"",""text"":""cwr-ase-edpdatagateway-dev-vm-01"",""marks"":[{""type"":""code""}]},{""type"":""text"",""text"":"" must be started in Azure.""}],""attrs"":{""localId"":""1508f1e3-d3c5-4200-984c-eac7ddf2f787"",""state"":""TODO""}},{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""After usage, the VM must be stopped to reduce cost.""}],""attrs"":{""localId"":""55934423-95d3-4ed3-984c-2ce9d40ec550"",""state"":""TODO""}}],""attrs"":{""localId"":""b1828406-1e16-4fd0-9d61-b7fe195d2117""}}
{adf}
# *Compute Management for VNet Gateway*

* When using {{SF_S_POWERBI_DEV_VGW}}, the Fabric capacity {{cwraseedpdevfc}} must be resumed.
* After usage, the capacity must be paused to save cost.

# *Trial Capacity Option*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""The VNet Data Gateway ""},{""type"":""text"",""text"":""cwr-ase-edp-dev-vdgw"",""marks"":[{""type"":""code""}]},{""type"":""text"",""text"":"" can optionally be assigned to a Trial Capacity.""}],""attrs"":{""localId"":""8f87142b-75dd-4bae-b49f-f310272f43bf"",""state"":""TODO""}}],""attrs"":{""localId"":""10f6f76d-e77f-4a61-be43-c42152915315""}}
{adf}",
CSCI-225,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,04/Aug/25 6:22 AM,14/Aug/25 11:37 AM,,Team Access to SF via PrivateLink Setup - Review,"This card is a review card in relation to [https://sigmahealthcare.atlassian.net/browse/CSCI-199|https://sigmahealthcare.atlassian.net/browse/CSCI-199] 

Work has already been done for this card.","* -Work for Team access to privatelink has been reviewed and passed-
** -A clear decision is made on how to organize team access to Snowflake via PrivateLink.-
** -Documentation of the current and proposed access methods for both SIGMA and CW.-
** -Identification of any blockers or dependencies.-
** -Agreement on next steps and responsible parties.-","To check if there is a Jira ticket for Developer Access/ VDI for Snowflake log in @user. If there isnt, create a new one. Story points: 3."
CSCI-223,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,04/Aug/25 6:02 AM,11/Aug/25 3:24 PM,,Adeel - On-prem DEV/PROD Database Environment Sync - part 2,"Background

{quote}Following a review of the on-prem DEV and PROD environments for key databases (StockDB, SCAX2012, ILS, TransactionStorage), discrepancies were identified in schema alignment and data volume. Adeel Syed provided a detailed comparison, highlighting schema mismatches in StockDB and data size differences in TransactionStorage and ILS.

Notably, SCAX2012 was recently refreshed and is up-to-date.

@user has proposed a prioritised refresh plan to align DEV with PROD, particularly to support accurate modelling and pipeline validation in Snowflake environments.

Rachel Wan confirmed that DEV environments are refreshed only upon request and are not automatically synced with PROD.

*Summary of Findings:*

* *SCAX2012*: Recently refreshed (11 July 2025) ‚Äì no further action required.
* *StockDB*: Schema drift identified across 7 tables (refer to _SchemaCompare.docx_) ‚Äì refresh required to align DEV with PROD.
* *TransactionStorage*: DEV has more records than PROD due to regular archiving in PROD (not duplication). Tables archived include:
** {{BranchOrderItems}}, {{BranchOrders}}, {{Store_IncrementalSOHChanges}}, {{Transactions}}, {{Transactions_Invoice}}
** {{Shipment_detail}}, {{Shipment_header}}
* *ILS*: Data size gaps observed ‚Äì refresh recommended but not urgent.

*Proposed Refresh Priority:*

# *StockDB* ‚Äì to resolve schema mismatches.
# *TransactionStorage* ‚Äì to align data volume and ensure DEV reflects current PROD state.
# *ILS* ‚Äì to be scheduled after the above.{quote}","*StockDB refresh*

* -NA - due to dev work from other project this DB (@user you may need to help me elaborate on this)-

*TransactionStorage refresh:*

* -TransactionStorage Restore complete-
* @user I‚Äôll need help creating further points from here.

*ILS refresh:*

* -Clarify risks that are associated with ILS refresh-","how soon do we need ILS refresh?
 - TBD @user

Will not proceeed with ILS refresh @user @user"
CSCI-222,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Aug/25 5:48 PM,23/Sep/25 3:40 PM,,Enable SSO login to snowflake - SSO,"Consdering the options between 

* MS Entra ID
* Okta

CK to have a follow-up call with Brent from Cloud to firm up the plan for SSO implementation

* -book next tuesday to follow up-

Meeting completed on Tues:

Implementation of Microsoft Entra ID SSO for Snowflake access across the entire data platform, enabling unified authentication for all user personas and tools including PowerBI, Tabular Editor 3, Azure Data Factory, DBT, and direct Snowflake access.","* --booking made for tuesday meeting--
* --To follow-up from the results of the call on Tuesday--
* --Decision made between MS Entra ID vs Okta- ‚úÖ *MS Entra ID Selected*-

* -Meeting with Brent from Cloud team scheduled- ‚úÖ *Completed*
* *Entra ID SSO configured and functional for Snowflake*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Work with Raveendran from our cloud team ""},{""type"":""mention"",""attrs"":{""id"":""557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84"",""accessLevel"":"""",""localId"":""5a778d84-3fcf-46a6-8eed-a75d321d3ac5""}},{""type"":""text"",""text"":"" and ""},{""type"":""mention"",""attrs"":{""id"":""712020:a3bb398f-73df-472d-a226-f67ed86b49ee"",""accessLevel"":"""",""localId"":""542bb25d-3701-49bc-be49-7b5f9c0a5bea""}},{""type"":""text"",""text"":"" ""}],""attrs"":{""localId"":""0a6a01ee-0890-4b42-99a3-17e68a53d8bb"",""state"":""DONE""}}],""attrs"":{""localId"":""340fd36c-fd9c-494d-93b5-65b83210118a""}}
{adf}
* *Service Principal OAuth setup for PowerBI Service scheduled refresh*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Work with Raveendran from our cloud team ""},{""type"":""mention"",""attrs"":{""id"":""557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84"",""accessLevel"":"""",""localId"":""29da3589-921e-4bd8-9020-7c2027ba319a""}},{""type"":""text"",""text"":"" and ""},{""type"":""mention"",""attrs"":{""id"":""712020:a3bb398f-73df-472d-a226-f67ed86b49ee"",""accessLevel"":"""",""localId"":""7cfbbe27-380e-4949-a2c5-ab2e425ff05a""}},{""type"":""text"",""text"":"" ""}],""attrs"":{""localId"":""7d2f681f-168c-4a83-8bc1-c56634d2d967"",""state"":""TODO""}}],""attrs"":{""localId"":""340fd36c-fd9c-494d-93b5-65b83210118a""}}
{adf}
* *Network security implemented (VNet Gateway)*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":"" ""},{""type"":""mention"",""attrs"":{""id"":""557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84"",""accessLevel"":"""",""localId"":""4d11860e-95ff-49b1-bcd3-89eb13c3af94""}},{""type"":""text"",""text"":"" - as part of the SSO can we also collaborate on this""}],""attrs"":{""localId"":""736cca4b-dae9-4882-9b4e-de06b4790373"",""state"":""TODO""}}],""attrs"":{""localId"":""819123db-3a96-41ca-8543-00cb537f3b64""}}
{adf}
* *All user personas can authenticate successfully*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Testing post entra ID sso dependencies completed""}],""attrs"":{""localId"":""c9b6f8b8-7224-4cd6-9ba4-1b8c34b4d47d"",""state"":""TODO""}}],""attrs"":{""localId"":""db194595-b904-41c9-8026-19c645d2aaac""}}
{adf}
* *Conditional access policies and MFA enforced*
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Testing post entra ID sso dependencies completed""}],""attrs"":{""localId"":""b541fd8a-1751-4451-945b-4cd78a6a3069"",""state"":""TODO""}}],""attrs"":{""localId"":""db194595-b904-41c9-8026-19c645d2aaac""}}
{adf}
* *Production rollout completed*","pretty sure I can update this @user 
I think we got a foloow up session but the result is to use VDI for DE‚Äôs for WFH

* I‚Äôll enter notes of phase 1 and 2 as per presentation and timeline/resource requirements

LMK if there‚Äôs anything extra I need ot add

Follow up session - as per @useremail - will elaborate on this.

@user Thanks for the update CK!

Need servicenow tickets for:

* *Entra ID SSO configured and functional for Snowflake*

Splitting out Jira tickets here for monitoring.

For reference @user @user

snowflake user group created

Issue split into:
|CSCI-462|Enable SSO login to snowflake - SCIM|"
CSCI-221,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Aug/25 2:28 PM,15/Aug/25 9:46 AM,,Implement Partial Full Apply Mode with Sliding Window Delete Tracking,"We need to follow up on the implementation of a partial full apply mode for handling very large datasets. Instead of a full apply, the approach will track deletes within a sliding window (e.g. last 6 months). This should also be integrated with archive and retention policy analysis on the source system.","* Define the parameters for the sliding window.
* Align with data retention and archival policies.
* Document the implementation plan and any dependencies.","need
- how data is archived

csci-186"
CSCI-220,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,31/Jul/25 5:29 PM,15/Aug/25 4:38 PM,,Source to Target for Facts - Fact_Warehouse_Location_inventory,"Mapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]

main facts to focus on are as per acceptance criteria

@user -will split this task out so that fits in to sprint cadence- done

notes from @user 

{quote}The Stock Fact_Warehouse_Inventory_Snapshot- the mapping is done based off the PBI05.supplychain.scax.dcsinventoryhistory

base tables: INVENTDIM and INVENTDIM. 
Things to note: 

* DataareaID- this defines the business entities. 
* read from the most current partition: This function gives the most current partition; SCAXLink.[dbo].[udf_GetPartition] (null)
* ensure Nolock is used for these reads.{quote}

{quote}

Fact_Warehouse_Inventory_Adjustment: Based of [BI_Presentation].[dbo].[WarehouseInventoryAdjustments]

E.g

{quote}","* Priority 1 and Priority 2 Fact tables on control table will be mapped
** -Reviewing Fact_Warehouse_Location_Inventory_Intra-
** -Reviewing Fact_Warehouse_Location_Inventory_Snapshot-
** Fact_Warehouse_Location_Inventory_Adjustment","Hey @user ,

Who created the details here? Description and ticket Acceptance Criteria dont seem to marry up?

Thanks,

Harrison

Reviewing Fact_Warehouse_Location_Inventory_Snapshot: 

Table is based of ILS.dbo.LocationInventory

This is a live table out of the WH Management system.

Fact_Warehouse_Inventory_Adjustment:

is based on the SC schematic model. As this was not part of the pervious EDP project."
CSCI-219,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,31/Jul/25 9:30 AM,04/Aug/25 5:18 PM,,Source to Target for Facts - Fact_Warehouse,"Mapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]

main facts to focus on are as per acceptance criteria

@user -will split this task out so that fits in to sprint cadence- done","* Priority 1 fact tables on control table will be mapped
** -Fact_Warehouse_Inventory_Intra-
** -Fact_Warehouse_Inventory_Snapshot-
** -Fact_Warehouse_Inventory_Adjustment-","@user will update ticket - 

ahve started mapping fields.

need overview of tables i.e. description and what should be housed and granularity

Harrison mentioned that one is snapshot
- e.g. AX inventory_intra_day - per hour

noted from @user that structure will be the same.

Discussed about stock incremental

@user @user - will discuss

The Stock Fact_Warehouse_Inventory_Snapshot- the mapping is done based off the PBI05.supplychain.scax.dcsinventoryhistory

base tables: INVENTDIM and INVENTDIM. 
Things to note: 

* DataareaID- this defines the business entities. 
* read from the most current partition: This function gives the most current partition; SCAXLink.[dbo].[udf_GetPartition] (null)
* ensure Nolock is used for these reads.

Fact_Warehouse_Inventory_Adjustment: Based of [BI_Presentation].[dbo].[WarehouseInventoryAdjustments]

E.g

marking task as done for sprint burndown

@user 
I‚Äôll carry over the notes to next card for completeness."
CSCI-218,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Jul/25 4:12 PM,30/Jul/25 4:27 PM,,SC BAU Sprint 4 Work,# Assisted Matthew Jones with his SNOW ticket request (*INC0444602/RITM0171602*). Needed help with creating a workspace to share his KPI Scorecard with his team,,
CSCI-217,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Jul/25 3:02 PM,01/Aug/25 9:16 AM,,Getting Alan Yuen CSCI access pack,Getting Alan Yuen access to CW SC resources and board.,"Getting Alan Yuen access to 

* -Board-
* Mergeco folder

* -Daily standup meetings-",
CSCI-216,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Jul/25 10:55 AM,01/Aug/25 5:48 PM,,meeting to book - Enable SSO login to snowflake,"Consdering the options between 

* MS Entra ID
* Okta

CK to have a follow-up call with Brent/ Cloud to firm up the plan for SSO implementation

* -book next tuesday to follow up-","* -booking made for tuesday meeting-
* To follow-up from the results of the call on Tuesday",
CSCI-215,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Jul/25 10:49 AM,15/Aug/25 3:53 PM,,Phil to get access ServiceNow via AD,,* SErviceNow access granted for @user @user and @user,"@user FYI - i‚Äôll get access from you soon.

Have spoken to mark Barton - would be best to get me Azure AD access that gives me the whole package - ticket changed in servicenow @user @user

will now follow up with Mikhail

in lieu of @user tickets I will have parity with Service now tickets in JIRA"
CSCI-214,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Jul/25 9:59 AM,09/Oct/25 8:39 AM,,Review of existing dbt transformation from Silver to Gold,,,"@user your account in Azure Devops is already changed to Basic:

Please confirm if you can access the repo at: [edp_supplychain_dbt - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp_supplychain_dbt?version=GBmaster]

placed this under [https://sigmahealthcare.atlassian.net/browse/CSCI-160|https://sigmahealthcare.atlassian.net/browse/CSCI-160]"
CSCI-212,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Jul/25 7:40 AM,18/Nov/25 5:18 PM,,Microsoft Engagement Planning Session Agenda for Nagaraj,"Nagaraj is the MS Solutions architect assigned to the CW/Sigma account.

Nagaraj has requested we provide a list of topics to cover in the PBI best practice sessions being held in August.

Need to create a session plan.",,
CSCI-211,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Jul/25 7:32 AM,12/Sep/25 6:17 PM,,Analysis of Existing EDP Project Transformations into STTM,,,
CSCI-210,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Jul/25 7:22 AM,09/Oct/25 8:39 AM,,Source-To-Target-Mapping (STTM) Document Template,"Template for Fact and DIM Source-To-Target-Mapping documentation.

*Document Link:* [https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&action=default&mobileredirect=true] 

*Control Table Template*

*Fact Table Template*",,
CSCI-208,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Jul/25 7:16 AM,01/Sep/25 2:30 AM,,ExpressRoute Monitoring (18 Aug - 1 Sept),Monitor ExpressRoute throughput utilisation,"# *Metric Visibility:*
#* ExpressRoute connection metrics (e.g., Ingress/Egress throughput) are visible in Azure Monitor or Network Insights.
#* Bandwidth usage can be correlated with ADF pipeline execution timelines.
# *Alerting Rules:*
#* Alerts are configured to trigger when bandwidth usage exceeds a defined threshold (e.g., 70% of provisioned bandwidth).
#* Alerts include pipeline metadata (e.g., pipeline name, run ID) when possible, to trace high-usage executions.
# *Log Collection:*
#* Network bandwidth logs (e.g., from Network Performance Monitor or Azure Network Watcher) are retained for a minimum of 30 days.
#* ADF pipeline run logs include integration runtime IPs or endpoints for cross-reference.
# *Validation Testing:*
#* At least one high-throughput ADF pipeline is executed to validate that bandwidth spikes are captured and logged as expected.
#* Test scenarios simulate concurrent pipeline runs to verify system responsiveness and alert thresholds.
# *Governance & Review:*
#* A monthly review is conducted to assess if bandwidth consumption patterns require scaling up/down of ExpressRoute or ADF optimization.
#* A weekly review to be initially done on until Oct 2025.","What is required in Sprint 5 - Aug 04 to Aug 15 for this ticket

We just need to baseline current usage, which is already available in the Azure Dashboard. Currently, we are passing thru AXON. 

We cannot establish yet our bandwidth requirements, but DBAs can estimate max daily data volume from the past 90 days.

@user I have truncated/ removed delta load the following tables so we can see how the ExpressRoute cope with a larger load (~ 550K - 2.1 M) in the next run

* SCAX2012.INVENTJOURNALTRANS ~ 1 M
* SCAX2012.INVENTJOURNALTABLE ~ 1.1 M
* SCAX2012.INVENTSUM ~ 600 K
* SCAX2012.PURCHLINE ~ 2.1 M

 @user

Issue split into:
|CSCI-380|ExpressRoute Monitoring - 1 Sep - 15th Sep|"
CSCI-207,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Jul/25 7:08 AM,15/Aug/25 4:39 PM,,Review of CI/CD Pipeline,Review output of [https://sigmahealthcare.atlassian.net/browse/CSCI-151|https://sigmahealthcare.atlassian.net/browse/CSCI-151] and [https://sigmahealthcare.atlassian.net/browse/CSCI-159|https://sigmahealthcare.atlassian.net/browse/CSCI-159],,"Stretch Sprint 5 goal

I have reviewed the CD pipeline and am satisfied with the following:

* *Branching strategy:* Use a feature branch, merge to {{master}}, then deploy to each environment upon approval.
* *Deployment monitoring:* Check DevOps Releases for deployment status and logs.
* *Script execution:* {{Config_meta.sql}} will be sequenced and run within {{deployment_master_scripts}}.
* *Versioning:* The current mechanism functions but remains limited. We still require a defined process for version naming, review and auto-validation.

*Next step:* Chloe to define the desired version control process with detailed requirements."
CSCI-206,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Jul/25 2:21 PM,21/Aug/25 3:32 PM,,Snowpark PBI API Environment Setup,Configure Snowpark to allow PBI APIs to be queried. Environment can then be used to productionize existing PBI Metadata reports for the SC team.,"* -Successfully pull PBI metadata via API call in Snowpark-
* -Be able to save this output as a table in Snowflake-","@userto approach this some time next week

@user - are you able to accept the use of Anaconda 3rd party packages for Snowpark?

can do @user. I will move to this as soon as I clear out my Snowflake ci/cd and SSO reviews. ETA ~Wed/Thurs.

Hi @user the *Anaconda Python packages* feature has been enabled in Snowflake, please check on your side. Cheers

Tested Snowpark query - we are now able to run queries within Snowpark (no longer receiving previous error).

However, am still unable to run a PBI API query (see below):

{code:python}import msal ## MS authentication package

## configure tenant and service principal details (note IDs replaced with false variables as they're sensitive info)
tenant_id = 'abc' ## CW Tenancy ID
client_id = 'abc' ## Service Princiapl ID
client_secret = 'abc' ## Service Princial 'Secret'

## set MS authentication URLs
authority_url = f""https://login.microsoftonline.com/{tenant_id}/""
scope_url = list()
scope_url.append('https://analysis.windows.net/powerbi/api/.default')

## configure credentials to MS and retrive access token
app = msal.ConfidentialClientApplication(
 client_id,client_credential=client_secret,
 authority=authority_url
)

result = app.acquire_token_for_client(scopes=scope_url)

token = result.get('access_token')

print(token){code}

Error received appears to be a networking issue:

Discussing with Chloe to identify who is best contact to resolve (Eugene or CK)

hi @user either Eugene or CK will be able to provide you with those details.

spoke wiht CK and reconfigured network rules.

CK/Chloe helped resolve the network issues.
Now encountering a syntax error when trying to create a python UDF. Need to break code down step-by-step to understand what the syntax error relates to.

Resolve syntax issues - was trying to orignally test connection to the APIs via a Snowsight Python Worksheet - have instead move code to a Python Notebook and now syntax errors have resolved.

Still unable to connect to the PBI API as the network rules created by Chloe has not been shared with my RBAC role - Chloe to action.

Able to successfully query 4 separate APIs:

* MSAL client authentication
* PBI Semantic Model Refreshes (GET)
* PBI Semantic Model Data Sources
* PBI Semantic Model Refresh (POST)

Able to successfully save test data to a SF Table in the Sandbox Schema."
CSCI-205,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Jul/25 2:21 PM,07/Aug/25 6:45 AM,,PBI to Snowflake VNet Gateway Connection POC,"POC to document and test how users will be able to connect to our VNet gateway to connect to SF from PBI.

Need to test:

* How to authenticate
* How users will need to be granted access
* At what level (in SF) do we grant permissions for access (do we do this at a DB level or tenancy level? etc)","* -To test and cover all requirements.-
* -Have all requirements documented (in Confluence/Sharepoint).-","Currently unable to connect via the Privatelink URL (cw-au.privatelink.snowflakecomputing.com) via PBI desktop - have asked Eugene to look into this

h2. How to Authenticate

Developers will start in PBI Desktop/Tabular Editor. To begin development, the user will select the Snowflake connector type and enter in their credentials:

* Server: cw-au.privatelink.snowflakecomputing.com
* Warehouse: <spoke‚Äôs dedicated PBI warehouse>
* Login details: SSO with AAD

From here a developer can select the table/s they wish to pull from Snowflake. As they‚Äôve signed in as themselves they will view all tables that they have access to in Snowflake.

Once selected, the developer will choose ‚Äòimport mode‚Äô for their data (we will discourage the use of DirectQuery).

From here they will develop and build their dashboard. Once completed they will publish to their Workspace in PBI Service.

Once published, the user will go to their semantic model settings. They will need to navigate to ‚ÄòGateway and cloud connections‚Äô, select +On+ for ‚ÄòUse an On-premises or VNet data gateway‚Äô. Locate to the VNet gateway and map the Snowflake data source to their relevant connection name (their connection will be to a Service Account, based on the spoke‚Äôs dedicated warehouse and roles granted). After selected, hit ‚Äòapply‚Äô and the semantic model is ready to be deployed and refreshed.

h2. What developers need access to

h3. Snowflake

* Snowflake login (setup with AAD SSO)
* Their spoke‚Äôs environment in SF
** Configured at the role level
*** Each role will have a dedicated warehouse to be used for PBI reports and refreshes
*** The roles grant access to the relevant DB‚Äôs/schema‚Äôs/table‚Äôs in SF
*** Each role will have a warehouse where users can query SF for analytics and sourcing data

h3. Power BI

* Power BI Desktop
* Tabular Editor Pro (if applicable for super users)
* PBI VNet Gateway connection created (for each spoke) at a service account level
** Access distinguished by the role and warehouse for that spoke
* PBI VNet Gateway connection shared (user permissions)
** This should be configured at an AD Group level

Eugene clarified that there are 2 things that are pending that need to happen to resolve this:

* DNS resolution
* Network team to build firewall settings

Once completed, users will be able to query SF‚Äôs privatelink even in PBI Desktop

Documentation complete:
[PBI_POC_VNet_Gateway.docx|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/PBI_POC_VNet_Gateway.docx?d=wcb0e96cdfbd94f6facd2d9a2b81261d6&csf=1&web=1&e=0FaBkg]"
CSCI-204,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Jul/25 11:23 AM,13/Aug/25 9:16 PM,,VNET Gateway testing support,Harrison to validate any requirements for Eugene to enable Jess C,,"Next Eugene Activites

* VNET Gateway testing
* Snowflake Privatelink/network policy config
* ExpressRoute - validate in standup what is required - likely monitoring
* CI/CD pipeline - done but waiting for review and test from Chloe

Just waiting for @user if support is required, I believe access is already provided.

Jess pls advise if you want to setup a call to go through the config.

@user 

Hi Eugene üôÇ 

FYI been testing connecting PBI to Snowflake today, so far so good with everything regarding the VNet gateway you configured üôÇ Thanks for all your help.

One thing I noticed is within PBI Desktop, I can‚Äôt seem to connect to our Snowflake server with the privatelink URL (cw-au.*privatelink*.snowflakecomputing.com). I have no issues in PBI Service.

Are you able to enable the use of the privatelink in the PBI Desktop app? Would we need to be able to do the same in tabular editor also?

Let me know, thanks! üôÇ

@user are you connecting directly from PowerBI? Or are you using a Semantic Model from PowerBI?

@user connecting directly from PBI desktop.

As discussed on our call just now, this question is now resolved üôÇ we are awaiting for the DNS resolution to be completed, plus for firewalls to be configured with the network team before this is able to be done."
CSCI-203,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Jul/25 11:17 AM,14/Aug/25 11:37 AM,,High Level Solution Design Walkthrough for Internal Data Engineering,Walkthrough with Alan Y,"Create high level architecture document and walk through 

* Snowflake and ADF design/ setup
* Framework setup
* Environment walkthrough
* Eugene Intro","* -Created High Level Logical Architecture and sent to the wider team-
* -Walk-through and discussion with Alan Y. on the solution design-
* -Updated the Snowflake design based on Alan‚Äôs suggestion-

 

 @user @user

I am working on documenting ADF logical architecture and pipeline design."
CSCI-202,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Jul/25 11:05 AM,15/Aug/25 3:45 PM,,SQL3 (PDB14) Delta Load,,"Sorry I‚Äôll need help with filling out the AC here. (from Phil)

as per [https://sigmahealthcare.atlassian.net/browse/CSCI-134|https://sigmahealthcare.atlassian.net/browse/CSCI-134]

We need to set up pipelines for the delta loads for the 5 tables.

|BranchOrderItems|
|BranchOrders|
|Store_IncrementalSOHChanges|
|Transactions|
|Transactions_Invoice|","4 tables that need to be moved to snowflake
- will move smallest first

Working on the patterns identification and performance with one table. Once successful we will build the pipeline for all tables.

Loading delta for transactions table. Identifying the patterns and testing the delta load. 

There are no valid date columns that can be used. We are using transactions_invoce to get data."
CSCI-201,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Jul/25 11:02 AM,14/Aug/25 9:49 AM,,ADF and Snowflake Implementation Walkthrough From Chloe (Ashutosh),"* Walkthrough of Integration and Transformation Framework
* Walkthrough of Development standards",2 new resources able to implement integration and transformation,will reach out to @user
CSCI-200,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Jul/25 11:01 AM,08/Aug/25 9:56 AM,,ADF and Snowflake Implementation Walkthrough From Chloe (Aswini),"* Walkthrough of Integration and Transformation Framework
* Walkthrough of Development standards",2 new resources able to implement integration and transformation,assgined to @user
CSCI-199,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,29/Jul/25 10:53 AM,05/Aug/25 10:05 AM,,Team Access to SF via PrivateLink Setup,"Discussions to include details as per below 

{quote}Scaffold of details as per below @user 

* *VPN Usage*
** Clarify whether CW currently uses a VPN for Snowflake access.
** Perhaps Determine if SIGMA uses a different VPN or the same infrastructure.
* *Communication Pathways*
** Document the communication flow and access methods for both SIGMA and CW when connecting to Snowflake.
** Identify any differences in network architecture or security protocols.
* *Integration Strategy*
** Explore options for combining or aligning access methods between SIGMA and CW.
** Evaluate the feasibility of a unified PrivateLink setup or maintaining separate configurations.{quote}

*Meeting Coordination*

* -A meeting has been scheduled for *10 am tomorrow* to discuss this topic in detail (as noted by Phillip Yuen).-","* A clear decision is made on how to organize team access to Snowflake via PrivateLink.
* Documentation of the current and proposed access methods for both SIGMA and CW.
* Identification of any blockers or dependencies.
* Agreement on next steps and responsible parties.","@user please get across this to drive

Caught up with @user
Meeting booked specifically regarding this conversation at 10 am tomorrow.

* @user to provide plan/details on how to approach this 
* @user will send details to CK @user to relay to cloud team. (Brent)
* @user to get access to servicenow with @user and FRank

@user to reach otu to @user

See doc at: [Developer Access - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5145/Developer-Access]

Pls review.

cc: @user @user @user @user

@user this card has been split to [https://sigmahealthcare.atlassian.net/browse/CSCI-225|https://sigmahealthcare.atlassian.net/browse/CSCI-225] as a review card.

@user thanks Eugene! this looks great üôÇ 

For the Azure VM‚Äôs as a backup - can you clarify in what kind of scenarios this would be required? How would this differ to using jumpbox?

Also for the non-CW domain users (eg: Harrison and Phillip) - in this architecture can they access CW data? Do they have access to using jumpbox etc? Maybe something else to consider üôÇ 

Cheers

@user the Azure VM is going to be used as a fallback when all the other options are not accessible, e.g. ExpressRoute is down, VPN Access not available, Jumpboxes not available.

For non-CW domain users, they may need CW accounts if there is not account federation yet.

cc: @user

@user Great thanks for clarifying üôÇ"
CSCI-198,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Jul/25 3:54 PM,28/Jul/25 4:57 PM,,"Onboard Adeel, Samer, Bhavya to CSCI JIRA","User story:
- as a _____ I want to _____ so that ______

As PO I want to onboard @user @user @user on to Jira so that they will be able to contribute to project.

Deliverable:

* show basic Jira functions as a video on teams.
* Create new confluence doc that updated from previous PRoduct devliery life cycle (PDLC)","Given:____
When:_____

Then: ______

@user @user @user to be able to:

* understand what‚Äôs progressing in Jira board
* create tickets
* create story estimates
* know what sections they are to look out for","Testing scope to be negotiated with @user 

@user as part of the recording.
@user 
@user"
CSCI-196,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Jul/25 11:11 AM,09/Oct/25 8:39 AM,,Appriss Test Data Load and DataShare,"* test connectivity in data share env ( week of 11/8)
* ingest test data from location (when data is ready)",* Data ingested from location,
CSCI-195,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Jul/25 9:55 AM,28/Jul/25 3:50 PM,,waseem access to JIRA,waseem access to Jira CSCI,waseem access to JIRA,
CSCI-194,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Jul/25 9:54 AM,11/Sep/25 9:34 AM,,Onboarding and statement of work,"Loading of Infocentric SOW into Plexus

Onboarding of Ashwini and Ashutosh into CW enviornment","Infocentric SOW completed

Onboarding of Ashwini and Ashutosh completed","Infocentric SOW loaded into Plexus and at CIO approval and signature stage, thanks to Frank Perez and Matthew Jones.

Onboarding initiated through Matthew Jones.

Thanks,

Harrison

Pretty sure this is done @user"
CSCI-193,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Jul/25 9:38 AM,25/Aug/25 2:31 PM,,Appriss Requirements Design and Walkthrough,"Requirements and Design Documentation completed in mergeCo Reporting and session setup to take Chloe, Harrison and Ashwini through.

Aswini will be responsible for delivering however Chloe should be included also as part if initial setup of Data Platform",Alignment between Waseem and team on deliverables for Appriss initiative,"CK and Waseem took team through the source data and outputs.

Marked as done"
CSCI-192,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Jul/25 8:53 AM,12/Sep/25 6:24 PM,,Data Platform Architecture and Design Documentation and Walkthrough with Internal Team,"Documentation in confluence or other of CW Data Platform Architecture and Design decisions.

Walkthrough for Alan and Shagun to provide feedback and approval",Any feedback from internal Data Engineering team considered and incorporated where required,"Working on the Solution Architecture and Design Document v.01. I have divided the works between Ashu and myself:

* Design Principles
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Add guidelines and principles that shape the solution design""}],""attrs"":{""localId"":""06223205-7d08-4dde-8da4-75e66813dc81"",""state"":""TODO""}}],""attrs"":{""localId"":""0460c10a-039d-443d-b840-4a951faedfd2""}}
{adf}
* Logical architecture - Interim Solution
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":"" Key decision driver, choice of technologies, design flows etc.""}],""attrs"":{""localId"":""790df446-a05d-42fd-ae2c-a13f696b52b3"",""state"":""TODO""}}],""attrs"":{""localId"":""00a154d5-047b-498b-ae88-a595990aade8""}}
{adf}
* Logical architecture - Future State:
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":"" Key decision driver, technologies, design flows etc.""}],""attrs"":{""localId"":""fa2e2ef1-98a8-42e3-b52b-63f22429fc0c"",""state"":""TODO""}}],""attrs"":{""localId"":""7436ecdc-a967-4dc7-935f-ce59a3bd3133""}}
{adf}
* Altis DLA: 
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Detailed explanation and how it works""}],""attrs"":{""localId"":""3522c235-82db-43ec-9c99-45c41cd3ef5e"",""state"":""TODO""}}],""attrs"":{""localId"":""0d2531ef-cb73-4349-9c23-6e8fc783b5f1""}}
{adf}
* Source systems: 
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Database name/ use-cases""}],""attrs"":{""localId"":""956df07f-abee-414a-92ee-cb0201ee3d60"",""state"":""TODO""}},{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""SQL Servers/ IP addresses ""}],""attrs"":{""localId"":""de5e8100-8dbc-4d4b-adb1-925d1bf6e8d1"",""state"":""TODO""}}],""attrs"":{""localId"":""e2a36481-0b41-4c86-a19c-b42086993884""}}
{adf}

 @user @user

will split task to sprint 8"
CSCI-191,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,28/Jul/25 8:23 AM,18/Aug/25 9:43 AM,,ADF and Snowflake Implementation Walkthrough for New Resources,"* Walkthrough of Integration and Transformation Framework
* Walkthrough of Development standards",* 2 new resources able to implement integration and transformation,"Sessions booked for Snowflake and ADF walkthrough

Onboarding tasks for Ashu and Aswini 

* -Access in Azure and Snowflake-
* -Walk through ADF templates and existing integrations-
* -Walk through Snowflake schemas and high level setup-
* -Walk through development processes and conventions-
* -Allocate Jira tasks -

 @user @user

Next: 

* Deep dive sessions in Altis DLA framework
* Walk-through sessions for the wider project team

next steps;|

- @user @user ADF pipeline 

* @user to walk thru PR process

trigger job in dev

cc @user for review (PR)"
CSCI-190,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Jul/25 2:37 PM,01/Aug/25 11:01 AM,,Create Data Out Transformation Pipeline Template,"Devops: [TASK 198087|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/198087]

Build a reusable ADF pipeline template for performing data transformations on staged data before loading it into curated Snowflake tables. The pipeline should be parameterized, support incremental and full loads, and include basic logging and error handling.","* Pipeline template accepts dynamic inputs (e.g., table name, load type).

* Supports transformation logic using SQL or stored procedures.","Data Out template tested successful on Parallel load for StockDB.BranchHierarchyInfo_VW.

Next step: 

To test this scenario:

* Sequential load/ Full Apply/ Deleted records from source

Data Out template tested successful on both Sequential and Parallel load. @user @userTest table created in STG_TRNS schema, using FULL APPLY load type."
CSCI-186,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Jul/25 11:19 AM,01/Aug/25 4:13 PM,,Adeel - Document Archive Policy,"Archiving policy is updated [here|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/03.%20Test%20and%20Deploy/Archiving%20Criteria.xlsx?d=w582498ed19ab4a92b3702a0c2d5cceb7&csf=1&web=1&e=RPM4tt ]
_________________________________________

Following recent discussions on the DEV/PROD database environment alignment, Chloe and Adeel will collaborate on the below

# *Documenting the Archive Policy*
#* Define retention periods
#* Specify archiving criteria (e.g., based on date or status columns)
#* Include any additional relevant details","* -Archive policy document is created and shared with relevant stakeholders-
* -Policy includes clear retention periods and archiving criteria-","Phil to drive activity, assistance required for Adeel.

@user

I have caught up with @user 

Archiving policy is as per attached [link|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/03.%20Test%20and%20Deploy/Archiving%20Criteria.xlsx?d=w582498ed19ab4a92b3702a0c2d5cceb7&csf=1&web=1&e=RPM4tt]

Will update description

@user archiving policy now present - ready for you to review.

Deep link to sheet here - [Archiving Criteria.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/03.%20Test%20and%20Deploy/Archiving%20Criteria.xlsx?d=w582498ed19ab4a92b3702a0c2d5cceb7&csf=1&web=1&e=TTktBA&nav=MTVfezk3OThEODNGLUQ5NzctNEMzMC05RUIxLTE3MTBCQUI2NkE4Nn0]

Adeel carry over from this ticket to be transitioned to Sprint 5 @user plus assign to Adeel when done

@user Review of Archival policy by Chloe and go back to Adeel to incorporate.

Plan to be put in place as part of review

I‚Äôve reviewed Adeel analysis on the current retention policy. To revisit this with ticket CSCI 221"
CSCI-185,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Jul/25 10:10 AM,04/Aug/25 6:03 AM,,Adeel - On-prem DEV/PROD Database Environment Sync,"Following a review of the on-prem DEV and PROD environments for key databases (StockDB, SCAX2012, ILS, TransactionStorage), discrepancies were identified in schema alignment and data volume. Adeel Syed provided a detailed comparison, highlighting schema mismatches in StockDB and data size differences in TransactionStorage and ILS. 

Notably, SCAX2012 was recently refreshed and is up-to-date.

@user has proposed a prioritised refresh plan to align DEV with PROD, particularly to support accurate modelling and pipeline validation in Snowflake environments. 

Rachel Wan confirmed that DEV environments are refreshed only upon request and are not automatically synced with PROD.

*Summary of Findings:*

* *SCAX2012*: Recently refreshed (11 July 2025) ‚Äì no further action required.
* *StockDB*: Schema drift identified across 7 tables (refer to _SchemaCompare.docx_) ‚Äì refresh required to align DEV with PROD.
* *TransactionStorage*: DEV has more records than PROD due to regular archiving in PROD (not duplication). Tables archived include:
** {{BranchOrderItems}}, {{BranchOrders}}, {{Store_IncrementalSOHChanges}}, {{Transactions}}, {{Transactions_Invoice}}
** {{Shipment_detail}}, {{Shipment_header}}
* *ILS*: Data size gaps observed ‚Äì refresh recommended but not urgent.

*Proposed Refresh Priority:*

# *StockDB* ‚Äì to resolve schema mismatches.
# *TransactionStorage* ‚Äì to align data volume and ensure DEV reflects current PROD state.
# *ILS* ‚Äì to be scheduled after the above.

@Rachel Wan ‚Äì to advise on the formal process for requesting refreshes for these databases. 
Once confirmed, requests to be raised.","*StockDB Refresh*

* Schema drift issues resolved.
* -DEV schema matches PROD schema as per {{SchemaCompare.docx}}.-

*TransactionStorage Refresh*

* -Data volume aligned between DEV and PROD.-
* -DEV duplicates addressed.-
* -Tables listed by Adeel (e.g., {{BranchOrderItems}}, {{Transactions}}, {{Shipment_detail}}) archived appropriately in PROD.-

*ILS Refresh*

* -Data volume aligned between DEV and PROD.-
* -Refresh scheduled post StockDB and TransactionStorage updates.-

*Confirmation*

* -Rachel Wan to advise on the formal process for initiating refresh requests.-
* -Refresh timelines and responsible parties confirmed.-","@user descriptions udpate FYI

Adeel drive DEV refresh requests and discussions

Phil to drive activity, assistance required for Adeel.

@user

I have caught up with @user 

{quote}* got to go ahead for ILS DB refresh
* currently needing to work out on the refresh plan as it will be performed for the first time
* schema drift in Stockdb exists due to the dev work going on with other project.
** if this stockdb is refreshed, then we need to replicate those schema drifts in dev env again
*** discussion with Chloe @userhappened this afternoon - stockdb refresh is not needed as for now.

{quote}

Adeel carry over from this ticket to be transitioned to Sprint 5 @user plus assign to Adeel when done

StockDB to not be refreshed, as DEV Schema is per new build. Validate Go Live of new changes to ensure we dont go in earlier.

SCAX2012 =- refreshed recently

TransactioStorage - refreshing being worked through

ils - refreshing being worked through

@user

approvals gained for ILS refresh. @user

Expected to finish today
NOted:
- refresh risk for scale - 

@user discuss about the above

to split out task:

* ILS refresh risk
* Monitoring transactionstorage restore - in progress.

Will split out card.

@user done."
CSCI-184,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,24/Jul/25 9:48 AM,19/Nov/25 11:50 AM,,Data Transformation Discussion Per SIGMA Issues,"Harrison to discuss with Chloe a couple issues in SIGMA Ingestion and Transformation that I want to clarify for CW Snowflake Ingestion and Transformation

* Issues with ETL_Update stamping
** ETL_UPDATE in the Fact is altered even if the Fact metrics, attributes or Dim Key relationships are unchanged. An attribute of a Dim that the object connects to has changed but the Key values havent, yet the ETL_Update stamp is altered. means there are significanlty more changes in historical data in Fact then should be, this impacts ability to perform incremental refreshes etc
* MERGE issue
** In SIGMA environment the MERGE never handled deletes from the Fact very well, want to ensure this wont be repliacted
** i.e. we have an agg object populated from a Base Fact where when records are deleted from Base Fact the merge doesnt work",,"Hey @user ,

Lets include @user in Arch session next week with @user to discuss these.

Thanks,

Harrison"
CSCI-181,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Jul/25 10:14 AM,25/Jul/25 11:12 AM,,Vnet Data Gateway Setup,"PBI ticket: ______

taken from notes on Tuesday 22/07

* Task to be collaborated between @user and @user 
* Vnet gateway requires Fabric Licence (i.e. F2 capacity)
** Note: cost concerns were raised by Eugene
* CK: Will create a Vnet Data gateway (as Eugene doesn‚Äôt have the rights at time of writing)
* Eugene: Will presumably be in charge of setup - Will confirm.

Acceptance criteira:

* Vnet Data gateway to be created","* Vnet Data gateway to be created 
* Eugene granted access to gateway
* Documentation of setup","VNet Data Gateway for Dev is created at: [Fabric|https://app.fabric.microsoft.com/groups/me/gateways?experience=fabric-developer]

Naming convention is: *cwr-ase-edp-<environment>-vdgw*

User assignment as follows: 

Setup Instructions: 

* in Fabric Console, navigate to Settings | Manage connections and gateways:
* 
Select the Virtual network gateways tab
* Click on + New and fill up the form
* 

cc: @user @user @user"
CSCI-180,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,22/Jul/25 10:09 AM,03/Sep/25 9:40 AM,,Create baseline and incremental ExpressRoute usage,,,"Hey @user 
Would it be OK to catch up with you about descriptions here?

I just wanted to know whether I can close this as configuring expressroute is already done.

@user baseline is covered by the ExpressRoute logs, we just have to establish specific durations."
CSCI-179,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,22/Jul/25 10:07 AM,13/Aug/25 8:33 AM,,Raw Source Objects for Dimensions,"Focusing on objects not connected to SC existing PBI Semantic Models, but other objects in BUS Matrix. Provide not just PBI05 object but the relevant objects from actual source for this object.","Column G ‚ÄúActual Source‚Äù in BUS Matrix filled in with relevant SQL Server/MySQL object details.

*Next Step:* Source-To-Target Mapping","Started working on Dims Source Mapping

session booked

session. part 3 booked tomorrow"
CSCI-178,CSCI,CW Cloud Data Platform Interim Solution,Subtask,Done,Medium,22/Jul/25 10:06 AM,02/Aug/25 1:23 AM,,Configure SHIR QoS,Tracking development at: [Task 196417 Implement QoS to SHIR|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/196417/],,"Created a ticket with MS for QoS, implementation created issues

Received some guidance from MS, continuing work on this.

Unfortunately, this is not viable as with the QoS in place, it won‚Äôt allow connection to the on-premise databbase.

# Without QoS, we can connect
# We setup a QoS
# With QoS, it fails"
CSCI-176,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Jul/25 4:47 PM,15/Aug/25 4:38 PM,,Source to Target for Facts - DC Inventory,"Source to target mapping for Inventory history related Facts - filling out sheet that covers:

* Fact_DC_inventory_history
* Fact_DC_inventory_intra
* Fact_DC_Inventory_Location_History
* Fact_DC_Inventory_Adjustments","* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Inventory%20Source-to-target%20document%20-%20Facts.xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=uvHOeg&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no","This task will/should be split out - I can do this.

@user

need ot map the BI tables for DC

is approaching secondary mapping for DC

To verify ‚ÄòData type‚Äô whether it‚Äôs needed to be specified.
\ @user@user

* Fact_DC_inventory_intra

Harrison had entered a few more fields. SAFETYSTOCKQTY, DamagedQty, ¬†UnsellableQty and ExpiredQty.

I have done some research to to be able to get these information out of the ERP system. 
expired qty can possibly be found from INVENTBATCH table but when i checked the table it doesn't contain any useful information. 

Have started a conversation with CWR AX Consultants and jess to see if can get this data out. 

@user , we might want to think about this table. As i think we cant get a lot these fields out of the scale system."
CSCI-171,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Jul/25 3:55 PM,01/Sep/25 2:20 AM,,Create Source-to-Target Map for Dim_Product_UOM - Mapping,Source to target mapping for Dim_Product_UOM - filling out sheet as per Source-to-target dims,"* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no
{adf:display=block}
{""type"":""taskList"",""content"":[{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Descriptions"",""marks"":[{""type"":""strong""}]},{""type"":""text"",""text"":"" are clear, unambiguous, and provide both business and technical context.""}],""attrs"":{""localId"":""2b889b40-6284-4ea5-b742-52fa6b13827f"",""state"":""TODO""}},{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Source details"",""marks"":[{""type"":""strong""}]},{""type"":""text"",""text"":"" are accurate and validated against the current production environment.""}],""attrs"":{""localId"":""33d27bf9-950d-4e05-b79d-9434ee46ae5a"",""state"":""TODO""}},{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Derived columns"",""marks"":[{""type"":""strong""}]},{""type"":""text"",""text"":"" include a concise explanation of the derivation logic.""}],""attrs"":{""localId"":""a35204ac-7404-4ae1-8237-226609754794"",""state"":""TODO""}},{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""The completed mapping is reviewed and validated by a peer or data steward."",""marks"":[{""type"":""strike""}]}],""attrs"":{""localId"":""0e777f9c-cf50-4a62-b19a-a7bff82987a6"",""state"":""TODO""}},{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""Any mismatches or gaps between the mapping and the production environment are resolved and documented.""}],""attrs"":{""localId"":""66d56fe0-d06b-47ca-bd8e-7cd2ee438f34"",""state"":""TODO""}},{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""The final mapping file is uploaded to the designated SharePoint location and stakeholders are notified.""}],""attrs"":{""localId"":""5a1a8415-d6cd-45cb-aecd-7d2f5d399b60"",""state"":""TODO""}},{""type"":""taskItem"",""content"":[{""type"":""text"",""text"":""(Optional) If required, update downstream documentation or metadata repositories to reflect the new mappings.""}],""attrs"":{""localId"":""3cb522be-3cd3-4959-a49f-762bb795418e"",""state"":""TODO""}}],""attrs"":{""localId"":""0fa1744a-4f74-410c-8921-a11b661237b7""}}
{adf}","Completed the mapping for Dim_Product_UOM. Creating Review ticket for @user to review.

Cc: @user

buffed the AC @user - LMK if this is appropriate"
CSCI-170,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Jul/25 3:43 PM,01/Sep/25 2:20 AM,,Create Source-to-Target Map for Dim_Employee - Mapping,Source to target mapping for Dim_Employee - filling out sheet as per Source-to-target dims,"* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",
CSCI-169,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Jul/25 3:42 PM,01/Sep/25 2:18 AM,,Create Source-to-Target Map for Dim_PO - part 1,Source to target mapping for Dim_PO - filling out sheet as per Source-to-target dims,"* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no","@user Can you provide the scope of this dimension? Is it related to Store POs or DC POs

@user - will catchup with @user after standup today

Issue split into:
|CSCI-378|Create Source-to-Target Map for Dim_PO - part 1|"
CSCI-168,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Jul/25 3:41 PM,01/Sep/25 2:20 AM,,Create Source-to-Target Map for Dim_Location - Mapping,Source to target mapping for Dim_LOCATION - filling out sheet as per Source-to-target dims,"* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no",
CSCI-167,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Jul/25 3:40 PM,01/Sep/25 2:20 AM,,Create Source-to-Target Map for Dim_DC - Mapping,Source to target mapping for Dim_DC - filling out sheet as per Source-to-target dims,"* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no

* *Descriptions* are clear, unambiguous, and provide both business and technical context.
* *Source details* are accurate and validated against the current production environment.
* *Derived columns* include a concise explanation of the derivation logic.
* -The completed mapping is reviewed and validated by a peer or data steward.-
* Any mismatches or gaps between the mapping and the production environment are resolved and documented.
* The final mapping file is uploaded to the designated SharePoint location and stakeholders are notified.
* (Optional) If required, update downstream documentation or metadata repositories to reflect the new mappings.","Mapping of DIM_DC is complete, created a ticket and assigned to @user for review.

CC: @user

@user I‚Äôve included the mapping from current source system and the gold layer mapping from previous EDP project.

buffed the AC @user - LMK if this is appropriate"
CSCI-166,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Jul/25 3:28 PM,15/Aug/25 3:59 PM,,Create Source-to-Target Map for Dim_Store,Source to target mapping for Dim_Store - filling out sheet as per Source-to-target dims,"* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no","Added the source mapping for DIM store. Separate ticket is created for review 

@user"
CSCI-165,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Jul/25 3:25 PM,01/Sep/25 2:20 AM,,Create Source-to-Target Map for Dim_Date - Mapping,Source to target mapping for Dim_Date - filling out sheet as per Source-to-target dims,"* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no

* *Descriptions* are clear, unambiguous, and provide both business and technical context.
* *Source details* are accurate and validated against the current production environment.
* *Derived columns* include a concise explanation of the derivation logic.
* -The completed mapping is reviewed and validated by a peer or data steward.-
* Any mismatches or gaps between the mapping and the production environment are resolved and documented.
* The final mapping file is uploaded to the designated SharePoint location and stakeholders are notified.
* (Optional) If required, update downstream documentation or metadata repositories to reflect the new mappings.","Added mapping for Date Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]

@user I have created a review ticket ad assigned to @user

@user thank you.
Can you link that ticket here? Thanks.
cc @user

Hey I think your tickets (along with other DIM tickets) are all done except Dim_PO @user

cc @user - I‚Äôll make it neat for you to review all at once where possible"
CSCI-164,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Jul/25 3:24 PM,17/Sep/25 2:26 PM,,Create Source-to-Target Map for Dim_Product,Source to target mapping for Dim_Product - filling out sheet as per Source-to-target dims,"* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled
** Pres Column name
** Data type
** Description
** Source
*** Server
*** DB
*** Schema
*** TAble 
*** Column
*** Derived? Yes or no","@user Added the source mapping from DIM_PRODICT source table from PBI05, Also included the Gold layer mapping DimProduct table from pervious EDP project.
[Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=ES7jdo&nav=MTVfezg3MTk2QTA1LURBNDgtNERBOS1CREVELTI0MTBDNzEwNTFFRH0]

@user Conceptual ERD from EDP Gold layer

Harrison and Alan to review Dims.
 @user to create new ticket if needed."
CSCI-163,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,21/Jul/25 10:04 AM,01/Aug/25 4:07 PM,,QA Snowflake ingested data,"Comparing data between:

* source SQL DB (StockDB and SCAX2012) 
* Snowflake tables and views",* Complete match between source and Snowflake tables,"Tested DB:

* TDB08AX2012.StockDB

What has been tested:

* mismatch
* missing tables
* missing data types
* duplicates
* Key column null value check
* Data range check

*Results as per Adeel Email:*

# RowCount mismatch

All tables are matched except the BranchInfoHierarchy

_ÓÑç_

|*Server*|*Database*|*Table*|*DBRowCount*|*SFRowCount*|*Results*|
|TDB08AX2012|StockDb|BranchInfoHierarchy|901|921|Not Matching|

¬†

# Below tables are missing

* MultiBuyTriggers
* ProductGroup
* ProductNetworkCosts
* ProductNetworkCostsHistory
* ProductsDrugMatch
* SOHAdjustmentTypes
* SupplierDetails

¬†

¬†

# Data Type mismatch
## All tables were ingested into Snowflake with columns as text. Views were created for type conversion to match the source, but the following column mappings are incorrect.

_ÓÑç_

{adf:display=block}
{""type"":""table"",""attrs"":{""isNumberColumnEnabled"":false,""layout"":""center"",""localId"":""efef3cf3-771c-4d29-93ab-ba8ba18cb5ab""},""content"":[{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""colspan"":3,""background"":""yellow""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Source Table""}]}]},{""type"":""tableCell"",""attrs"":{""colspan"":3,""background"":""yellow""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""SF View""}]}]},{""type"":""tableCell"",""attrs"":{},""content"":[{""type"":""paragraph""}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""background"":""#daf2d0""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""TABLE_NAME"",""marks"":[{""type"":""strong""}]}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#daf2d0""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""COLUMN_NAME"",""marks"":[{""type"":""strong""}]}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#daf2d0""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""DATA_TYPE"",""marks"":[{""type"":""strong""}]}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#c0e6f5""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""TABLE_NAME"",""marks"":[{""type"":""strong""}]}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#c0e6f5""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""COLUMN_NAME"",""marks"":[{""type"":""strong""}]}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#c0e6f5""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""DATA_TYPE"",""marks"":[{""type"":""strong""}]}]}]},{""type"":""tableCell"",""attrs"":{},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Results"",""marks"":[{""type"":""strong""}]}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""background"":""#daf2d0""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Catalogues""}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#daf2d0""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""EndDate""}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#daf2d0""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""smalldatetime""}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#c0e6f5""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""CATALOGUES_VW""}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#c0e6f5""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""ENDDATE""}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#c0e6f5""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""TEXT""}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#c0e6f5""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Not Matched""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{""background"":""#daf2d0""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Catalogues""}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#daf2d0""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""StartDate""}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#daf2d0""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""smalldatetime""}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#c0e6f5""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""CATALOGUES_VW""}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#c0e6f5""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""STARTDATE""}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#c0e6f5""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""TEXT""}]}]},{""type"":""tableCell"",""attrs"":{""background"":""#c0e6f5""},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""not matched""}]}]}]}]}
{adf}

¬†

# Duplicate Checks

Result: Passed - No duplicate records were found

¬†

# Key Column null value check

Result: Passed - No null values found in key columns

¬†

# Date Range Validation
Invalid date values were identified in the following fields across all tables:

* ETL_ROW_EFFECTIVE_DATE
* ETL_ROW_EXPIRY_DATE

@user FYI

Note:

* need to know archiving policy
* know whether we should be pulling from archives? or direct ax2012
** Point - need to have in sync with snowflake.

I have addressed all the gaps and issues raised by Adeel for StockDB. He will retest later and let us know.

Adeel will also commence testing the SCAX2012 tables.

To address data gaps for SCAX2012

will catchup about duplicates today @user @user

The FULL APPLY load type for tracking source deletions has been successfully tested."
CSCI-159,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jul/25 8:26 PM,15/Aug/25 4:39 PM,,Snowflake - Configure CD pipeline in DevOps,"Build and configure a CD pipeline to automate Snowflake deployments via DevOps. 

Development task: [Task 192950 Configure Snowflake Release Pipeline|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/192950]","* CD pipeline deploys Snowflake scripts automatically upon version control updates.
* Environment-specific configurations are supported and validated.
* Deployment logs and rollback options are available and functional.","for review by @user

approved and merged

cc @user

hi Eugene, I've got this error when I ran the Snowflake cd pipeline. Do you know what is missing?"
CSCI-155,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jul/25 4:31 PM,04/Aug/25 6:48 AM,,PowerBI - Infras/ gateway setup,"This is a placeholder for PowerBI infras requirements. We don‚Äôt need to take any action now. Jess and Harrison will communicate/ confirm the details in early August once we know more. 

@user",,"CK to create Vnet gateway
Eugene to create connections

I created 2 PowerBI Connections in Fabric, one using the On-Premise Data Gateway (SF_S_POWERBI_DEV) and the other using VNet Data Gateway (SF_S_POWERBI_DEV_VGW).

I also allowed the group [Azure-Permissions-dev-edp-aus-Contributor - Microsoft Azure|https://portal.azure.com/#view/Microsoft_AAD_IAM/GroupDetailsMenuBlade/~/Members/groupId/1d2b4ba7-8619-4000-a219-f488b2256e3c/menuId/] use access to it.

When using the connection, please follow the following steps to enable the compute:

# *SF_S_POWERBI_DEV* - start the VM [cwr-ase-edpdatagateway-dev-vm-01 - Microsoft Azure|https://portal.azure.com/#@mychemist.com.au/resource/subscriptions/dc516e92-8716-44f9-b09c-fc5ca9cdd01a/resourceGroups/cwr-ase-dpdev-rg/providers/Microsoft.Compute/virtualMachines/cwr-ase-edpdatagateway-dev-vm-01/overview]. Once done, please stop the VM to reduce cost.
# *SF_S_POWERBI_DEV_VGW* - resume the Fabric capacity [cwraseedpdevfc - Microsoft Azure|https://portal.azure.com/#@mychemist.com.au/resource/subscriptions/dc516e92-8716-44f9-b09c-fc5ca9cdd01a/resourceGroups/cwr-ase-dpdev-rg/providers/Microsoft.Fabric/capacities/cwraseedpdevfc/overview]. Make sure to Pause it after use to save on cost. Currently, only CK and I can resume the capacity. 

Note:

* For the VNet Data Gateway, *cwr-ase-edp-dev-vdgw*, it can also be assigned to a Trial Capacity, see below: 

cc: @user @user @user @user @user

@user this is in review but task is basically done.

@user marked as done and crated a review task"
CSCI-154,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jul/25 3:45 PM,01/Aug/25 4:51 PM,,Analyse timing of on-prem SQL server table refreshes - part 2,"Currently, PBI05 SQL Server is at 95-100% CPU capacity. Issue stems from daily batch jobs running whilst SC team are trying to refresh their semantic models.

To help ease this issue, need to develop a report/dashboard that details the daily timings of when each table in PBI05 is being refreshed.",,"Made below updates to PBI Dashboard:

* Updated SQL script to include the ‚ÄòObject type‚Äô (ie Table or View)
* Factored in new logic for Linked Server objects (included a manual override column to flag)
* Fixed the exception for PBI table FactStoreInventoryERPEOMSummary
* Advised Rachel F that they are referencing 2 TBI03 (test server) objects (these will not be refreshed periodically)

Need to follow up Bhavya to ask about:

* PDB08 and PDB15
** Do these servers contain data that is refresh as part of an ETL batch job
* Gain ‚Äòview server state permission‚Äô access to PBI03 and PDB servers as needed

Expected to be done today"
CSCI-153,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jul/25 3:28 PM,01/Aug/25 4:06 PM,,SCAX2012 Integration - Very large tables,"Implementing a bulk load parquet pattern and dual ingestion strategies (historical + delta) for very large tables:

|Database|Schema|Table|
|SCAX2012|dbo|CUSTINVOICEJOUR|
|SCAX2012|dbo|CustInvoiceTrans|
|SCAX2012|dbo|PurchLine|
|SCAX2012|dbo|VENDINVOICETRANS|","# Bulk load pattern successfully ingests ~3.8k Parquet files without failure or performance issues.
# Historical and delta ingestion logic is implemented and validated for target tables.
# End-to-end ingestion pipeline passes functional testing","|Database|Schema|Table|
|SCAX2012|dbo|CUSTINVOICEJOUR ~ 32.6 Mil|
|SCAX2012|dbo|CustInvoiceTrans ~ 385.7 Mil|
|SCAX2012|dbo|PurchLine ~ 181.5 mil|
|SCAX2012|dbo|VENDINVOICETRANS ~ 181 Mil|

Created external stages to get row count from the total parquet files

Successfully loaded below tables:

* SCAX2012.VENDINVOICETRANS at 181,086,639 row count
* SCAX2012.PURCHLINE at 181,467,367 row count

Historical and delta load implemented and unit tested for all 4 above mentioned tables. Next step: - Adeel to QA data - Chloe to monitor the job load @user @user @user@user 

I‚Äôve scheduled the delta load job to run at 4am my time @user if you want to jump in to observe ExpressRoute stats.

@user - to check followup ticket in backlog

Awaiting Adeel‚Äôs QA results

Chloe to address issues raised by Adeel.

@user DQ issues raised by Adeel, Chloe investigating

expected to wrap up today
- testing/QA with @user"
CSCI-152,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jul/25 3:25 PM,15/Aug/25 3:48 PM,,StockDb Integration - part 2,"Build data ingestion pipelines for the StockDb database.

Pending a few more tables from StockDb to be ingested to Snowflake next sprint:

|MultiBuyTriggers|
|ProductGroup|
|ProductNetworkCosts|
|ProductsDrugMatch|
|SOHAdjustmentTypes|
|SupplierDetails|

And the below which will require a historical load + delta pattern: 

|Table|RawData|Refresh Freq|¬†Total Row Count¬†|
|ProductNetworkCostsHistory|Yes|Real Time|########|","Tables available in Snowflake and pass validation test

Phil will catch up wiht @user to write this soon.","@user part 2 of the task split for sprint 4

for the big table, will need to spin off as a different task. @user @user

@user Created the delta job and asked @user to create some insert, update & delete at source database to track in delta load into snowflake.

@user Below records are inserted deleted and updated in ProductNetworkCostsHistory table.

HISTORYID Inserted
136742278
136742277

HISTORYID Deleted
136742275
136742274

HISTORYID Updated
136742273
136742272
--PublishedDate updated from '2025-01-31' to '2025-02-01'

to spin off testing ticket for this @user
can you tag me afterwards if need help with acceptance criteria

@user,Dev work is completed for all tables in scope. Need to create a separate ticket for some testing on the big table ingestion (PRODUCTNETWORKCOSTSHISTORY)."
CSCI-151,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jul/25 3:13 PM,15/Aug/25 4:39 PM,,Snowflake Deployment Pipeline - Versioning & Parameterisation,"* *Finalise* {{config_meta.sql}} *Versioning Logic*
** Define and implement logic to track version history and changes within the {{config_meta.sql}} script.
* *Parameterise Deployment Execution Scripts*
** Introduce variables and parameters to support flexible, environment-specific deployments in DevOps.
* *Test Initial Deployment Pipeline*
** Execute and validate the first round of deployment to ensure pipeline functionality and identify any issues.","# {{config_meta.sql}} includes clear versioning logic and documentation.
# DevOps scripts support configurable parameters for different environments.
# Initial deployment completes successfully with logs and validation checks.",", I've updated the below:

* Updated deployment scripts with added parameters for the snowflake environment and version¬†
* Updated the config_meta.sql scripts with proper test values/ column names. They are still test values though.¬†
* Created the edp-snowflake-cd pipeline to test my configuration setup¬†
* Added the Library's group variables and allow pipeline permission for edp-snowflake-cd.
* Updated the cd.yml file to reference the group variables from the Library

Eugene to implement the CD pipeline. Will need to test after it is up and running

{{@Chloe Tran }}fill out the other variable groups for deployment

Eugene has working mechanism, requires review by Chloe to approval.

Closing this ticket and creating new ticket for review post other prioritised items being completed - likely Sprint 7.

Thanks,

Harrison"
CSCI-150,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jul/25 12:55 PM,18/Jul/25 8:51 PM,,Update SQL Source Documentation with DBA Lineage Information,"Adeel from DBA team has provided us information on the underlying data source lineage of PBI05 and PBI03 objects.

Task to incorporate this information with our existing documentation. As well as identify any gaps in the lineage information provided, to ensure all PBI objects are mapped to a root source.",,
CSCI-149,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jul/25 12:43 PM,30/Jul/25 4:12 PM,,SC BAU Sprint 3 Work,"# Helping to troubleshoot PBI Report/Workspace access issue for e-Comm team, for a user from Sigma
## Madumitha messaged via Teams on 15/7
# Assisted Liang with access to PBI Gateway connections for SC team",,
CSCI-148,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Jul/25 11:57 AM,01/Oct/25 3:29 PM,,Data Engineering code standards,"# *Define Coding and Naming Standards*
#* Establish conventions for SQL, Python, and pipeline components.
# *Document Deployment and CI/CD Patterns*
#* Outline standardised approaches for Snowflake and DevOps workflows.
# *Create Data Modelling Guidelines*
#* Provide guidance on dimensional modelling, schema design, and partitioning strategies.
# *Develop Onboarding and Knowledge Base Materials*
#* Compile key resources, templates, and walkthroughs for new team members.","* Core documentation is published and accessible to the team.
* Standards and patterns are reviewed and approved by key stakeholders.
* Onboarding materials are complete and tested with at least one new team member.","@user can you please elaborate for which are would this be? Off hand, these are the following where we need coding standards:

* Azure Data Factory
* Snowflake Development (DDLs and Transformations)
* Semantic Models
* PowerBI Reports

Pls advise.

cc: @user

This is more of a place-holder, Chloe to specify further which code standard is required. @user @user I will add the details as per Eugene‚Äôs comment

Hey @user ,

Adding @user to this card as I think he and you could discuss to align what standards and templates are required for future MergeCo Data Platform. Then could divvy up documenting etc.

Thanks,

Harrison

Will combine this with [https://sigmahealthcare.atlassian.net/browse/CSCI-416|https://sigmahealthcare.atlassian.net/browse/CSCI-416]"
CSCI-147,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Jul/25 11:56 AM,01/Sep/25 2:46 AM,,Configure ExpressRoute,"Azure DevOps Link: [User Story 191094 Configure ExpressRoute Peering|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/191094]

* Set up the QoS policy on the SHIR VM and monitor performance and bandwidth usage during the 2 AM‚Äì6 AM window.
* Adjust ADF settings (concurrent connections, DIUs, copy parallelism) as needed based on test results.
* Capture baseline and incremental bandwidth data to inform future scaling decisions.
* Review results together before considering any further controls at the ExpressRoute or firewall level.
* If the VM-level QoS proves effective, we can avoid more complex network changes. Once the Megaport rebuild is complete and the new connectivity is validated, we‚Äôll work with Brent and the cloud team to coordinate the next phase, including a phased ramp-up of Snowflake loads and the Private Endpoint migration.

@user the task description above is as per CK‚Äôs email on on ExpressRoute Limitation on 11 Jul 2025. Please feel free to make any edits or updates as you see fit.",,"Implementing the QoS on the SHIR machines but encountered an issue, currently working with MS support for resolving the issue.

Migration and upgrade required by <who? Cloud Team?>.

Not D&A team to perform.

Only Monitoring required in sprint 5 per [https://sigmahealthcare.atlassian.net/browse/CSCI-208|https://sigmahealthcare.atlassian.net/browse/CSCI-208]. Monitor ExpressRoute throughput utilisation.

@user

@user we already have a dashboard for monitoring the bandwidth usage of ExpressRoute but our traffic is not yet representative of prod levels. We do have a baseline we can work with.

Can you please create a task for the following: 

* Adjust ADF settings (concurrent connections, DIUs, copy parallelism) as needed based on test results."
CSCI-146,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Jul/25 11:50 AM,05/Aug/25 8:35 AM,,Meet with Rachel Wan on SSIS Transformation Packages,Sit down with team to ask questions and go through transformation logic as well as the access to go through oursleves,,"confirmed their availabilities.

@user to take Bhavya/samer`/adeel

Rachel Wan taken through what we are requesting for access to utilise Bhavya and Adeel for. We are hoping for 80% capacity from them starting 04/08 but we are waiting for in writing response from Rachel to confirm. Second session with Bhavya and Adeel completed to take them through.

Thanks,

Harrison"
CSCI-145,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Jul/25 11:48 AM,05/Aug/25 8:35 AM,,Access to SSIS packages,Raise request in SNOW for access and send email to Rachel Wan to clarify,,"Request Raised for SSIS package access : Request Number : *REQ0155399*

¬†

Raised for Amit , Jess and Chloe

@user can i get a screenshot of req and followups?
just want to move this along.

thanks

Had a chat with Heshan. Access is granted now. However, we might need Visual Studio license to open the packages. He is trying out different options if possible.

No further update. Will check on Friday with Heshan

Can only see a few SSIS packages, but can‚Äôt see all - investigating. @user

There is a lot of dependency on Stored procsedures in SSIS package. So, not much info can be pulled out directly from SSIS, except the names of the responsible Store Procs. Have got Dimension update store proc from Bhavya."
CSCI-144,CSCI,CW Cloud Data Platform Interim Solution,Task,Duplicate,Medium,16/Jul/25 11:27 AM,19/Nov/25 12:23 PM,,Source-To-Target for Facts and Dims in Prioritised Domains,,,
CSCI-143,CSCI,CW Cloud Data Platform Interim Solution,Task,Duplicate,Medium,16/Jul/25 11:25 AM,19/Nov/25 12:27 PM,,Source-To-Target for Facts and Dims in Prioritised Domains,"Source To Target mapping for all Facts and Dims identified for delivery in prioritised domains

# Conformed Master Data
# Inventory

Need to split Business Processes and Dims prioritised across this ticket and [https://sigmahealthcare.atlassian.net/browse/CSCI-118|https://sigmahealthcare.atlassian.net/browse/CSCI-118]",,
CSCI-140,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Jul/25 11:57 AM,25/Jul/25 9:47 AM,,Raw Source Object for core Business Processes,"Focusing on objects not connected to SC existing PBI Semantic Models, but other objects in BUS Matrix. Provide not just PBI05 object but the relevant objects from actual source for this object.","Column G ‚ÄúActual Source‚Äù in BUS Matrix filled in with relevant SQL Server/MySQL object details.

*Next Step:* Source-To-Target Mapping","Split into 2 tasks, one for Business Processes and Dims

Started working on Business Processes. Reviewing existing documents. Hvaing SSIS package access will help a lot

Only a few tables left now. Will be finishing off today

Source mapping completed

@usercan you book session for early next week - any fill outs to call out before DBA approaches this"
CSCI-139,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Jul/25 11:57 AM,30/Jul/25 2:57 PM,,Raw Source Object for core Business Processes and Dims,Focusing on objects connected to SC existing PBI Semantic Models. Provide not just PBI05 object but the relevant objects from actual source for this object.,"Column G ‚ÄúActual Source‚Äù in BUS Matrix filled in with relevant SQL Server/MySQL object details\ for objects included in SC PBI Semantic Models.

*Next Step:* Source-To-Target Mapping","Clairfy/review objects with Adeel.

Email sent to Adeep (28/7/25).

Identified we are missing 12 PBI objects with their lineage (these were not picked up in the initial discovery as these are indirect sources of other PBI objects)."
CSCI-138,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Jul/25 10:50 AM,18/Jul/25 12:31 PM,,Collation of PBI Admin questions for Microsoft,"Time to produce list of questions for Microsoft regarding PBI Admin and Setup

*Questions*

# How do we connect PBI to Snowflake with SSO and Privatelink most seamlessly",,"# Ask MS how best to connect Snowflake to PBI via Gateway settings (how do they advise we set this up)
## Ensure this solution enables us to maintain a ‚Äòprivatelink‚Äô between SF and PBI
# Show MS the bug between switching from a CW tenant to Sigma tenant and vice versa
## Why does the left-hand pane disappear?
## Why can‚Äôt we switch from CW back to Sigma? (Why does it default back to CW?)
## Why does the webpage ‚Äòglitch out‚Äô when trying to open settings in a semantic model, from a different tenant?

Email sent to CW (+ contractors) + Sigma team (16/7/25) to see if any other team members wish to raise any questions for MS.
Have asked for requests to be sent by COB 17/7/25.

@user Ask if for paginated reports we require an on-prem gateway? (Q came from Zhaneta/engineering team - they have been told this in the past)

From Dili:

+_From a DG perspective, if MS can provide information on whether PBI has the ability to bring in any access controls i.e. RBAC rules from Snowflake, on both desktop and Online Service (in case that differs)_+

Email sent - 18/7/25"
CSCI-136,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Jul/25 9:37 AM,24/Nov/25 9:11 AM,,Manhattan Active (MySQL) Integration part 1,"DATA IN for critical identified objects from Manhattan Active (MySQL) db

* confirm with RFoong decision for data lake extraction rather than API
* Whitelinsting
* Credential needed for GCP - login and testing connection
* Create pipeline

@user","Definition of done:

* Extract meta created
* Linked services created
* Pipeline run end to end","Decision required to be made on whether to connect to MySQL MAWM copy or connect directly through REST API to MAWM. Providing some notes from discussions below. Meeting setup for Thursday 31/07/2025 for this.

@userlet‚Äôs confirm our decision on this source early next week with @user and @user. Rachel F. mentioned Gustavo would be on leave for the whole September so we might need to schedule a call with them by end of August.

Status as of Monday (15.09.2025):

Will split tasks with you @user - will share load with @user

Issue split into:
|CSCI-461|Manhattan Active (MySQL) Integration part 2|

now connecting to manhattan active - new credential requred as current user is being decomissioned.

@user working on a custom proc for MYSQL to get the datatypes from the source system.

Issue split into:
|CSCI-524|Manhattan Active (MySQL) Integration - (Work on MySQL specific Procedure )|

readonlyuser deprecated and need new credentials to connect to Manhattan active.

Got access to a MYSQL database and work in progress to create a procedure in snowflake to get source data types. Once finished the ticket will go back to blocked until the service account is created for Manhattan Active database.

Credential Received.

Discovery exercise completed. 

Questions posted to database team and continue the second part."
CSCI-135,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Jul/25 9:35 AM,28/Aug/25 10:39 AM,,Manhattan Scale Integration,"DATA IN for critical identified objects from Manhattan Scale db (ILS)

Implement data ingestion for critical identity objects from the on-premises Manhattan Scale (ILS) database into the cloud data platform. This includes configuring the necessary Azure Data Factory (ADF) components to support automated and reliable data flow.

Latest update;

* Ticket raised. REQ0156142
* Currently with the infrastructure cloud team - Brent - Cloud services manager.
* !image-20250729-063227 (1ed4c70f-781d-46cb-9ced-b8db1e7f0691).png|width=961,height=959,alt=""image-20250729-063227.png""!","Definition of done:

* Extract meta created (@user )
* Linked services created
* Pipeline run end to end
* -Whitelisting of on-prem database server. (Done by @user )-","ils db being added to firewall per new ticket from CK per Chloe request

Following up with @user on this

@user can you follow this up

@user will do

Have followed up with @user on this - have updated description - and will get further updates on this shortly.

Update:
- need to whiltelist this in Azure firewall.

* Ticket raised. REQ0156142
* Currently with the infrastructure cloud team - Brent.
*

Have caught up with @user CK|
This is tested and resolved. 
CK will update this.

Have updated acceptance criteria 

* whitelisting on prem server done by @user - TYSM!

@user @user I have split this into part 1 and part 2 so that if this doesn‚Äôt finish there will be work flowing to the next sprint."
CSCI-134,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,15/Jul/25 9:33 AM,12/Aug/25 1:27 PM,,SQL3 (PDB14) Integration Historical Load (Parquet),"DATA IN for critical identified objects from SQL3 MyPOS db

|TDB14|TransactionStorage|BranchOrderItems|Yes|3 mins Merge from Store|467,808,855|
|TDB14|TransactionStorage|BranchOrders|Yes|3 mins Merge from Store|22,340,006|
|TDB14|TransactionStorage|Store_IncrementalSOHChanges|Yes|3 mins Merge from Store|8,302,390|
|TDB14|TransactionStorage|Transactions|Yes|3 mins Merge from Store|825,948,968|
|TDB14|TransactionStorage|Transactions_Invoice|Yes|3 mins Merge from Store|272,348,977|","Definition of done:

* Extract meta created
* Linked services created
* Pipeline run end to end","SQL3 Server upgraded, does that require new IP Address to be provided to us, whitelist etc.

@user Does this require new whitelisting etc? Need to confirm with @user

Working on bridging tasks between historical and delta(ongoing) run.

Done."
CSCI-133,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Jul/25 3:41 PM,01/Aug/25 4:51 PM,,Analyse timing of current Supply Chain Semantic Model refresh vs SQL Server refresh,"Upon completion of [https://sigmahealthcare.atlassian.net/browse/CSCI-132|https://sigmahealthcare.atlassian.net/browse/CSCI-132], need to analyse result findings with details to when each semantic model is being scheduled to refresh.

Need to understand the lag time and how long an average refresh is taking each day, per model.",,"need to show Rachel Foong for review

Expected to be done today"
CSCI-132,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Jul/25 3:40 PM,18/Jul/25 8:51 PM,,Analyse timing of on-prem SQL server table refreshes,"Currently, PBI05 SQL Server is at 95-100% CPU capacity. Issue stems from daily batch jobs running whilst SC team are trying to refresh their semantic models.

To help ease this issue, need to develop a report/dashboard that details the daily timings of when each table in PBI05 is being refreshed.",,"Initially was able to retrieve a ‚Äòlast modified‚Äô time for all tables and views in PBI05 using metadata from the system table *sys.tables*.

However upon further inspection, this modified time appears to only be when a table was last altered structurally, eg: creating a new column, or performing a DROP and CREATE statement etc

Was able to locate a different system table: *sys.dm_db_index_usage_stats*.

This table stores index usage stats. We can use the last modified date from this table to form as an approximation to when a table was last modified (if any data was inserted or deleted from a table, this is included within the modified date logic).

Currently need to request additional access from DB team to query this table (require VIEW SERVER STATE permissions). Have spoken to Rachel Wan and she is ok with approving this permission (at least for PBI05 initially). Awaiting access.

{code:sql}SELECT TOP 1 *
FROM sys.dm_db_index_usage_stats
WHERE database_id = DB_ID( 'SupplyChain')
AND OBJECT_ID in (select OBJECT_ID(name) from sys.tables)
ORDER BY last_user_update DESC;{code}

have split task to [part 2|https://sigmahealthcare.atlassian.net/browse/CSCI-154] for further work
this task can now be adjusted for sprint points and ready to be closed.

 @user"
CSCI-131,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,14/Jul/25 2:49 PM,01/Sep/25 12:29 PM,,MergeCo Conformed Reporting - Develop Inventory Agg Fact,"* Inventory
** SOH (Units + $)
*** Stock on Hand
** DOI
*** Days of Inventory
**** Avg SOH $ / avg Daily Sales $ at Cost (last 12 weeks)

*Tasks done:*

* Performed Data Discovery of SCAX2012 and PDB08 to locate and find where this data exists in CW (2 story points)
* Developed SQL views of data (1 story point)
* Performed validation/sense checking of data (0.5 story points)
* Gathered all requirements of what data we need to ingest in PBI05 (0.5 story points)","Develop facts from CW to create these KPIs

* -SOH Units-
* -SOH $-
* -Days of Inventory-","Could start this in Sprint 3 but then move to 4.

Breakout into 2 tasks

# Analysis of required objects to be ingested into SF from db‚Äôs
# Building of aggregations and ingestion
## ADF Pipeline

Currently fleshing this out and will fill out details for this + csci-205 + csci-206

SOH - PBI05.[BI_Presentation].[dbo].[AX_FCT_DC_STOCK]
Daily Sales - PBI05.[BI_Presentation].[dbo].[AX_FCT_DC_SALES]

Need to check/confirm what CW equivalent is:

* Confirmed
* Invoiced
* Ordered

SC use PBI05.[SupplyChain].[Scale].[*shipment_detail*] and PBI05.[SupplyChain].[Scale].[*shipment_header*] for ‚ÄúRequested Qty‚Äù and ‚ÄúDelivered Qty‚Äù

I will convert this to a checklist in AC

prioritise:

* inventory
* outbound performance

SC use [SupplyChain].[SCAX].[PurchLineSimple] - purch status contains 4 PO statuses:

* 1 = Open Order
* 2 = Received
* 3 = Invoiced
* 4 = Cancelled

Need to discuss if 2 = ‚ÄúConfirmed‚Äù from Sigma‚Äôs side. If so can leverage these statuses to define the Outbound Performance metrics

For DOT% - CW have an existing metric known as DIFOT% - which is % of orders that we on-time, which is defined as within 3 business days (with public holidays included)

Underlying source of these tables is PDB19B.SCAX2012.INVENTSUM + INVENTDIM

Have got across inventory
Currently on:
- discovery - outbound performance

udpated AC @user

Currently on outbound performance @user

to go thru with @useron Friday

----

aim:

* to break out by prioritisation of reports

Will split out this ticket @user today 

note - outbound performance."
CSCI-128,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,10/Jul/25 12:20 PM,18/Jul/25 8:45 PM,,Power BI - Create CI/CD POC with Git,"Needed for PBI deployment

Created this ticket on the back of 10am meeting with CK and @user - thanks @user for letting me know

Have placed 3 sprint points as a placeholder for now.

Feel free to edit this ticket where needed, or contact me so that I can do the admin/make necessary linkages.",,"@user i created a corresponding feature in DevOps [Feature 196244 Semantic Model and PowerBI Git Integration|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/196244]. 

cc: @user @user

Git - done.
POC - done

next steps
Creating report out of semantic model - in progress

NOte:
- eugene @user to work with Jess @user - 
book a meeting Tues for demo.
 @user @user @user

@user can we please split this in 2? one for CI and the other for CD? CI is completed

@user 

@user - done
[https://sigmahealthcare.atlassian.net/browse/CSCI-155|https://sigmahealthcare.atlassian.net/browse/CSCI-155] [https://sigmahealthcare.atlassian.net/browse/CSCI-156|https://sigmahealthcare.atlassian.net/browse/CSCI-156]

@user FYI the split out tasks"
CSCI-127,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,08/Jul/25 10:05 AM,18/Jul/25 3:51 PM,,Snowflake User Creation,Create Snowflake users for EDP team members,,"Chloe: Access granted from Chloe to Jess, Eugene, Chloe, Amit, Phil and Harrison

To create new user accounts for Bavya and Adeel and Sam in DBA team"
CSCI-126,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,03/Jul/25 10:26 AM,14/Jul/25 2:39 PM,,Snowflake - ADF service user - Key Pair authentication,"ADF to SF via User Account, currently uses password however no longer available for SF sign in. I have put in a temporary solution: update user type as LEGACY SERVICE, however this method will fade out on 1st Nov. 

 Key pair authentication method sign in required. 

To do: switch all Snowflake service accounts to use Key Pair authentication.",,"I have generated the key pairs and tested the connection successfully.¬†

¬†

¬†

However I am not able to upload to Azure Key Vault. To store a private key in Azure Key Vault, it needs to be combined with a certificate. I have to generate it as a pfx or pem files with CA/CSR signed certificate which seems very involved. My pem file doesn't include a certificate at this stage.

Anjali has raised ticket *RITM0169851* created to get help from the security or network team to issue certificates we can use.

Successfully uploaded the pem file to Key Vault using Azure CLI. Eugene to review method to add private key and passphrase via Terraform.

This is completed, please see [Task 195726 Create SSH KeyPair for Snowflake Authentication|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/195726]. Deployed in Dev, QA/SIT, Prod"
CSCI-125,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Jul/25 11:42 AM,18/Jul/25 3:51 PM,,SCAX2012 Integration,"DATA IN for critical identified objects from SCAX2012 db.

|TDB19b|PDB19B|SCAX2012|CUSTINVOICEJOUR|
|TDB19b|PDB19B|SCAX2012|CustInvoiceTrans|
|TDB19b|PDB19B|SCAX2012|INVENTDIM|
|TDB19b|PDB19B|SCAX2012|INVENTJOURNALNAME|
|TDB19b|PDB19B|SCAX2012|INVENTJOURNALTABLE|
|TDB19b|PDB19B|SCAX2012|INVENTJOURNALTRANS|
|TDB19b|PDB19B|SCAX2012|INVENTLOCATION|
|TDB19b|PDB19B|SCAX2012|INVENTSUM|
|TDB19b|PDB19B|SCAX2012|PurchLine|
|TDB19b|PDB19B|SCAX2012|VENDINVOICETRANS|","Definition of done:

* Extract meta created
* Linked services created
* Pipeline run end to end","Chloe: 3 tables ingested in Full Load pattern, need to test a delta load.

Adeel provided PK and no deletes. To review. Eugene mentioned they have seen deletes on some tables. Delete {color:#ff991f}checking{color} table. Soft deletes pattern DATA IN. Lower priority but plan in timeline post integrations

Jess C: CK document has BUS Matrix, analyse vs existing document. Overall CW including SC.

Amit required to play bigger role in existing legacy information.

The following tables have been fully ingested to SCAX2012:

# INVENTJOURNALTABLE ‚Äì DONE (historical + delta)
# INVENTJOURNALTRANS ‚Äì DONE (historical + delta)
# INVENTSUM ‚Äì DONE (historical + delta)
# INVENTDIM ‚Äì DONE (direct)
# INVENTJOURNALNAME ‚Äì DONE (direct)
# INVENTLOCATION ‚Äì DONE (direct)

We need to implement a pattern for parallel load for large 3 of parquet files ~3.8k files.

--

The other tables which need to be ingested via 2 patterns (historical + delta) have very large # of parquet files.

With the existing pipeline it will take days to complete a full historical load. It takes 1 ~ 1.2min to ingest 1 parquet file. 

|TABLE_NAME|FILE_COUNT|
|CUSTINVOICETRANS|3857 (~77hrs or 3.2 days)|
|PURCHLINE|1815 (~36hrs or 1.5 days)|
|CUSTINVOICEJOUR|303 (~6hrs)|
|VENDINVOICETRANS|1811 (~36hrs or 1.5 days)|

I tested a pattern where we can bulk load all the parquet files from external stage (blob storage). 

Next step: I need Adeel to copy the files to the specified location. I dont have access to do it. Will also request access from Eugene to do so myself."
CSCI-124,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Jul/25 11:42 AM,28/Aug/25 2:16 PM,,AXLink Integration,"DATA IN for critical identified objects from AXLink.

|Warehouse_ReservedItems|","Definition of done:

* -Extract meta created-
* -Linked services created-
* -Pipeline run end to end-","planning to catchup with @user 

- what is a 'bridging db‚Äù"
CSCI-123,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Jul/25 11:39 AM,25/Jul/25 12:22 PM,,DFIO Manhattan Source System,"Follow up on understanding who can assist in obtaining source system details for DFIO data for a future state solution.

Who can provide this info?

What is system details?

How do we connect?

What objects do we require?",,"Requested Login for DFIO SCI .

Access to be provided to Amit for credentials to login and review.

Requested access to setup session with manhattan team.

# My SQL environment setup for Manhattan - need to confirm this is a Copy of data from Source System and frequency of timing.
## Are we to connect to MySQL Manhattan Active Copy or is another Source System Required?
## Is CK able to assist in this? Does he know who if not able to know about MySQL environment setup

*Next Step:* Amit to share email and book call with required Infra/CK for MySQL understanding

Reached out to Manhattan for time slots availability for a discussion. Bhavi Sanjay Mehta <[bhmehta@manh.com|mailto:bhmehta@manh.com]> is the contact person from Manhattan

Manhattan has mentioned that they have shared the required details with Project Management team . Have advised to reach out to Amila. Dropped a note to him. Will update once I hear from him.

Had a word with Amila. He mentioned that he has some info and will share with us soon. 
Will talk to Supply Chain team for current extract of data and source report.

No further update from Amila

Followed up again with Amila. Asked for an ETA.@user

Got Below responses from Amila. I think we got our answers for now. 

# Details of MySQL environment

¬†

AE: (from attached email)

Direct from our Gateway (IP: 192.168.42.65 or 88 ‚Äì both PowerBI gateway servers)

To MySQL server (172.19.232.11)

Through IP whitelisting/firewall rules

Standard TCP connection on port 3306

¬†

¬†

# Best way to connect mainly from data extraction purposes

AE: Above, Analitics team use a MYSQL connector . not sure how they use it. Rachel Foong should be able to provide more info

¬†

# Is it the live data source or a copy

AE: Copy

¬†

# If so , refresh frequency

AE: (from attached email)

‚ÄúWe worked with the CloudOps team and confirmed that there is no lag or issue with the Data Save replica database.‚Äù

¬†

# Any Schema Documentation

We don‚Äôt have this, maybe check with ¬†Rachel Foong

(see ‚ÄúMAWM ‚Äì Questions‚Äù mail)

¬†

# Any mapping document between old Scale and New MAWM Data

We don‚Äôt have this, maybe check with ¬†Rachel Foong

Next steps: 
what is the best way to connect for ingestion purpose. I think we can explore Azure/Snowflake options and reconnect later. 

Schema Documentation: We should touch base with MAWM team . Point of action for later

Mapping Document: We should touch base with MAWM team . Point of action for later

@user @user Any thoughts?

Hey @user ,

Not sure what additional details you have added that weren't known 2 weeks ago from Teams and Stand-Up discussions.

You have said ask another person a lot throughout this response also. Can I confirm you are going to own DFIO portion for future analysis to understand where the data currently lives? Where will it live in future? Details etc? CK can help I would say.

For Manhattan Active, we have got those details from CK a few weeks ago. We need to know how many DC‚Äôs on it and the roll out plan, core objects to pull etc. If not same as Manhattan Scale.

Thanks,

Harrison

MOving to done - 2 new tickets for SPrint 5 - Need ticket numbers as per backlog standup @user"
CSCI-122,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Jul/25 11:14 AM,03/Jul/25 4:46 PM,,Identifying the underlying source of Supply Chain Facts and Dims - DFIO (Share Location),"Goal:

* Identifying the source of Supply Chain Facts and Dims

Acceptance criteria:

* Location address of the source of Supply Chain Facts and Dims for the DFIO Data Domain",,"Have extracted the network drive file path for each data source stored on the cwvault network drive.

Info is saved here: [Supply Chain Data Sources - Network Drive (DFIO).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Supply%20Chain%20Data%20Sources%20-%20Network%20Drive%20(DFIO).xlsx?d=w46b6938fd3884f85ba06f0c7dae7b97b&csf=1&web=1&e=QjFGGs]"
CSCI-121,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Jul/25 11:07 AM,18/Jul/25 8:45 PM,,Snowflake - CI/CD high level design,"Collaborate with Eugene to establish the approach for Snowflake deployment. 

Test the deployment pipeline for the 1st round of deployment.","Deployment steps are well defined and documented.

1st deployment pipeline runs end to end","* Workspace created in Snowflake.
* Folder structure is also defined and agreed between Eugene and Chloe.

Waiting for Key Pair ticket ‚Äú‚Äú to be completed. To support Eugene , test and validate Eugene CI/CD pipeline build ‚Äú‚Äú.

Eugene and Chloe discussing on settling on approach. POC to test approach.

Need to prepare deployment script 
Fleshing out req
Call booked on Tues (Mon Eugene‚Äôs time)

Ongoing requirements and design for CI/CD delivery.

We‚Äôve got high level design for managing meta changes and version logging"
CSCI-117,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Jul/25 10:36 AM,19/Nov/25 12:33 PM,,Metrics Definitions for Prioritised Phase 1 Deliverables,"Metrics Definitions Document created for prioritised Business Processes for Phase 1 of delivery.

As per outcome of [https://sigmahealthcare.atlassian.net/browse/CSCI-115|https://sigmahealthcare.atlassian.net/browse/CSCI-115] Phase 1 will include Critical Conformed *Master Data* and Business Processes and Dimensions relating to *Inventory Data Domain*.",,"[https://cwretail.atlassian.net/wiki/spaces/ITEDP/pages/503709725/Stock+ETL+Business+calculation|https://cwretail.atlassian.net/wiki/spaces/ITEDP/pages/503709725/Stock+ETL+Business+calculation] 

Documentation from EDP 1.0 relating to metric definitions for Inventory (Store and DC)

Writing documentation for Metric Definitions in this file:

[Business Metric Definition Glossary.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Business%20Metric%20Definition%20Glossary.xlsx?d=w8c9d0bf3b8484cb9a623c328b2e11bcd&csf=1&web=1&e=aycLBN]

Discussion on current BUS Matrix session 08/07/2025. Notes to be clarified and updating document per decisions. Share actions.

*Harrison required to create new tasks for Source-To-Target.*

Adeel responded back on PBI05 Lineage. Jess C has appended to BUS Matrix for sources.

*Jess C to Action:* PBI03 was also a source mentioned in some lineage, can we trace lineage for these back to actual source (PDB19B, PDB08, SQL3 etc). CK email may answer this last part.

*Eugene:* There were some PBI objects that could not be traced back (difficulty in tracing logic therefore decision is made) to actual source therefore decision was made in EDP project to use the PBI object rather then the source db. *Amit and Bhavya should have this background as well*.

@user Just updating this comment:

* All notes made from the Bus Matrix discussion on 8/7/25 are now updated and reflected in the document
* Have emailed Adeel re: PBI03/ETLSRV - Adeel has advised (in person) he is working through this currently and will get back to us
** Note the underlying code to populate these tables is nested and not straight forward, hence why it is taking some extended time (this is due to legacy code and processes that have been expanded on over the years)"
CSCI-116,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Jul/25 10:31 AM,09/Oct/25 8:38 AM,,Detailed Timeline for Phased Delivery,Detailed timeline with resource allocation for Project Phased delivery,"Detailed timeline created, shared and reviewed internally and with key Business Stakeholders","Location for Project Planning
[https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2B73F1D3-F32E-423D-84FA-C5DDB20E6040%7D&file=Project%20Planning.xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2B73F1D3-F32E-423D-84FA-C5DDB20E6040%7D&file=Project%20Planning.xlsx&action=default&mobileredirect=true]"
CSCI-114,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,01/Jul/25 10:25 AM,03/Jul/25 1:20 PM,,Identifying the underlying source of Supply Chain Facts and Dims - Manhattan (MySQL),"Goal:

* Identifying the source of Supply Chain Facts and Dims at the server-database-table level
** Including the underlying SQL code written

Acceptance criteria:

* Location address of the source of Supply Chain Facts and Dims from Manhattan system (MySQL database)",,"Have completed the source details of all MySQL Server connections, down to the table level:

* Server.Database.Table

Each source detail is mapped against a table within each Semantic Model.

Info saved: [Supply Chain Data Sources - MySQL.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Supply%20Chain%20Data%20Sources%20-%20MySQL.xlsx?d=w9d74faf0c7f94fb2b66fe77c9ddc8d24&csf=1&web=1&e=tsaVfZ]"
CSCI-113,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Jun/25 2:50 PM,04/Jul/25 2:51 PM,,Create link services for source databases,"* To test source database credentials which are saved in the Azure Key Vault. 
* Create linked services in ADF for integration works to commence once ready

Definition of done:

* New linked services created for the source databases
* Deployment config file for dev config-dev.csv",,"I've created a new PR with new linked services and a config-dev file. 

I was able to successfully test the credentials and created linked services for the following databases in ADF:

* AXLINK
* SCAX2012

With the other servers/ databases:

* CWMgtStoreInvoices
* General_Reference
* SpsWhsPurchase
* StockDb
* ILS¬†

I got error such as ""The account is disabled"" or ¬†""A network-related or instance-specific error occurred while establishing a connection to SQL Server. Make sure the SQL Database firewall allows the integration runtime to access"".

I will reach out to Anjali on logins validation.

the connection for 192.168.29.105 - TDB08AX2012 went through successfully.¬†

* 192.168.29.78 - TDB15 still has the same error

Cannot connect to SQL Database. Please contact SQL server team for further support. Server: '192.168.29.78', Database: 'ILS', User: 'ServAC_EDPADF_dSQL07'. Check the linked service configuration is correct, and make sure the SQL Database firewall allows the integration runtime to access.

Anjali to raise a ticket to get TBD15 firewall fixed

Fill in the below information in Key Vault:

user=os.getenv('SNOWFLAKE_USER'),
password=os.getenv('SNOWFLAKE_PASSWORD'),
account=os.getenv('SNOWFLAKE_ACCOUNT'),
warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),
database=os.getenv('SNOWFLAKE_DATABASE'),
schema=os.getenv('SNOWFLAKE_SCHEMA')

New ticket for ‚ÄùADF to SF via User Account, currently uses password however no longer available for SF sign in. Key pair authentication method sign in required‚Äù

new ticket created"
CSCI-112,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,30/Jun/25 10:07 AM,18/Jul/25 12:43 PM,,SC BAU Sprint 2 Work,"# Gateway connection Bug fix
## [https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/194797|https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/194797] 
# Refresh Analysis Dashboard
## [https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/185354|https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/185354]",,"Assisted SC team with a gateway connection bug for one of their Semantic Models:

[https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/194797|https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/194797] 

Task took 1 hr to complete

Completed the development of a ‚ÄúRefresh Analysis‚Äù dashboard for Supply Chain.

Dashboard is built using PBI Service metadata, called via PBI APIs.

[https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/185354|https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/185354] 

As part of the development, I have created a python notebook environment setup to create and call PBI APIs - this notebook can be leveraged for data extraction for the interim project.

Task has taken 2 days additional work."
CSCI-111,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,27/Jun/25 10:08 AM,27/Jun/25 10:33 AM,,Link Eugene's tickets from Azure,,,
CSCI-110,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,27/Jun/25 10:08 AM,27/Jun/25 10:38 AM,,Merge CSCI-83 and CSCI84,,,
CSCI-109,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,25/Jun/25 10:24 AM,27/Jun/25 11:09 AM,,Phil to figure out how to visualise Jess's story points and capacity.,"Problem:

* Jess is currently having 2 places where her efforts are placed
* Jess‚Äôs effort/sprint points are not constant in both 
** [Operations Analytics Devops team/board|https://dev.azure.com/MyChemist/Operations%20Analytics]
** CW cloud data platform (this board)
* We are trying to measure how many points she has so that we know what her capacity is from a product/project management and agile-Scrum perspective.

Objective:

* To Measure and visualise how much capacity Jess has on any given Sprint

Definition of Done:

* To be able to discover a method of visibility so that we can effectively see the capacity Jess has on her sprints.
* To have the above discovery implemented in JIRA/Devops.",,We‚Äôll basically have 2 boards concurrently in standup. Both boards will be visibile during standup.
CSCI-104,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,24/Jun/25 2:44 PM,24/Jun/25 2:44 PM,,test task 2,,,
CSCI-103,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Jun/25 3:43 PM,25/Jun/25 10:09 AM,,Azure tasks that need - devops linking,"Phil - To link tasks that are as per listed below and re-create them in JIRA

* 192465
* 192466
* 185363
* 185353",,Have spoken to @user - I will need to create separate new tasks to be in backlog.
CSCI-102,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Jun/25 3:35 PM,23/Jun/25 3:35 PM,,test task,,,
CSCI-101,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Jun/25 3:24 PM,24/Jun/25 2:45 PM,,SC tasks not clearing in sprint,"Problem:

* JIRA tickets aren‚Äôt clearing, thus tickets in ‚Äòdone‚Äô column are carrying over 

Solution:

* Phil to check and redefine status columns where necessary

Definition of done:

* upon closing of sprint, tickets in ‚Äòdone‚Äô are closed",,
CSCI-98,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,20/Jun/25 4:04 PM,30/Jun/25 3:11 PM,,Jess-Amit Bus Matrix Handover Tasks,"List of action items from Amit from [https://sigmahealthcare.atlassian.net/browse/CSCI-34|https://sigmahealthcare.atlassian.net/browse/CSCI-34]:

# -Document Source Systems Models and Reports- - already captured by multiple tickets
# Are there any similar reports coming from two different source system?
# Capture all Flat/Junk‚ÄôFlag Dimensions used in existing reporting
# Capture different kind of Date dimensions used in existing reporting. Like Order Date, Invoiced Date, Shipped Date, Delivery Date etc
# -Grouping of Measures: group all time intelligence KPIs coming from same Measure into one bucket- - already captured in an existing ticket [https://sigmahealthcare.atlassian.net/browse/CSCI-80|https://sigmahealthcare.atlassian.net/browse/CSCI-80] 
# Look for any source system that is not captured in Bus Matrix document
# Look for Data sets ingested from Mobile Dock. These are not captured in documents
# Look for Data Sets captured from MAWM

*Definition of done:*

* questions above answered as either a list or spun off to different tasks.",,All points covered as part of tasks [https://sigmahealthcare.atlassian.net/browse/CSCI-94|https://sigmahealthcare.atlassian.net/browse/CSCI-94] and [https://sigmahealthcare.atlassian.net/browse/CSCI-81|https://sigmahealthcare.atlassian.net/browse/CSCI-81]
CSCI-97,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,20/Jun/25 3:44 PM,27/Jun/25 10:30 AM,,Review and update Bus Matrix with Analysis from Supply Chain SM's,"Upon completion of [https://sigmahealthcare.atlassian.net/browse/CSCI-81|https://sigmahealthcare.atlassian.net/browse/CSCI-81] and [https://sigmahealthcare.atlassian.net/browse/CSCI-94|https://sigmahealthcare.atlassian.net/browse/CSCI-94], populate the results from this analysis to update our Bus Matrix document to include all Facts and Dims identified in the supply chain current state. 

Identify also which facts and dims are not being used by the supply chain team, already created in the Bus Matrix.

Acceptance Criteria:

* Update Bus Matrix document with Dims and Facts identified from Supply Chain",,"Have updated my supply chain discovery notes to the Bus Matrix doc for Dimensions.
Have re-formatted the page to tidy up some descriptions being in the ‚Äòcomments‚Äô field and some in the ‚Äòdefinition‚Äô field.

*Dimension Definition:*

Refers to what kind of columns/data is included in the listed Dimensions

*Comments:*

Any callouts relating to how Supply Chain are currently reporting/data they have requested

Have also added flags to highlight if Dimensions are currently being used by SC and if will be required in future (those listed as N are tables in question - need to speak with Amit to clarify)"
CSCI-96,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,20/Jun/25 10:07 AM,07/Jul/25 9:48 AM,,ADF - Extraction pipeline template - PARQUET file,"The data is too large and couldn't be loaded directly (Direct) from the source DBs. Hence CW database admins helped to extract them and upload to blob as PARQUET files. Once it is processed, the next loads are incremental and direct based on the Last Extracted Date Time column.

Objective:

* Create a test pipeline template for PARQUET file to do full loads on large source. 
* e.g source - TBD14

*Definition of done:*

* Test pipeline is up and running. Data is landed on blob storage",,"Implement and test delta load

@user is it okay if you entered in definition of done for this task? thanks!

Incremental load is tested successfully where data is landed on the TEST blob container

Parquet ADF template to copy to blob successfully‚Ä¶ Pending Storage Integration to test end to end flow.

Awaiting Enda/Brent action on SNOW ticket, Harrison following up given Enda is now on leave till end of July.

Thanks,

Harrison

Pipeline failed due to missing parameters. Need further investigation.

Pipeline able to pick up however unable to decompress Parquet format in Snowflake. I will have a look on how to resolve this

this pipeline template is successfully tested end to end"
CSCI-95,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,20/Jun/25 9:43 AM,07/Jul/25 9:48 AM,,Create a Data Ingestion plan,"* To identify the first 3 sources (databases) to be ingested first to EDP Snowflake
* Definition of Done:
** Outline the database names, tables name
** Get approval from relevant stakeholders (CK, Harrison, Rachel etc.) to ingest to Snowflake",,"I‚Äôve been working with Eugene to better understand the data ingestion challenges encountered during EDP 1.0. Based on those insights, we plan to implement *two pipelines per data source*, particularly for those with high data volumes:

# *Initial Load Pipeline*
This pipeline will handle a one-time bulk load. The DBA team will extract data from the on-premises databases and upload it to Azure Blob Storage as *CSV or Parquet* files. We understand that using Parquet format can help mitigate data quality issues. Once the files are available in Blob Storage, we‚Äôll trigger the pipeline to ingest the data into Snowflake.
# *Incremental Load Pipeline*
This pipeline will manage ongoing data updates by loading incremental changes *directly* from the source databases.

To support this approach, I‚Äôll be setting up an ADF template specifically for extracting data from Parquet files during the initial load phase.

So I believe we need the assistance from Rachel's / DBA team at least in the below:

* Help us validate our current data source mapping is accurate and complete.
* Extract data from the confirmed databases and upload the data to Azure Blob Storage in¬†*Parquet*¬†format.

Provided a list of source to ingest. Harrison has sent email to Rachel to request her team‚Äôs support. [List of data sources to be extracted as parquet.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/List%20of%20data%20sources%20to%20be%20extracted%20as%20parquet.xlsx?d=w385e099996914351bc23a629ff81840d&csf=1&web=1&e=1l5y1g&xsdata=MDV8MDJ8fDdlOGY1NzhjNzQ0NTQxOTliYjkzMDhkZGI0N2I0NTQ1fDFkZWMyOWI0OWE1ZDQxZmM5Yzc1Y2RiMmNiOWM3ZmU2fDB8MHw2Mzg4NjUxNjM2OTI2NTM0NTl8VW5rbm93bnxWR1ZoYlhOVFpXTjFjbWwwZVZObGNuWnBZMlY4ZXlKRFFTSTZJbFJsWVcxelgwRlVVRk5sY25acFkyVmZVMUJQVEU5R0lpd2lWaUk2SWpBdU1DNHdNREF3SWl3aVVDSTZJbGRwYmpNeUlpd2lRVTRpT2lKUGRHaGxjaUlzSWxkVUlqb3hNWDA9fDF8TDJOb1lYUnpMekU1T2pGa01URmpZekEzT0RZelpUUTBPR05oWlRNelpUVTBObVV4WVRobE5UTTJRSFJvY21WaFpDNTJNaTl0WlhOellXZGxjeTh4TnpVd09URTVOVFkzT1RNMHw4NTk3ZDdkYmM1MmI0Y2ZmYmI5MzA4ZGRiNDdiNDU0NXw5YjJjMDc1ZGI1MGU0NjI0YTE4Yzk2YTExZWUzZDc0OQ%3D%3D&sdata=cW9oMEdyOW5Vc3FJSG85dEt2Vnc4eUNOR1U1NGk3QnFjZ1ZBSC9lUVRrOD0%3D&ovuser=fd21b7ac-6a50-45ae-99ea-568c8161f6d0%2CChloeT%40altis.com.au]

Now waiting for Rachel‚Äôs response.

I've sent out the email to get data ingestion approval."
CSCI-94,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,19/Jun/25 12:34 PM,24/Jun/25 3:00 PM,,Analysing and Grouping Facts from Supply Chain,"A list of 62 Fact tables have been identified across all the Supply Chain semantic models.

Need to understand the grain and facts within each of these tables, and determine if we can group some of these facts together.

*Definition of Done:* 

* Inspect each individual fact and identify the metrics/facts stored as well as the grain of the table
* Group like-facts together to create a more concise list of facts",,"Work has been completed in below excel doc:

[Supply Chain Fact Discovery.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Supply%20Chain%20Fact%20Discovery.xlsx?d=w3ea322123bc345a4b0981cc3df1bc6f1&csf=1&web=1&e=U9cJpy]

Initial Discovery complete. 

Of all the unique 62 fact tables, have estimated these can be grouped into 20 distinct fact tables (10 of these tables deemed not to be applicable fact tables).

Below depicts the ‚ÄòBusiness Areas‚Äô that these fact tables relate to:

* DFIO (3)
* Inbound/Outbound Shipments (17)
* Inventory (22)
* Sales (6)
* ‚ÄòOther‚Äô (4)
* Not Applicable (10)

Some challenges:

It‚Äôs noted that there were a lot of similar fact tables generated with the same metric values but at a +_different aggregate_+. It appears this has been done with performance considerations in mind, due to some tables being incredibly large (eg: Table *FactDCInboundPurchaseOrdersERP* in SM ‚ÄòDC Inbound Shipments Semantic Model‚Äô is in excess of +100M+ rows).

Note: there were a couple of tables which are ‚Äòfactless facts‚Äô - something to consider for our BI modelling"
CSCI-93,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,19/Jun/25 8:11 AM,19/Nov/25 12:27 PM,,Designing Future State - Pairing BI application design with Business requirements definition,"Objective:

* Prepare designs with business requirements so that we are developing what uses will need.",,
CSCI-92,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,19/Jun/25 8:11 AM,19/Nov/25 12:27 PM,,Designing Future State - BI application Design,"Objective/outcome:

* To optimise design visualisation in PowerBI that will meet Supply chain‚Äôs needs.

Reason:

* This will be part of reviewing existing reports and seeing how we can display data better to our users.
* This would be done lieu with requirements.",,
CSCI-91,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,19/Jun/25 8:10 AM,19/Nov/25 12:27 PM,,Designing Future state - Identifying and proposing which data source (on prem) and which data grains should be connected to ADF pipeline.,"Objective:

* We want to find out from each data source which particle data grains should we be uploading into ADF

Reason:

* In order to increase performance, we should only upload what we need from data sources. These sources may be on- premises, which may take time for both initial load and bulk load/streaming.",,
CSCI-90,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,19/Jun/25 8:09 AM,19/Nov/25 12:28 PM,,Designing Future State - Proposing which reports should be developed through usage metrics,"Outcome:

* To find out what are the top 10 reports that are currently being used by supply chain (Operations Analytics)

Reason:

* We can then use the top 10 reports to find out what data sources are being queried the most
* We want to create the most impact for supply chain team by delivering on reports that affect them the most.",,
CSCI-89,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jun/25 4:04 PM,18/Jul/25 8:26 PM,,Snowflake Git Repository configuration,"[Devops |https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/192817]192817

Objective:

* To create a separate Azure DevOps repository for Snowflake so that SQL scripts and objects can be managed independently of other resources.

Definition of Done: 

* Snowflake repo created with organized folder structure
* Branch policies enforce code review and approval
* Access granted only to authorized Snowflake contributors
* Branch protection and restrictions are applied (e.g., no force-push, mandatory PR reviews)",,
CSCI-88,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jun/25 2:41 PM,24/Jun/25 2:39 PM,,ADF - Extraction pipeline template - Rdbms/ SQL server,"Objective:

* Create a test pipeline template for Rdbms/ SQL server
* e.g source - TBD14

*Definition of done:*

* Test pipeline is up and running. Data is landed on blob storage",,"Implement and test delta load

@user is it okay if you entered in definition of done for this task? thanks!

Incremental load is tested successfully where data is landed on the TEST blob container"
CSCI-87,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jun/25 12:38 PM,19/Nov/25 12:28 PM,,Designing Future state - propose connection between source (snowflake) and semantic Models,"Objective:

* We want to come with a design where we can connnect the right Dim and Fact tables from snowflake to the models that are created by PowerBI",,
CSCI-86,CSCI,CW Cloud Data Platform Interim Solution,Task,Duplicate,Medium,18/Jun/25 12:24 PM,19/Nov/25 12:29 PM,,Understanding the Data Lineage from BI Servers to Underlying Source System,"Discussion between Jess and Chloe and Rachel (DBA Team).

Goal to understand all the transformations/steps/systems between the current Power BI Semantic Model connection to the true Source System.

Outcome:

* Liaise with Rachel Wan - ETL steps on SSIS packages",,
CSCI-85,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jun/25 12:23 PM,19/Nov/25 12:30 PM,,Understanding Current state - Identifying Source of semantic models (from DIMs and Facts),"This task builds on the dims and facts identification task as it pulls in Multiple DIMs and Facts.

[https://sigmahealthcare.atlassian.net/browse/CSCI-83|https://sigmahealthcare.atlassian.net/browse/CSCI-83] 

AND
 [https://sigmahealthcare.atlassian.net/browse/CSCI-84|https://sigmahealthcare.atlassian.net/browse/CSCI-84] 

Outcome:

* Pinpoint what sources are primarily used (Which Fact and Dim that we want)",,
CSCI-84,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jun/25 12:20 PM,07/Jul/25 12:23 PM,,Identifying the underlying source of Supply Chain Facts,"Goal:

* Identifying the source of Supply Chain Facts at the server-database-table level
** Including the underlying SQL code written

Acceptance criteria:

* Location address of the source of Supply Chain Facts

Postulation:

* Identifying the tables on which server that has most usage and prioritising it by 1,2,3 so that we can make the most client impact in our migration.

What does this solve:

* Performance issues
* Access issues.",,merged with [https://sigmahealthcare.atlassian.net/browse/CSCI-83|https://sigmahealthcare.atlassian.net/browse/CSCI-83]
CSCI-83,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,18/Jun/25 12:15 PM,01/Jul/25 11:14 AM,,Identifying the underlying source of Supply Chain Facts and Dims - SQL Server environments,"Goal:

* Identifying the source of Supply Chain Facts and Dims at the server-database-table level
** Including the underlying SQL code written

Acceptance criteria:

* Location address of the source of Supply Chain Facts and Dims

Postulation:

* Identifying the tables on which server that has most usage and prioritising it by 1,2,3 so that we can make the most client impact in our migration.

What does this solve:

* Performance issues
* Access issues.",,"Merged with [https://sigmahealthcare.atlassian.net/browse/CSCI-84|https://sigmahealthcare.atlassian.net/browse/CSCI-84] @user

Link provided below in chat by @user 

[https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B30678332-5293-4A0C-84A6-26488C56A8CD%7D&file=Supply%20Chain%20Data%20Sources%20-%20SQL.xlsx&wdOrigin=TEAMS-MAGLEV.p2p_ns.rwc&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B30678332-5293-4A0C-84A6-26488C56A8CD%7D&file=Supply%20Chain%20Data%20Sources%20-%20SQL.xlsx&wdOrigin=TEAMS-MAGLEV.p2p_ns.rwc&action=default&mobileredirect=true] 

Thanks,

Harrison

Have completed the source details of all SQL Server connections, down to the table level:

* Server.Database.Schema.Table(View)

Each source detail is mapped against a table within each Semantic Model.

For SQL connections, there are 2 methods to connect in PBI:

# Selecting a table/view from a picklist against the given database
# Custom SQL query

Source details have been mapped for each SQL connection ‚Äòtype‚Äô."
CSCI-82,CSCI,CW Cloud Data Platform Interim Solution,Task,Duplicate,Medium,17/Jun/25 11:40 AM,19/Nov/25 12:30 PM,,Understanding Current State - Identifying KPIs used in reports,"Acceptance critieria:

* Listing KPIs that are currently used for Current reports that are in used in CW Supply Chain.",,
CSCI-81,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,17/Jun/25 11:39 AM,24/Jun/25 2:39 PM,,Grouping DIMs that can be merged,"h3. Dimensions

¬†

List of Dim‚Äôs used across models (list based on table name in semantic model containing ‚ÄòDim‚Äô):

* DimDFIOSKU
* DimDFIOProduct
* DimService
* DimDate
* DimItemCode
* DimProduct
* DimProductMAWM
* DimProductUOM
* DimProductUOM_MAWM
* DimStore
* DimOmniStore
* DimStoreGroup
* DimAutoStoreSKUs
* DimDCCycleCountMasterPlan
* DimDCLocation
* DimDCNonShippingWeekdays
* DimDistributionCentre
* DimVendor
* DimExemptedSuppliers
* DimExpiryStatus
* DimManufacturer
* DimMobileDOCK

¬†

{color:#bf2600}*Action:*{color} Group dim‚Äôs that can be merged as a single Dim (eg: store, omni-store and store group)

*Definition of Done:*

* DIMs identified and mapped - with business definitions.
* Presented as a list",,"Work being completed in this excel doc:
[Supply Chain Dimension Discovery.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Supply%20Chain%20Dimension%20Discovery.xlsx?d=wd4f6edcd6a8c420d9e177ac2fc5b8d88&csf=1&web=1&e=RFejOX]

Initial discovery complete.

Have queried each Dim table to understand if some of the above can be grouped. Have estimated we can have 9 unique Dim‚Äôs to cover all dimension requirements for current supply chain reporting:

Note: (blank) in the above refers to us dropping these 3 tables as Dims (upon inspection, 1 table is actually a fact - just labelled as a dim, the other 2 are not proper dimension tables (eg: 1 column)).

All work has been saved to the linked excel workbook.

Of all Dimensions listed, DimProduct is going to be the most detailed to create. Upon inspection there are many Product Identifiers used across multiple systems/processes (noted in ‚ÄòDim Product‚Äô sheet of excel workbook):

* MyChemID (internal product ID - key identifier)
* SKU ID
* SKU Ref
* PLU
* Item_Code (VERY similar to MyChemID but different)
* ItemWHSKey
* Product_SK
* ProductUOMIDKey (ID for product-UOM grain - lowest grain we need to establish)
* ProductBarcode"
CSCI-80,CSCI,CW Cloud Data Platform Interim Solution,Task,Duplicate,Medium,17/Jun/25 11:39 AM,19/Nov/25 12:30 PM,,Grouping SC by KPI/Measure type,"Currently across the 42 semantic models, we have *692 unique/individual measures*. Some of these can be referencing the same value but expressing this as a different output (eg: *sales* in absolute $'s, *sales* as a %, *sales* last 60 days, etc).

Attached is a list of all measures as of 16.6.25.

[^Supply Chain Measures.xlsx]

{color:#bf2600}*Action:*{color} Work through this list to group by KPI/measure type (eg: group sales as absolute, sales as % and sales last 60 days as ‚ÄòSales Group‚Äô etc).

*Definition of done:*

* KPI and measures are identified and paired with business definitions as a table.",,"@user is it okay to review this definition of done? thanks.

Hey @user , @user ,

My thought this should be not immediate priority.

Below are some of the analysis and deliverables I think we should include.

But @user please overlay your thoughts on this, rip what I said apart, add your thoughts on ways to step through the Dimensional Modelling delivery so we can all agree.

Happy for you to come up with thoughts on top of this, remove and add what you like then maybe we have a session next week to confirm how we implement. 

*Business Process Review*

* Business Processes captured in Tactical models vs captured in existing documentation
** Whats included in doc that shouldnt be
** Whats not included in doc that should be
** Any definition or source information easily captured from this analysis?

*Dimension Review*

* Review Dimensions captured in Tactical models vs captured in existing documentation
** Whats included in doc that shouldnt be
** Whats included in doc that should be
** Any definition or source information easily captured from this analysis? Merging of Dims etc

*BUS Matrix*

* Completion of HL BUS Matrix after above, mapping of Business Processes and Dims
* Include High Level Source mapping where feasible, has already been attempted by Amit and have EDP documents to assist also

*Business Metrics Definition*

* Business Metrics list
* Mapping to core Business Process
* Metric Definitions
* Probably worth doing iteratively based on priority Business Processes

*Data Source Mapping*

* Business Process mapping to
** Source System
** DB
** Object and transformation

*Dimensional Modelling*

* Source to Target documentation
** Business Process(Fact) - PK, Grain, column mapping, metric calculation
** Dimension (Dim) - PK, Grain, column mapping, attribute definition

cc @user 

Thanks,

Harrison

Hi @user,

Your points below align to the convo @user and I had yesterday.

Agree that this task needs to be completed after we have reviewed the business processes/facts first (task [https://sigmahealthcare.atlassian.net/browse/CSCI-94|https://sigmahealthcare.atlassian.net/browse/CSCI-94]). The dimension review has already been completed ([https://sigmahealthcare.atlassian.net/browse/CSCI-81|https://sigmahealthcare.atlassian.net/browse/CSCI-81]).

Chloe and I spoke that after the fact review has been complete that I will then go into our Bus Matrix document and update/add any dims and facts that are not currently included.

The data source mapping work I think will need to be its own separate task initially. From what I can see from the current Supply Chain Semantic Models, there are a lot of hops their data source goes through to go from source system to report. Majority of their connections in PBI service are to the *PBI05* server.

Example of some of the lineage:

PBI05 (BI Server) ‚Üê PDB08 (AX 2012 Server) ‚Üê Azure Blob Storage (for a CSV file load)

Chloe and I are going to look into this further next week to understand what transformations are done between each hop/source üôÇ 

I have details of the source connection info that supply chain team are using in PBI service, so once we unpack the lineage from the actual source system, we can join the dots to complete this task."
CSCI-79,CSCI,CW Cloud Data Platform Interim Solution,Task,Duplicate,Medium,17/Jun/25 11:37 AM,19/Nov/25 12:30 PM,,Grouping Semantic Models,"Rachel and the Supply Chain team have already grouped their own semantic models into +*12 ‚Äòmaster models‚Äô*+ which have already been created:

h4. DC

# [DC Cycle Count|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/f062aba7-efc7-4147-8cf2-7af21874a874/details?experience=power-bi]
# [DC Employee Performance|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/2f02aa2e-3858-4015-ab7f-9f4b78495405/details?experience=power-bi]
# [DC Inbound Shipments|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/596c4534-aae5-45ce-948c-71c836b3875d/details?experience=power-bi]
# [DC Inventory Location|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/9deaba9d-b41b-4f5c-a3e1-19fba0e4404a/details?experience=power-bi]
# [DC Inventory|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/d9fa86fd-cc2c-4082-bb3b-119fd7c4d3b7/details?experience=power-bi]
# [DC MobileDOCK|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/3066b4b6-c9fb-46d9-89d2-4f6e7dbf5ff7/details?experience=power-bi]
# [DC Outbound Shipments|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/0fd5f627-fafc-411e-aea3-77d6838ec2dc/details?experience=power-bi]
# [DC Shipment Wave|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/a07880d1-5275-4d9a-bbc5-68b416e367d5/details?experience=power-bi]

h4. Store

# [DFIO|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/0234fec9-bfdc-45fe-880c-18832b409343/details?experience=power-bi]
# [Employee Safety|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/cfb5a6a9-0ad9-4bea-baf0-4f81ce8d2993/details?experience=power-bi]
# [Store Inventory|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/ef4f1f73-051a-4464-bf57-eac4f2d82053/details?experience=power-bi]
# [Store Sales|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/fd95fa38-7b01-4f36-b20c-19d72bc0d7ca/details?experience=power-bi]

¬†

Currently the Supply Chain team are working to re-map all their existing semantic models to reference only from these above 12 ‚Äòmaster semantic models' to become as their new source.

There are currently *42 semantic models* in the Operations Analytics workspace in total (excluding their sandbox workspace). These 42 include the above 12, meaning there are currently *30 semantic models* that are created that do not form as part of this ‚Äòmaster layer‚Äô.

The team are in the work-in-progress stage of this migration/re-mapping of their semantic models. For example currently the ‚ÄòS&OP Projections‚Äô semantic model only has inputs coming from the ‚Äòmaster layer‚Äô, whereas the ‚ÄòSPS KPIs‚Äô semantic model currently has inputs only coming from a DB source (nothing from the master layer).

¬†

The advice would be:

* Continue to re-map all their current semantic models to this master layer, to reduce the number of queries against the database servers
* Work with the SC team to establish if we can group these models further (eg: can we group ‚ÄòDC Inbound Shipments‚Äô, ‚ÄòDC Outbound Shipments‚Äô and ‚ÄòDC Shipment Wave‚Äô as 1 single ‚ÄòDC Shipments‚Äô model?)

*definition of done:*

* Semanitc models mapped - with business definitions.",,@user is it okay to review this definition of done? Thanks.
CSCI-78,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,17/Jun/25 11:35 AM,27/Jun/25 9:48 AM,Planning,Requirements Gathering questions,"Document [here|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Requirements%20Gathering%20-%20Draft%201.docx?d=w82871d69c81340c7a2edd5160b87d809&csf=1&web=1&e=GPMYyh] for review.

Objective:

* Story on this sprint - -to update the requriements/getting definitions- to update on questions to approach to Rachel
** If not, finding out our point of contact.
* Highlighting what the top 3 Use cases are.

Definition of done;

* -Questions listed to ask Rachel/her team-
** questions reviewed by the team.
* t-op 3 priorities listed-
* Find out who our point of contact is.

Expectation from stakeholders:

* *they should know what they want.*
* They will know what their top 3 priorities are when it comes to reports",,"Hi @user - In preparation for tomorrow‚Äôs meeting about Supply chain Modelling Progress and Plan , I have prepared a document on what type of questions we can approach Rachel with.

Feel free to have a look before the meetup tomorrow.

Thanks,
Phil"
CSCI-77,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,17/Jun/25 10:08 AM,24/Jun/25 2:39 PM,,Review of BUS Matrix Dimension Mapping and Source,"Review BUS Matrix Draft completed by Chloe in [https://sigmahealthcare.atlassian.net/browse/CSCI-76|https://sigmahealthcare.atlassian.net/browse/CSCI-76] .

# Include Source System mapping in column identified in BUS Matrix sheet
# Review Dimensional Mapping in BUS Matrix",,Reviewed the Bus Matrix Detailed and added the sources for reporting and true sources. Looks good.
CSCI-76,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,17/Jun/25 10:04 AM,24/Jun/25 2:39 PM,,BUS Matrix Draft 1.0,"Creation of draft BUS Matrix for Supply Chain Data Modelling.

Sheet ‚ÄúBUS Matrix‚Äú in [https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true]",,created a Bus Matrix based on Amit‚Äôs business processes and dimensions
CSCI-75,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,17/Jun/25 10:03 AM,24/Jun/25 2:39 PM,,Dimension Identification,"Creation of working document with Dimensions Identified for Supply Chain Data Modelling. Both conformed and BP specific (junk).

Sheet ‚ÄúDimension‚Äú in [https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true]",,
CSCI-74,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,17/Jun/25 10:03 AM,24/Jun/25 2:39 PM,,Business Process Identification,"Creation of working document with critical Business Processes Identified for Supply Chain Data Modelling.

Sheet ‚ÄúBusiness Process‚Äú in [https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true]",,
CSCI-71,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Jun/25 4:14 PM,18/Jul/25 3:51 PM,,StockDb Integration - part 1,"Build data ingestion pipelines for the StockDb database.

Definition of done:

* Extract meta created
* Linked services created
* Pipeline run end to end",Tables available in Snowflake and pass validation test,"Pipeline and extract meta setup. The following tables have been ingested:

* BUYERS
* PRODUCTS @user@user

Pending a few more tables from StockDb to be ingested to Snowflake next sprint:

|MultiBuyTriggers|
|ProductGroup|
|ProductNetworkCosts|
|ProductsDrugMatch|
|SOHAdjustmentTypes|
|SupplierDetails|

And the below which will require a historical load + delta pattern: 

|Table|RawData|Refresh Freq|¬†Total Row Count¬†|
|ProductNetworkCostsHistory|Yes|Real Time|########|

@user @user

I‚Äôll duplicate this task:
- part 1 reflects work done in this sprint
- part 2 reflects work to be done on next sprint

 @user"
CSCI-69,CSCI,CW Cloud Data Platform Interim Solution,Story,Done,Medium,16/Jun/25 4:13 PM,20/Jun/25 10:33 AM,,Business Process (Current State),,,
CSCI-68,CSCI,CW Cloud Data Platform Interim Solution,Story,Done,Medium,16/Jun/25 4:13 PM,23/Jun/25 9:43 AM,,BUS Matrix,,,
CSCI-67,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Jun/25 4:13 PM,24/Jun/25 2:39 PM,,Infrastructure,"Objective:

* to setup environments so that data can be loaded onto servers to be ready to ETL
** This will entail the set up of 
*** ADF
*** Snowflake
**** DEV
**** SIT
**** UAT
**** Prod",,
CSCI-64,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Jun/25 4:13 PM,24/Jun/25 2:40 PM,,Snowflake GIT Integration approach,"Establish the approach - 3 options Flyway/ Devops V - R script, Flyway R Script or Snowflake declarative 

*Definition of done*

* To research on Snowflake‚Äôs offering 
* Chloe/ Eugene to agree and confirm the Git approach for Snowflake",,"@user is it okay if you entered in definition of done for this task? thanks!

Discussed with Eugene the options with Snowflake deployments:

* Flyway/ Devops V - R script - this was implemented at Sigma. Very complex and high maintenance 
* Flyway R Script - Similar to the above but still complex/ high maintenance. 
* Snowflake declarative - new feature, simpler than the above"
CSCI-61,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/May/25 11:04 AM,17/Jun/25 10:36 AM,,CW BUS matrix,"expected 22/5

alongside with business identification, we will need the bus Matrix so that a semantic model can be generated and thus generate business reports and other data products.

[Business Process sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Business%20Matrix_Supply_Chain.xlsx?d=wcb15417700074f2ea34019d98b5c6170&csf=1&web=1&e=WCRVjt&nav=MTVfezQ0QTYyMDkzLUEwNTEtNDlCNS05ODZDLUVBREIyNkQxMkMyMH0]",,"expected 22/5

@user has added updates to this ticket - will add link to this ticket and align with Amit‚Äôs work

Updated Jira with link to BUS matrix

Updating:

* Business Process
* Dimensions"
CSCI-60,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/Apr/25 3:32 PM,09/Jul/25 2:57 PM,,Discovery - Altis to Engage with cyber / Vinay on what's we are trying to achieve,,,"Hey @user can you give me some info about Vinay when you have the chance?

@user I will follow up with Rhubesh. I am not across this activity."
CSCI-59,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,12/May/25 11:04 AM,09/Jul/25 12:37 PM,,CW SC - Business Process Identification,"We‚Äôll need to identify the CW SC business processes - namely to identify what are the business transactions

- warehouse, purchase, DFIO

done list:

* inventory
* SAles
* DFIO
* @user to update.",,"expected 22/5

|Section|Status|
|Sales|Complete|
|Supply Chain Source Systems Identification|Complete|
|Warehouse Management|70% Complete|
|Purchases|50% Complete|
|Inventory Management|40% Complete|
|DFIO|Not Started|"
CSCI-58,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Jun/25 3:13 PM,24/Jun/25 2:39 PM,,ADF - Snowflake connectivity,"Objective:

* to Ensure secure housing and transfer of data from Source, to Snowflake, and also a secure-link.

Acceptance criteria:

* to have data not being able to accessed by the internet (i.e. an open port) but onliy allowed authenticated users to be able to access data.
* Data will not be interceptable either from source/warehouse/azure servers and anywhere in between i.e. (secure layer)",,"This is addressed here: [User Story 191088 Configure ADF Linked Services|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/191088]

ADF is connected to Snowflake via private link."
CSCI-57,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Jun/25 3:13 PM,14/Jul/25 2:39 PM,,Snowflake - Privatelink URL,"[devops 191105|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/191105]

To connect to Snowflake over PrivateLink, so data never leaves Microsoft‚Äôs backbone.

see reference:¬†[+Azure Private Link and Snowflake | Snowflake Documentation+|https://docs.snowflake.com/en/user-guide/privatelink-azure]

Current status - Blocked:
- We are waiting on DNS integration - Awaiting on cloud team for setup.",,"This is mosty done, just waiting for the DNS integration, see [User Story 191105 Snowflake PrivateLink|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/191105] for reference.

* Snowflake account has PrivateLink enabled
* ADF and SHIR can resolve and reach the endpoint
* Query latency is consistent with private path

Next step: DNS integration

@user - Can we know what we are waiting for on this? We‚Äôre just not clear on the ticket in regards to this. I‚Äôll raise it on standup."
CSCI-56,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Jun/25 3:12 PM,24/Jun/25 2:39 PM,,Snowflake - DLA setup,"Objective: 

* To create Data Ingestion Framework.

Description: 

* Run setup scripts for DLA framework:

# DLA tables
# DLA stored procs
# DLA views",,
CSCI-55,CSCI,CW Cloud Data Platform Interim Solution,Task,Duplicate,Medium,05/Jun/25 3:27 PM,20/Jun/25 1:23 PM,,Review of existing SC semantic models part 2 - Connecting with BUS Matrix -,"GAP analysis 

* Comparing documented business process and existing semantic layer
** 2 way check.
** 
Reviewing existing Supply Chain Semanit models for CW and seeing where improvements are needed.",,
CSCI-54,CSCI,CW Cloud Data Platform Interim Solution,Task,Duplicate,Medium,15/May/25 9:24 AM,20/Jun/25 1:23 PM,,Review of existing CW supply chain semantic models,Reviewing existing Supply Chain Semanit models for CW and seeing where improvements are needed.,,
CSCI-53,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Jun/25 9:26 AM,24/Jun/25 2:39 PM,,Eugene's Azure Devops task migration to JIRA,"Migrating tasks from [devops|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_sprints/backlog/Enterprise%20Data%20Platform%20Implementation/Enterprise%20Data%20Platform%20Implementation/Snowflake%20Integration?workitem=192242] to JIRA

Will ask for automation tooling if required",,
CSCI-52,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,11/Jun/25 1:26 PM,09/Jul/25 12:37 PM,,Sample business METRIC and DEFINITIONS,"[Sample Business Metric and Definitions.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Sample%20Business%20Metric%20and%20Definitions.xlsx?d=w212314dd6ad0413881539bb97fb30134&csf=1&web=1&e=sL7AiX]

* Parts of EWM
* Parts of Supplier
* Whole of KPI reporting
* -Whole of supply chain-",,
CSCI-50,CSCI,CW Cloud Data Platform Interim Solution,Story,Done,Medium,13/Jun/25 11:27 AM,18/Jul/25 12:42 PM,,BUS Matrix,"* -Grouping of different Models together ( roughly belonging to similar business process/ Source Systems)-
* -Different measures used in models and reports-
* KPIs used in reports
* -Dimensions used in reports- 
* -Facts used in reports-

If you can align Measures/KPIs & dimensions ¬†to business process to the best of your knowledge , that would be awesome. Need not be 100 % accurate.

*Definition of Done:*
- List of KPI, Measures, and Dimensions identified and linked to business processes.",,"h3. Grouping Semantic Models

Rachel and the Supply Chain team have already grouped their own semantic models into +*12 ‚Äòmaster models‚Äô*+ which have already been created:

h4. DC

# [DC Cycle Count|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/f062aba7-efc7-4147-8cf2-7af21874a874/details?experience=power-bi]
# [DC Employee Performance|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/2f02aa2e-3858-4015-ab7f-9f4b78495405/details?experience=power-bi]
# [DC Inbound Shipments|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/596c4534-aae5-45ce-948c-71c836b3875d/details?experience=power-bi]
# [DC Inventory Location|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/9deaba9d-b41b-4f5c-a3e1-19fba0e4404a/details?experience=power-bi]
# [DC Inventory|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/d9fa86fd-cc2c-4082-bb3b-119fd7c4d3b7/details?experience=power-bi]
# [DC MobileDOCK|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/3066b4b6-c9fb-46d9-89d2-4f6e7dbf5ff7/details?experience=power-bi]
# [DC Outbound Shipments|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/0fd5f627-fafc-411e-aea3-77d6838ec2dc/details?experience=power-bi]
# [DC Shipment Wave|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/a07880d1-5275-4d9a-bbc5-68b416e367d5/details?experience=power-bi]

h4. Store

# [DFIO|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/0234fec9-bfdc-45fe-880c-18832b409343/details?experience=power-bi]
# [Employee Safety|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/cfb5a6a9-0ad9-4bea-baf0-4f81ce8d2993/details?experience=power-bi]
# [Store Inventory|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/ef4f1f73-051a-4464-bf57-eac4f2d82053/details?experience=power-bi]
# [Store Sales|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/fd95fa38-7b01-4f36-b20c-19d72bc0d7ca/details?experience=power-bi]

Currently the Supply Chain team are working to re-map all their existing semantic models to reference only from these above 12 ‚Äòmaster semantic models' to become as their new source. 

There are currently *42 semantic models* in the Operations Analytics workspace in total (excluding their sandbox workspace). These 42 include the above 12, meaning there are currently *30 semantic models* that are created that do not form as part of this ‚Äòmaster layer‚Äô.

The team are in the work-in-progress stage of this migration/re-mapping of their semantic models. For example currently the ‚ÄòS&OP Projections‚Äô semantic model only has inputs coming from the ‚Äòmaster layer‚Äô, whereas the ‚ÄòSPS KPIs‚Äô semantic model currently has inputs only coming from a DB source (nothing from the master layer).

The advice would be:

* Continue to re-map all their current semantic models to this master layer, to reduce the number of queries against the database servers
* Work with the SC team to establish if we can group these models further (eg: can we group ‚ÄòDC Inbound Shipments‚Äô, ‚ÄòDC Outbound Shipments‚Äô and ‚ÄòDC Shipment Wave‚Äô as 1 single ‚ÄòDC Shipments‚Äô model?)

h3. Measures

Currently across the 42 semantic models, we have *692 unique/individual measures*. Some of these can be referencing the same value but expressing this as a different output (eg: *sales* in absolute $'s, *sales* as a %, *sales* last 60 days, etc).

Attached is a list of all measures as of 16.6.25.

[^Supply Chain Measures.xlsx]

{color:#bf2600}*Action:*{color} Work through this list to group by KPI/measure type (eg: group sales as absolute, sales as % and sales last 60 days as ‚ÄòSales Group‚Äô etc).

h3. Dimensions

List of Dim‚Äôs used across models (list based on table name in semantic model containing ‚ÄòDim‚Äô):

* DimDFIOSKU
* DimDFIOProduct
* DimService
* DimDate
* DimItemCode
* DimProduct
* DimProductMAWM
* DimProductUOM
* DimProductUOM_MAWM
* DimStore
* DimOmniStore
* DimStoreGroup
* DimAutoStoreSKUs
* DimDCCycleCountMasterPlan
* DimDCLocation
* DimDCNonShippingWeekdays
* DimDistributionCentre
* DimVendor
* DimExemptedSuppliers
* DimExpiryStatus
* DimManufacturer
* DimMobileDOCK

{color:#bf2600}*Action:*{color} Group dim‚Äôs that can be merged as a single Dim (eg: store, omni-store and store group)

h3. Facts

List of Facts used across models (list based on table name in semantic model containing ‚ÄòFact‚Äô):

|Fact12MonthsDCSales|
|Fact90DaysStoreItemTransactionSummary|
|FactCostToServeSummary|
|FactDCCycleCount|
|FactDCEmployeeHoursWorked|
|FactDCInboundASN|
|FactDCInboundPurchaseOrdersERP|
|FactDCInboundPurchaseOrdersERP_detail|
|FactDCInboundPurchaseOrdersPOResponse|
|FactDCInboundPutaway|
|FactDCInboundPutawaySummarybyPO|
|FactDCInboundReceipts|
|FactDCInventoryAdjustmentsERP|
|FactDCInventoryERP|
|FactDCInventoryERPLatest|
|FactDCInventoryERPSummaryByBucket|
|FactDCInventoryLocation_MAWM|
|FactDCInventoryLocationCombined|
|FactDCInventoryLocationWMS|
|FactDCInventoryLocationWMS_MAWM|
|FactDCInventoryMovementERP|
|FactDCInventoryPPFvsReserve|
|FactDCInventoryRMACredits|
|FactDCInventoryUnitCost|
|FactDCInventoryWMSLatest|
|FactDCLocationSummary|
|FactDCOpenPOLinesERP|
|FactDCOutboundSales|
|FactDCOutboundSalesSummary|
|FactDCOutboundShipmentContainer|
|FactDCOutboundShipmentInvoices|
|FactDCOutboundShipmentOrders|
|FactDCOutboundShipmentsSummaryByERPID|
|FactDCOutboundShipmentsSummaryByERPIDOnTime|
|FactDCPicks|
|FactDCPicksSummary|
|FactDCRangingReview|
|FactDCShipmentFailure_MAWM|
|FactDCShipmentWave_MAWM|
|FactDCTransferOrderReceipts|
|FactDCTransferOrderSummary|
|FactDCWaveVolumeSummary_MAWM|
|FactDFIOHistoricalProjections|
|FactDFIOLCPTracker|
|FactDFIOProjectionsRejected|
|FactDFIOStockOnOrder|
|FactDFIOStoreDCBuyerNameLatest|
|FactDockToStockSummarybyPO|
|FactInboundOutboundSummary|
|FactNonConformancePOList|
|FactProductCAS|
|FactProductSupplier|
|FactStoreInventoryERPEOMSummary|
|FactStoreInventoryERPLatest|
|FactStoreInventoryERPLatestSummary|
|FactStoreInventoryERPWeeklySummary|
|FactStoreSalesERP|
|FactStoreSalesERP_Last 60 Days|
|FactStoreSalesWeeklySummary|
|FactStoreStockSales|
|vw_Store_IncrementalSOHChanges_Fact_CurrentDay|
|ZZ - Fact Base Tables|

{color:#bf2600}*Action:*{color} Understand the grain of these facts to see if some of these can be grouped (also to understand what fact ‚ÄòZZ - Fact Base Tables‚Äô refers to)

Need to append work from [https://sigmahealthcare.atlassian.net/browse/CSCI-84|https://sigmahealthcare.atlassian.net/browse/CSCI-84] to the Bus Matrix document - outlining which source tables map to which business process.

Will be using tasks for now @user

Moved to done with no impact to sprint points."
CSCI-49,CSCI,CW Cloud Data Platform Interim Solution,Story,Done,Medium,23/Apr/25 3:32 PM,09/Jul/25 12:37 PM,,Discovery - Engage infra set up for Azure tenancy,,,
CSCI-48,CSCI,CW Cloud Data Platform Interim Solution,Story,Done,Medium,23/Apr/25 3:32 PM,09/Jul/25 2:57 PM,,Story - Source Snowflake Azure Tenancy,,,
CSCI-47,CSCI,CW Cloud Data Platform Interim Solution,Story,Done,Medium,12/May/25 11:04 AM,09/Jul/25 12:37 PM,Planning,Data Source Identification,Needing to identify and document where data sources are (and where unknowns are) so that we can model data effectively,,"expected 22/5

Source Systems are documented"
CSCI-46,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Jun/25 3:18 PM,24/Jun/25 2:39 PM,,Snowflake - Environment Setup,To setup high level Snowflake structure for Chemist Warehouse which follows similar design and naming conventions already established for Sigma Healthcare.,,"[^high level org CW.xlsx]

¬†

{adf:display=block}
{""type"":""table"",""attrs"":{""isNumberColumnEnabled"":false,""layout"":""center"",""localId"":""d298ba13-74bf-4ac0-8517-1f606bf29616""},""content"":[{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""EDP_PROD""}]}]},{""type"":""tableCell"",""attrs"":{""colspan"":11},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""A single database will separate data layers by schema.""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""CONTROL""}]}]},{""type"":""tableCell"",""attrs"":{""colspan"":11},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Control Schema -¬† metadata, configuration settings, control tables, and job orchestration information""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""LDN""}]}]},{""type"":""tableCell"",""attrs"":{""colspan"":11},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Landing Schema ‚Äì untransformed copies of source system data held for a short period""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""STG""}]}]},{""type"":""tableCell"",""attrs"":{""colspan"":11},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Staging Schema ‚Äì historical data (untransformed) and transformed data joining relevant data sources and applying stipulated business rules""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""STG_TRNS""}]}]},{""type"":""tableCell"",""attrs"":{""colspan"":11},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Staging Transformed Schema ‚Äì transformed data joining relevant data sources and applying stipulated business rules""}]}]}]},{""type"":""tableRow"",""content"":[{""type"":""tableCell"",""attrs"":{},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""PRES""}]}]},{""type"":""tableCell"",""attrs"":{""colspan"":11},""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Presentation Schema ‚Äì information presented to users in a clear structure to support pre-defined and self-service reporting""}]}]}]}]}
{adf}"
CSCI-45,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Jun/25 3:15 PM,03/Jul/25 10:04 AM,,End to end data flow - TBD14 - BrandOrders,"End to end data flow testing from on-prem source Transaction Storage (TBD14 - BrandOrders) to Snowflake. 

Being blocked by Storage Integration. 

*Definition of done*",,"Create a test pipeline based on source Transaction Storage (TBD14 - BrandOrders)

@user is it okay if you entered in definition of done for this task? thanks!

Storage Integration was enabled yesterday however missing firewall rules to allow reversed traffic. Error: 

Anjali to create a SNOW ticket and Enda/ cloud team to action next steps.

Awaiting Enda/Brent action on SNOW ticket, Harrison following up given Enda is now on leave till end of July.

Thanks,

Harrison

Storage Integration completed. Pipeline ran end to end."
CSCI-44,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Jun/25 3:14 PM,16/Jul/25 2:17 PM,,Azure Devops setup - CICD pipeline,,,"Eugene created the following repo for Data Factory: [edp-data-factory - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory?version=GBmaster]

Pipeline is setup and ready for code deployment.

Chloe successfully tested with feature branch.

PR for CI created at: [Pull request 21856: Implemented #192947 - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/a9d4deeb-ad55-465e-9425-438e29d991d5/pullrequest/21856]

Sample pipeline run can be viewed at: [Pipelines - Run 20250619.4|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_build/results?buildId=32990&view=results]

Updated branch policies also to include the CI check on PRs:"
CSCI-43,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,23/May/25 1:06 PM,24/Jun/25 2:39 PM,,Azure network and infrastructure,"Effort and ETA required for Network and Infrastructure teams in IT to allow CW Snowflake setup before Chloe can begin work.

CK and Xavier to provide details post conversation with Rod on resource availability and effort involved",,"outcome - ETA of the work - as this is the dependency Chloe has for setup of CW snowflake

Havent' got an eta yet but getting breakdown of work @user

@user FYI

ADF instances created

Self-hosted integration runtime (SHIR) created"
CSCI-37,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Jun/25 1:25 PM,17/Jul/25 3:12 PM,,Business Matrix Review,"To be done after Jess does her BI modelling task

Objective:
- To review Fact - Dim table concept so that it will be able to be used to create data visualisations/Data products.

[Business Matrix_Supply_Chain.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true]",,"reviewed the dim tables and shared my comments in the spread sheet

Need few inputs from @user on newly added dimensions to review and add comments

|Product UOM|
|Manufacturer|
|Service Region|
|DC Master Plan|
|Sales Purchase Type|

Session to be booked for Bus Matrix Review for making decisions. @user to book

Uploaded EDP project Data Source Details document

To review previous BUS matrix from EDP 1.0

 @user 

to be followed up on tuesday

Review in Progress . Have reviewed couple of docs ¬†. Added my findings in a separate Tab. ¬†Will add more tomorrow

¬†

Hi @user & @user , will connect you tomorrow ¬†on how to accommodate them in our main doc

Created DIM_Currency Table Structure. Team to provide comments@user @user

Review complete. Observations are added to Matrix spread sheet."
CSCI-36,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,05/Jun/25 3:13 PM,24/Jun/25 2:39 PM,,ADF - DLA setup,"Objective: To create Data Ingestion Framework in ADF

Description:

* Import DLA extraction templates 
* Create Linked Services (snowflake private link, key vault and blob storage, service user etc.)
* Create Global environments (DB, Warehouse, Service Users etc.)",,"High level setup has been complete:

* Linked Services created (snowflake private link, key vault and blob storage, service user etc.)
* Global environments created (DB, Warehouse, Service Users etc.)"
CSCI-34,CSCI,CW Cloud Data Platform Interim Solution,Task,Done,Medium,16/Jun/25 11:04 AM,24/Jun/25 2:39 PM,,Amit > Jess - Semantic model handover,"Actionable handover points:

* What models are identified
* What is unsure
* What needs to be modelled
* Which reports (that we know of ) are these models contributing to.",,"Hi All,

We had the handover meeting and below are the tasks needs to be done 

1. Document Source Systems Models and Reports. Jess is already doing this
2. Is there any similar reports coming from two different source system

# Capture all Flat/Junk Dimensions used in existing reporting
# Capture different kind of Date dimensions used in existing reporting. Like Order Date, Invoiced Date, Shipped Date, Delivery Date etc.
# Grouping of Measures: group all time intelligence KPIs coming from same Measure into one bucket
# Looks for any source system that is not captured in Bus Matrix document
# Look for Data sets ingested from Mobile Dock . These are not captured in documents
# Look for Data Sets captured from MAWM

 @user @user@user"
CSCI-33,CSCI,CW Cloud Data Platform Interim Solution,Task,Duplicate,Medium,16/Jun/25 11:04 AM,20/Jun/25 1:23 PM,,Documentation of existing CW supply chain - Semantic models,,,
