{"issue_key": "CSCI-770", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Dec/25 9:51 AM", "updated": "18/Dec/25 9:32 AM", "labels": [], "summary": "Appriss worksheet response", "description": "h2. Overview\n\nThis task involves reviewing comments related to Appriss data. The goal is to identify and address any issues or concerns raised in the comments. This review is crucial for maintaining data quality and ensuring accurate reporting, ultimately improving the reliability of Appriss data for decision-making.\n\nh2. Steps\n\n# Access the Appriss comment review dashboard.\n# Filter comments by date range (last 7 days).\n# Read each comment carefully, noting any issues, concerns, or questions.\n# Investigate any reported data discrepancies or errors.\n# Document findings and resolutions for each comment.\n# Escalate any unresolved issues to the appropriate team.\n# Update the comment status to \"Resolved\" or \"In Progress\" as appropriate.\n\nh2. Acceptance Criteria\n\n* [ ] All comments from the last 7 days have been reviewed.\n* [ ] Each comment has a documented resolution or action plan.\n* [ ] All identified data discrepancies have been investigated and addressed.\n* [ ] The Appriss comment review dashboard is updated with the current status of each comment.\n* [ ] Any escalated issues have been assigned to the appropriate team.", "acceptance_criteria": "Given, When, Then", "comments": "Response is now shared with Appriss \n\n@user @user \n\nWorksheet link: [https://trebu.sharepoint.com/:x:/r/sites/ProjectManagement/_layouts/15/guestaccess.aspx?e=4%3Atz1jyA&at=9&share=Eb28Xewu_llMqK3p2YLR2KwBt7ctofD_Dodo4-rm-SmXZQ|https://trebu.sharepoint.com/:x:/r/sites/ProjectManagement/_layouts/15/guestaccess.aspx?e=4%3Atz1jyA&at=9&share=Eb28Xewu_llMqK3p2YLR2KwBt7ctofD_Dodo4-rm-SmXZQ]\n\nHey @user - I believe there are more than 1 question that might need respond?\n\nJust to confirm we have addressed all of them and would you share a screenshot so we can close this one?\n\nHi @user @user @user , All questions raised is now answered.", "text": "Summary\nAppriss worksheet response\n\n---\n\nDescription\nh2. Overview\n\nThis task involves reviewing comments related to Appriss data. The goal is to identify and address any issues or concerns raised in the comments. This review is crucial for maintaining data quality and ensuring accurate reporting, ultimately improving the reliability of Appriss data for decision-making.\n\nh2. Steps\n\n# Access the Appriss comment review dashboard.\n# Filter comments by date range (last 7 days).\n# Read each comment carefully, noting any issues, concerns, or questions.\n# Investigate any reported data discrepancies or errors.\n# Document findings and resolutions for each comment.\n# Escalate any unresolved issues to the appropriate team.\n# Update the comment status to \"Resolved\" or \"In Progress\" as appropriate.\n\nh2. Acceptance Criteria\n\n* [ ] All comments from the last 7 days have been reviewed.\n* [ ] Each comment has a documented resolution or action plan.\n* [ ] All identified data discrepancies have been investigated and addressed.\n* [ ] The Appriss comment review dashboard is updated with the current status of each comment.\n* [ ] Any escalated issues have been assigned to the appropriate team.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nResponse is now shared with Appriss \n\n@user @user \n\nWorksheet link: [https://trebu.sharepoint.com/:x:/r/sites/ProjectManagement/_layouts/15/guestaccess.aspx?e=4%3Atz1jyA&at=9&share=Eb28Xewu_llMqK3p2YLR2KwBt7ctofD_Dodo4-rm-SmXZQ|https://trebu.sharepoint.com/:x:/r/sites/ProjectManagement/_layouts/15/guestaccess.aspx?e=4%3Atz1jyA&at=9&share=Eb28Xewu_llMqK3p2YLR2KwBt7ctofD_Dodo4-rm-SmXZQ]\n\nHey @user - I believe there are more than 1 question that might need respond?\n\nJust to confirm we have addressed all of them and would you share a screenshot so we can close this one?\n\nHi @user @user @user , All questions raised is now answered.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 0}}
{"issue_key": "CSCI-769", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Dec/25 9:47 AM", "updated": "22/Dec/25 8:34 AM", "labels": [], "summary": "Work breakdown for RF tactical access", "description": "h2. Overview\n\nThis card outlines the work required to establish RF tactical access. Providing RF tactical access will enable secure and reliable communication in environments where traditional network infrastructure is unavailable. This is crucial for maintaining operational effectiveness and situational awareness in critical scenarios.\n\nh2. Steps\n\n# Define the specific requirements for RF tactical access, including frequency bands, data rates, and security protocols.\n# Identify and procure the necessary hardware and software components, such as radios, antennas, and encryption modules.\n# Configure and integrate the RF equipment to establish a functional tactical network.\n# Conduct thorough testing of the RF tactical access solution to ensure performance and reliability under various conditions.\n# Develop comprehensive documentation and training materials for users of the RF tactical access system.\n# Deploy the RF tactical access solution in the designated operational environment.\n# Provide ongoing support and maintenance for the RF tactical access system.\n\nh2. Acceptance Criteria\n\n* [ ] The RF tactical access solution meets the defined requirements for frequency bands, data rates, and security protocols.\n* [ ] The RF equipment is properly configured and integrated, allowing for seamless communication.\n* [ ] The RF tactical access solution demonstrates reliable performance under simulated operational conditions.\n* [ ] Comprehensive documentation and training materials are available for users.\n* [ ] The RF tactical access solution is successfully deployed in the designated operational environment.\n* [ ] A support and maintenance plan is in place for the RF tactical access system.", "acceptance_criteria": "Given, When, Then", "comments": "*The solution will be:*\n\n* Create data share between Sigma & CW SF\n* Create a dedicated warehouse for Rachel’s team on CW SF\n* Her warehouse can only access the 5 x agreed Sigma datasets\n* Everything will be automated (CI/CD) – means alignment to security and quick turnaround for future data sharing requests from any team\n\n \n\n*Cost:*\n\n* For Rachel’s warehouse, start with extra small, apply a cap amount of our choosing\n* If above doesn’t suffice then we would dial up accordingly\n\n \n\n*Timelines:*\n\n* Est delivery date is +*26*++^*th*^+ +*Feb*+ (we aim to finish by +*18*++^*th*^+ +*Feb*++)+\n* Detailed plan below (basically 3 x weeks effort between Jag & Harry)\n* We estimate this work to start in Feb 2026 because this is when we expect to have production environment ready", "text": "Summary\nWork breakdown for RF tactical access\n\n---\n\nDescription\nh2. Overview\n\nThis card outlines the work required to establish RF tactical access. Providing RF tactical access will enable secure and reliable communication in environments where traditional network infrastructure is unavailable. This is crucial for maintaining operational effectiveness and situational awareness in critical scenarios.\n\nh2. Steps\n\n# Define the specific requirements for RF tactical access, including frequency bands, data rates, and security protocols.\n# Identify and procure the necessary hardware and software components, such as radios, antennas, and encryption modules.\n# Configure and integrate the RF equipment to establish a functional tactical network.\n# Conduct thorough testing of the RF tactical access solution to ensure performance and reliability under various conditions.\n# Develop comprehensive documentation and training materials for users of the RF tactical access system.\n# Deploy the RF tactical access solution in the designated operational environment.\n# Provide ongoing support and maintenance for the RF tactical access system.\n\nh2. Acceptance Criteria\n\n* [ ] The RF tactical access solution meets the defined requirements for frequency bands, data rates, and security protocols.\n* [ ] The RF equipment is properly configured and integrated, allowing for seamless communication.\n* [ ] The RF tactical access solution demonstrates reliable performance under simulated operational conditions.\n* [ ] Comprehensive documentation and training materials are available for users.\n* [ ] The RF tactical access solution is successfully deployed in the designated operational environment.\n* [ ] A support and maintenance plan is in place for the RF tactical access system.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n*The solution will be:*\n\n* Create data share between Sigma & CW SF\n* Create a dedicated warehouse for Rachel’s team on CW SF\n* Her warehouse can only access the 5 x agreed Sigma datasets\n* Everything will be automated (CI/CD) – means alignment to security and quick turnaround for future data sharing requests from any team\n\n \n\n*Cost:*\n\n* For Rachel’s warehouse, start with extra small, apply a cap amount of our choosing\n* If above doesn’t suffice then we would dial up accordingly\n\n \n\n*Timelines:*\n\n* Est delivery date is +*26*++^*th*^+ +*Feb*+ (we aim to finish by +*18*++^*th*^+ +*Feb*++)+\n* Detailed plan below (basically 3 x weeks effort between Jag & Harry)\n* We estimate this work to start in Feb 2026 because this is when we expect to have production environment ready", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 1}}
{"issue_key": "CSCI-750", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "10/Dec/25 10:02 AM", "updated": "18/Dec/25 11:14 AM", "labels": [], "summary": "Appriss query refine", "description": "h2. Overview\n\nThe existing Appriss Datashare ELT query requires refinement to improve accuracy, maintainability, and alignment with the current data product standards. This work ensures the ELT logic is consistent with the latest Appriss data-share requirements, resolves gaps identified during QA, and prepares the pipeline for downstream reporting dependencies.\n\nh2. Steps\n\n# Review the current Appriss Datashare ELT SQL and document all existing joins, filters, transformations, and business rules.\n# Identify inconsistencies, redundant logic, missing filters, or misaligned business definitions based on updated Appriss documentation and stakeholder feedback.\n# Update and refine the SQL ELT logic to reflect the correct business rules, ensuring alignment with Bronze → Silver conventions and naming standards.\n# Validate refined output against expected Appriss samples (e.g., benchmark extracts, QA logs, historical outputs).\n# Optimise query performance where appropriate (index use, partition pruning, CTE structure, join ordering).\n# Peer review with Data Engineering and validate with Appriss SME before promoting the updated query.\n# Update documentation, lineage, and add comments within the SQL code for ongoing maintainability.\n\nh2. Deliverables\n\n* Refined and validated Appriss Datashare ELT SQL query.\n* Comparison document outlining logic changes vs previous version.\n* Updated data lineage / documentation in Confluence & Alation.\n* Peer-review sign-off and test results.\n\nh2. Acceptance Criteria\n\n* All business rules in the refined ELT logic align with current Appriss data share specifications.\n* Output datasets match expected Appriss benchmarks with no material discrepancies.\n* SQL code passes peer review and conforms to internal coding standards.\n* Performance is equal to or better than the existing ELT job.\n* Documentation and lineage links are updated and published.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAppriss query refine\n\n---\n\nDescription\nh2. Overview\n\nThe existing Appriss Datashare ELT query requires refinement to improve accuracy, maintainability, and alignment with the current data product standards. This work ensures the ELT logic is consistent with the latest Appriss data-share requirements, resolves gaps identified during QA, and prepares the pipeline for downstream reporting dependencies.\n\nh2. Steps\n\n# Review the current Appriss Datashare ELT SQL and document all existing joins, filters, transformations, and business rules.\n# Identify inconsistencies, redundant logic, missing filters, or misaligned business definitions based on updated Appriss documentation and stakeholder feedback.\n# Update and refine the SQL ELT logic to reflect the correct business rules, ensuring alignment with Bronze → Silver conventions and naming standards.\n# Validate refined output against expected Appriss samples (e.g., benchmark extracts, QA logs, historical outputs).\n# Optimise query performance where appropriate (index use, partition pruning, CTE structure, join ordering).\n# Peer review with Data Engineering and validate with Appriss SME before promoting the updated query.\n# Update documentation, lineage, and add comments within the SQL code for ongoing maintainability.\n\nh2. Deliverables\n\n* Refined and validated Appriss Datashare ELT SQL query.\n* Comparison document outlining logic changes vs previous version.\n* Updated data lineage / documentation in Confluence & Alation.\n* Peer-review sign-off and test results.\n\nh2. Acceptance Criteria\n\n* All business rules in the refined ELT logic align with current Appriss data share specifications.\n* Output datasets match expected Appriss benchmarks with no material discrepancies.\n* SQL code passes peer review and conforms to internal coding standards.\n* Performance is equal to or better than the existing ELT job.\n* Documentation and lineage links are updated and published.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 2}}
{"issue_key": "CSCI-749", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Dec/25 10:22 AM", "updated": "12/Dec/25 2:43 PM", "labels": [], "summary": "Appriss audit data release", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Started working on this\n\nHi @user ,\n\n \n\nPlease find attached the query for AUDIT data. This would be a new table for Audit information. Please let me know when you create the table in Snowflake and load data. I will then create the Benchmark query and share it with you.\n\nMain query and Benchmark query, both are now shared with Mike and data is also populated in Snowflake.", "text": "Summary\nAppriss audit data release\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nStarted working on this\n\nHi @user ,\n\n \n\nPlease find attached the query for AUDIT data. This would be a new table for Audit information. Please let me know when you create the table in Snowflake and load data. I will then create the Benchmark query and share it with you.\n\nMain query and Benchmark query, both are now shared with Mike and data is also populated in Snowflake.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 3}}
{"issue_key": "CSCI-748", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Dec/25 10:21 AM", "updated": "12/Dec/25 2:43 PM", "labels": [], "summary": "investigate benchmark transaction issue for benchmark validation data for Appriss", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "I did the investigation from my end. It looks like a legitimate transaction to me. Below is the findings\n\nFor : 35358290519654432\n\nCheck For Invoice with ID 35358290519654432 Legitimate Invoice \nCheck for Electronic Payment Payment Approved, Invoice not voided \nCheck Item Level Data Legitimate Invoice \nCheck for Audit Data with Abandoned Sales No records of abandoned Sales (SaleactivityID =27) Audit records shows it is legitimate invoice\n\nChecked at Snowflake end as well\n\nFor: 35357668968628352\n\nCheck For Invoice with ID 35358290519654432 Legitimate Invoice \nCheck for Electronic Payment Payment Approved, Invoice not voided ( positive pay) \nCheck Item Level Data Legitimate Invoice All line items has got positive qty sold \nCheck for Audit Data with Abandoned Sales No records of abandoned Sales (SaleactivityID =27) Audit records shows it is legitimate invoice\n\nChecked at Snowflake end as well\n\nIt appears to be a legitimate .", "text": "Summary\ninvestigate benchmark transaction issue for benchmark validation data for Appriss\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nI did the investigation from my end. It looks like a legitimate transaction to me. Below is the findings\n\nFor : 35358290519654432\n\nCheck For Invoice with ID 35358290519654432 Legitimate Invoice \nCheck for Electronic Payment Payment Approved, Invoice not voided \nCheck Item Level Data Legitimate Invoice \nCheck for Audit Data with Abandoned Sales No records of abandoned Sales (SaleactivityID =27) Audit records shows it is legitimate invoice\n\nChecked at Snowflake end as well\n\nFor: 35357668968628352\n\nCheck For Invoice with ID 35358290519654432 Legitimate Invoice \nCheck for Electronic Payment Payment Approved, Invoice not voided ( positive pay) \nCheck Item Level Data Legitimate Invoice All line items has got positive qty sold \nCheck for Audit Data with Abandoned Sales No records of abandoned Sales (SaleactivityID =27) Audit records shows it is legitimate invoice\n\nChecked at Snowflake end as well\n\nIt appears to be a legitimate .", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 4}}
{"issue_key": "CSCI-743", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Dec/25 11:37 AM", "updated": "16/Dec/25 9:55 AM", "labels": [], "summary": "Cognos || implementation and review", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "The offshore and DB team is now briefed about the Solution development. Task created for DB Team *REQ0165663 /  RITM0181203*", "text": "Summary\nCognos || implementation and review\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nThe offshore and DB team is now briefed about the Solution development. Task created for DB Team *REQ0165663 /  RITM0181203*", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 5}}
{"issue_key": "CSCI-742", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Dec/25 11:33 AM", "updated": "23/Dec/25 9:47 AM", "labels": [], "summary": "Setting up DBT Cloud", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nSetting up DBT Cloud\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 6}}
{"issue_key": "CSCI-741", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "04/Dec/25 12:40 PM", "updated": "05/Dec/25 11:42 AM", "labels": [], "summary": "PAM- Cognos Onboarding Questionnaire", "description": "h3. Context\n\n* To help answer [PAM - Cognos Cube Onboarding Questionaire.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/SecurityOperations/_layouts/15/Doc.aspx?sourcedoc=%7B2D64A06A-1AD2-4C17-8246-3845EE9EA80E%7D&file=PAM%20-%20Cognos%20Cube%20Onboarding%20Questionaire.xlsx&wdLOR=c3791FC8F-67AC-4008-A97F-2C072FB52811&action=default&mobileredirect=true&isSPOFile=1&xsdata=MDV8MDJ8fGE5ZmQ2YzIxMzM0ODQ5NmQ3MGQ3MDhkZTMyYzliNjA2fDhiYzAyODBlYjNhZDQzOGFiYThhMmNmMDBkMTlhZjRhfDB8MHw2MzkwMDQwMzkwNTY4Nzk4MzF8VW5rbm93bnxWR1ZoYlhOVFpXTjFjbWwwZVZObGNuWnBZMlY4ZXlKRFFTSTZJbFJsWVcxelgwRlVVRk5sY25acFkyVmZVMUJQVEU5R0lpd2lWaUk2SWpBdU1DNHdNREF3SWl3aVVDSTZJbGRwYmpNeUlpd2lRVTRpT2lKUGRHaGxjaUlzSWxkVUlqb3hNWDA9fDF8TDJOb1lYUnpMekU1T2pFd09UaGpNbUl3TFdJM1pHVXROR1JsWkMwNE9XRmhMV1U0Tnpkak1qazJPR013WWw4ME9HUTNNVGxtTkMweVkyWTRMVFE0TnpRdE9XWTJOUzA0TXpNeE5HRmlNV1E1T1RKQWRXNXhMbWRpYkM1emNHRmpaWE12YldWemMyRm5aWE12TVRjMk5EZ3dOekV3TkRnME1BPT18NTJhOTVhMDgwNTAwNGNmOWY1ZTIwOGRlMzJjOWI2MDV8Y2ZjNjliOThlMjE0NGY1ZGJlNTI1NWJhMzBlZTU1YzM%3D&sdata=ejA2eFdMY3YxRW1TS0VuT1E5Vmo3M2NUcXZvOExESEI0SnVYQXJ4d210VT0%3D&ovuser=1dec29b4-9a5d-41fc-9c75-cdb2cb9c7fe6%2CHan.Li%40sigmahealthcare.com.au]\n\nh3. Acceptance criteria\n\n* All question answer and sent back to security team\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nPAM- Cognos Onboarding Questionnaire\n\n---\n\nDescription\nh3. Context\n\n* To help answer [PAM - Cognos Cube Onboarding Questionaire.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/SecurityOperations/_layouts/15/Doc.aspx?sourcedoc=%7B2D64A06A-1AD2-4C17-8246-3845EE9EA80E%7D&file=PAM%20-%20Cognos%20Cube%20Onboarding%20Questionaire.xlsx&wdLOR=c3791FC8F-67AC-4008-A97F-2C072FB52811&action=default&mobileredirect=true&isSPOFile=1&xsdata=MDV8MDJ8fGE5ZmQ2YzIxMzM0ODQ5NmQ3MGQ3MDhkZTMyYzliNjA2fDhiYzAyODBlYjNhZDQzOGFiYThhMmNmMDBkMTlhZjRhfDB8MHw2MzkwMDQwMzkwNTY4Nzk4MzF8VW5rbm93bnxWR1ZoYlhOVFpXTjFjbWwwZVZObGNuWnBZMlY4ZXlKRFFTSTZJbFJsWVcxelgwRlVVRk5sY25acFkyVmZVMUJQVEU5R0lpd2lWaUk2SWpBdU1DNHdNREF3SWl3aVVDSTZJbGRwYmpNeUlpd2lRVTRpT2lKUGRHaGxjaUlzSWxkVUlqb3hNWDA9fDF8TDJOb1lYUnpMekU1T2pFd09UaGpNbUl3TFdJM1pHVXROR1JsWkMwNE9XRmhMV1U0Tnpkak1qazJPR013WWw4ME9HUTNNVGxtTkMweVkyWTRMVFE0TnpRdE9XWTJOUzA0TXpNeE5HRmlNV1E1T1RKQWRXNXhMbWRpYkM1emNHRmpaWE12YldWemMyRm5aWE12TVRjMk5EZ3dOekV3TkRnME1BPT18NTJhOTVhMDgwNTAwNGNmOWY1ZTIwOGRlMzJjOWI2MDV8Y2ZjNjliOThlMjE0NGY1ZGJlNTI1NWJhMzBlZTU1YzM%3D&sdata=ejA2eFdMY3YxRW1TS0VuT1E5Vmo3M2NUcXZvOExESEI0SnVYQXJ4d210VT0%3D&ovuser=1dec29b4-9a5d-41fc-9c75-cdb2cb9c7fe6%2CHan.Li%40sigmahealthcare.com.au]\n\nh3. Acceptance criteria\n\n* All question answer and sent back to security team\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 7}}
{"issue_key": "CSCI-740", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Nov/25 9:44 AM", "updated": "15/Dec/25 9:21 AM", "labels": [], "summary": "Cognos || performance improvement investigation", "description": "Detail to be fill in\n\nh3. *Context*\n\nUsers are intermittently unable to see application content in Cognos. IBM’s initial investigation indicates a correlation with JVM memory pressure; JVM size has been increased as recommended.\nFurther internal analysis shows the issue occurs mainly on Mondays and Tuesdays and coincides with heavy execution of the _Buyer Category Performance Drill-Through Report_, which queries large-volume data across multiple tables.\nThis report appears to be a significant contributor to JVM load and may be impacting overall Cognos stability.\n\n----\n\nh3. *Acceptance Criteria*\n\n* Performance investigation completed within estimation.\n* Root cause confirmed and documented with evidence (logs, query analysis, JVM metrics).\n* Proposed optimisation approach reduces report execution time and/or JVM load.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Started working on this. The objective is to analyze the problem and come up with a solution approach. This task to continue in next sprint.\n\nThanks @user, Just fyi will move this exact card to next sprint so feel free to add note here as needed\n\nAnalysis complete. Creating queries for optimization. 2 queries need to be created for AUS and NZL\n\nQuery for AUS is now complete\n\nQueries for NZL is also prepared now.", "text": "Summary\nCognos || performance improvement investigation\n\n---\n\nDescription\nDetail to be fill in\n\nh3. *Context*\n\nUsers are intermittently unable to see application content in Cognos. IBM’s initial investigation indicates a correlation with JVM memory pressure; JVM size has been increased as recommended.\nFurther internal analysis shows the issue occurs mainly on Mondays and Tuesdays and coincides with heavy execution of the _Buyer Category Performance Drill-Through Report_, which queries large-volume data across multiple tables.\nThis report appears to be a significant contributor to JVM load and may be impacting overall Cognos stability.\n\n----\n\nh3. *Acceptance Criteria*\n\n* Performance investigation completed within estimation.\n* Root cause confirmed and documented with evidence (logs, query analysis, JVM metrics).\n* Proposed optimisation approach reduces report execution time and/or JVM load.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nStarted working on this. The objective is to analyze the problem and come up with a solution approach. This task to continue in next sprint.\n\nThanks @user, Just fyi will move this exact card to next sprint so feel free to add note here as needed\n\nAnalysis complete. Creating queries for optimization. 2 queries need to be created for AUS and NZL\n\nQuery for AUS is now complete\n\nQueries for NZL is also prepared now.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 8}}
{"issue_key": "CSCI-739", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "27/Nov/25 10:45 AM", "updated": "05/Dec/25 9:40 AM", "labels": [], "summary": "Appriss Tactical || Generate Updated Benchmark Dataset with New Logic Applied", "description": "h3. Context\n\n* Create 7* dataset for each Appriss generated script with\n* selected 10 stores\n** locations:\n*** B004: Chadstone Amcal (Customer data enabled)\n*** B005: Chadstone SC CWH\n*** B057: Hurstville CWH\n*** B222: Eastwood CWH\n*** B509: Watergardens TC\n*** B532: Sydney Central CWH\n*** B859: Melbourne Amcal (Customer data enabled)\n* Extraction Period: April 28, 2025 to May 16, 2025\n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "@user Please note the extraction period is now Extraction Period: April 28, 2025 to May 16, 2025 per Frank\n\nFYI @user\n\nThanks @user for the update. Will proceed accordingly. @user , We need to reload data for the said date range for all tables.\n\nPicking this one now. Work in Progress", "text": "Summary\nAppriss Tactical || Generate Updated Benchmark Dataset with New Logic Applied\n\n---\n\nDescription\nh3. Context\n\n* Create 7* dataset for each Appriss generated script with\n* selected 10 stores\n** locations:\n*** B004: Chadstone Amcal (Customer data enabled)\n*** B005: Chadstone SC CWH\n*** B057: Hurstville CWH\n*** B222: Eastwood CWH\n*** B509: Watergardens TC\n*** B532: Sydney Central CWH\n*** B859: Melbourne Amcal (Customer data enabled)\n* Extraction Period: April 28, 2025 to May 16, 2025\n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n@user Please note the extraction period is now Extraction Period: April 28, 2025 to May 16, 2025 per Frank\n\nFYI @user\n\nThanks @user for the update. Will proceed accordingly. @user , We need to reload data for the said date range for all tables.\n\nPicking this one now. Work in Progress", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 9}}
{"issue_key": "CSCI-738", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "27/Nov/25 10:43 AM", "updated": "03/Dec/25 10:03 AM", "labels": [], "summary": "Appriss Tactical || Join Transactions with Audit Logs", "description": "h3. Context\n\n* Design and implement a Snowflake dataset/view that joins transaction datasets with audit log data, applying the correct mapping logic and exposing required columns for Appriss Retail to validate QA issues.\n* Add audit data to ardm_tender simliar to ardm_header\n* Add audit data to ardm_event simliar to ardm_header\n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Combined dataset contains all columns requested by Appriss.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Update as of now\n\n# Add Audit Log data for Voided transactions into ARDM_EVENT.-- Completed.\n# Add PLU information in Product Reference Data( New Request came today) -- Completed.\n# Add Audit log data for voided transactions into ARDM_TENDER  -- In Progress\nETA for 1,2 & 3 -> Tuesday, 2nd Dec\n\nHi All,\n\nPlease find below the updates as of now:\n\n \n\n# Add Audit Log data for Voided transactions into ARDM_EVENT.-- Completed.\n# Add PLU information in Product Reference Data( New Request came 1^st^ Dec) -- Completed.\n# Add Audit log data for voided transactions into ARDM_TENDER  -- Completed", "text": "Summary\nAppriss Tactical || Join Transactions with Audit Logs\n\n---\n\nDescription\nh3. Context\n\n* Design and implement a Snowflake dataset/view that joins transaction datasets with audit log data, applying the correct mapping logic and exposing required columns for Appriss Retail to validate QA issues.\n* Add audit data to ardm_tender simliar to ardm_header\n* Add audit data to ardm_event simliar to ardm_header\n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Combined dataset contains all columns requested by Appriss.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nUpdate as of now\n\n# Add Audit Log data for Voided transactions into ARDM_EVENT.-- Completed.\n# Add PLU information in Product Reference Data( New Request came today) -- Completed.\n# Add Audit log data for voided transactions into ARDM_TENDER  -- In Progress\nETA for 1,2 & 3 -> Tuesday, 2nd Dec\n\nHi All,\n\nPlease find below the updates as of now:\n\n \n\n# Add Audit Log data for Voided transactions into ARDM_EVENT.-- Completed.\n# Add PLU information in Product Reference Data( New Request came 1^st^ Dec) -- Completed.\n# Add Audit log data for voided transactions into ARDM_TENDER  -- Completed", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 10}}
{"issue_key": "CSCI-737", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "26/Nov/25 10:08 AM", "updated": "05/Dec/25 12:00 PM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical || Creating View for extraction script", "description": "Create a set of Snowflake views to support the Appriss Tactical extraction process.\nThese views will sit on top of the raw source tables ingested via ADF, apply all required SQL transformation logic, and expose clean, consumable outputs for datashare.\n\n*Deliverables*\n\n* *Finalised Snowflake views* representing the curated, transformed outputs required for Appriss data share.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAppriss Tactical || Creating View for extraction script\n\n---\n\nDescription\nCreate a set of Snowflake views to support the Appriss Tactical extraction process.\nThese views will sit on top of the raw source tables ingested via ADF, apply all required SQL transformation logic, and expose clean, consumable outputs for datashare.\n\n*Deliverables*\n\n* *Finalised Snowflake views* representing the curated, transformed outputs required for Appriss data share.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 11}}
{"issue_key": "CSCI-736", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "26/Nov/25 8:25 AM", "updated": "02/Dec/25 9:51 AM", "labels": ["NLP_BI"], "summary": "NLP BI || Datashare establishment", "description": "h3. *Objective*\n\nh3. \nEstablish Snowflake Data Share from CW Snowflake to Sigma for key Supply Chain datasets. The share must enable read-only access for Sigma stakeholders and ensure controlled, auditable data exchange with no manual intervention beyond initial setup. Target tables currently sit in EDP_DEV.STG_SOURCE_SUPPLYCHAIN (CW Bronze Layer):\n\n* *DIM_PRODUCT*\n* *DIM_STORE*\n* *FCT_SALES*\n* *FCT_STOCK*\n\nAccess to be provided to:\n\n* [Girish.Bhatta@sigmahealthcare.com.au|mailto:Girish.Bhatta@sigmahealthcare.com.au]\n* [Dunyin.Gu@sigmahealthcare.com.au|mailto:Dunyin.Gu@sigmahealthcare.com.au]\n* [Phoebe.Song@sigmahealthcare.com.au|mailto:Phoebe.Song@sigmahealthcare.com.au]\n\n----\n\nh3. *Acceptance criteria*\n\n* Secure Share created in CW Snowflake\n* Tables added: DIM_PRODUCT, DIM_STORE, FCT_SALES, FCT_STOCK for mentioned users\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nNLP BI || Datashare establishment\n\n---\n\nDescription\nh3. *Objective*\n\nh3. \nEstablish Snowflake Data Share from CW Snowflake to Sigma for key Supply Chain datasets. The share must enable read-only access for Sigma stakeholders and ensure controlled, auditable data exchange with no manual intervention beyond initial setup. Target tables currently sit in EDP_DEV.STG_SOURCE_SUPPLYCHAIN (CW Bronze Layer):\n\n* *DIM_PRODUCT*\n* *DIM_STORE*\n* *FCT_SALES*\n* *FCT_STOCK*\n\nAccess to be provided to:\n\n* [Girish.Bhatta@sigmahealthcare.com.au|mailto:Girish.Bhatta@sigmahealthcare.com.au]\n* [Dunyin.Gu@sigmahealthcare.com.au|mailto:Dunyin.Gu@sigmahealthcare.com.au]\n* [Phoebe.Song@sigmahealthcare.com.au|mailto:Phoebe.Song@sigmahealthcare.com.au]\n\n----\n\nh3. *Acceptance criteria*\n\n* Secure Share created in CW Snowflake\n* Tables added: DIM_PRODUCT, DIM_STORE, FCT_SALES, FCT_STOCK for mentioned users\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 12}}
{"issue_key": "CSCI-735", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Nov/25 11:11 AM", "updated": "10/Dec/25 9:55 AM", "labels": [], "summary": "Power BI SSO to Snowflake", "description": "h3. Context\n\nEnable and configure Microsoft Power BI to connect to Snowflake using single sign-on (SSO) via Microsoft Entra ID and External OAuth. This setup allows users to access Snowflake data without requiring an on-premises Power BI Gateway. The configuration includes security integration creation in Snowflake, network policy validation, role mapping, and troubleshooting of common SSO issues.\n\nh3. Acceptance criteria\n\n* Power BI security integration created and verified in Snowflake.\n* Users can connect to Snowflake via Power BI with Microsoft Entra ID SSO.\n* Verification of access using login history queries.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Hi @user can you pls help us create a SSO integration for PowerBI in Snowflake using AccountAdmin privilege: \n\nInstruction in this doc \n\n[+Power BI SSO to Snowflake | Snowflake Documentation+|https://url.au.m.mimecastprotect.com/s/C3TBCBNq3QhxWxoqT6h9C2R4Yj?domain=docs.snowflake.com] \n\nIf you have any question, please reach out to Jess and Eugene, \n\n@user @user @user\n\nHey @user,\n\nWould you able to work with @user on this one after Appriss?\n\nThanks\n\nperformance issue to be review", "text": "Summary\nPower BI SSO to Snowflake\n\n---\n\nDescription\nh3. Context\n\nEnable and configure Microsoft Power BI to connect to Snowflake using single sign-on (SSO) via Microsoft Entra ID and External OAuth. This setup allows users to access Snowflake data without requiring an on-premises Power BI Gateway. The configuration includes security integration creation in Snowflake, network policy validation, role mapping, and troubleshooting of common SSO issues.\n\nh3. Acceptance criteria\n\n* Power BI security integration created and verified in Snowflake.\n* Users can connect to Snowflake via Power BI with Microsoft Entra ID SSO.\n* Verification of access using login history queries.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nHi @user can you pls help us create a SSO integration for PowerBI in Snowflake using AccountAdmin privilege: \n\nInstruction in this doc \n\n[+Power BI SSO to Snowflake | Snowflake Documentation+|https://url.au.m.mimecastprotect.com/s/C3TBCBNq3QhxWxoqT6h9C2R4Yj?domain=docs.snowflake.com] \n\nIf you have any question, please reach out to Jess and Eugene, \n\n@user @user @user\n\nHey @user,\n\nWould you able to work with @user on this one after Appriss?\n\nThanks\n\nperformance issue to be review", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 13}}
{"issue_key": "CSCI-734", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Nov/25 11:07 AM", "updated": "25/Nov/25 1:28 PM", "labels": [], "summary": "Connect to Snowflake in the Power BI Service", "description": "*Description:*\nConfigure the Power BI service to connect to Snowflake, optionally enabling Microsoft Entra ID single sign-on (SSO). This includes configuring administrative settings across Snowflake, Power BI, and Azure, and updating semantic models to support SSO or basic authentication.\n\n*Acceptance Criteria:*\n\n* Power BI Admin portal configured to send Microsoft Entra tokens to Snowflake.\n* Semantic models updated to use Microsoft Entra ID credentials for SSO where required.\n* Verification of connectivity and authentication tested successfully i", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nConnect to Snowflake in the Power BI Service\n\n---\n\nDescription\n*Description:*\nConfigure the Power BI service to connect to Snowflake, optionally enabling Microsoft Entra ID single sign-on (SSO). This includes configuring administrative settings across Snowflake, Power BI, and Azure, and updating semantic models to support SSO or basic authentication.\n\n*Acceptance Criteria:*\n\n* Power BI Admin portal configured to send Microsoft Entra tokens to Snowflake.\n* Semantic models updated to use Microsoft Entra ID credentials for SSO where required.\n* Verification of connectivity and authentication tested successfully i\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 14}}
{"issue_key": "CSCI-733", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Nov/25 9:25 AM", "updated": "05/Dec/25 11:41 AM", "labels": ["NLP_BI"], "summary": "NLP BI || Retail Knowledge Transfer", "description": "h3. Context\n\n* Knowledge Transfer on retail data \n\nh3. Deliverables\n\nBelow information are provided:\n\n# *Store IDs* for all MyChemist locations now converted to Amcal.\n# *Mapping rules* between CW products and IQVIA products.\n# CWG exclusive product (done)\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "@user - Can you review and confirm if any additional information is needed or not?\n\nThanks @user. I have a follow-up question about the Amcal Branch ID file and will connect with you when you are available tomorrow.\n\n@user Could you pls. review the CWH_IQVIA_PRODUCT_MAPPING file and check if the CWH product can be mapped to the product in the Dim_Product_Retail_Master_VW using IQVIA ID. Pls. let us know if any qs. Thanks.\n\nRequired information and data is shared with Data Science Team\n\nHi @user , Could you please confirm that you have got all info and data that you needed from CWH team for now? I can then close this ticket.\n\n@user yes we’ve got all the datasets for now. Just to let you know that we might come back to you later as the business users start using the application and raise some important questions not covered by the current datasets. Thanks for your help.", "text": "Summary\nNLP BI || Retail Knowledge Transfer\n\n---\n\nDescription\nh3. Context\n\n* Knowledge Transfer on retail data \n\nh3. Deliverables\n\nBelow information are provided:\n\n# *Store IDs* for all MyChemist locations now converted to Amcal.\n# *Mapping rules* between CW products and IQVIA products.\n# CWG exclusive product (done)\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n@user - Can you review and confirm if any additional information is needed or not?\n\nThanks @user. I have a follow-up question about the Amcal Branch ID file and will connect with you when you are available tomorrow.\n\n@user Could you pls. review the CWH_IQVIA_PRODUCT_MAPPING file and check if the CWH product can be mapped to the product in the Dim_Product_Retail_Master_VW using IQVIA ID. Pls. let us know if any qs. Thanks.\n\nRequired information and data is shared with Data Science Team\n\nHi @user , Could you please confirm that you have got all info and data that you needed from CWH team for now? I can then close this ticket.\n\n@user yes we’ve got all the datasets for now. Just to let you know that we might come back to you later as the business users start using the application and raise some important questions not covered by the current datasets. Thanks for your help.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 15}}
{"issue_key": "CSCI-732", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Nov/25 9:24 AM", "updated": "05/Dec/25 3:02 PM", "labels": ["NLP_BI"], "summary": "NLP BI || Snowflake Table review/refresh", "description": "*Objective*\n\nReview data quality issue identified in meeting regarding missing values.\n\nDesign and implement automated refresh processes for CW Retail Bronze tables, enabling continuous ingestion of the latest data. Implement delta or change-data-capture logic where applicable to ensure efficient updates and maintain historical accuracy.\n\nTarget tables:\n\n* *DIM_PRODUCT*\n* *DIM_STORE*\n* *FCT_SALES*\n* *FCT_STOCK*\n\nAll tables must be refreshed reliably and be made query-ready for downstream consumption via datashare\n\nAcceptance Criteria\n\n* Automated refresh pipelines for DIM_PRODUCT, DIM_STORE, FCT_SALES, FCT_STOCK\n* Delta/CDC logic implemented where applicable\n* Scheduling and monitoring enabled\n* Validation evidence (row count checks, sample reconciliations)", "acceptance_criteria": "Given, When, Then", "comments": "Data Refreshed until 26/11/2025. Now reviewing with Amit. Will continue this process for a few days. Once we are happy with the stats. I will create the PR to publish the schedule.\n\nduplicates identified in the data and noticed the data is getting deleted in the source table. Need to implement a logic to accommodate the deletions.\n\nPR created and merge changes are rebased with master and waiting for approval.\n\nChanges are now published. Job will run every day at 12:30pm", "text": "Summary\nNLP BI || Snowflake Table review/refresh\n\n---\n\nDescription\n*Objective*\n\nReview data quality issue identified in meeting regarding missing values.\n\nDesign and implement automated refresh processes for CW Retail Bronze tables, enabling continuous ingestion of the latest data. Implement delta or change-data-capture logic where applicable to ensure efficient updates and maintain historical accuracy.\n\nTarget tables:\n\n* *DIM_PRODUCT*\n* *DIM_STORE*\n* *FCT_SALES*\n* *FCT_STOCK*\n\nAll tables must be refreshed reliably and be made query-ready for downstream consumption via datashare\n\nAcceptance Criteria\n\n* Automated refresh pipelines for DIM_PRODUCT, DIM_STORE, FCT_SALES, FCT_STOCK\n* Delta/CDC logic implemented where applicable\n* Scheduling and monitoring enabled\n* Validation evidence (row count checks, sample reconciliations)\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nData Refreshed until 26/11/2025. Now reviewing with Amit. Will continue this process for a few days. Once we are happy with the stats. I will create the PR to publish the schedule.\n\nduplicates identified in the data and noticed the data is getting deleted in the source table. Need to implement a logic to accommodate the deletions.\n\nPR created and merge changes are rebased with master and waiting for approval.\n\nChanges are now published. Job will run every day at 12:30pm", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 16}}
{"issue_key": "CSCI-731", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "24/Nov/25 11:22 PM", "updated": "26/Nov/25 10:38 AM", "labels": [], "summary": "Placeholder - New Table extraction from source to bronze", "description": "h3. Context\n\n* Placeholder card to extract any new table identify as needed for LLM project\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Base on identified table needed to support LLM project\n\n* Tables are extracted from source and ingested into CW SF Bronze\n* Historical Data loaded", "comments": "No additional table needed as of this sprint", "text": "Summary\nPlaceholder - New Table extraction from source to bronze\n\n---\n\nDescription\nh3. Context\n\n* Placeholder card to extract any new table identify as needed for LLM project\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nBase on identified table needed to support LLM project\n\n* Tables are extracted from source and ingested into CW SF Bronze\n* Historical Data loaded\n\n---\n\nComments\nNo additional table needed as of this sprint", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 17}}
{"issue_key": "CSCI-728", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Nov/25 12:49 PM", "updated": "05/Dec/25 9:32 AM", "labels": [], "summary": "Error Investigation || SF SSO to PBI via PrivateLink", "description": "h3. Context\n\n* An investigation is required into a Power BI Desktop → Snowflake SSO authentication issue occurring when connecting via PrivateLink. Microsoft Support has requested further validation steps and clarification on the configuration.\n\nh3. Objective\n\n* Analyse the SSO authentication error encountered in Power BI Desktop when connecting to Snowflake through PrivateLink and follow suggestion as per MS support\n* Prepare required details or logs for Microsoft Support if escalation is needed.\n\nh3. Acceptance criteria\n\n* Technical findings on the root cause (where identifiable).\n* Updated configuration steps or corrections if required.\n* PBI is able to connect to SF when Private link turned on\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Based on my investigation, we need also to add PowerBI and Snowflake SSO Integration:\n\n* [+Connect to Snowflake with Power BI - Power BI | Microsoft Learn+|https://learn.microsoft.com/en-us/power-bi/connect-data/service-connect-snowflake]\n* [+Power BI SSO to Snowflake | Snowflake Documentation+|https://docs.snowflake.com/en/user-guide/oauth-powerbi]\n\nSeparate base on task\n\n* [+Connect to Snowflake with Power BI - Power BI | Microsoft Learn+|https://learn.microsoft.com/en-us/power-bi/connect-data/service-connect-snowflake] – CSCI-734\n\n[+Power BI SSO to Snowflake | Snowflake Documentation+|https://docs.snowflake.com/en/user-guide/oauth-powerbi] +-+ CSCI-735", "text": "Summary\nError Investigation || SF SSO to PBI via PrivateLink\n\n---\n\nDescription\nh3. Context\n\n* An investigation is required into a Power BI Desktop → Snowflake SSO authentication issue occurring when connecting via PrivateLink. Microsoft Support has requested further validation steps and clarification on the configuration.\n\nh3. Objective\n\n* Analyse the SSO authentication error encountered in Power BI Desktop when connecting to Snowflake through PrivateLink and follow suggestion as per MS support\n* Prepare required details or logs for Microsoft Support if escalation is needed.\n\nh3. Acceptance criteria\n\n* Technical findings on the root cause (where identifiable).\n* Updated configuration steps or corrections if required.\n* PBI is able to connect to SF when Private link turned on\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nBased on my investigation, we need also to add PowerBI and Snowflake SSO Integration:\n\n* [+Connect to Snowflake with Power BI - Power BI | Microsoft Learn+|https://learn.microsoft.com/en-us/power-bi/connect-data/service-connect-snowflake]\n* [+Power BI SSO to Snowflake | Snowflake Documentation+|https://docs.snowflake.com/en/user-guide/oauth-powerbi]\n\nSeparate base on task\n\n* [+Connect to Snowflake with Power BI - Power BI | Microsoft Learn+|https://learn.microsoft.com/en-us/power-bi/connect-data/service-connect-snowflake] – CSCI-734\n\n[+Power BI SSO to Snowflake | Snowflake Documentation+|https://docs.snowflake.com/en/user-guide/oauth-powerbi] +-+ CSCI-735", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 18}}
{"issue_key": "CSCI-716", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Nov/25 12:35 PM", "updated": "22/Dec/25 4:38 PM", "labels": [], "summary": "Source to Target Mapping || FactWarehouseInventoryHistory", "description": "Source to target mapping for FactWarehouseInventoryHistory- filling out sheet as per Source-to-target Fact", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** Table\n*** Column\n*** Derived? Yes or no", "comments": "Bring card up to sprint 14 @user - fyi\n\n[Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=G9oYh5&nav=MTVfe0RGM0EyMDAyLUFBMEQtNDc3MS05QzUzLTIwMzMwMjJEMDMyOH0]\n\n@user Please review\n\nAs discussed with Bhavya, removed WarehouseLocation attribute from model (this cannot be done where the source is from AX - WarehouseLocation will sit on the FactWarehouseInventoryLocation-Intra/History models instead).\n\nRenamed StockCostPrice to AverageRealCost.\nAdded AverageNetworkCost and AverageStoreCost (based on advice from Bhavya).\nRemoved StockAmount (this is removed from the platinum model, but will be added into the gold layer).", "text": "Summary\nSource to Target Mapping || FactWarehouseInventoryHistory\n\n---\n\nDescription\nSource to target mapping for FactWarehouseInventoryHistory- filling out sheet as per Source-to-target Fact\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** Table\n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nBring card up to sprint 14 @user - fyi\n\n[Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=G9oYh5&nav=MTVfe0RGM0EyMDAyLUFBMEQtNDc3MS05QzUzLTIwMzMwMjJEMDMyOH0]\n\n@user Please review\n\nAs discussed with Bhavya, removed WarehouseLocation attribute from model (this cannot be done where the source is from AX - WarehouseLocation will sit on the FactWarehouseInventoryLocation-Intra/History models instead).\n\nRenamed StockCostPrice to AverageRealCost.\nAdded AverageNetworkCost and AverageStoreCost (based on advice from Bhavya).\nRemoved StockAmount (this is removed from the platinum model, but will be added into the gold layer).", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 19}}
{"issue_key": "CSCI-715", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Nov/25 12:33 PM", "updated": "22/Dec/25 4:38 PM", "labels": [], "summary": "Source to Target Mapping || FactWarehouseInventoryIntra", "description": "Source to target mapping for FactWarehouseInventoryIntra- filling out sheet as per Source-to-target Fact", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** Table\n*** Column\n*** Derived? Yes or no", "comments": "[Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=KeEcaO&nav=MTVfezhBRDI5QzE1LTY0RTctNEJBMi1CNzNGLUZEQjlCMkMyMTc2MX0]\n\n@user Please review\n\nAs discussed with Bhavya, removed WarehouseLocation attribute from model (this cannot be done where the source is from AX - WarehouseLocation will sit on the FactWarehouseInventoryLocation-Intra/History models instead).\n\nRenamed StockCostPrice to AverageRealCost.\nAdded AverageNetworkCost and AverageStoreCost (based on advice from Bhavya).\nRemoved StockAmount (this is removed from the platinum model, but will be added into the gold layer).", "text": "Summary\nSource to Target Mapping || FactWarehouseInventoryIntra\n\n---\n\nDescription\nSource to target mapping for FactWarehouseInventoryIntra- filling out sheet as per Source-to-target Fact\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** Table\n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\n[Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=KeEcaO&nav=MTVfezhBRDI5QzE1LTY0RTctNEJBMi1CNzNGLUZEQjlCMkMyMTc2MX0]\n\n@user Please review\n\nAs discussed with Bhavya, removed WarehouseLocation attribute from model (this cannot be done where the source is from AX - WarehouseLocation will sit on the FactWarehouseInventoryLocation-Intra/History models instead).\n\nRenamed StockCostPrice to AverageRealCost.\nAdded AverageNetworkCost and AverageStoreCost (based on advice from Bhavya).\nRemoved StockAmount (this is removed from the platinum model, but will be added into the gold layer).", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 20}}
{"issue_key": "CSCI-713", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Nov/25 12:32 PM", "updated": "22/Dec/25 12:18 PM", "labels": [], "summary": "Source to Target Mapping || FactStoreInventoryHistory", "description": "Source to target mapping for FactStoreInventoryHistory- filling out sheet as per Source-to-target Fact", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** Table\n*** Column\n*** Derived? Yes or no", "comments": "Made some amendments (based on new changes made to FactStoreInventoryIntra):\n\n* StockCostPrice been renamed to AverageRealCost\n* StockOnHandValue, InboundValue and OutboundValue all removed (these will be added to the gold layer product instead)\n* AverageNetworkCost and AverageStoreCost added", "text": "Summary\nSource to Target Mapping || FactStoreInventoryHistory\n\n---\n\nDescription\nSource to target mapping for FactStoreInventoryHistory- filling out sheet as per Source-to-target Fact\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** Table\n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nMade some amendments (based on new changes made to FactStoreInventoryIntra):\n\n* StockCostPrice been renamed to AverageRealCost\n* StockOnHandValue, InboundValue and OutboundValue all removed (these will be added to the gold layer product instead)\n* AverageNetworkCost and AverageStoreCost added", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 21}}
{"issue_key": "CSCI-712", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Nov/25 12:31 PM", "updated": "22/Dec/25 12:09 PM", "labels": [], "summary": "Source to Target Mapping || FactStoreInventoryIntra", "description": "Source to target mapping for FactStoreInventoryIntra- filling out sheet as per Source-to-target Fact", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** Table\n*** Column\n*** Derived? Yes or no", "comments": "@user - Can you pls review this one? thx\n\nMade some amendments (based on new changes made to model):\n\n* StockCostPrice been renamed to AverageRealCost\n* StockOnHandValue, InboundValue and OutboundValue all removed (these will be added to the gold layer product instead)\n* AverageNetworkCost and AverageStoreCost all added\n** Copied over column values based on note Bhavya left regarding Opening/Closing SOH Network/Store Average Amount", "text": "Summary\nSource to Target Mapping || FactStoreInventoryIntra\n\n---\n\nDescription\nSource to target mapping for FactStoreInventoryIntra- filling out sheet as per Source-to-target Fact\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** Table\n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\n@user - Can you pls review this one? thx\n\nMade some amendments (based on new changes made to model):\n\n* StockCostPrice been renamed to AverageRealCost\n* StockOnHandValue, InboundValue and OutboundValue all removed (these will be added to the gold layer product instead)\n* AverageNetworkCost and AverageStoreCost all added\n** Copied over column values based on note Bhavya left regarding Opening/Closing SOH Network/Store Average Amount", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 22}}
{"issue_key": "CSCI-707", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "20/Nov/25 8:21 PM", "updated": "25/Nov/25 5:20 PM", "labels": [], "summary": "Appriss Tactical || Data Mapping Queries for Voided Transactions & Total Amount Fields - ARDM_HEADER", "description": "h3. *Context*\n\nAmit raised a data-mapping query regarding the fields *Total Amount*, *Calculated Total*, and *Original Total* in the _ARDM_HEADER_ table, noting that all three currently have the same mapping. Clarification is required from the Appriss team on the intended logic and definitions.\nAdditionally, review and updates are required for the mapping of *voided transactions*, based on customer feedback and sample data to be provided.\n\nh3. *Objective*\n\nConfirm the correct data-mapping rules for total amount fields and finalise the approach for handling voided transactions within the ARDM schema.\n\nh3. *Deliverables*\n\n* Confirmed definitions and mapping rules for Total Amount, Calculated Total, and Original Total.\n* Updated mapping rules for voided invoices including required columns.\n* Revised ARDM data mapping document or schema definition.\n* Summary of decisions and rationale shared with the team.", "acceptance_criteria": "* Mapping rules for total amount fields are clarified and approved.\n* Final approach for voided transactions is documented and aligns with customer guidance.", "comments": "Analysis done! Found a solution to combine two separate data sets into one. Will continue working on this in the next Sprint\n\nWork in Progress. Planning to finish by tomorrow.\n\nARDM_HEADER is now complete, and queries are shared with @user", "text": "Summary\nAppriss Tactical || Data Mapping Queries for Voided Transactions & Total Amount Fields - ARDM_HEADER\n\n---\n\nDescription\nh3. *Context*\n\nAmit raised a data-mapping query regarding the fields *Total Amount*, *Calculated Total*, and *Original Total* in the _ARDM_HEADER_ table, noting that all three currently have the same mapping. Clarification is required from the Appriss team on the intended logic and definitions.\nAdditionally, review and updates are required for the mapping of *voided transactions*, based on customer feedback and sample data to be provided.\n\nh3. *Objective*\n\nConfirm the correct data-mapping rules for total amount fields and finalise the approach for handling voided transactions within the ARDM schema.\n\nh3. *Deliverables*\n\n* Confirmed definitions and mapping rules for Total Amount, Calculated Total, and Original Total.\n* Updated mapping rules for voided invoices including required columns.\n* Revised ARDM data mapping document or schema definition.\n* Summary of decisions and rationale shared with the team.\n\n---\n\nAcceptance Criteria\n* Mapping rules for total amount fields are clarified and approved.\n* Final approach for voided transactions is documented and aligns with customer guidance.\n\n---\n\nComments\nAnalysis done! Found a solution to combine two separate data sets into one. Will continue working on this in the next Sprint\n\nWork in Progress. Planning to finish by tomorrow.\n\nARDM_HEADER is now complete, and queries are shared with @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 23}}
{"issue_key": "CSCI-702", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "20/Nov/25 9:34 AM", "updated": "21/Nov/25 11:58 AM", "labels": [], "summary": "Knowledge Transfer - Chloe & Ashu", "description": "*Context*\nKnowledge transfer session between *Chloe* and *Ashu* to ensure continuity of work and coverage of Chloe’s tasks during *26 Nov – 5 Dec*, and any additional support as needed.\n\n*Objective*\nEnsure Ashu has full visibility and understanding of Chloe’s responsibilities, workflows, and in-flight tasks so that work can continue smoothly during the specified period.\n\n*Steps*\n\n* Chloe to walk through current tasks, priorities, outstanding actions\n\n*Deliverables*\n\n* Handover notes / documentation created where available.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nKnowledge Transfer - Chloe & Ashu\n\n---\n\nDescription\n*Context*\nKnowledge transfer session between *Chloe* and *Ashu* to ensure continuity of work and coverage of Chloe’s tasks during *26 Nov – 5 Dec*, and any additional support as needed.\n\n*Objective*\nEnsure Ashu has full visibility and understanding of Chloe’s responsibilities, workflows, and in-flight tasks so that work can continue smoothly during the specified period.\n\n*Steps*\n\n* Chloe to walk through current tasks, priorities, outstanding actions\n\n*Deliverables*\n\n* Handover notes / documentation created where available.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 24}}
{"issue_key": "CSCI-698", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Nov/25 1:38 PM", "updated": "20/Nov/25 9:22 AM", "labels": [], "summary": "Review procedure to get the data type from source system for MySQL", "description": "h3. Context\n\n* Test the snowflake procedure to get the data types of the columns from the source system for MYSQL.\n\nh3. Objective\n\n* Load the data for each table and auto generate the view with the source system datatypes \n\nh3. Acceptance criteria\n\n* Load tables from Manhattan Active and generate views with column data types from same as the source system. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nReview procedure to get the data type from source system for MySQL\n\n---\n\nDescription\nh3. Context\n\n* Test the snowflake procedure to get the data types of the columns from the source system for MYSQL.\n\nh3. Objective\n\n* Load the data for each table and auto generate the view with the source system datatypes \n\nh3. Acceptance criteria\n\n* Load tables from Manhattan Active and generate views with column data types from same as the source system. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 25}}
{"issue_key": "CSCI-697", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "17/Nov/25 11:12 PM", "updated": "18/Nov/25 3:30 PM", "labels": [], "summary": "Alation || \"Configuring Power BI Tenant Settings for Alation API Service\"", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAlation || \"Configuring Power BI Tenant Settings for Alation API Service\"\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 26}}
{"issue_key": "CSCI-695", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Nov/25 9:06 AM", "updated": "21/Nov/25 12:05 PM", "labels": [], "summary": "ADF Linked Service Clean up", "description": "h3. Context\n\n* Rename the linked services according to the naming standards.\n\nh3. Acceptance criteria\n\n* All linked services should follow the same naming standards.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Linked services cannot be renamed. I will document the details of the source system and linked services and delete the unused the linked services.\n\n@user\n\nDetails documented in the operational document. Linked service deleted and waiting for PR post call with Eugene.\n\nThis done. PR created and will be approved on 24/11 just to avoid an issue over the weekend.", "text": "Summary\nADF Linked Service Clean up\n\n---\n\nDescription\nh3. Context\n\n* Rename the linked services according to the naming standards.\n\nh3. Acceptance criteria\n\n* All linked services should follow the same naming standards.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nLinked services cannot be renamed. I will document the details of the source system and linked services and delete the unused the linked services.\n\n@user\n\nDetails documented in the operational document. Linked service deleted and waiting for PR post call with Eugene.\n\nThis done. PR created and will be approved on 24/11 just to avoid an issue over the weekend.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 27}}
{"issue_key": "CSCI-694", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Nov/25 9:06 AM", "updated": "18/Nov/25 1:14 PM", "labels": [], "summary": "Snowflake || Post-Migration Schemas Clean up", "description": "h3. Context\n\n* Perform the clean-up of the dev environments post migration to Bronze Schema. \n\nh3. Acceptance criteria\n\n* Remove all unused/ migrated stg_ schemas from the EDP_DEV database \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Below schemas has been deleted. We have 30 days to undrop these with snowflake time travel feature.\n\nEDP_DEV.STG_AXLINK\nEDP_DEV.STG_CWMGTSTOREINVOICES\nEDP_DEV.STG_GENERAL_REFERENCE\nEDP_DEV.STG_ILS\nEDP_DEV.STG_MANHATTAN_ACTIVE\nEDP_DEV.STG_MANHATTAN_ACTIVE_DEFAULT_DCINVENTORY\nEDP_DEV.STG_SCAX2012\nEDP_DEV.STG_SKU\nEDP_DEV.STG_SPSWHSPURCHASE\nEDP_DEV.STG_STOCKDB\nEDP_DEV.STG_TRANSACTION_STORAGE\nEDP_DEV.STG_TRNS\nEDP_DEV.TEST\nEDP_DEV.STG_TRNS", "text": "Summary\nSnowflake || Post-Migration Schemas Clean up\n\n---\n\nDescription\nh3. Context\n\n* Perform the clean-up of the dev environments post migration to Bronze Schema. \n\nh3. Acceptance criteria\n\n* Remove all unused/ migrated stg_ schemas from the EDP_DEV database \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nBelow schemas has been deleted. We have 30 days to undrop these with snowflake time travel feature.\n\nEDP_DEV.STG_AXLINK\nEDP_DEV.STG_CWMGTSTOREINVOICES\nEDP_DEV.STG_GENERAL_REFERENCE\nEDP_DEV.STG_ILS\nEDP_DEV.STG_MANHATTAN_ACTIVE\nEDP_DEV.STG_MANHATTAN_ACTIVE_DEFAULT_DCINVENTORY\nEDP_DEV.STG_SCAX2012\nEDP_DEV.STG_SKU\nEDP_DEV.STG_SPSWHSPURCHASE\nEDP_DEV.STG_STOCKDB\nEDP_DEV.STG_TRANSACTION_STORAGE\nEDP_DEV.STG_TRNS\nEDP_DEV.TEST\nEDP_DEV.STG_TRNS", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 28}}
{"issue_key": "CSCI-693", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "12/Nov/25 9:03 AM", "updated": "22/Dec/25 8:26 AM", "labels": [], "summary": "Update CI configuration for snowflake-infra", "description": "Update the CI configuration files to reference the new snowflake-infra folder location and ensure all pipelines work as expected.", "acceptance_criteria": "", "comments": "", "text": "Summary\nUpdate CI configuration for snowflake-infra\n\n---\n\nDescription\nUpdate the CI configuration files to reference the new snowflake-infra folder location and ensure all pipelines work as expected.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 29}}
{"issue_key": "CSCI-692", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "12/Nov/25 9:03 AM", "updated": "22/Dec/25 8:26 AM", "labels": [], "summary": "Move base directory to base-infra folder", "description": "Relocate the entire base directory within the repository to a new folder named base-infra as part of the repo reorganization.", "acceptance_criteria": "", "comments": "", "text": "Summary\nMove base directory to base-infra folder\n\n---\n\nDescription\nRelocate the entire base directory within the repository to a new folder named base-infra as part of the repo reorganization.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 30}}
{"issue_key": "CSCI-691", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "12/Nov/25 9:03 AM", "updated": "22/Dec/25 8:26 AM", "labels": [], "summary": "Update CI configuration for base-infra", "description": "Modify the continuous integration (CI) configuration files to ensure they reference the new base-infra folder location correctly.", "acceptance_criteria": "", "comments": "", "text": "Summary\nUpdate CI configuration for base-infra\n\n---\n\nDescription\nModify the continuous integration (CI) configuration files to ensure they reference the new base-infra folder location correctly.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 31}}
{"issue_key": "CSCI-690", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "12/Nov/25 9:03 AM", "updated": "22/Dec/25 8:26 AM", "labels": [], "summary": "Copy edp-snowflake-infrastructure to snowflake-infra folder", "description": "Copy the contents of the edp-snowflake-infrastructure repository into a new folder named snowflake-infra within the consolidated repository.", "acceptance_criteria": "", "comments": "", "text": "Summary\nCopy edp-snowflake-infrastructure to snowflake-infra folder\n\n---\n\nDescription\nCopy the contents of the edp-snowflake-infrastructure repository into a new folder named snowflake-infra within the consolidated repository.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 32}}
{"issue_key": "CSCI-689", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "12/Nov/25 9:03 AM", "updated": "13/Nov/25 7:52 AM", "labels": [], "summary": "Rename edp-infrastructure repository to edp-infra-provision", "description": "Rename the existing edp-infrastructure repository to edp-infra-provision to reflect the new consolidated structure.", "acceptance_criteria": "", "comments": "edp-infrastructure renamed to [edp-infra-provision - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infra-provision]", "text": "Summary\nRename edp-infrastructure repository to edp-infra-provision\n\n---\n\nDescription\nRename the existing edp-infrastructure repository to edp-infra-provision to reflect the new consolidated structure.\n\n---\n\nComments\nedp-infrastructure renamed to [edp-infra-provision - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infra-provision]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 33}}
{"issue_key": "CSCI-688", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Nov/25 8:41 AM", "updated": "14/Nov/25 3:05 PM", "labels": [], "summary": "Investigate Null Value in Table", "description": "h3. *Context*\n\nAn issue has been identified in the {{TransactionStorage}} database, {{BranchOrderItems}} table — where all records appear to contain *null values*. This table is part of the *bulk upload automation* process, and the data issue may impact downstream integrations and reporting.\n\nh3. *Scope*\n\n* analysis on {{TransactionStorage.BranchOrderItems}}.\n* Review of bulk upload automation flow related to this table.", "acceptance_criteria": "* Root cause analysis done and issue identified\n* Remediation (if applicable).\n* Confirmation of table is loaded successfully after fix", "comments": "Issue was due to the Parquet file headers are case sensitive. It is not fixed, and the data is loaded successfully. I would be spending some more time to check what happens to the procedure when there are spaces in the columns name. cc-@user", "text": "Summary\nInvestigate Null Value in Table\n\n---\n\nDescription\nh3. *Context*\n\nAn issue has been identified in the {{TransactionStorage}} database, {{BranchOrderItems}} table — where all records appear to contain *null values*. This table is part of the *bulk upload automation* process, and the data issue may impact downstream integrations and reporting.\n\nh3. *Scope*\n\n* analysis on {{TransactionStorage.BranchOrderItems}}.\n* Review of bulk upload automation flow related to this table.\n\n---\n\nAcceptance Criteria\n* Root cause analysis done and issue identified\n* Remediation (if applicable).\n* Confirmation of table is loaded successfully after fix\n\n---\n\nComments\nIssue was due to the Parquet file headers are case sensitive. It is not fixed, and the data is loaded successfully. I would be spending some more time to check what happens to the procedure when there are spaces in the columns name. cc-@user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 34}}
{"issue_key": "CSCI-685", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:26 AM", "updated": "05/Dec/25 5:16 PM", "labels": [], "summary": "EDP Infrastructure || Repo Configuration - Repo for ALTIDA CI/CD", "description": "Implement full CI/CD and IaC for the ALTIDA framework in a dedicated isolated Git repository, so that ALTIDA is fully automated (no click-ops), minimally coupled to the core data platform, and can be cleanly removed and replaced in the future without refactoring other repos or shared infrastructure.", "acceptance_criteria": "*Repository & Isolation:*\n\n* ALTIDA has its own Git repo containing all ALTIDA code, config, IaC and pipeline definitions.\n* No ALTIDA code/config/IaC lives in any core platform or shared infrastructure repo. Other repos can reference ALTIDA only via documented loosely coupled integration points (eg. queues, file locations, APIs).\n* Disabling ALTIDA pipelines and archiving the ALTIDA repo does not require code changes in other repos beyond removing those integration references.\n\n*IaC and no Click-Ops:*\n\n* All Snowflake and cloud resources required by ALTIDA are defined and managed only via IaC in ALTIDA repo (no portal or Snowflake UI changes).\n* A new environment (eg. DEV/SIT/UAT) can be provisioned from empty to full working ALTIDA by running the documented CI/CD pipeline, with no manual steps.\n\n*CI/CD Configuration:*\n\n* ALTIDA has dedicated CI/CD pipelines in its repo for build, test/validation and deploy to each environment.\n* Environment differences are handled via parameters and secrets (eg. Key Vault) not by branching code or hand-editing config in the UI.\n\n*Security & Access:*\n\n* ALTIDA uses its own service accounts and roles (Snowflake, cloud) defined in IaC with least privilege, limited to the layers/resources it needs (Landing/Bronze) and not to curated/core platform components\n* No shared “god” roles or service accounts exists between ALTIDA and other platform components. Secrets are stored only in approved secret stores, not in code or pipeline definitions.\n\n*Replaceability/Decommissioning:*\n\n* The ALTIDA repo includes a short, tested decommission procedure describing how to disable ALTIDA, remove its resources and revoke its identities without impacting the rest of the platform.\n* There are no hard dependencies on ALTIDA internals in other repos (no imports of ALTIDA libraries or assumptions about ALTIDA-specific structures). Integration contracts are documented so a future replacement data ingestion solution can plug into the same interfaces.", "comments": "Hi @user please add the Acceptance Criteria when you are ready.\n\n@user can you please rename the edp-snowflake repo to edp-etl-atilda-legacy. This repo will be used for ALTIDA CI/CD purpose. \n\nFYI @user\n\n@user @user this is renamed to: {{edp-etl-atilda-legacy}}\n\n@user thank you!", "text": "Summary\nEDP Infrastructure || Repo Configuration - Repo for ALTIDA CI/CD\n\n---\n\nDescription\nImplement full CI/CD and IaC for the ALTIDA framework in a dedicated isolated Git repository, so that ALTIDA is fully automated (no click-ops), minimally coupled to the core data platform, and can be cleanly removed and replaced in the future without refactoring other repos or shared infrastructure.\n\n---\n\nAcceptance Criteria\n*Repository & Isolation:*\n\n* ALTIDA has its own Git repo containing all ALTIDA code, config, IaC and pipeline definitions.\n* No ALTIDA code/config/IaC lives in any core platform or shared infrastructure repo. Other repos can reference ALTIDA only via documented loosely coupled integration points (eg. queues, file locations, APIs).\n* Disabling ALTIDA pipelines and archiving the ALTIDA repo does not require code changes in other repos beyond removing those integration references.\n\n*IaC and no Click-Ops:*\n\n* All Snowflake and cloud resources required by ALTIDA are defined and managed only via IaC in ALTIDA repo (no portal or Snowflake UI changes).\n* A new environment (eg. DEV/SIT/UAT) can be provisioned from empty to full working ALTIDA by running the documented CI/CD pipeline, with no manual steps.\n\n*CI/CD Configuration:*\n\n* ALTIDA has dedicated CI/CD pipelines in its repo for build, test/validation and deploy to each environment.\n* Environment differences are handled via parameters and secrets (eg. Key Vault) not by branching code or hand-editing config in the UI.\n\n*Security & Access:*\n\n* ALTIDA uses its own service accounts and roles (Snowflake, cloud) defined in IaC with least privilege, limited to the layers/resources it needs (Landing/Bronze) and not to curated/core platform components\n* No shared “god” roles or service accounts exists between ALTIDA and other platform components. Secrets are stored only in approved secret stores, not in code or pipeline definitions.\n\n*Replaceability/Decommissioning:*\n\n* The ALTIDA repo includes a short, tested decommission procedure describing how to disable ALTIDA, remove its resources and revoke its identities without impacting the rest of the platform.\n* There are no hard dependencies on ALTIDA internals in other repos (no imports of ALTIDA libraries or assumptions about ALTIDA-specific structures). Integration contracts are documented so a future replacement data ingestion solution can plug into the same interfaces.\n\n---\n\nComments\nHi @user please add the Acceptance Criteria when you are ready.\n\n@user can you please rename the edp-snowflake repo to edp-etl-atilda-legacy. This repo will be used for ALTIDA CI/CD purpose. \n\nFYI @user\n\n@user @user this is renamed to: {{edp-etl-atilda-legacy}}\n\n@user thank you!", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 35}}
{"issue_key": "CSCI-684", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:25 AM", "updated": "24/Nov/25 2:02 PM", "labels": [], "summary": "EDP Infrastructure || IaC Repo Configuration - edp-visualize-publish-dashboards", "description": "", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nEDP Infrastructure || IaC Repo Configuration - edp-visualize-publish-dashboards\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 36}}
{"issue_key": "CSCI-683", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:25 AM", "updated": "24/Nov/25 1:59 PM", "labels": [], "summary": "EDP Infrastructure || IaC Repo Configuration - edp-model-publish-semantic", "description": "", "acceptance_criteria": "Given, When, Then", "comments": "see development tracking at: [User Story 213682 Rename Semantic Model Repository|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213682]", "text": "Summary\nEDP Infrastructure || IaC Repo Configuration - edp-model-publish-semantic\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nsee development tracking at: [User Story 213682 Rename Semantic Model Repository|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213682]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 37}}
{"issue_key": "CSCI-682", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:25 AM", "updated": "18/Dec/25 10:56 AM", "labels": [], "summary": "EDP Infrastructure || IaC Repo Configuration - edp-transform-build-warehouse", "description": "edp-transform-build-warehouse\n\nThis is repo naming reserved for dbt and transformation workflow.", "acceptance_criteria": "Given, When, Then", "comments": "see development tracking at: [User Story 213686 Rename Snowflake Repository|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213686]\n\nwe need a new repo for this @user. This is intended for dbt transformation workflow I believe.\n\n@user - Do you need new card for rename {{edp-snowflak}} to {{edp-etl-atilda-legacy  }}? Per Chloe edp-transform-build-warehouse is reserve for DBT?\n\n@user this is already done. we need a ticket eventually for the dbt repo.\n\nHi @user @user this is repo naming reserved for dbt and transformation workflow.", "text": "Summary\nEDP Infrastructure || IaC Repo Configuration - edp-transform-build-warehouse\n\n---\n\nDescription\nedp-transform-build-warehouse\n\nThis is repo naming reserved for dbt and transformation workflow.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nsee development tracking at: [User Story 213686 Rename Snowflake Repository|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213686]\n\nwe need a new repo for this @user. This is intended for dbt transformation workflow I believe.\n\n@user - Do you need new card for rename {{edp-snowflak}} to {{edp-etl-atilda-legacy  }}? Per Chloe edp-transform-build-warehouse is reserve for DBT?\n\n@user this is already done. we need a ticket eventually for the dbt repo.\n\nHi @user @user this is repo naming reserved for dbt and transformation workflow.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 38}}
{"issue_key": "CSCI-681", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:25 AM", "updated": "25/Nov/25 10:35 AM", "labels": [], "summary": "EDP Infrastructure || IaC Repo Configuration - edp-ingest-orchestrate-pipelines", "description": "Rename [edp-data-factory|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/branches] to edp-ingest-orchestrate-pipelines", "acceptance_criteria": "Given, When, Then", "comments": "@user @user @user I will be renaming the repo [edp-data-factory|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/branches] to edp-ingest-orchestrate-pipelines can you please confirm that you have checked in pending work in your branches?\n\nOnce confirmed, I will rename and notify you, then you can check out your branches from the new repository name, contents are the same, there should be no changes in functionality.\n\n@user @user @user just waiting for your confirmation to move ahead with this.\n\n@user All good from my side! @user @user @user can you please confirm if you have pending changes in ADF that need to be committed to your feature branch before the renaming action?\n\nAll good from my side as well @user @user\n\nConfirmed by everyone @user Please proceed!\n\nHi @user , please proceed 🙂\n\nthis is completed: [User Story 213677 Rename Data Factory Repository|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213677]\n\nEugene rectified the issue with security on the publish branch. Detail of the fixes is at [User Story 213677 Rename Data Factory Repository|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213677]", "text": "Summary\nEDP Infrastructure || IaC Repo Configuration - edp-ingest-orchestrate-pipelines\n\n---\n\nDescription\nRename [edp-data-factory|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/branches] to edp-ingest-orchestrate-pipelines\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n@user @user @user I will be renaming the repo [edp-data-factory|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/branches] to edp-ingest-orchestrate-pipelines can you please confirm that you have checked in pending work in your branches?\n\nOnce confirmed, I will rename and notify you, then you can check out your branches from the new repository name, contents are the same, there should be no changes in functionality.\n\n@user @user @user just waiting for your confirmation to move ahead with this.\n\n@user All good from my side! @user @user @user can you please confirm if you have pending changes in ADF that need to be committed to your feature branch before the renaming action?\n\nAll good from my side as well @user @user\n\nConfirmed by everyone @user Please proceed!\n\nHi @user , please proceed 🙂\n\nthis is completed: [User Story 213677 Rename Data Factory Repository|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213677]\n\nEugene rectified the issue with security on the publish branch. Detail of the fixes is at [User Story 213677 Rename Data Factory Repository|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213677]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 39}}
{"issue_key": "CSCI-678", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:20 AM", "updated": "16/Dec/25 3:32 PM", "labels": [], "summary": "EDP Infrastructure || IaC Repo Configuration - edp-infra-provision", "description": "Combine edp-infrastructure and edp-snowflake-infrastructure into a single repo.\n\nSteps:\n\n# clone edp-infrastructure to edp-infrastructure-backup\n# rename edp-infrastructure to edp-infra-provision\n# move the whole base directory to base-infra\n# update base-infra ci to reflect new folder location\n# copy edp-snowflake-infrastructure to snowflake-infra\n# update snowflake-infra ci to reflect new folder location", "acceptance_criteria": "* Both source repositories are merged into {{edp-infra-provision}}.\n* {{edp-infrastructure}} and {{edp-snowflake-infrastructure}} modules retain separate Terraform state files and CI/CD pipelines.\n* Folder structure clearly separates Azure and Snowflake components (e.g., {{/azure}} and {{/snowflake}}).\n* All pipeline definitions are updated to reflect new repository paths but maintain independent workflows.\n* Backend configurations (state files, key vaults, storage accounts) remain unchanged.\n* All contributors are notified pre- and post-migration.\n* Documentation and pipeline badges updated to reflect new repository location.\n* Validation builds for both areas succeed after migration.", "comments": "current repo cloned to [edp-infrastructure-backup - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infrastructure-backup?version=GBmaster]\n\nsee development tracking: [User Story 213659 Merge and Rename Infrastructure Repositories|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213659]\n\n@user", "text": "Summary\nEDP Infrastructure || IaC Repo Configuration - edp-infra-provision\n\n---\n\nDescription\nCombine edp-infrastructure and edp-snowflake-infrastructure into a single repo.\n\nSteps:\n\n# clone edp-infrastructure to edp-infrastructure-backup\n# rename edp-infrastructure to edp-infra-provision\n# move the whole base directory to base-infra\n# update base-infra ci to reflect new folder location\n# copy edp-snowflake-infrastructure to snowflake-infra\n# update snowflake-infra ci to reflect new folder location\n\n---\n\nAcceptance Criteria\n* Both source repositories are merged into {{edp-infra-provision}}.\n* {{edp-infrastructure}} and {{edp-snowflake-infrastructure}} modules retain separate Terraform state files and CI/CD pipelines.\n* Folder structure clearly separates Azure and Snowflake components (e.g., {{/azure}} and {{/snowflake}}).\n* All pipeline definitions are updated to reflect new repository paths but maintain independent workflows.\n* Backend configurations (state files, key vaults, storage accounts) remain unchanged.\n* All contributors are notified pre- and post-migration.\n* Documentation and pipeline badges updated to reflect new repository location.\n* Validation builds for both areas succeed after migration.\n\n---\n\nComments\ncurrent repo cloned to [edp-infrastructure-backup - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infrastructure-backup?version=GBmaster]\n\nsee development tracking: [User Story 213659 Merge and Rename Infrastructure Repositories|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213659]\n\n@user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 40}}
{"issue_key": "CSCI-677", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:19 AM", "updated": "16/Dec/25 3:54 PM", "labels": [], "summary": "EDP Infrastructure || IaC Repo Structures - Implementation Plan", "description": "h3. Context\n\n* Rename repositories to be aligned with implementation across different organizations and geographical locations.\n\nh3. Objective\n\n* Rename current repositories for clarity as to which organizations and geographical locations they are applicable to.\n\nh3. Steps \n\n# Notify developers of the pending change in repository name\n# Update the repository name\n# Update CI/CD pipelines to use the new repository name if needed\n# Validate that CI/CD pipelines work\n# Developers to update their environments (VS Code, Data Factory Studio, Workspaces) to use the new repository name\n\nh3. Acceptance criteria\n\n* renamed repositories aligned with target organizations and geographical location\n\nh3. Assumptions - (Optional)\n\n* naming conventions for naming repositories, including the organization and geographical location.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "@user i created a whole feature in Azure DevOps for the repo restructuring:\n\n[Feature 213658 Align EDP Repository Naming with Enterprise Standards|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213658]\n\nif you want, you can update this and related tasks to match what is in there.", "text": "Summary\nEDP Infrastructure || IaC Repo Structures - Implementation Plan\n\n---\n\nDescription\nh3. Context\n\n* Rename repositories to be aligned with implementation across different organizations and geographical locations.\n\nh3. Objective\n\n* Rename current repositories for clarity as to which organizations and geographical locations they are applicable to.\n\nh3. Steps \n\n# Notify developers of the pending change in repository name\n# Update the repository name\n# Update CI/CD pipelines to use the new repository name if needed\n# Validate that CI/CD pipelines work\n# Developers to update their environments (VS Code, Data Factory Studio, Workspaces) to use the new repository name\n\nh3. Acceptance criteria\n\n* renamed repositories aligned with target organizations and geographical location\n\nh3. Assumptions - (Optional)\n\n* naming conventions for naming repositories, including the organization and geographical location.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n@user i created a whole feature in Azure DevOps for the repo restructuring:\n\n[Feature 213658 Align EDP Repository Naming with Enterprise Standards|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/213658]\n\nif you want, you can update this and related tasks to match what is in there.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 41}}
{"issue_key": "CSCI-675", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:14 AM", "updated": "21/Nov/25 1:51 PM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical || Share Audit Data with APPRISS", "description": "", "acceptance_criteria": "Given, When, Then", "comments": "Hey @user - what is the expected outcome of “sharing audit data”? \n\nIs to review and address some of the issue raised by appriss?", "text": "Summary\nAppriss Tactical || Share Audit Data with APPRISS\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nHey @user - what is the expected outcome of “sharing audit data”? \n\nIs to review and address some of the issue raised by appriss?", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 42}}
{"issue_key": "CSCI-674", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:13 AM", "updated": "21/Nov/25 1:47 PM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical || APPRISS Transform query ARDM Ref Location", "description": "", "acceptance_criteria": "Given, When, Then", "comments": "Query Build is complete and is shared with the Engineering team", "text": "Summary\nAppriss Tactical || APPRISS Transform query ARDM Ref Location\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nQuery Build is complete and is shared with the Engineering team", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 43}}
{"issue_key": "CSCI-673", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:13 AM", "updated": "21/Nov/25 5:07 PM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical || APPRISS Transform query ARDM Ref Product", "description": "h3. Context\n\nDevelop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.\n\n*Objects to Develop:*\n\n* ORDER_HEADER - done\n* EVENT \n* ITEM\n* TENDER\n* REFERENCE_CUSTOMER\n* REFERENCE_PRODUCT\n* REFERENCE_LOCATION\n* Placeholder - SUMMARY TABLE\n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* Transformation script from source to generate APPRISS output\n\nh3. Acceptance criteria\n\n* SQL transformation models are created and committed for all eight objects listed.\n* Each transform query follows the standard CTE and naming convention structure.\n* Data types and key fields are consistent across all models.\n* Transformations run successfully with no data loss or schema mismatches.\n* QA validation confirms accurate mapping and record counts\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Query prepared and shared with Mike", "text": "Summary\nAppriss Tactical || APPRISS Transform query ARDM Ref Product\n\n---\n\nDescription\nh3. Context\n\nDevelop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.\n\n*Objects to Develop:*\n\n* ORDER_HEADER - done\n* EVENT \n* ITEM\n* TENDER\n* REFERENCE_CUSTOMER\n* REFERENCE_PRODUCT\n* REFERENCE_LOCATION\n* Placeholder - SUMMARY TABLE\n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* Transformation script from source to generate APPRISS output\n\nh3. Acceptance criteria\n\n* SQL transformation models are created and committed for all eight objects listed.\n* Each transform query follows the standard CTE and naming convention structure.\n* Data types and key fields are consistent across all models.\n* Transformations run successfully with no data loss or schema mismatches.\n* QA validation confirms accurate mapping and record counts\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nQuery prepared and shared with Mike", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 44}}
{"issue_key": "CSCI-672", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:13 AM", "updated": "20/Nov/25 10:16 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical || APPRISS Transform query ARDM Item Discount", "description": "", "acceptance_criteria": "Given, When, Then", "comments": "[^ARDM_ITEM_DISCOUNT.txt]", "text": "Summary\nAppriss Tactical || APPRISS Transform query ARDM Item Discount\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n[^ARDM_ITEM_DISCOUNT.txt]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 45}}
{"issue_key": "CSCI-670", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:12 AM", "updated": "08/Dec/25 11:40 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical || APPRISS Transform query ARDM Tender", "description": "h3. Context\n\nDevelop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.\n\n*Objects to Develop:*\n\n* ORDER_HEADER - done\n* EVENT \n* ITEM\n* TENDER\n* REFERENCE_CUSTOMER\n* REFERENCE_PRODUCT\n* REFERENCE_LOCATION\n* Placeholder - SUMMARY TABLE\n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* Transformation script from source to generate APPRISS output\n\nh3. Acceptance criteria\n\n* SQL transformation models are created and committed for all eight objects listed.\n* Each transform query follows the standard CTE and naming convention structure.\n* Data types and key fields are consistent across all models.\n* Transformations run successfully with no data loss or schema mismatches.\n* QA validation confirms accurate mapping and record counts\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "+*ARDM_TENDER*+\n\n \n\nQuery build is complete based on my understanding and documentation available. I have got  few question for Business Team/APPRISS\n\n \n\n# Amount : if electronic payment exists then electronic payment else sum of other payments\n# Type: One invoice will have only one type : direction order will be as below\n\nCASH, CREDIT, CHEQUE, ACCOUNT\n\n# Card Expiry is defaulted to 12/99 . Is that correct? We don’t have card expiry date captured anyway.\n# Cash back is mapped to Cashout Amount . Is it correct? What is the definition?\n# Authorization code  is not correctly mapped Electronic Ref is hashed card key. It should be mapped to Approved code. changed in my query\n# Tender data will be null for any payment other that electronic payment.\n# Gift Balance is always null. It should be mapped with Gift Balance in Tender Data. Changed accordingly.\n# 'card_number' hard coded as 'TOKENIZED' , it should be electronicref which is hashed Key of actual card number. Changed accordingly.\n# Bin Number was wrongly mapped . Now it is mapped to Leading 9 characters of the hashed Card number\n# electronic_receipt is mapped blank . It should be the actual electronic receipt? Mapped to Electronic receipt. Please let me know in case it needs to be blank \n\nSample Data", "text": "Summary\nAppriss Tactical || APPRISS Transform query ARDM Tender\n\n---\n\nDescription\nh3. Context\n\nDevelop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.\n\n*Objects to Develop:*\n\n* ORDER_HEADER - done\n* EVENT \n* ITEM\n* TENDER\n* REFERENCE_CUSTOMER\n* REFERENCE_PRODUCT\n* REFERENCE_LOCATION\n* Placeholder - SUMMARY TABLE\n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* Transformation script from source to generate APPRISS output\n\nh3. Acceptance criteria\n\n* SQL transformation models are created and committed for all eight objects listed.\n* Each transform query follows the standard CTE and naming convention structure.\n* Data types and key fields are consistent across all models.\n* Transformations run successfully with no data loss or schema mismatches.\n* QA validation confirms accurate mapping and record counts\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n+*ARDM_TENDER*+\n\n \n\nQuery build is complete based on my understanding and documentation available. I have got  few question for Business Team/APPRISS\n\n \n\n# Amount : if electronic payment exists then electronic payment else sum of other payments\n# Type: One invoice will have only one type : direction order will be as below\n\nCASH, CREDIT, CHEQUE, ACCOUNT\n\n# Card Expiry is defaulted to 12/99 . Is that correct? We don’t have card expiry date captured anyway.\n# Cash back is mapped to Cashout Amount . Is it correct? What is the definition?\n# Authorization code  is not correctly mapped Electronic Ref is hashed card key. It should be mapped to Approved code. changed in my query\n# Tender data will be null for any payment other that electronic payment.\n# Gift Balance is always null. It should be mapped with Gift Balance in Tender Data. Changed accordingly.\n# 'card_number' hard coded as 'TOKENIZED' , it should be electronicref which is hashed Key of actual card number. Changed accordingly.\n# Bin Number was wrongly mapped . Now it is mapped to Leading 9 characters of the hashed Card number\n# electronic_receipt is mapped blank . It should be the actual electronic receipt? Mapped to Electronic receipt. Please let me know in case it needs to be blank \n\nSample Data", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 46}}
{"issue_key": "CSCI-669", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:11 AM", "updated": "21/Nov/25 1:47 PM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical || APPRISS Transform query ARDM Item", "description": "h3. Context\n\nDevelop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.\n\n*Objects to Develop:*\n\n* ORDER_HEADER \n* EVENT \n* ITEM\n* TENDER\n* CUSTOMER\n* REFERENCE_CUSTOMER\n* REFERENCE_PRODUCT\n* REFERENCE_LOCATION\n* SUMMARY TABLE\n\nh3. Deliverables\n\n* Transformation script from source to generate APPRISS output\n\nh3. Acceptance criteria\n\n* SQL transformation models are created and committed for all eight objects listed.\n* Each transform query follows the standard CTE and naming convention structure.\n* Data types and key fields are consistent across all models.\n* Transformations run successfully with no data loss or schema mismatches.\n* QA validation confirms accurate mapping and record counts\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "+*ARDM_ITEM*+\n\nQuery build is complete based on my understanding and documentation available. I have got  few question for Business Team/APPRISS\n\n \n\n# Order id  & ID  Same mapping. They both mapped to Invoice Id Is this expected?\n# Definition of Department: Is it Product Group? Product Category is highest level, Product Group is one level down\n# TRANSACTION_CLASS : Need definition , did not have any info in document\n# Supplier ID  : Should it not be Manufacturer/ Main Supplier . Supplier ID is internal to CWH and checked that we are not sharing any supplier name in any file. So supplier ID might not be of any use\n# We found some differences between APRISS Document and APPRISS Schema which was actually shared with APPRISS team. As per discussion with James, we are taking APPRISS Schema as bible and creating data points based on that.\n\n \n\nSample Data:", "text": "Summary\nAppriss Tactical || APPRISS Transform query ARDM Item\n\n---\n\nDescription\nh3. Context\n\nDevelop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.\n\n*Objects to Develop:*\n\n* ORDER_HEADER \n* EVENT \n* ITEM\n* TENDER\n* CUSTOMER\n* REFERENCE_CUSTOMER\n* REFERENCE_PRODUCT\n* REFERENCE_LOCATION\n* SUMMARY TABLE\n\nh3. Deliverables\n\n* Transformation script from source to generate APPRISS output\n\nh3. Acceptance criteria\n\n* SQL transformation models are created and committed for all eight objects listed.\n* Each transform query follows the standard CTE and naming convention structure.\n* Data types and key fields are consistent across all models.\n* Transformations run successfully with no data loss or schema mismatches.\n* QA validation confirms accurate mapping and record counts\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n+*ARDM_ITEM*+\n\nQuery build is complete based on my understanding and documentation available. I have got  few question for Business Team/APPRISS\n\n \n\n# Order id  & ID  Same mapping. They both mapped to Invoice Id Is this expected?\n# Definition of Department: Is it Product Group? Product Category is highest level, Product Group is one level down\n# TRANSACTION_CLASS : Need definition , did not have any info in document\n# Supplier ID  : Should it not be Manufacturer/ Main Supplier . Supplier ID is internal to CWH and checked that we are not sharing any supplier name in any file. So supplier ID might not be of any use\n# We found some differences between APRISS Document and APPRISS Schema which was actually shared with APPRISS team. As per discussion with James, we are taking APPRISS Schema as bible and creating data points based on that.\n\n \n\nSample Data:", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 47}}
{"issue_key": "CSCI-668", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:10 AM", "updated": "25/Nov/25 5:20 PM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical || APPRISS Transform query ARDM Event", "description": "h3. Context\n\nDevelop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.\n\n*Objects to Develop:*\n\n* ORDER_HEADER \n* EVENT \n* ITEM\n* TENDER\n* CUSTOMER\n* REFERENCE_CUSTOMER\n* REFERENCE_PRODUCT\n* REFERENCE_LOCATION\n* SUMMARY TABLE\n\nh3. Deliverables\n\n* Transformation script from source to generate APPRISS output\n\nh3. Acceptance criteria\n\n* SQL transformation models are created and committed for all eight objects listed.\n* Each transform query follows the standard CTE and naming convention structure.\n* Data types and key fields are consistent across all models.\n* Transformations run successfully with no data loss or schema mismatches.\n* QA validation confirms accurate mapping and record counts\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "@user - Is Event done but just pending business clarification to finalise?\n\n[^ARDM_EVENT.txt]\n\nFYI @user @user\n\nHi @user Han, yes, the Event is complete, pending business clarifications. Something is not looking right in the existing attachment. Reuploaded the query and shared it with Mike as well.", "text": "Summary\nAppriss Tactical || APPRISS Transform query ARDM Event\n\n---\n\nDescription\nh3. Context\n\nDevelop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.\n\n*Objects to Develop:*\n\n* ORDER_HEADER \n* EVENT \n* ITEM\n* TENDER\n* CUSTOMER\n* REFERENCE_CUSTOMER\n* REFERENCE_PRODUCT\n* REFERENCE_LOCATION\n* SUMMARY TABLE\n\nh3. Deliverables\n\n* Transformation script from source to generate APPRISS output\n\nh3. Acceptance criteria\n\n* SQL transformation models are created and committed for all eight objects listed.\n* Each transform query follows the standard CTE and naming convention structure.\n* Data types and key fields are consistent across all models.\n* Transformations run successfully with no data loss or schema mismatches.\n* QA validation confirms accurate mapping and record counts\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n@user - Is Event done but just pending business clarification to finalise?\n\n[^ARDM_EVENT.txt]\n\nFYI @user @user\n\nHi @user Han, yes, the Event is complete, pending business clarifications. Something is not looking right in the existing attachment. Reuploaded the query and shared it with Mike as well.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 48}}
{"issue_key": "CSCI-667", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:05 AM", "updated": "20/Nov/25 8:25 PM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical || Create New Schema for Tactical Solution", "description": "", "acceptance_criteria": "Given, When, Then", "comments": "Using existing datashare from POC", "text": "Summary\nAppriss Tactical || Create New Schema for Tactical Solution\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nUsing existing datashare from POC", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 49}}
{"issue_key": "CSCI-666", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:05 AM", "updated": "26/Nov/25 9:53 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical || Once Off Historical Data Load", "description": "h3. Context\n\nPending on [https://sigmahealthcare.atlassian.net/browse/CSCI-626|https://sigmahealthcare.atlassian.net/browse/CSCI-626]\n\nTo load historical data parquet\n\nh3. Acceptance criteria\n\n* Historical data showing for each table\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Pending finalised script", "text": "Summary\nAppriss Tactical || Once Off Historical Data Load\n\n---\n\nDescription\nh3. Context\n\nPending on [https://sigmahealthcare.atlassian.net/browse/CSCI-626|https://sigmahealthcare.atlassian.net/browse/CSCI-626]\n\nTo load historical data parquet\n\nh3. Acceptance criteria\n\n* Historical data showing for each table\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nPending finalised script", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 50}}
{"issue_key": "CSCI-665", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:04 AM", "updated": "21/Nov/25 1:44 PM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical Data Movement || Benchmark", "description": "", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAppriss Tactical Data Movement || Benchmark\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 51}}
{"issue_key": "CSCI-664", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:04 AM", "updated": "21/Nov/25 9:59 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical Data Movement || Ref Product", "description": "", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAppriss Tactical Data Movement || Ref Product\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 52}}
{"issue_key": "CSCI-663", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:04 AM", "updated": "21/Nov/25 9:59 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical Data Movement || Ref Location", "description": "", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAppriss Tactical Data Movement || Ref Location\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 53}}
{"issue_key": "CSCI-662", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:04 AM", "updated": "21/Nov/25 9:59 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical Data Movement || ARDM Item Discount", "description": "", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAppriss Tactical Data Movement || ARDM Item Discount\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 54}}
{"issue_key": "CSCI-661", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:04 AM", "updated": "21/Nov/25 9:59 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical Data Movement || ARDM Item", "description": "", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAppriss Tactical Data Movement || ARDM Item\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 55}}
{"issue_key": "CSCI-660", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:03 AM", "updated": "21/Nov/25 9:59 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical Data Movement || ARDM Tender", "description": "", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAppriss Tactical Data Movement || ARDM Tender\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 56}}
{"issue_key": "CSCI-659", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:03 AM", "updated": "21/Nov/25 9:59 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical Data Movement || ARDM Event", "description": "", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAppriss Tactical Data Movement || ARDM Event\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 57}}
{"issue_key": "CSCI-658", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Nov/25 11:02 AM", "updated": "21/Nov/25 9:59 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Appriss Tactical Data Movement || ARDM Header", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAppriss Tactical Data Movement || ARDM Header\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 58}}
{"issue_key": "CSCI-657", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "07/Nov/25 2:15 PM", "updated": "17/Nov/25 8:36 AM", "labels": [], "summary": "PR - Bulk load pipeline", "description": "h3. Context\n\n* Create PR for the Bulk upload pipeline.\n\n* \n\nh3. Object -\n\n* Eugene to confirm what object to configure to push the changes for global parameters\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nPR - Bulk load pipeline\n\n---\n\nDescription\nh3. Context\n\n* Create PR for the Bulk upload pipeline.\n\n* \n\nh3. Object -\n\n* Eugene to confirm what object to configure to push the changes for global parameters\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 59}}
{"issue_key": "CSCI-656", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "07/Nov/25 10:10 AM", "updated": "18/Dec/25 9:22 AM", "labels": [], "summary": "EDP Infra || Infra & Security provision for SIT UAT PROD-Sprint 13", "description": "h3. Objective\n\nProvision secure environments with consistent configurations across SIT, UAT and PROD. Achieve full environment parity and compliance alignment.\n\nh3. Steps \n\n# Replicate IaC templates for SIT, UAT, PROD\n# Validate VNet, Private Link, RBAC and logging setup\n\nh3. Deliverables\n\n* SIT, UAT, PROD environments deployed via IaC\n* Security approval sign-offs", "acceptance_criteria": "* 100% parity across SIT, UAT, PROD\n* Security posture matches PROD baseline.\n* All environments deployed via IaC (no manual setup).", "comments": "Discussed requirements with Anjali and Eugene.\n\nAnjali to raise a request to SysOps team.\n\n@user to work with @user to document the technical requirements for this stream of work.\n\nFYI @user\n\n@user Hey any update on this?\n\n@user Will look the details of the tickets raised by Anjali once have access to see other’s tickets on Service-Now .\n\n[^EDP REQUESTS HELP.docx]\n\n@user I will keep this in review until Chloe comes back, just in case she is after something more.", "text": "Summary\nEDP Infra || Infra & Security provision for SIT UAT PROD-Sprint 13\n\n---\n\nDescription\nh3. Objective\n\nProvision secure environments with consistent configurations across SIT, UAT and PROD. Achieve full environment parity and compliance alignment.\n\nh3. Steps \n\n# Replicate IaC templates for SIT, UAT, PROD\n# Validate VNet, Private Link, RBAC and logging setup\n\nh3. Deliverables\n\n* SIT, UAT, PROD environments deployed via IaC\n* Security approval sign-offs\n\n---\n\nAcceptance Criteria\n* 100% parity across SIT, UAT, PROD\n* Security posture matches PROD baseline.\n* All environments deployed via IaC (no manual setup).\n\n---\n\nComments\nDiscussed requirements with Anjali and Eugene.\n\nAnjali to raise a request to SysOps team.\n\n@user to work with @user to document the technical requirements for this stream of work.\n\nFYI @user\n\n@user Hey any update on this?\n\n@user Will look the details of the tickets raised by Anjali once have access to see other’s tickets on Service-Now .\n\n[^EDP REQUESTS HELP.docx]\n\n@user I will keep this in review until Chloe comes back, just in case she is after something more.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 60}}
{"issue_key": "CSCI-655", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "07/Nov/25 9:55 AM", "updated": "17/Nov/25 8:45 AM", "labels": ["Ingestion"], "summary": "Stage to Bronze Migration - Sprint 12", "description": "h3. Context\n\n* This is to implement the strategic solution and update the imported tables to move to BRONZE schema from STG schema except Supply Chain tactical solution schema. \n\nh3. Objective\n\n* Migrate tables to “Bronze” schema and update ALTIDA DATA IN configurations and Azure pipelines to load data in “Bronze” schema.\n\n|*Database* |*No of tables*|\n|AXLINK|1|\n|CWMgtStoreInvoices|5|\n|General_Reference|2|\n|ILS|13|\n|SCAX2012|10|\n|SKU|7|\n|SpsWhsPurchase|6|\n|StockDb|26|\n|StoreOrders|2|\n|TransactionStorage|11|\n\n \n\nh3. Steps \n\n# Create the new schema “Bronze”\n# Clone all the tables & views to from all STG_<Source Name> to “Bronze” Schema.\n# Update ALTIDA configuration for all the tables in each data source \n# Test the pipeline is able to successfully load the data to tables \n# \n\nh3. Deliverables\n\n* -Create New Schema-\n* -Clone Objects -\n* -Update ALTIDA configuration -\n* -Update & Test ADF pipelines loading data in the “Bronze” tables-\n*", "acceptance_criteria": "Complete the deliverables for each source. \n\n* -TransactionStorage -\n* -StockDb-", "comments": "Discussing with Mike about the table names standards now that all the tables will be in the same schema\n\nNaming convention finalised and will resume the task. Have to start from the beginning.\n\nBlock by [https://sigmahealthcare.atlassian.net/browse/CSCI-688|https://sigmahealthcare.atlassian.net/browse/CSCI-688]\n\nCompleted except two pipelines were not run end to end due to the number of records. Need to check with Database team on the plan for those", "text": "Summary\nStage to Bronze Migration - Sprint 12\n\n---\n\nDescription\nh3. Context\n\n* This is to implement the strategic solution and update the imported tables to move to BRONZE schema from STG schema except Supply Chain tactical solution schema. \n\nh3. Objective\n\n* Migrate tables to “Bronze” schema and update ALTIDA DATA IN configurations and Azure pipelines to load data in “Bronze” schema.\n\n|*Database* |*No of tables*|\n|AXLINK|1|\n|CWMgtStoreInvoices|5|\n|General_Reference|2|\n|ILS|13|\n|SCAX2012|10|\n|SKU|7|\n|SpsWhsPurchase|6|\n|StockDb|26|\n|StoreOrders|2|\n|TransactionStorage|11|\n\n \n\nh3. Steps \n\n# Create the new schema “Bronze”\n# Clone all the tables & views to from all STG_<Source Name> to “Bronze” Schema.\n# Update ALTIDA configuration for all the tables in each data source \n# Test the pipeline is able to successfully load the data to tables \n# \n\nh3. Deliverables\n\n* -Create New Schema-\n* -Clone Objects -\n* -Update ALTIDA configuration -\n* -Update & Test ADF pipelines loading data in the “Bronze” tables-\n*\n\n---\n\nAcceptance Criteria\nComplete the deliverables for each source. \n\n* -TransactionStorage -\n* -StockDb-\n\n---\n\nComments\nDiscussing with Mike about the table names standards now that all the tables will be in the same schema\n\nNaming convention finalised and will resume the task. Have to start from the beginning.\n\nBlock by [https://sigmahealthcare.atlassian.net/browse/CSCI-688|https://sigmahealthcare.atlassian.net/browse/CSCI-688]\n\nCompleted except two pipelines were not run end to end due to the number of records. Need to check with Database team on the plan for those", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 61}}
{"issue_key": "CSCI-653", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "06/Nov/25 8:49 AM", "updated": "14/Nov/25 7:48 AM", "labels": [], "summary": "Appriss Tactical || Sync TransactionReturns PDB14 to PBI05", "description": "h3. Context\n\n* This task is part of the _Appriss Tactical Delivery_ initiative. \n\nh3. Objective\n\n* The objective is to synchronize table called TransactionReturns from PDB14.TransactionStorage to PBI05.TransactionsArchive \n\nh3. Approach\n\n* a scheduled job will run every 30 (it can be changed) to trigger ETL that perform a merge (Insert or Update) for the last two days of the data\n\nh3. Acceptance criteria\n\n* Synchronized Table from SQL3 → PBI05\n* Data validation", "acceptance_criteria": "Given, When, Then", "comments": "Objects created\n\nTable | [TransactionReturns] in PBI05.Trasna..Arch..\n\nSAJ | called APPRISS_Sync_DB14_ToBI05_TransactionReturns located IN D/PIS01\n\nSSIS | Sync_DB14_ToBI05_TransactionReturns.dtsx\n\nraising CR for deployment\n\n@user @user @user \n\nA CR has been raised, once approved we will deploy it to prod \n\n[CHG0052911 | Change Request | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/change_request.do%3Fsys_id%3D0de4c50733c5b25047764f945d5c7b0f%26sysparm_view%3D%26sysparm_domain%3Dnull%26sysparm_domain_scope%3Dnull]\n\ndeployed \nnext is to monitor and validate the data\n\nwill confirm the result tomw", "text": "Summary\nAppriss Tactical || Sync TransactionReturns PDB14 to PBI05\n\n---\n\nDescription\nh3. Context\n\n* This task is part of the _Appriss Tactical Delivery_ initiative. \n\nh3. Objective\n\n* The objective is to synchronize table called TransactionReturns from PDB14.TransactionStorage to PBI05.TransactionsArchive \n\nh3. Approach\n\n* a scheduled job will run every 30 (it can be changed) to trigger ETL that perform a merge (Insert or Update) for the last two days of the data\n\nh3. Acceptance criteria\n\n* Synchronized Table from SQL3 → PBI05\n* Data validation\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nObjects created\n\nTable | [TransactionReturns] in PBI05.Trasna..Arch..\n\nSAJ | called APPRISS_Sync_DB14_ToBI05_TransactionReturns located IN D/PIS01\n\nSSIS | Sync_DB14_ToBI05_TransactionReturns.dtsx\n\nraising CR for deployment\n\n@user @user @user \n\nA CR has been raised, once approved we will deploy it to prod \n\n[CHG0052911 | Change Request | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/change_request.do%3Fsys_id%3D0de4c50733c5b25047764f945d5c7b0f%26sysparm_view%3D%26sysparm_domain%3Dnull%26sysparm_domain_scope%3Dnull]\n\ndeployed \nnext is to monitor and validate the data\n\nwill confirm the result tomw", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 62}}
{"issue_key": "CSCI-652", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Nov/25 11:32 AM", "updated": "21/Nov/25 5:07 PM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Issue Investigation - Issue 13", "description": "*Objective*\nInvestigate and resolve the rounding discrepancy in *Invoice ID: 35358912688554000*, where the total amount in the header ($100.00) does not align with the sum of the line items (approximately $19.00). The tender record also reflects $100.00, suggesting a data integrity or rounding issue between header, line item, and tender tables.\n\n*Steps*\n\n# Confirm with stakeholders whether the total amount should always equal the sum of all line items (excluding rounding tolerances) in the secure database schema.\n# Data Engineering team to validate if the discrepancy originates from:\n#* Source system calculation or rounding logic, or\n#* ETL transformation or aggregation logic in the current data pipeline.\n# Identify all affected invoices exhibiting similar discrepancies.", "acceptance_criteria": "*Acceptance Criteria*\n\n* *Root Cause Identified:* Source of the discrepancy (source system vs ETL logic) is clearly documented.\n* *Fix Implemented:* Data logic corrected and validated so that header totals reconcile with summed line items (within accepted rounding limits).\n* *Validation Passed:* QA confirms that post-fix, total header and item-level values are consistent across test invoices.", "comments": "AC Nov 4: CW to investigate\n\nAC Nov 18: Total value received by staff from Customer : $100\n\nMode of Payment: Cash\n\nLine Item Total: $15.98\n\nRounding: $.02\n\nTotal Line Item: $16\n\nChange given to Customer: $84\n\nInvoice Header Amount value and Invoice Line item amount sum may differ whenever there is a cash payment involved. In this invoice:35358912688554000 if someone pays cash as payment and gives $100 to the cashier for products worth $16. Then the Total Amount from the product line item would be $16, and Header Invoice would be $100. It does not take the change returned ($84 ) into account.", "text": "Summary\nAPPRISS Issue Investigation - Issue 13\n\n---\n\nDescription\n*Objective*\nInvestigate and resolve the rounding discrepancy in *Invoice ID: 35358912688554000*, where the total amount in the header ($100.00) does not align with the sum of the line items (approximately $19.00). The tender record also reflects $100.00, suggesting a data integrity or rounding issue between header, line item, and tender tables.\n\n*Steps*\n\n# Confirm with stakeholders whether the total amount should always equal the sum of all line items (excluding rounding tolerances) in the secure database schema.\n# Data Engineering team to validate if the discrepancy originates from:\n#* Source system calculation or rounding logic, or\n#* ETL transformation or aggregation logic in the current data pipeline.\n# Identify all affected invoices exhibiting similar discrepancies.\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria*\n\n* *Root Cause Identified:* Source of the discrepancy (source system vs ETL logic) is clearly documented.\n* *Fix Implemented:* Data logic corrected and validated so that header totals reconcile with summed line items (within accepted rounding limits).\n* *Validation Passed:* QA confirms that post-fix, total header and item-level values are consistent across test invoices.\n\n---\n\nComments\nAC Nov 4: CW to investigate\n\nAC Nov 18: Total value received by staff from Customer : $100\n\nMode of Payment: Cash\n\nLine Item Total: $15.98\n\nRounding: $.02\n\nTotal Line Item: $16\n\nChange given to Customer: $84\n\nInvoice Header Amount value and Invoice Line item amount sum may differ whenever there is a cash payment involved. In this invoice:35358912688554000 if someone pays cash as payment and gives $100 to the cashier for products worth $16. Then the Total Amount from the product line item would be $16, and Header Invoice would be $100. It does not take the change returned ($84 ) into account.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 63}}
{"issue_key": "CSCI-641", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Nov/25 11:07 AM", "updated": "18/Dec/25 11:12 AM", "labels": ["SupplyChainP1"], "summary": "Data Modelling || FactWarehouseInventoryHistory", "description": "Draw physical star schema design (Platinum Layer) for the fact table, defining:\n\n* Column names\n* Column types\n* Primary Keys\n* Foreign Keys\n* Granularity\n* Unique constraints\n\nPlease produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]\n\nPreliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] \n\nPreliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]", "acceptance_criteria": "Given, When, Then", "comments": "[https://lucid.app/lucidchart/ab68325d-d632-4ea7-b305-a8d40606bc5c/edit?invitationId=inv_dfe592d5-a572-47ea-870c-995eeec337a1&page=9ZadQqz2rMGN#|https://lucid.app/lucidchart/ab68325d-d632-4ea7-b305-a8d40606bc5c/edit?invitationId=inv_dfe592d5-a572-47ea-870c-995eeec337a1&page=9ZadQqz2rMGN#] \n\nTo be reviewed by @user", "text": "Summary\nData Modelling || FactWarehouseInventoryHistory\n\n---\n\nDescription\nDraw physical star schema design (Platinum Layer) for the fact table, defining:\n\n* Column names\n* Column types\n* Primary Keys\n* Foreign Keys\n* Granularity\n* Unique constraints\n\nPlease produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]\n\nPreliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] \n\nPreliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n[https://lucid.app/lucidchart/ab68325d-d632-4ea7-b305-a8d40606bc5c/edit?invitationId=inv_dfe592d5-a572-47ea-870c-995eeec337a1&page=9ZadQqz2rMGN#|https://lucid.app/lucidchart/ab68325d-d632-4ea7-b305-a8d40606bc5c/edit?invitationId=inv_dfe592d5-a572-47ea-870c-995eeec337a1&page=9ZadQqz2rMGN#] \n\nTo be reviewed by @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 64}}
{"issue_key": "CSCI-640", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Nov/25 11:07 AM", "updated": "02/Dec/25 8:55 AM", "labels": ["SupplyChainP1"], "summary": "Data Modelling || FactWarehouseInventoryIntra", "description": "Draw physical star schema design (Platinum Layer) for the fact table, defining:\n\n* Column names\n* Column types\n* Primary Keys\n* Foreign Keys\n* Granularity\n* Unique constraints\n\nPlease produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]\n\nPreliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] \n\nPreliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nData Modelling || FactWarehouseInventoryIntra\n\n---\n\nDescription\nDraw physical star schema design (Platinum Layer) for the fact table, defining:\n\n* Column names\n* Column types\n* Primary Keys\n* Foreign Keys\n* Granularity\n* Unique constraints\n\nPlease produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]\n\nPreliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] \n\nPreliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 65}}
{"issue_key": "CSCI-638", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Nov/25 11:07 AM", "updated": "02/Dec/25 8:55 AM", "labels": ["SupplyChainP1"], "summary": "Data Modelling || FactStoreInventoryHistory", "description": "Draw physical star schema design (Platinum Layer) for the fact table, defining:\n\n* Column names\n* Column types\n* Primary Keys\n* Foreign Keys\n* Granularity\n* Unique constraints\n\nPlease produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]\n\nPreliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] \n\nPreliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nData Modelling || FactStoreInventoryHistory\n\n---\n\nDescription\nDraw physical star schema design (Platinum Layer) for the fact table, defining:\n\n* Column names\n* Column types\n* Primary Keys\n* Foreign Keys\n* Granularity\n* Unique constraints\n\nPlease produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]\n\nPreliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] \n\nPreliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 66}}
{"issue_key": "CSCI-637", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Nov/25 11:06 AM", "updated": "25/Nov/25 4:59 PM", "labels": ["SupplyChainP1"], "summary": "Data Modelling || FactStoreInventoryIntra", "description": "Draw physical star schema design (Platinum Layer) for the fact table, defining:\n\n* Column names\n* Column types\n* Primary Keys\n* Foreign Keys\n* Granularity\n* Unique constraints\n\nPlease produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]\n\nPreliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] \n\nPreliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nData Modelling || FactStoreInventoryIntra\n\n---\n\nDescription\nDraw physical star schema design (Platinum Layer) for the fact table, defining:\n\n* Column names\n* Column types\n* Primary Keys\n* Foreign Keys\n* Granularity\n* Unique constraints\n\nPlease produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]\n\nPreliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] \n\nPreliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 67}}
{"issue_key": "CSCI-635", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Nov/25 11:03 AM", "updated": "10/Dec/25 9:53 AM", "labels": ["APPRISSModelling"], "summary": "Data Modelling || FactSalesRetailElectronicPayment", "description": "Draw physical star schema design (Platinum Layer) for the fact table, defining:\n\n* Column names\n* Column types\n* Primary Keys\n* Foreign Keys\n* Granularity\n* Unique constraints\n\nPlease produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]\n\nPreliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] \n\nPreliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nData Modelling || FactSalesRetailElectronicPayment\n\n---\n\nDescription\nDraw physical star schema design (Platinum Layer) for the fact table, defining:\n\n* Column names\n* Column types\n* Primary Keys\n* Foreign Keys\n* Granularity\n* Unique constraints\n\nPlease produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]\n\nPreliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] \n\nPreliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 68}}
{"issue_key": "CSCI-633", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Nov/25 10:59 AM", "updated": "21/Nov/25 4:07 PM", "labels": ["SupplyChainP1"], "summary": "Data Modelling || DimWarehouseLocation", "description": "Draw physical star schema design (Platinum Layer) for the fact table, defining:\n\n* Column names\n* Column types\n* Primary Keys\n* Foreign Keys\n* Granularity\n* Unique constraints\n\nPlease produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]\n\nPreliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] \n\nPreliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]", "acceptance_criteria": "Given, When, Then", "comments": "Hi @user - please have a review of the below and let me know if you’re happy for me to add this to our Enterprise Data Model.\n\nThanks!\n\n!CW Data Modelling.png|width=369,alt=\"CW Data Modelling.png\"!\n\nCreated model based off of current Supply Chain reporting + Chat GPT suggestions\n\n@user Nice one, happy for you to update confluence.\n\nUpdated final version to include sample data for some of the descriptive columns (included columns that could be interpreted ambiguously)\n\n!CW Data Modelling (1).png|width=414,alt=\"CW Data Modelling (1).png\"!", "text": "Summary\nData Modelling || DimWarehouseLocation\n\n---\n\nDescription\nDraw physical star schema design (Platinum Layer) for the fact table, defining:\n\n* Column names\n* Column types\n* Primary Keys\n* Foreign Keys\n* Granularity\n* Unique constraints\n\nPlease produce diagram using Lucid Charts. Here is example of expected format: [Enterprise Data Model - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimEmployee]\n\nPreliminary data mappings and early design for Supply Chain: [Silver Dimension Tables Mapping Info.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2097EB3A-B6C8-402C-85D8-2FE97E6D4BC6%7D&file=Silver%20Dimension%20Tables%20Mapping%20Info.xlsx&action=default&mobileredirect=true] \n\nPreliminary data mappings and early design for Retail: [MergeCo Data Team - Retail Data Model - Silver - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F02%2E%20Commercial%20Data%20Platform%2FRetail%20Data%20Model%20%2D%20Silver&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx&noAuthRedirect=1]\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nHi @user - please have a review of the below and let me know if you’re happy for me to add this to our Enterprise Data Model.\n\nThanks!\n\n!CW Data Modelling.png|width=369,alt=\"CW Data Modelling.png\"!\n\nCreated model based off of current Supply Chain reporting + Chat GPT suggestions\n\n@user Nice one, happy for you to update confluence.\n\nUpdated final version to include sample data for some of the descriptive columns (included columns that could be interpreted ambiguously)\n\n!CW Data Modelling (1).png|width=414,alt=\"CW Data Modelling (1).png\"!", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 69}}
{"issue_key": "CSCI-632", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Nov/25 11:35 AM", "updated": "11/Nov/25 9:44 AM", "labels": [], "summary": "EDP Infra || Adding Static Application Security Scan to ADF CI pipeline", "description": "h3. Objective\n\nIntegrate Static Application Security Testing into ADF CI pipeline to automatically scan ADF resource templates for security vulnerabilities and non-compliance before deployment.\n\n----\n\nh3. Steps\n\n----\n\nh3. Deliverables\n\n* Updated CI/CD YAML pipeline with a dedicated SAST scan step.\n* Configuration files for the SAST tool defining scan rules.\n* Documentation on how to view and address SAST scan results.\n\n----", "acceptance_criteria": "* -The ADF CI pipeline successfully executes the static security scan on every run.-\n* -The scan covers all critical ADF components.-\n* -Fail build on Critical Vulnerabilities-\n* -Security Report Published-", "comments": "see code tracking task: [Task 212773 Add Static Security Scan to ADF CI Pipeline|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/212773]\n\nPR submitted: [Pull request 23128: Implemented #212773 - Update ADF CI pipeline configuration for improved security scanning - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/a9d4deeb-ad55-465e-9425-438e29d991d5/pullrequest/23128]", "text": "Summary\nEDP Infra || Adding Static Application Security Scan to ADF CI pipeline\n\n---\n\nDescription\nh3. Objective\n\nIntegrate Static Application Security Testing into ADF CI pipeline to automatically scan ADF resource templates for security vulnerabilities and non-compliance before deployment.\n\n----\n\nh3. Steps\n\n----\n\nh3. Deliverables\n\n* Updated CI/CD YAML pipeline with a dedicated SAST scan step.\n* Configuration files for the SAST tool defining scan rules.\n* Documentation on how to view and address SAST scan results.\n\n----\n\n---\n\nAcceptance Criteria\n* -The ADF CI pipeline successfully executes the static security scan on every run.-\n* -The scan covers all critical ADF components.-\n* -Fail build on Critical Vulnerabilities-\n* -Security Report Published-\n\n---\n\nComments\nsee code tracking task: [Task 212773 Add Static Security Scan to ADF CI Pipeline|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/212773]\n\nPR submitted: [Pull request 23128: Implemented #212773 - Update ADF CI pipeline configuration for improved security scanning - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/a9d4deeb-ad55-465e-9425-438e29d991d5/pullrequest/23128]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 70}}
{"issue_key": "CSCI-631", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "31/Oct/25 9:58 AM", "updated": "21/Nov/25 1:43 PM", "labels": [], "summary": "Create Naming Convention for Repos", "description": "h3. Context\n\n* We need a simple, durable Git repo naming standard for the Enterprise Data Platform (EDP) that works across clouds/regions without future renames. Use lowercase *kebab-case* and avoid embedding env/region/provider in names.\n\nh3. Objective\n\n* Adopt a task-oriented pattern and pair it with CI/CD parameters for multi-cloud/region deployments.\n*Naming pattern:* {{edp-<capability>-<task>[-<asset>]}}\nExamples: {{edp-infra-provision}}, {{edp-ingest-orchestrate-pipelines}}, {{edp-transform-build-warehouse}}, {{edp-model-publish-semantic}}, {{edp-visualize-publish-dashboards}}. (Lowercase, hyphens only.)\n\nh3. Steps \n\n# \n# Publish standard and examples in Confluence; notify teams.\n# Rename existing repos to the new pattern (no env/region/provider in slug). \n# Update pipelines to use a *matrix* for cloud/region (e.g., {{CLOUD=az|aws}}, {{REGION=au-ae|eu}}).\n# Configure Terraform state backends per cloud/region: *azurerm* on Azure, *s3* on AWS. \n# (If using ADF) Deploy from *publish* artifacts/ARM templates, not raw JSON.\n\nh3. Deliverables\n\n* Documented naming standard with examples.\n* Renamed repos aligned to {{edp-<capability>-<task>[-<asset>]}}.\n* CI/CD updated to matrix by cloud/region; TF backends isolated per target; ADF using publish flow. \n\nh3. Assumptions - (Optional)\n\n* \n\nh3.", "acceptance_criteria": "* *Given* the standard, *when* repos are reviewed, *then* each name matches {{edp-<capability>-<task>[-<asset>]}} in lowercase kebab-case with no env/region/provider. \n* *Given* multi-cloud/region needs, *when* pipelines run, *then* jobs fan out via *matrix* and deploy successfully per target.\n* *Given* Terraform usage, *when* state is initialized, *then* Azure uses *azurerm* and AWS uses *s3* backends with separate keys/workspaces per region/env.\n* *Given* ADF deployments, *then* CI consumes *publish* templates/parameters to promote between environments.", "comments": "edp-infra-provision\n Infrastructure IaC for cloud resources (VNets, KV, IR, etc.)\nedp-infra-provision-snowflake\n Snowflake account objects (roles, warehouses, databases, grants) via IaC\nedp-ingest-orchestrate-pipelines\n ADF today; room for AWS (Glue/Airflow) later, selected by CI vars\nedp-transform-build-warehouse\n dbt project for transformations\nedp-model-publish-semantic\n\nedp-visualize-publish-dashboards\n Power BI reports as artifacts with CI-driven deployment\n\nand the repo structure: \n/modules # shared, cloud-agnostic\n/cloud/az/... # AU/NZ specifics\n/cloud/aws/... # future AWS\n/overlays/eu/ # EU-only policy/params shared globally (residency, keys, logs, PII rules) \n/overlays/<org>/eu # EU-only policy/params org(cwr,sigma) specific (residency, keys, logs, PII rules)\n\ncreated a new confluence page for [https://sigmahealthcare.atlassian.net/wiki/x/AQDkZw|https://sigmahealthcare.atlassian.net/wiki/x/AQDkZw]", "text": "Summary\nCreate Naming Convention for Repos\n\n---\n\nDescription\nh3. Context\n\n* We need a simple, durable Git repo naming standard for the Enterprise Data Platform (EDP) that works across clouds/regions without future renames. Use lowercase *kebab-case* and avoid embedding env/region/provider in names.\n\nh3. Objective\n\n* Adopt a task-oriented pattern and pair it with CI/CD parameters for multi-cloud/region deployments.\n*Naming pattern:* {{edp-<capability>-<task>[-<asset>]}}\nExamples: {{edp-infra-provision}}, {{edp-ingest-orchestrate-pipelines}}, {{edp-transform-build-warehouse}}, {{edp-model-publish-semantic}}, {{edp-visualize-publish-dashboards}}. (Lowercase, hyphens only.)\n\nh3. Steps \n\n# \n# Publish standard and examples in Confluence; notify teams.\n# Rename existing repos to the new pattern (no env/region/provider in slug). \n# Update pipelines to use a *matrix* for cloud/region (e.g., {{CLOUD=az|aws}}, {{REGION=au-ae|eu}}).\n# Configure Terraform state backends per cloud/region: *azurerm* on Azure, *s3* on AWS. \n# (If using ADF) Deploy from *publish* artifacts/ARM templates, not raw JSON.\n\nh3. Deliverables\n\n* Documented naming standard with examples.\n* Renamed repos aligned to {{edp-<capability>-<task>[-<asset>]}}.\n* CI/CD updated to matrix by cloud/region; TF backends isolated per target; ADF using publish flow. \n\nh3. Assumptions - (Optional)\n\n* \n\nh3.\n\n---\n\nAcceptance Criteria\n* *Given* the standard, *when* repos are reviewed, *then* each name matches {{edp-<capability>-<task>[-<asset>]}} in lowercase kebab-case with no env/region/provider. \n* *Given* multi-cloud/region needs, *when* pipelines run, *then* jobs fan out via *matrix* and deploy successfully per target.\n* *Given* Terraform usage, *when* state is initialized, *then* Azure uses *azurerm* and AWS uses *s3* backends with separate keys/workspaces per region/env.\n* *Given* ADF deployments, *then* CI consumes *publish* templates/parameters to promote between environments.\n\n---\n\nComments\nedp-infra-provision\n Infrastructure IaC for cloud resources (VNets, KV, IR, etc.)\nedp-infra-provision-snowflake\n Snowflake account objects (roles, warehouses, databases, grants) via IaC\nedp-ingest-orchestrate-pipelines\n ADF today; room for AWS (Glue/Airflow) later, selected by CI vars\nedp-transform-build-warehouse\n dbt project for transformations\nedp-model-publish-semantic\n\nedp-visualize-publish-dashboards\n Power BI reports as artifacts with CI-driven deployment\n\nand the repo structure: \n/modules # shared, cloud-agnostic\n/cloud/az/... # AU/NZ specifics\n/cloud/aws/... # future AWS\n/overlays/eu/ # EU-only policy/params shared globally (residency, keys, logs, PII rules) \n/overlays/<org>/eu # EU-only policy/params org(cwr,sigma) specific (residency, keys, logs, PII rules)\n\ncreated a new confluence page for [https://sigmahealthcare.atlassian.net/wiki/x/AQDkZw|https://sigmahealthcare.atlassian.net/wiki/x/AQDkZw]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 71}}
{"issue_key": "CSCI-630", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "31/Oct/25 9:42 AM", "updated": "07/Nov/25 10:10 AM", "labels": [], "summary": "Tech Requirement capture for Create Request for Azure Subscription SIT/ UAT", "description": "Clone from [https://sigmahealthcare.atlassian.net/browse/CSCI-274|https://sigmahealthcare.atlassian.net/browse/CSCI-274] to capture effort in capturing tech requirement on this card\n\n[^Technical Requirements- SIT & UAT.docx]\n\nCreate request for creation of SIT and UAT Azure Subscription for Data Platform with the following RBAC:\n\nh4. 1. Subscription Provisioning\n\n* Ensure each subscription includes:\n** Proper subnets provisioned within their respective VNets.\n** Associated Key Vaults, blob storage, and other foundational resources.\n\nh4. 2. Access Management\n\n* Azure AD user groups created for:\n** Contributors\n** Readers\n** Any other roles required for environment access.\n* Assign appropriate RBAC roles to these groups within the new subscriptions.\n\nh4. 3. Firewall Configuration\n\n* Firewall rules updates to:\n** Allow SIT and UAT subnets access to on-premises resources.\n** Enable outbound access from SIT and UAT to the internet.\n** Specifically allow Self-hosted Integration Runtime (SHIR) outbound access for software installation.\n** Existing patterns for the dev environment are followed in SIT/UAT\n\nh4. 4. Routing & Patterns\n\n* Apply existing firewall routing patterns used in other environments to maintain consistency.\n* Ensure both on-prem and Azure firewall rules are updated accordingly.\n\nReference from prev raised tickets:\n\n[+Feature 146776+|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/146776]{{: Azure Subscriptions}}\n\n[+Task 148083+|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/148083]{{: Create Request for Dev Azure Subscription Creation- REQ0136676 created}}\n\n[+Task 148097+|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/148097]{{: Create QA Azure Subscription}}\n\n[+Task 148114+|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/148114]{{: Create Request to Configure Azure Firewall}}", "acceptance_criteria": "h4. 1. *Subscription Provisioning*\n\n* Ensure each subscription includes:\n** Proper *subnets* provisioned within their respective *VNets*.\n** Associated *Key Vaults*, *blob storage*, and other foundational resources.\n\nh4. 2. *Access Management*\n\n* *Azure AD user groups created* for:\n** *Contributors*\n** *Readers*\n** Any other roles required for environment access.\n* Assign appropriate *RBAC roles* to these groups within the new subscriptions.\n\nh4. 3. *Firewall Configuration*\n\n* *Firewall rules* updates to:\n** Allow *SIT and UAT subnets* access to *on-premises resources*.\n** Enable *outbound access* from SIT and UAT to the internet.\n** Specifically allow *Self-hosted Integration Runtime (SHIR)* outbound access for software installation.\n\nh4. 4. *Routing & Patterns*\n\n* Apply existing *firewall routing patterns* used in other environments to maintain consistency.\n* Ensure both *on-prem* and *Azure firewall rules* are updated accordingly.", "comments": "", "text": "Summary\nTech Requirement capture for Create Request for Azure Subscription SIT/ UAT\n\n---\n\nDescription\nClone from [https://sigmahealthcare.atlassian.net/browse/CSCI-274|https://sigmahealthcare.atlassian.net/browse/CSCI-274] to capture effort in capturing tech requirement on this card\n\n[^Technical Requirements- SIT & UAT.docx]\n\nCreate request for creation of SIT and UAT Azure Subscription for Data Platform with the following RBAC:\n\nh4. 1. Subscription Provisioning\n\n* Ensure each subscription includes:\n** Proper subnets provisioned within their respective VNets.\n** Associated Key Vaults, blob storage, and other foundational resources.\n\nh4. 2. Access Management\n\n* Azure AD user groups created for:\n** Contributors\n** Readers\n** Any other roles required for environment access.\n* Assign appropriate RBAC roles to these groups within the new subscriptions.\n\nh4. 3. Firewall Configuration\n\n* Firewall rules updates to:\n** Allow SIT and UAT subnets access to on-premises resources.\n** Enable outbound access from SIT and UAT to the internet.\n** Specifically allow Self-hosted Integration Runtime (SHIR) outbound access for software installation.\n** Existing patterns for the dev environment are followed in SIT/UAT\n\nh4. 4. Routing & Patterns\n\n* Apply existing firewall routing patterns used in other environments to maintain consistency.\n* Ensure both on-prem and Azure firewall rules are updated accordingly.\n\nReference from prev raised tickets:\n\n[+Feature 146776+|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/146776]{{: Azure Subscriptions}}\n\n[+Task 148083+|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/148083]{{: Create Request for Dev Azure Subscription Creation- REQ0136676 created}}\n\n[+Task 148097+|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/148097]{{: Create QA Azure Subscription}}\n\n[+Task 148114+|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/148114]{{: Create Request to Configure Azure Firewall}}\n\n---\n\nAcceptance Criteria\nh4. 1. *Subscription Provisioning*\n\n* Ensure each subscription includes:\n** Proper *subnets* provisioned within their respective *VNets*.\n** Associated *Key Vaults*, *blob storage*, and other foundational resources.\n\nh4. 2. *Access Management*\n\n* *Azure AD user groups created* for:\n** *Contributors*\n** *Readers*\n** Any other roles required for environment access.\n* Assign appropriate *RBAC roles* to these groups within the new subscriptions.\n\nh4. 3. *Firewall Configuration*\n\n* *Firewall rules* updates to:\n** Allow *SIT and UAT subnets* access to *on-premises resources*.\n** Enable *outbound access* from SIT and UAT to the internet.\n** Specifically allow *Self-hosted Integration Runtime (SHIR)* outbound access for software installation.\n\nh4. 4. *Routing & Patterns*\n\n* Apply existing *firewall routing patterns* used in other environments to maintain consistency.\n* Ensure both *on-prem* and *Azure firewall rules* are updated accordingly.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 72}}
{"issue_key": "CSCI-629", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Oct/25 1:40 PM", "updated": "07/Nov/25 2:15 PM", "labels": [], "summary": "Review Password Expiry Policy for all database accounts in DEV", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "@user - can you please confirm? thx\n\nDBA confirmed password expiry is set for only user and requested them to remove that", "text": "Summary\nReview Password Expiry Policy for all database accounts in DEV\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n@user - can you please confirm? thx\n\nDBA confirmed password expiry is set for only user and requested them to remove that", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 73}}
{"issue_key": "CSCI-628", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Oct/25 9:11 AM", "updated": "18/Nov/25 4:21 AM", "labels": [], "summary": "EDP infra|| Update IaC to Apply Subnet Change for Snowflake SSO PrivateLink Access", "description": "*Objective*\nIncorporate the subnet configuration changes into the existing Infrastructure-as-Code (IaC) to support Snowflake SSO connectivity via PrivateLink. Ensure IaC reflects the updated subnet design validated during connectivity testing.\n\n*Steps*\n\n* Review the subnet configuration validated by Raveen and Chloe.\n* Update the IaC modules/templates to include the new subnet configuration.\n* Validate the PrivateLink access from Azure environment using the updated IaC.\n* Commit and push the changes to the IaC repository in Azure DevOps.\n* Notify stakeholders once updates are deployed and verified.\n\n*Deliverables*\n\n* Updated IaC configuration reflecting new subnet setup.\n* Verified PrivateLink access for Snowflake SSO post-deployment.\n* Version-controlled IaC changes in Azure DevOps repository.", "acceptance_criteria": "*Acceptance Criteria*\n\n* IaC successfully deploys infrastructure with updated subnet configuration.\n* Snowflake SSO PrivateLink access verified post-deployment.\n* No manual changes required outside IaC.\n* Changes reviewed and approved by Chloe and Raveen.", "comments": "@user just want to confirm, if you remove the setting, does the snowflake private endpoint not become accessible to the VDI browser?\n\nHey @user , yes, I think so. Especially in a SSO scenario\n\nsee [Task 212755 Update Snowflake subnet private endpoint policy|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/212755] for development.\n\n@user can you please review the PR: [Pull request 23103: Implemented #212755 - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-snowflake-infrastructure/pullrequest/23103?_a=files]\n\ncc: @user @user\n\n@user to generate service account\n\nWaiting for deployment approval from @user\n\nI believe this change is done. Can you pls confirm @user \n\nFYI @user", "text": "Summary\nEDP infra|| Update IaC to Apply Subnet Change for Snowflake SSO PrivateLink Access\n\n---\n\nDescription\n*Objective*\nIncorporate the subnet configuration changes into the existing Infrastructure-as-Code (IaC) to support Snowflake SSO connectivity via PrivateLink. Ensure IaC reflects the updated subnet design validated during connectivity testing.\n\n*Steps*\n\n* Review the subnet configuration validated by Raveen and Chloe.\n* Update the IaC modules/templates to include the new subnet configuration.\n* Validate the PrivateLink access from Azure environment using the updated IaC.\n* Commit and push the changes to the IaC repository in Azure DevOps.\n* Notify stakeholders once updates are deployed and verified.\n\n*Deliverables*\n\n* Updated IaC configuration reflecting new subnet setup.\n* Verified PrivateLink access for Snowflake SSO post-deployment.\n* Version-controlled IaC changes in Azure DevOps repository.\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria*\n\n* IaC successfully deploys infrastructure with updated subnet configuration.\n* Snowflake SSO PrivateLink access verified post-deployment.\n* No manual changes required outside IaC.\n* Changes reviewed and approved by Chloe and Raveen.\n\n---\n\nComments\n@user just want to confirm, if you remove the setting, does the snowflake private endpoint not become accessible to the VDI browser?\n\nHey @user , yes, I think so. Especially in a SSO scenario\n\nsee [Task 212755 Update Snowflake subnet private endpoint policy|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/212755] for development.\n\n@user can you please review the PR: [Pull request 23103: Implemented #212755 - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-snowflake-infrastructure/pullrequest/23103?_a=files]\n\ncc: @user @user\n\n@user to generate service account\n\nWaiting for deployment approval from @user\n\nI believe this change is done. Can you pls confirm @user \n\nFYI @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 74}}
{"issue_key": "CSCI-627", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Oct/25 1:43 PM", "updated": "07/Nov/25 7:53 PM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Create APPRISS Transform query", "description": "h3. Context\n\nDevelop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.\n\n*Objects to Develop:*\n\n* ORDER_HEADER - done\n* EVENT \n* ITEM\n* TENDER\n* CUSTOMER\n* REFERENCE_CUSTOMER\n* REFERENCE_PRODUCT\n* REFERENCE_LOCATION\n* Placeholder - SUMMARY TABLE\n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* Transformation script from source to generate APPRISS output\n\nh3. Acceptance criteria\n\n* SQL transformation models are created and committed for all eight objects listed.\n* Each transform query follows the standard CTE and naming convention structure.\n* Data types and key fields are consistent across all models.\n* Transformations run successfully with no data loss or schema mismatches.\n* QA validation confirms accurate mapping and record counts\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "+*ARDM_HEADER*+\n\nQuery build is complete based on my understanding and documentation available. I have got  few question for Business Team/APPRISS\n\n \n\n# Order id  & order Number Same mapping. They both mapped to Invoice Id Is this expected?\n# Definition of Total Amount :  Invoice Header Amount value and Invoice Line item amount sum may differ whenever there is a cash payment involved. An example would give you a better understanding. ( Invoice ID : 801125062148140) If someone pays cash as payment and gives $50 to cashier  for product worth $34.50 . Then Total Amount from product line item would be 34.50 and Header Invoice would be $50. It does not take change returned ($15.50 ) into account. Which field is expected? Please confirm from APPRISS\n# Total Amount, Calculated Total, Original Total : They all have same mapping . Is there any definition shared by APPRISS team for the same?\n# We found some differences between APRISS Document and APPRISS Schema which was actually shared with APPRISS team. As per discussion with James, we are taking APPRISS Schema as bible and creating data points based on that.\n\n \n\nSample Data:\n\n+*ARDM_EVENT*+\n\n \n\nQuery Building is in Progress. As discussed during call, we have got a few questions and statements\n\n \n\n# Manager id mapping is wrong. We do not have that mapping in transactions. In document, ManagerID is mapped to InForStaffID. This filed cpatures if any CWH staff purchases items from a store. It is not manager. Manager id will be populated as null.\n# Voided Flag: need definition: Is it voided if the whole invoice is cancelled ? Or any item in that invoice is voided\n# We found some differences between APRISS Document and APPRISS Schema which was actually shared with APPRISS team. As per discussion with James, we are taking APPRISS Schema as bible and creating data points based on that.\n\nClosing this card as sprint closure, remaining queries to have individual cards in the next sprint", "text": "Summary\nCreate APPRISS Transform query\n\n---\n\nDescription\nh3. Context\n\nDevelop and implement SQL transformation queries for all core APPRISS data objects to populate the *Silver (Transform)* layer from the *Bronze (Raw)* layer. This ensures consistent, validated, and standardized data structures for downstream analytics and secure database use.\n\n*Objects to Develop:*\n\n* ORDER_HEADER - done\n* EVENT \n* ITEM\n* TENDER\n* CUSTOMER\n* REFERENCE_CUSTOMER\n* REFERENCE_PRODUCT\n* REFERENCE_LOCATION\n* Placeholder - SUMMARY TABLE\n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* Transformation script from source to generate APPRISS output\n\nh3. Acceptance criteria\n\n* SQL transformation models are created and committed for all eight objects listed.\n* Each transform query follows the standard CTE and naming convention structure.\n* Data types and key fields are consistent across all models.\n* Transformations run successfully with no data loss or schema mismatches.\n* QA validation confirms accurate mapping and record counts\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n+*ARDM_HEADER*+\n\nQuery build is complete based on my understanding and documentation available. I have got  few question for Business Team/APPRISS\n\n \n\n# Order id  & order Number Same mapping. They both mapped to Invoice Id Is this expected?\n# Definition of Total Amount :  Invoice Header Amount value and Invoice Line item amount sum may differ whenever there is a cash payment involved. An example would give you a better understanding. ( Invoice ID : 801125062148140) If someone pays cash as payment and gives $50 to cashier  for product worth $34.50 . Then Total Amount from product line item would be 34.50 and Header Invoice would be $50. It does not take change returned ($15.50 ) into account. Which field is expected? Please confirm from APPRISS\n# Total Amount, Calculated Total, Original Total : They all have same mapping . Is there any definition shared by APPRISS team for the same?\n# We found some differences between APRISS Document and APPRISS Schema which was actually shared with APPRISS team. As per discussion with James, we are taking APPRISS Schema as bible and creating data points based on that.\n\n \n\nSample Data:\n\n+*ARDM_EVENT*+\n\n \n\nQuery Building is in Progress. As discussed during call, we have got a few questions and statements\n\n \n\n# Manager id mapping is wrong. We do not have that mapping in transactions. In document, ManagerID is mapped to InForStaffID. This filed cpatures if any CWH staff purchases items from a store. It is not manager. Manager id will be populated as null.\n# Voided Flag: need definition: Is it voided if the whole invoice is cancelled ? Or any item in that invoice is voided\n# We found some differences between APRISS Document and APPRISS Schema which was actually shared with APPRISS team. As per discussion with James, we are taking APPRISS Schema as bible and creating data points based on that.\n\nClosing this card as sprint closure, remaining queries to have individual cards in the next sprint", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 75}}
{"issue_key": "CSCI-626", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Oct/25 1:41 PM", "updated": "21/Nov/25 1:17 PM", "labels": [], "summary": "Create APPRISS historical file in Parquet", "description": "h3. Context\n\nTo perform once off historical load of 8 weeks data for Appriss - will require\n\n* All transform script ready in order to extract\n\nh3. Acceptance criteria\n\nParquet files generated for all appriss data share tables\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "@user from me to action this i need a query From @user\n\nPending all extraction script\n\nThis is no longer require as historical data ingestion can be done via ADF pipeline", "text": "Summary\nCreate APPRISS historical file in Parquet\n\n---\n\nDescription\nh3. Context\n\nTo perform once off historical load of 8 weeks data for Appriss - will require\n\n* All transform script ready in order to extract\n\nh3. Acceptance criteria\n\nParquet files generated for all appriss data share tables\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n@user from me to action this i need a query From @user\n\nPending all extraction script\n\nThis is no longer require as historical data ingestion can be done via ADF pipeline", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 76}}
{"issue_key": "CSCI-625", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Oct/25 1:40 PM", "updated": "20/Nov/25 8:24 PM", "labels": [], "summary": "APPRISS historical load to Snowflake", "description": "h3. Context\n\nPending on [https://sigmahealthcare.atlassian.net/browse/CSCI-626|https://sigmahealthcare.atlassian.net/browse/CSCI-626]\n\nTo load historical data parquet\n\nh3. Acceptance criteria\n\n* Historical data showing for each table\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAPPRISS historical load to Snowflake\n\n---\n\nDescription\nh3. Context\n\nPending on [https://sigmahealthcare.atlassian.net/browse/CSCI-626|https://sigmahealthcare.atlassian.net/browse/CSCI-626]\n\nTo load historical data parquet\n\nh3. Acceptance criteria\n\n* Historical data showing for each table\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 77}}
{"issue_key": "CSCI-623", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Oct/25 1:31 PM", "updated": "18/Nov/25 1:46 PM", "labels": [], "summary": "APRRISS || Service Account for PDB 14 to Azure Dev", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Ticket raised under RITM0178360\n\nRequest approved under RITM0178360\n\nNo longer require will use SSIS to populate needed table to PDB 14 which SF is currently accessible", "text": "Summary\nAPRRISS || Service Account for PDB 14 to Azure Dev\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nTicket raised under RITM0178360\n\nRequest approved under RITM0178360\n\nNo longer require will use SSIS to populate needed table to PDB 14 which SF is currently accessible", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 78}}
{"issue_key": "CSCI-622", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Oct/25 1:30 PM", "updated": "15/Dec/25 3:40 PM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS || Whitelisting for PDB 14 to Azure Dev", "description": "h3. Context\n\n* Need to whitelist on-prem server PDB14 to our SHIR (Self-Hosted Integration Runtime) so that we can ingest data into Snowflake (via ADF)\n\nh3. Objective\n\n* Need to whitelist on-prem server PDB14 to our SHIR (Self-Hosted Integration Runtime) so that we can ingest data into Snowflake (via ADF)\n* This data is required for the APPRISS project\n\nh3. Steps \n\n# Raise firewall request via ServiceNow (be sure to use the correct ticket type, otherwise it will get rejected) - [https://cwretail.service-now.com/sp?id=sc_cat_item&sys_id=4e233585db8f8050633287f43a961929|https://cwretail.service-now.com/sp?id=sc_cat_item&sys_id=4e233585db8f8050633287f43a961929] \n## If unsure how to complete the request, copy ticket *RITM0177824*\n## Specify all details in the ticket request, including the server IP address to whitelist (on-prem SQL server IP) and our SHIR IP (*172.29.84.64/27 (SHIR - cwr-ase-edp-shir-dev-vmss)*)\n# After ticket is raised it will go to the *Security* team - they will ask us lots of questions about what we’re trying to do (even though it is specified within the ticket)\n# After Security approve the ticket, it will go to the *Network* team automatically\n## Note: we need to ask the Service Desk team to create an additional task assigned to this ticket to the *Cloud* team - in the current Firewall ServiceNow ticketing flow, this does not get automatically created. We can only do this *after* Security have approved the ticket (see *RITM0177824* as an example)\n# For the whitelisting to be fully completed, both the network and cloud team need to complete their tasks\n# Once we receive confirmation that firewall listing has been done on both sides, we can test this connection in ADF\n## Note: to do this we need a service account create that can login onto PDB14, however note this has already been done and credentials are stored in the Key Vault\n### UserName: *M2PSCSQL03N2-Prod-SQL-LocalAccountName-PDB14-ReadOnlyReplica*\n### Password: *M2PSCSQL03N2-Prod-SQL-LocalAccountPassword-PDB14-ReadOnlyReplica*\n\nFor any help on creating the ServiceNow ticket/following up on assignees, speak with @user as she has helped us out with creating many firewall/whitelisting requests.\n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Ticket under - RITM0178308\n\nUpdate as of 20251117\n\nUpdate completed pending testing from team\n\n@user to test\n\n@user \n\nTesting failed - receiving a firewall error:\n\nNeed to confirm with Sam and DBA team that there’s no issues with the User Account that’s been provided\n\nTicket raise for IT Cloud Team - REQ0164316/RITM0179824\n\nI’ve raised a ticket with the DBA team to check the access to PDB14 for the service account user (ServAC_EDPADF_pSQL03). Noticed that when logged into SSMS this user doesn’t have the ability to query any tables on PDB14.\nTicket: *RITM0179858*\n\n@user - security reviewing this one now.", "text": "Summary\nAPPRISS || Whitelisting for PDB 14 to Azure Dev\n\n---\n\nDescription\nh3. Context\n\n* Need to whitelist on-prem server PDB14 to our SHIR (Self-Hosted Integration Runtime) so that we can ingest data into Snowflake (via ADF)\n\nh3. Objective\n\n* Need to whitelist on-prem server PDB14 to our SHIR (Self-Hosted Integration Runtime) so that we can ingest data into Snowflake (via ADF)\n* This data is required for the APPRISS project\n\nh3. Steps \n\n# Raise firewall request via ServiceNow (be sure to use the correct ticket type, otherwise it will get rejected) - [https://cwretail.service-now.com/sp?id=sc_cat_item&sys_id=4e233585db8f8050633287f43a961929|https://cwretail.service-now.com/sp?id=sc_cat_item&sys_id=4e233585db8f8050633287f43a961929] \n## If unsure how to complete the request, copy ticket *RITM0177824*\n## Specify all details in the ticket request, including the server IP address to whitelist (on-prem SQL server IP) and our SHIR IP (*172.29.84.64/27 (SHIR - cwr-ase-edp-shir-dev-vmss)*)\n# After ticket is raised it will go to the *Security* team - they will ask us lots of questions about what we’re trying to do (even though it is specified within the ticket)\n# After Security approve the ticket, it will go to the *Network* team automatically\n## Note: we need to ask the Service Desk team to create an additional task assigned to this ticket to the *Cloud* team - in the current Firewall ServiceNow ticketing flow, this does not get automatically created. We can only do this *after* Security have approved the ticket (see *RITM0177824* as an example)\n# For the whitelisting to be fully completed, both the network and cloud team need to complete their tasks\n# Once we receive confirmation that firewall listing has been done on both sides, we can test this connection in ADF\n## Note: to do this we need a service account create that can login onto PDB14, however note this has already been done and credentials are stored in the Key Vault\n### UserName: *M2PSCSQL03N2-Prod-SQL-LocalAccountName-PDB14-ReadOnlyReplica*\n### Password: *M2PSCSQL03N2-Prod-SQL-LocalAccountPassword-PDB14-ReadOnlyReplica*\n\nFor any help on creating the ServiceNow ticket/following up on assignees, speak with @user as she has helped us out with creating many firewall/whitelisting requests.\n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nTicket under - RITM0178308\n\nUpdate as of 20251117\n\nUpdate completed pending testing from team\n\n@user to test\n\n@user \n\nTesting failed - receiving a firewall error:\n\nNeed to confirm with Sam and DBA team that there’s no issues with the User Account that’s been provided\n\nTicket raise for IT Cloud Team - REQ0164316/RITM0179824\n\nI’ve raised a ticket with the DBA team to check the access to PDB14 for the service account user (ServAC_EDPADF_pSQL03). Noticed that when logged into SSMS this user doesn’t have the ability to query any tables on PDB14.\nTicket: *RITM0179858*\n\n@user - security reviewing this one now.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 79}}
{"issue_key": "CSCI-621", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Oct/25 11:52 AM", "updated": "07/Nov/25 4:40 PM", "labels": ["Ingestion"], "summary": "Stage to Bronze Migration - Sprint 11", "description": "h3. Context\n\n* This is to implement the strategic solution and update the imported tables to move to BRONZE schema from STG schema except Supply Chain tactical solution schema. \n\nh3. Objective\n\n* Migrate tables to “Bronze” schema and update ALTIDA DATA IN configurations and Azure pipelines to load data in “Bronze” schema.\n\n|*Database* |*No of tables*|\n|AXLINK|1|\n|CWMgtStoreInvoices|5|\n|General_Reference|2|\n|ILS|13|\n|SCAX2012|10|\n|SKU|7|\n|SpsWhsPurchase|6|\n|StockDb|26|\n|StoreOrders|2|\n|TransactionStorage|11|\n\n \n\nh3. Steps \n\n# Create the new schema “Bronze”\n# Clone all the tables & views to from all STG_<Source Name> to “Bronze” Schema.\n# Update ALTIDA configuration for all the tables in each data source \n# Test the pipeline is able to successfully load the data to tables \n# Once all the pipelines are able to load data to all the table ingested. Drop the STG_<Source Name> Schemas \n\nh3. Deliverables\n\n* Create New Schema\n* Clone Objects \n* Update ALTIDA configuration \n* Update & Test ADF pipelines loading data in the “Bronze” tables\n* Clean-up of the Stg_<Source Name>\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Complete the deliverables for each source.\n\n* -AXLINK-\n* -ILS-\n* -SKU-\n* TransactionStorage\n* -CWMgtStoreInvoices-\n* -General_Reference-\n* -SCAX2012-\n* -SpsWhsPurchase-\n* StockDb", "comments": "Discussing with Mike about the table names standards now that all the tables will be in the same schema\n\nNaming convention finalised and will resume the task. Have to start from the beginning.", "text": "Summary\nStage to Bronze Migration - Sprint 11\n\n---\n\nDescription\nh3. Context\n\n* This is to implement the strategic solution and update the imported tables to move to BRONZE schema from STG schema except Supply Chain tactical solution schema. \n\nh3. Objective\n\n* Migrate tables to “Bronze” schema and update ALTIDA DATA IN configurations and Azure pipelines to load data in “Bronze” schema.\n\n|*Database* |*No of tables*|\n|AXLINK|1|\n|CWMgtStoreInvoices|5|\n|General_Reference|2|\n|ILS|13|\n|SCAX2012|10|\n|SKU|7|\n|SpsWhsPurchase|6|\n|StockDb|26|\n|StoreOrders|2|\n|TransactionStorage|11|\n\n \n\nh3. Steps \n\n# Create the new schema “Bronze”\n# Clone all the tables & views to from all STG_<Source Name> to “Bronze” Schema.\n# Update ALTIDA configuration for all the tables in each data source \n# Test the pipeline is able to successfully load the data to tables \n# Once all the pipelines are able to load data to all the table ingested. Drop the STG_<Source Name> Schemas \n\nh3. Deliverables\n\n* Create New Schema\n* Clone Objects \n* Update ALTIDA configuration \n* Update & Test ADF pipelines loading data in the “Bronze” tables\n* Clean-up of the Stg_<Source Name>\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nComplete the deliverables for each source.\n\n* -AXLINK-\n* -ILS-\n* -SKU-\n* TransactionStorage\n* -CWMgtStoreInvoices-\n* -General_Reference-\n* -SCAX2012-\n* -SpsWhsPurchase-\n* StockDb\n\n---\n\nComments\nDiscussing with Mike about the table names standards now that all the tables will be in the same schema\n\nNaming convention finalised and will resume the task. Have to start from the beginning.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 80}}
{"issue_key": "CSCI-620", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Oct/25 11:52 AM", "updated": "19/Nov/25 1:30 PM", "labels": [], "summary": "Snowflake Service Account for IaC", "description": "This account is used by Terraform (via CI/CD pipeline) to:\n\n* Authenticate into Snowflake (usually via *key pair auth*).\n* Create, modify, and manage objects (databases, schemas, warehouses, roles, grants, integrations).\n* Apply changes across multiple environments consistently.", "acceptance_criteria": "||Area||Setting||\n|*Service User*|SVC_IAC_SNOWFLAKE|\n|*Primary Role*|{{ROLE_INFRA_ADMIN_SNOWFLAKE}}|\n|*Parent Role*|{{SECURITYADMIN}}|\n|*Auth*|RSA Key Pair|\n|*Scope*|Database, Warehouse, Role, Integration, Grant|\n|*Privileged Avoidance*|No ACCOUNTADMIN, no CREATE USER|\n|*Pipeline Integration*|Via Azure DevOps using Key Vault secrets|", "comments": "Confirmed naming convention with Mike:\n\n* User: SVC_IAC_SNOWFLAKE\n* Role: ROLE_INFRA_ADMIN_SNOWFLAKE\n\n* Chloe to script out user and role in Snowflake\n* @user to generate key pair auth using IaC Terraform. \n* Chloe/ Eugene to document to AKV manual key rotation manual process.\n\nFYI\n\n@user @user @user\n\n@user since this is a common service account across the different environments, I will create the key pair in the Snowflake Infrastructure Key Vault.\n\ncc: @user\n\nPlease see [User Story 212758 Provision Snowflake Service Account for IaC Deployments|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/212758] for code development tracking.\n\ncc: @user @user @user\n\n@user sure thanks Eugene\n\nI have created the role *ROLE_INFRA_ADMIN_SNOWFLAKE* in Snowflake (steps 1 - 3). Pending steps 4 - 5 when key pair authentication created in key vault \n\nBelow are the scripts\n\n-- 1. Create the IAC service role and data warehouse\nUSE ROLE SECURITYADMIN;\n\nCREATE ROLE IF NOT EXISTS ROLE_INFRA_ADMIN_SNOWFLAKE COMMENT = 'Role used for IaC deployments';\nCREATE WAREHOUSE IF NOT EXISTS INFRA_ADMIN_WH COMMENT = 'WH used for IaC deployments';\n\n-- 2. Grant necessary privileges\nGRANT CREATE DATABASE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE INTEGRATION ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE ROLE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE SCHEMA TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\n\n-- 3. Allow escalation only when approved\nGRANT ROLE ROLE_INFRA_ADMIN_SNOWFLAKE TO ROLE SECURITYADMIN;\n\n-- 4. Create the service account user\nCREATE USER IF NOT EXISTS SVC_IAC_SNOWFLAKE\n LOGIN_NAME = 'SVC_IAC_SNOWFLAKE'\n DISPLAY_NAME = 'SVC_IAC_SNOWFLAKE'\n DEFAULT_ROLE = ROLE_INFRA_ADMIN_SNOWFLAKE\n DEFAULT_WAREHOUSE = 'INFRA_ADMIN_WH'\n MUST_CHANGE_PASSWORD = FALSE\n DISABLED = FALSE\n RSA_PUBLIC_KEY = '<your-public-key>'\n COMMENT = 'Service account for Snowflake IaC deployments';\n\n-- 5. Grant the role to the user\nGRANT ROLE ROLE_INFRA_ADMIN_SNOWFLAKE TO USER SVC_IAC_SNOWFLAKE;\n\n@user @user @user @user\n\n@user pls advise what other account you want added to this, especially the one for dbt.\n\ncc: @user @user\n\nLe's leave the dbt for later.\n\n@user can you please approve these releases:\n[edp-snowflake-infrastructure-release|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_release?view=mine&_a=releases&definitionId=6]\n\nI think this is approved and deployed? Let me know if otherwise , thx- @user", "text": "Summary\nSnowflake Service Account for IaC\n\n---\n\nDescription\nThis account is used by Terraform (via CI/CD pipeline) to:\n\n* Authenticate into Snowflake (usually via *key pair auth*).\n* Create, modify, and manage objects (databases, schemas, warehouses, roles, grants, integrations).\n* Apply changes across multiple environments consistently.\n\n---\n\nAcceptance Criteria\n||Area||Setting||\n|*Service User*|SVC_IAC_SNOWFLAKE|\n|*Primary Role*|{{ROLE_INFRA_ADMIN_SNOWFLAKE}}|\n|*Parent Role*|{{SECURITYADMIN}}|\n|*Auth*|RSA Key Pair|\n|*Scope*|Database, Warehouse, Role, Integration, Grant|\n|*Privileged Avoidance*|No ACCOUNTADMIN, no CREATE USER|\n|*Pipeline Integration*|Via Azure DevOps using Key Vault secrets|\n\n---\n\nComments\nConfirmed naming convention with Mike:\n\n* User: SVC_IAC_SNOWFLAKE\n* Role: ROLE_INFRA_ADMIN_SNOWFLAKE\n\n* Chloe to script out user and role in Snowflake\n* @user to generate key pair auth using IaC Terraform. \n* Chloe/ Eugene to document to AKV manual key rotation manual process.\n\nFYI\n\n@user @user @user\n\n@user since this is a common service account across the different environments, I will create the key pair in the Snowflake Infrastructure Key Vault.\n\ncc: @user\n\nPlease see [User Story 212758 Provision Snowflake Service Account for IaC Deployments|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/212758] for code development tracking.\n\ncc: @user @user @user\n\n@user sure thanks Eugene\n\nI have created the role *ROLE_INFRA_ADMIN_SNOWFLAKE* in Snowflake (steps 1 - 3). Pending steps 4 - 5 when key pair authentication created in key vault \n\nBelow are the scripts\n\n-- 1. Create the IAC service role and data warehouse\nUSE ROLE SECURITYADMIN;\n\nCREATE ROLE IF NOT EXISTS ROLE_INFRA_ADMIN_SNOWFLAKE COMMENT = 'Role used for IaC deployments';\nCREATE WAREHOUSE IF NOT EXISTS INFRA_ADMIN_WH COMMENT = 'WH used for IaC deployments';\n\n-- 2. Grant necessary privileges\nGRANT CREATE DATABASE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE INTEGRATION ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE ROLE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE SCHEMA TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\n\n-- 3. Allow escalation only when approved\nGRANT ROLE ROLE_INFRA_ADMIN_SNOWFLAKE TO ROLE SECURITYADMIN;\n\n-- 4. Create the service account user\nCREATE USER IF NOT EXISTS SVC_IAC_SNOWFLAKE\n LOGIN_NAME = 'SVC_IAC_SNOWFLAKE'\n DISPLAY_NAME = 'SVC_IAC_SNOWFLAKE'\n DEFAULT_ROLE = ROLE_INFRA_ADMIN_SNOWFLAKE\n DEFAULT_WAREHOUSE = 'INFRA_ADMIN_WH'\n MUST_CHANGE_PASSWORD = FALSE\n DISABLED = FALSE\n RSA_PUBLIC_KEY = '<your-public-key>'\n COMMENT = 'Service account for Snowflake IaC deployments';\n\n-- 5. Grant the role to the user\nGRANT ROLE ROLE_INFRA_ADMIN_SNOWFLAKE TO USER SVC_IAC_SNOWFLAKE;\n\n@user @user @user @user\n\n@user pls advise what other account you want added to this, especially the one for dbt.\n\ncc: @user @user\n\nLe's leave the dbt for later.\n\n@user can you please approve these releases:\n[edp-snowflake-infrastructure-release|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_release?view=mine&_a=releases&definitionId=6]\n\nI think this is approved and deployed? Let me know if otherwise , thx- @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 81}}
{"issue_key": "CSCI-619", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "27/Oct/25 8:51 AM", "updated": "07/Nov/25 1:54 PM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Issue Investigation - Issue 12", "description": "*Objective*\n\nDetermine if the \"missing\" transaction number (like 22845953) on benchmark images is a necessary field for the final secure database schema, and if so, initiate the process to add it to the source data pipeline. \n\n*Steps*\n\n* Confirm with stakeholders if the on-receipt transaction number is required for the secure environment.\n* Dev Team to confirm if this transaction number exists in the raw source data.\n* Add transaction data to source data for", "acceptance_criteria": "*Acceptance Criteria:*\n\n* Decision Made: A clear business decision on whether the transaction number is a required field is documented.\n* Source Confirmed The team has confirmed if the missing transaction number is available in the existing raw source data.", "comments": "Need more information on how this image is genarted and what is the definition of Transaction# \n\nIn this example the invoice id is 35358913223065664\n\nOrigin TransactionID 22845953 is a ranmdom id generated at store end for one of the line items in this invoice", "text": "Summary\nAPPRISS Issue Investigation - Issue 12\n\n---\n\nDescription\n*Objective*\n\nDetermine if the \"missing\" transaction number (like 22845953) on benchmark images is a necessary field for the final secure database schema, and if so, initiate the process to add it to the source data pipeline. \n\n*Steps*\n\n* Confirm with stakeholders if the on-receipt transaction number is required for the secure environment.\n* Dev Team to confirm if this transaction number exists in the raw source data.\n* Add transaction data to source data for\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria:*\n\n* Decision Made: A clear business decision on whether the transaction number is a required field is documented.\n* Source Confirmed The team has confirmed if the missing transaction number is available in the existing raw source data.\n\n---\n\nComments\nNeed more information on how this image is genarted and what is the definition of Transaction# \n\nIn this example the invoice id is 35358913223065664\n\nOrigin TransactionID 22845953 is a ranmdom id generated at store end for one of the line items in this invoice", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 82}}
{"issue_key": "CSCI-618", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "25/Oct/25 4:06 AM", "updated": "22/Dec/25 8:27 AM", "labels": [], "summary": "rename edp-powebi-reports", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nrename edp-powebi-reports", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 83}}
{"issue_key": "CSCI-617", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "25/Oct/25 4:06 AM", "updated": "22/Dec/25 8:27 AM", "labels": [], "summary": "rename edp-snowflake", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nrename edp-snowflake", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 84}}
{"issue_key": "CSCI-616", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "25/Oct/25 4:06 AM", "updated": "22/Dec/25 8:27 AM", "labels": [], "summary": "rename edp-semantic-models", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nrename edp-semantic-models", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 85}}
{"issue_key": "CSCI-615", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "25/Oct/25 4:06 AM", "updated": "22/Dec/25 8:27 AM", "labels": [], "summary": "revise repository naming coventions to include organization and geographical location", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nrevise repository naming coventions to include organization and geographical location", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 86}}
{"issue_key": "CSCI-614", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "25/Oct/25 4:05 AM", "updated": "22/Dec/25 8:27 AM", "labels": [], "summary": "rename edp-data-factory", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nrename edp-data-factory", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 87}}
{"issue_key": "CSCI-613", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "25/Oct/25 4:05 AM", "updated": "22/Dec/25 8:27 AM", "labels": [], "summary": "rename edp-snowflake-infrastructure", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nrename edp-snowflake-infrastructure", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 88}}
{"issue_key": "CSCI-612", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "25/Oct/25 4:04 AM", "updated": "22/Dec/25 8:27 AM", "labels": [], "summary": "rename edp-infrastructure", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nrename edp-infrastructure", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 89}}
{"issue_key": "CSCI-611", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "24/Oct/25 3:34 PM", "updated": "28/Oct/25 9:53 AM", "labels": [], "summary": "MergeCo - REtail tactical reporting-Sprint 11", "description": "h3. Context\n\n* The mergeco report is a report that is used by Execs to track the health of the business. Vikesh would like a series of reports that will help with this so he can make the correct decisions on the business.\n\nh3. Objective\n\n* To help with the availability of data that is used for producing Mergeco reports\n* Tables to ingest\n** FCT_CUSTOMER_COUNT_HOURLY_AUS - Correct Data\n** FCT_CUSTOMER_COUNT_HOURLY_NZL - Correct Data\n** SubCategoryBudgetDaily - Request data as Parquet files empty\n\nh3. Steps \n\n# Ingest Dim tables \n# Ingest Fact tables \n# Ingest Historical Data\n\nh3. Deliverables\n\n* \n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Create Extract Meta for delta & historical data \"}],\"attrs\":{\"localId\":\"50df6b4c-d7a0-435c-badb-b1db13ed0f02\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Create Pipelines \"}],\"attrs\":{\"localId\":\"0be7bc70-2c7e-46d0-b93b-f03e7ee404ba\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Load historical data for big tables\"}],\"attrs\":{\"localId\":\"8ccec245-dca7-4ae9-95a5-0d86a402aed5\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"3af7df84-c13f-4a98-9ae1-d189349ee0a2\"}}\n{adf}\n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Data available for Mergeco reporting", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nMergeCo - REtail tactical reporting-Sprint 11\n\n---\n\nDescription\nh3. Context\n\n* The mergeco report is a report that is used by Execs to track the health of the business. Vikesh would like a series of reports that will help with this so he can make the correct decisions on the business.\n\nh3. Objective\n\n* To help with the availability of data that is used for producing Mergeco reports\n* Tables to ingest\n** FCT_CUSTOMER_COUNT_HOURLY_AUS - Correct Data\n** FCT_CUSTOMER_COUNT_HOURLY_NZL - Correct Data\n** SubCategoryBudgetDaily - Request data as Parquet files empty\n\nh3. Steps \n\n# Ingest Dim tables \n# Ingest Fact tables \n# Ingest Historical Data\n\nh3. Deliverables\n\n* \n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Create Extract Meta for delta & historical data \"}],\"attrs\":{\"localId\":\"50df6b4c-d7a0-435c-badb-b1db13ed0f02\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Create Pipelines \"}],\"attrs\":{\"localId\":\"0be7bc70-2c7e-46d0-b93b-f03e7ee404ba\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Load historical data for big tables\"}],\"attrs\":{\"localId\":\"8ccec245-dca7-4ae9-95a5-0d86a402aed5\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"3af7df84-c13f-4a98-9ae1-d189349ee0a2\"}}\n{adf}\n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Data available for Mergeco reporting\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 90}}
{"issue_key": "CSCI-609", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "24/Oct/25 1:27 PM", "updated": "31/Oct/25 9:44 AM", "labels": [], "summary": "Snow ticket tracker", "description": "h3. Context\n\n* Create spreadsheet to capture all active SNOW ticket for visibility\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "h3. Acceptance criteria\n\n* Sheet created and access being shared", "comments": "Hey @user ,\n\nAs discussed please start capture all active SNOW ticket for the project and share the access to @user and myself.\n\nThanks\n\nPlease see attachment- @user", "text": "Summary\nSnow ticket tracker\n\n---\n\nDescription\nh3. Context\n\n* Create spreadsheet to capture all active SNOW ticket for visibility\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nh3. Acceptance criteria\n\n* Sheet created and access being shared\n\n---\n\nComments\nHey @user ,\n\nAs discussed please start capture all active SNOW ticket for the project and share the access to @user and myself.\n\nThanks\n\nPlease see attachment- @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 91}}
{"issue_key": "CSCI-608", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "24/Oct/25 11:16 AM", "updated": "10/Nov/25 9:48 AM", "labels": [], "summary": "MergeCo Reporting || Report Presentation Update", "description": "To obtain enhance requirement for report update.\n\nh3. Acceptance criteria\n\n* Understand enhancement need\n* Report update\n* UAT\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nMergeCo Reporting || Report Presentation Update\n\n---\n\nDescription\nTo obtain enhance requirement for report update.\n\nh3. Acceptance criteria\n\n* Understand enhancement need\n* Report update\n* UAT\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 92}}
{"issue_key": "CSCI-607", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "24/Oct/25 11:07 AM", "updated": "10/Dec/25 9:39 AM", "labels": [], "summary": "Merge Supply Chain Documentation || End to End PBI solution", "description": "h3. Context\n\nCreate consolidated documentation for the MergeCo tactical Supply Chain reporting solution, covering the full end-to-end pipeline—from source ingestion to data modelling, semantic layer, metrics alignment, and PBI visualisation. \n\nh3. \n\nh3. Acceptance criteria\n\n* *Finalised End-to-End Documentation* capturing the full MergeCo Tactical Supply Chain reporting solution (source → pipeline → model → metrics → Power BI report).\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Wip report - [https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design]", "text": "Summary\nMerge Supply Chain Documentation || End to End PBI solution\n\n---\n\nDescription\nh3. Context\n\nCreate consolidated documentation for the MergeCo tactical Supply Chain reporting solution, covering the full end-to-end pipeline—from source ingestion to data modelling, semantic layer, metrics alignment, and PBI visualisation. \n\nh3. \n\nh3. Acceptance criteria\n\n* *Finalised End-to-End Documentation* capturing the full MergeCo Tactical Supply Chain reporting solution (source → pipeline → model → metrics → Power BI report).\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nWip report - [https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 93}}
{"issue_key": "CSCI-606", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "24/Oct/25 10:02 AM", "updated": "17/Dec/25 11:23 AM", "labels": [], "summary": "EDP Infra || Repository Renaming", "description": "h3. Context\n\n* Rename repositories to be aligned with implementation across different organizations and geographical locations.\n\nh3. Objective\n\n* Rename current repositories for clarity as to which organizations and geographical locations they are applicable to.\n\nh3. Steps \n\n# Notify developers of the pending change in repository name\n# Update the repository name\n# Update CI/CD pipelines to use the new repository name if needed\n# Validate that CI/CD pipelines work\n# Developers to update their environments (VS Code, Data Factory Studio, Workspaces) to use the new repository name\n\nh3. Acceptance criteria\n\n* renamed repositories aligned with target organizations and geographical location\n\nh3. Assumptions - (Optional)\n\n* naming conventions for naming repositories, including the organization and geographical location.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "@user can you please advise the naming convention i should change these to? Thanks.\n\ncc: @user @user\n\n@user do we already have the naming conventions written in confluence?\n\n@user [https://sigmahealthcare.atlassian.net/wiki/x/AQDkZw|https://sigmahealthcare.atlassian.net/wiki/x/AQDkZw]", "text": "Summary\nEDP Infra || Repository Renaming\n\n---\n\nDescription\nh3. Context\n\n* Rename repositories to be aligned with implementation across different organizations and geographical locations.\n\nh3. Objective\n\n* Rename current repositories for clarity as to which organizations and geographical locations they are applicable to.\n\nh3. Steps \n\n# Notify developers of the pending change in repository name\n# Update the repository name\n# Update CI/CD pipelines to use the new repository name if needed\n# Validate that CI/CD pipelines work\n# Developers to update their environments (VS Code, Data Factory Studio, Workspaces) to use the new repository name\n\nh3. Acceptance criteria\n\n* renamed repositories aligned with target organizations and geographical location\n\nh3. Assumptions - (Optional)\n\n* naming conventions for naming repositories, including the organization and geographical location.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n@user can you please advise the naming convention i should change these to? Thanks.\n\ncc: @user @user\n\n@user do we already have the naming conventions written in confluence?\n\n@user [https://sigmahealthcare.atlassian.net/wiki/x/AQDkZw|https://sigmahealthcare.atlassian.net/wiki/x/AQDkZw]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 94}}
{"issue_key": "CSCI-605", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "24/Oct/25 9:36 AM", "updated": "19/Nov/25 9:21 AM", "labels": [], "summary": "Service Account Creation for Manhattan Active Database- RITM0178021", "description": "* *Short:* {{servac_edp_5wv2qm398}}\n* *Long:* {{servac_edp_mysql_manhattan_active_prod}}\n\nWe need *servac_edp_5wv2qm398* created", "acceptance_criteria": "Given, When, Then", "comments": "The Cloud Team is trying to move most service accounts to Group Managed Service Account across the Org, hence need more details before they provision the service account- Meeting set up for today. \n\ncc- @user @user @user\n\nMoving this to @user to discuss with supply chain on getting service account created in Manhattan\n\nAccess granted and able to connect to tables", "text": "Summary\nService Account Creation for Manhattan Active Database- RITM0178021\n\n---\n\nDescription\n* *Short:* {{servac_edp_5wv2qm398}}\n* *Long:* {{servac_edp_mysql_manhattan_active_prod}}\n\nWe need *servac_edp_5wv2qm398* created\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nThe Cloud Team is trying to move most service accounts to Group Managed Service Account across the Org, hence need more details before they provision the service account- Meeting set up for today. \n\ncc- @user @user @user\n\nMoving this to @user to discuss with supply chain on getting service account created in Manhattan\n\nAccess granted and able to connect to tables", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 95}}
{"issue_key": "CSCI-604", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 11:21 PM", "updated": "07/Nov/25 10:07 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Issue Investigation - Issue 11", "description": "*Objective*\n\nWe are seeing a lot of transactions where the tax amount and discount amount are exactly the same. In fact there are over 100 000 transactions like this but only 25 000 where the values are different. Is this intended and in line with their data?\n\nExample invoice ids are: 35358290519654400 & 35357668974658200\n\n*Steps*\n\n* Confirm if the on-receipt transaction number is required for the secure environment.\n* confirm if this transaction number exists in the raw source data.\n* analyse the source data to find any relevant flags or indicators.\n* Create and link a new card for any necessary pipeline fix.", "acceptance_criteria": "*Acceptance Criteria:*\n\n* Identify issue root cause.\n* Purpose fix where needed", "comments": "Original Invoice is 35357668974658280 & 35358290519654432\n\nChecked the data . There is no discount associated in this invoice as Base price and unitprice is same. \n\nThe data at APPRISS end does not look good. Need to revisit the code for file generation for Discount.Looks like need a code fix", "text": "Summary\nAPPRISS Issue Investigation - Issue 11\n\n---\n\nDescription\n*Objective*\n\nWe are seeing a lot of transactions where the tax amount and discount amount are exactly the same. In fact there are over 100 000 transactions like this but only 25 000 where the values are different. Is this intended and in line with their data?\n\nExample invoice ids are: 35358290519654400 & 35357668974658200\n\n*Steps*\n\n* Confirm if the on-receipt transaction number is required for the secure environment.\n* confirm if this transaction number exists in the raw source data.\n* analyse the source data to find any relevant flags or indicators.\n* Create and link a new card for any necessary pipeline fix.\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria:*\n\n* Identify issue root cause.\n* Purpose fix where needed\n\n---\n\nComments\nOriginal Invoice is 35357668974658280 & 35358290519654432\n\nChecked the data . There is no discount associated in this invoice as Base price and unitprice is same. \n\nThe data at APPRISS end does not look good. Need to revisit the code for file generation for Discount.Looks like need a code fix", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 96}}
{"issue_key": "CSCI-603", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 11:21 PM", "updated": "05/Nov/25 11:52 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Issue Investigation - Issue 10", "description": "*Issue Detail:*\n*DO 7550 — BM 05 GC Purchase - BM 19 GC Redemption*\nSource data does not indicate gift card payments. For *invoice ID 35357668968628300*, the benchmark receipt shows a gift card payment, but source data records it as a credit card payment. Please confirm how gift card tenders can be identified and whether gift card numbers are available in the data.\n\n*Steps*\n\nThe correct source field for identifying Gift Card tenders is defined and confirmed by DBA/Business .\n\nThe source for a valid Gift Card number for purchases/payments is identified and confirmed.\n\nA new card is created and linked to implement schema and pipeline changes for Gift Cards.", "acceptance_criteria": "*Acceptance Criteria:*\n\n* Identify issue root cause.\n* Purpose fix where needed", "comments": "Original Invoice ID is 35357668968628352\n\nHeader info says: Fully paid electronically\n\nTender Info: Has one record for this invoice\n\nAudit data says Electronic Type as Credit, and Electronic receipt says Visa Prepaid, which signifies gift card", "text": "Summary\nAPPRISS Issue Investigation - Issue 10\n\n---\n\nDescription\n*Issue Detail:*\n*DO 7550 — BM 05 GC Purchase - BM 19 GC Redemption*\nSource data does not indicate gift card payments. For *invoice ID 35357668968628300*, the benchmark receipt shows a gift card payment, but source data records it as a credit card payment. Please confirm how gift card tenders can be identified and whether gift card numbers are available in the data.\n\n*Steps*\n\nThe correct source field for identifying Gift Card tenders is defined and confirmed by DBA/Business .\n\nThe source for a valid Gift Card number for purchases/payments is identified and confirmed.\n\nA new card is created and linked to implement schema and pipeline changes for Gift Cards.\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria:*\n\n* Identify issue root cause.\n* Purpose fix where needed\n\n---\n\nComments\nOriginal Invoice ID is 35357668968628352\n\nHeader info says: Fully paid electronically\n\nTender Info: Has one record for this invoice\n\nAudit data says Electronic Type as Credit, and Electronic receipt says Visa Prepaid, which signifies gift card", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 97}}
{"issue_key": "CSCI-602", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 11:21 PM", "updated": "05/Nov/25 11:52 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Issue Investigation - Issue 9", "description": "*Issue Detail:*\n*DO 7535 — Tender Card Number - Tender Token*\nThe field *tender.card_number* always shows the value {{'TOKENIZED'}}. Please explain why the actual card or token values are not visible and confirm if this is intentional masking or data issue.", "acceptance_criteria": "*Acceptance Criteria:*\nIdentify issue root cause.", "comments": "Card Number is hard coded 'TOKENIZED' in data transform script\n\nWe do not store full card number in our system. We have hashed carddetails .\n\nIn our current code it is hardcoded as 'Tokenized' .\n\nIt can be changed to Hashed Electronic Ref for better visibility of vcard from APPRISS end", "text": "Summary\nAPPRISS Issue Investigation - Issue 9\n\n---\n\nDescription\n*Issue Detail:*\n*DO 7535 — Tender Card Number - Tender Token*\nThe field *tender.card_number* always shows the value {{'TOKENIZED'}}. Please explain why the actual card or token values are not visible and confirm if this is intentional masking or data issue.\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria:*\nIdentify issue root cause.\n\n---\n\nComments\nCard Number is hard coded 'TOKENIZED' in data transform script\n\nWe do not store full card number in our system. We have hashed carddetails .\n\nIn our current code it is hardcoded as 'Tokenized' .\n\nIt can be changed to Hashed Electronic Ref for better visibility of vcard from APPRISS end", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 98}}
{"issue_key": "CSCI-601", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 11:21 PM", "updated": "05/Nov/25 11:53 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Issue Investigation - Issue 8", "description": "*Issue Detail:*\n*DO 7530 — Secure Order No vs Order ID*\nSource invoice ID values are not unique. Example *35358913223065600* appears multiple times with different amounts. Please confirm whether these represent multiple transactions or data duplication, and clarify how to distinguish them if valid.\n\nSteps\n\n*Uniqueness Defined:* confirm with business if the identical {{order_id}} values with different amounts are one transaction or several.\n\n*Key Identified:* Business to advise business rule to define Unique Keys and DBA to update query", "acceptance_criteria": "*Acceptance Criteria:*\n\n* Identify issue root cause\n* New card to be create and linked for any fix needed", "comments": "Original Invoice ID is 35358913223065664\n\nHeader info says: Full paid electronically\n\nTender Info: Has got one record for this invoice\n\n we have only one record in headewr for the said invoice ids. \n\nAs thye screenshot shows , There are 6 line items under one header , 6 different products were bought in this invoice", "text": "Summary\nAPPRISS Issue Investigation - Issue 8\n\n---\n\nDescription\n*Issue Detail:*\n*DO 7530 — Secure Order No vs Order ID*\nSource invoice ID values are not unique. Example *35358913223065600* appears multiple times with different amounts. Please confirm whether these represent multiple transactions or data duplication, and clarify how to distinguish them if valid.\n\nSteps\n\n*Uniqueness Defined:* confirm with business if the identical {{order_id}} values with different amounts are one transaction or several.\n\n*Key Identified:* Business to advise business rule to define Unique Keys and DBA to update query\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria:*\n\n* Identify issue root cause\n* New card to be create and linked for any fix needed\n\n---\n\nComments\nOriginal Invoice ID is 35358913223065664\n\nHeader info says: Full paid electronically\n\nTender Info: Has got one record for this invoice\n\n we have only one record in headewr for the said invoice ids. \n\nAs thye screenshot shows , There are 6 line items under one header , 6 different products were bought in this invoice", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 99}}
{"issue_key": "CSCI-600", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 11:20 PM", "updated": "05/Nov/25 11:53 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Issue Investigation - Issue 7", "description": "*Issue Detail:*\n*DO 7517 — Tender - Tender Type - Tender Card Type*\nTender details for *invoice ID 35357668968628300* do not match benchmark data (missing indication of gift card tender). Additionally, *invoice ID 35358291367034900* has no tender data despite a payment in the benchmark. Please verify tender source data completeness.\n\n*Steps*\n\nBusiness to provide the correct tender identification method for the Gift Card issue.\n\n locate and provide the missing tender data for the second invoice ID.\n\nCreate new technical card(s) to address any confirmed mapping errors or missing data ingestion.", "acceptance_criteria": "*Acceptance Criteria:*\nIdentify issue root cause.", "comments": "Original Invoice ID is 35357668968628352\n\nHeader info says: Full paid electronically\n\nTender Info: Has got one record for this invoice\n\nAudit data says Elctronic Type as Credit and Electronic receipt says Visa Prepaid, which signifies giftcard/prepaid card", "text": "Summary\nAPPRISS Issue Investigation - Issue 7\n\n---\n\nDescription\n*Issue Detail:*\n*DO 7517 — Tender - Tender Type - Tender Card Type*\nTender details for *invoice ID 35357668968628300* do not match benchmark data (missing indication of gift card tender). Additionally, *invoice ID 35358291367034900* has no tender data despite a payment in the benchmark. Please verify tender source data completeness.\n\n*Steps*\n\nBusiness to provide the correct tender identification method for the Gift Card issue.\n\n locate and provide the missing tender data for the second invoice ID.\n\nCreate new technical card(s) to address any confirmed mapping errors or missing data ingestion.\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria:*\nIdentify issue root cause.\n\n---\n\nComments\nOriginal Invoice ID is 35357668968628352\n\nHeader info says: Full paid electronically\n\nTender Info: Has got one record for this invoice\n\nAudit data says Elctronic Type as Credit and Electronic receipt says Visa Prepaid, which signifies giftcard/prepaid card", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 100}}
{"issue_key": "CSCI-599", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 11:20 PM", "updated": "05/Nov/25 11:52 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Issue Investigation - Issue 6", "description": "*DO 7584 — Benchmark 04 Tender - Tender Void?*\nFor order *35357668974658200*, benchmark data shows two tenders (one voided, one Mastercard), while Secure shows only one Mastercard tender. If the voided tender is expected in Secure, please identify where it can be found in the raw data.\n\n*Acceptance Criteria:*\nIdentify issue root cause.", "acceptance_criteria": "*Acceptance Criteria:*\nIdentify issue root cause.", "comments": "AC Oct 30: Original Invoice ID is : 35357668974658280\n\nTransactions Data Shows Only one item was sold\n\nElectronic Data(Tender) shows 2 records used for payments and one was cancelled\n\nAudit Data Reiterates the same. So data is tied.\n\nDid APPRISS Team receive only one record? or two? for issue 21(DO-7524) , they had received two records. If APPRISS has received one record, need to check extraction code for Tender Data to check why a cancelled tender record was missed\n\nOriginal Invoice ID is : 801125062148140\n\nTransactions Data Shows total amount paid is $50 full in cash\n\nElectronic Data(Tender) shows 0 records confirming payment type as cash\n\nDid APPRISS Team receive only one record? or two? for issue 21(DO-7524) , they had received two records. Need to check extraction code for Tender Data to check why a cancelled tender record was missed", "text": "Summary\nAPPRISS Issue Investigation - Issue 6\n\n---\n\nDescription\n*DO 7584 — Benchmark 04 Tender - Tender Void?*\nFor order *35357668974658200*, benchmark data shows two tenders (one voided, one Mastercard), while Secure shows only one Mastercard tender. If the voided tender is expected in Secure, please identify where it can be found in the raw data.\n\n*Acceptance Criteria:*\nIdentify issue root cause.\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria:*\nIdentify issue root cause.\n\n---\n\nComments\nAC Oct 30: Original Invoice ID is : 35357668974658280\n\nTransactions Data Shows Only one item was sold\n\nElectronic Data(Tender) shows 2 records used for payments and one was cancelled\n\nAudit Data Reiterates the same. So data is tied.\n\nDid APPRISS Team receive only one record? or two? for issue 21(DO-7524) , they had received two records. If APPRISS has received one record, need to check extraction code for Tender Data to check why a cancelled tender record was missed\n\nOriginal Invoice ID is : 801125062148140\n\nTransactions Data Shows total amount paid is $50 full in cash\n\nElectronic Data(Tender) shows 0 records confirming payment type as cash\n\nDid APPRISS Team receive only one record? or two? for issue 21(DO-7524) , they had received two records. Need to check extraction code for Tender Data to check why a cancelled tender record was missed", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 101}}
{"issue_key": "CSCI-598", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 11:20 PM", "updated": "05/Nov/25 11:52 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Issue Investigation - Issue 5", "description": "*Issue Detail:*\n*DO 7525 — BM 11 and BM 12 - Cash Refund or Line Void*\nOrders *35358291367034900* appear both as Cash Refund and Line Void in benchmarks. In Secure, Item Return Flag is populated but Item Voided Flag is not. Please confirm whether this should be treated as a Cash Refund or Item Void and clarify identification rules in source data.", "acceptance_criteria": "*Acceptance Criteria:*\nIdentify issue root cause.", "comments": "Correct Invoice ID is : 35358291367034992\n\nHeader confirms $2 cash Refund\n\nNo entry in Electronic Payments Data ( Tender) confirms it’s a cash Refund\n\nAudit Data chekcs : Chec ks if any prevevious refund was issued, Check Original Invoice ID against which refund needs to be made (35357669285600948) then confimrs refund of $2 cash refund. added retunred quantity as 1. So all data is tied together and as per benchmarking", "text": "Summary\nAPPRISS Issue Investigation - Issue 5\n\n---\n\nDescription\n*Issue Detail:*\n*DO 7525 — BM 11 and BM 12 - Cash Refund or Line Void*\nOrders *35358291367034900* appear both as Cash Refund and Line Void in benchmarks. In Secure, Item Return Flag is populated but Item Voided Flag is not. Please confirm whether this should be treated as a Cash Refund or Item Void and clarify identification rules in source data.\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria:*\nIdentify issue root cause.\n\n---\n\nComments\nCorrect Invoice ID is : 35358291367034992\n\nHeader confirms $2 cash Refund\n\nNo entry in Electronic Payments Data ( Tender) confirms it’s a cash Refund\n\nAudit Data chekcs : Chec ks if any prevevious refund was issued, Check Original Invoice ID against which refund needs to be made (35357669285600948) then confimrs refund of $2 cash refund. added retunred quantity as 1. So all data is tied together and as per benchmarking", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 102}}
{"issue_key": "CSCI-597", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 11:19 PM", "updated": "05/Nov/25 11:52 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Issue Investigation - Issue 4", "description": "*Issue Detail:*\n*DO 7524 — Benchmark 09 - Multi Payment*\nFor order *35357669071783700*, benchmark data shows two payment cards (Visa and Mastercard) each for $38.46, but Secure data shows only one Visa tender. Please verify if this order is truly a multi-payment transaction, and clarify the correct tender details.\n\n*Acceptance Criteria:*\nIdentify issue root cause.", "acceptance_criteria": "*Acceptance Criteria:*\nIdentify issue root cause.", "comments": "", "text": "Summary\nAPPRISS Issue Investigation - Issue 4\n\n---\n\nDescription\n*Issue Detail:*\n*DO 7524 — Benchmark 09 - Multi Payment*\nFor order *35357669071783700*, benchmark data shows two payment cards (Visa and Mastercard) each for $38.46, but Secure data shows only one Visa tender. Please verify if this order is truly a multi-payment transaction, and clarify the correct tender details.\n\n*Acceptance Criteria:*\nIdentify issue root cause.\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria:*\nIdentify issue root cause.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 103}}
{"issue_key": "CSCI-596", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 11:19 PM", "updated": "05/Nov/25 11:52 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Issue Investigation - Issue 3", "description": "*Issue Detail:*\n*DO 7451 — BM 02 - Item - Item List Amount*\nBenchmark Transaction *35358912688554000* has three items with discounts. Items show identical *amount* and *list_amount* values. The *list_amount* should be the price before discounts, while *amount* is after discounts. Please investigate why these values are the same.", "acceptance_criteria": "*Acceptance Criteria:*\nIdentify issue root cause.", "comments": "Looks like same invoice id mentioned as line 19 (Internal Ticket: DO-7480). Our records shows different items as shown in line 19.\nSuspecting wrong invoice ids mentioned in spreadsheet.Could you please sahre the Branch id and Tinmestamp so that we will try to point the right invoice id from ur end", "text": "Summary\nAPPRISS Issue Investigation - Issue 3\n\n---\n\nDescription\n*Issue Detail:*\n*DO 7451 — BM 02 - Item - Item List Amount*\nBenchmark Transaction *35358912688554000* has three items with discounts. Items show identical *amount* and *list_amount* values. The *list_amount* should be the price before discounts, while *amount* is after discounts. Please investigate why these values are the same.\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria:*\nIdentify issue root cause.\n\n---\n\nComments\nLooks like same invoice id mentioned as line 19 (Internal Ticket: DO-7480). Our records shows different items as shown in line 19.\nSuspecting wrong invoice ids mentioned in spreadsheet.Could you please sahre the Branch id and Tinmestamp so that we will try to point the right invoice id from ur end", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 104}}
{"issue_key": "CSCI-595", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 11:19 PM", "updated": "05/Nov/25 11:52 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Issue Investigation - Issue 2", "description": "*Issue Detail:*\n*DO 7480 — BM 02 - Item - Item Discount to 0.00 displayed as Item Void*\nPlease investigate whether the third item in benchmark transaction *35358912688554000* should be voided. The data flags this as void in the source data but the benchmark summaries do not.\n\n*Acceptance Criteria:*\nIdentify issue root cause.", "acceptance_criteria": "*Acceptance Criteria:*\nIdentify issue root cause.", "comments": "35358912688554000 is a legit transactions. 3rd transactions with 2 cents as amount is fr rounding . Mychemid is always null for rounding transactions and transaction type is 4 for roundng transactions", "text": "Summary\nAPPRISS Issue Investigation - Issue 2\n\n---\n\nDescription\n*Issue Detail:*\n*DO 7480 — BM 02 - Item - Item Discount to 0.00 displayed as Item Void*\nPlease investigate whether the third item in benchmark transaction *35358912688554000* should be voided. The data flags this as void in the source data but the benchmark summaries do not.\n\n*Acceptance Criteria:*\nIdentify issue root cause.\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria:*\nIdentify issue root cause.\n\n---\n\nComments\n35358912688554000 is a legit transactions. 3rd transactions with 2 cents as amount is fr rounding . Mychemid is always null for rounding transactions and transaction type is 4 for roundng transactions", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 105}}
{"issue_key": "CSCI-594", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 11:18 PM", "updated": "05/Nov/25 11:52 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Issue Investigation - Issue 1", "description": "h3. Context\n\n*Issue ID:* DO-7437\n*Issue Detail:*\nBenchmark 01 - Header - Void Ticket Flg\nBenchmark transaction *35358290519654400* is not flagged as voided, despite being listed as voided in the benchmark summary. Please investigate the source data.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "*Acceptance Criteria:*\nIdentify issue root cause.", "comments": "got messed up in CSV original invoice id is 35358290519654432", "text": "Summary\nAPPRISS Issue Investigation - Issue 1\n\n---\n\nDescription\nh3. Context\n\n*Issue ID:* DO-7437\n*Issue Detail:*\nBenchmark 01 - Header - Void Ticket Flg\nBenchmark transaction *35358290519654400* is not flagged as voided, despite being listed as voided in the benchmark summary. Please investigate the source data.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\n*Acceptance Criteria:*\nIdentify issue root cause.\n\n---\n\nComments\ngot messed up in CSV original invoice id is 35358290519654432", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 106}}
{"issue_key": "CSCI-593", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 4:57 PM", "updated": "21/Nov/25 1:43 PM", "labels": [], "summary": "Create EDP Share", "description": "h3. Context\n\n* Create scaffolding for Snowflake data share\n\nh3. Objective\n\n* Create foundation for data sharing in SF\n\nh3. Deliverables\n\n* Create repo called “EDP_Publish”\n* Create new database called “EDP_Publish”\n* Create new schema inside EDP_Publish called “ED_Appriss_Share”\n* Create SF Share object\n* Set up E2E CI/CD pipeline\n\nh3. Acceptance criteria\n\n* Data share functionality enabled in EDP", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nCreate EDP Share\n\n---\n\nDescription\nh3. Context\n\n* Create scaffolding for Snowflake data share\n\nh3. Objective\n\n* Create foundation for data sharing in SF\n\nh3. Deliverables\n\n* Create repo called “EDP_Publish”\n* Create new database called “EDP_Publish”\n* Create new schema inside EDP_Publish called “ED_Appriss_Share”\n* Create SF Share object\n* Set up E2E CI/CD pipeline\n\nh3. Acceptance criteria\n\n* Data share functionality enabled in EDP\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 107}}
{"issue_key": "CSCI-589", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Oct/25 10:09 PM", "updated": "08/Dec/25 1:45 PM", "labels": [], "summary": "DimOrgAreaStore + DimOrgArea + BridgeOrgAreaManager + Employee", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "TBC", "comments": "Duplicate with CSCI-446", "text": "Summary\nDimOrgAreaStore + DimOrgArea + BridgeOrgAreaManager + Employee\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nTBC\n\n---\n\nComments\nDuplicate with CSCI-446", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 108}}
{"issue_key": "CSCI-580", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 9:20 AM", "updated": "21/Nov/25 1:40 PM", "labels": ["APPRISS_Tactical_Solution"], "summary": "Move data from Source to APPRISS Schema", "description": "h3. Context\n\nMove data to new Appriss production schema\n\nh3. Objective\n\nCreate automated pipeline to move data from Appriss source(on prem SQL) to Appriss SF database on a daily basis\n\nh3. Steps \n\nh3. Deliverables\n\n* Create new repo called “Appriss_Tactical”\n* Create table/view objects for Apprisss\n* Create ADF pipelines to load data from source to “Appriss_Tactical”", "acceptance_criteria": "* Code to be deployed to DEV, SIT, UAT, PROD\n* No changes/deployments made manually", "comments": "", "text": "Summary\nMove data from Source to APPRISS Schema\n\n---\n\nDescription\nh3. Context\n\nMove data to new Appriss production schema\n\nh3. Objective\n\nCreate automated pipeline to move data from Appriss source(on prem SQL) to Appriss SF database on a daily basis\n\nh3. Steps \n\nh3. Deliverables\n\n* Create new repo called “Appriss_Tactical”\n* Create table/view objects for Apprisss\n* Create ADF pipelines to load data from source to “Appriss_Tactical”\n\n---\n\nAcceptance Criteria\n* Code to be deployed to DEV, SIT, UAT, PROD\n* No changes/deployments made manually", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 109}}
{"issue_key": "CSCI-578", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Oct/25 9:01 AM", "updated": "08/Dec/25 11:40 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "APPRISS Tactical || APPRISS tactical solution design", "description": "h3. Objective\n\n* Prepare design document for APPRISS tactical solution\n\nh3. Deliverables\n\n* Solution design available in Confluence\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "h3. Acceptance criteria\n\n* Solution Design Document uploaded to Confluence", "comments": "", "text": "Summary\nAPPRISS Tactical || APPRISS tactical solution design\n\n---\n\nDescription\nh3. Objective\n\n* Prepare design document for APPRISS tactical solution\n\nh3. Deliverables\n\n* Solution design available in Confluence\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nh3. Acceptance criteria\n\n* Solution Design Document uploaded to Confluence", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 110}}
{"issue_key": "CSCI-575", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Oct/25 2:32 PM", "updated": "05/Nov/25 9:48 AM", "labels": ["APPRISS_Tactical_Solution"], "summary": "EDP Infra||Set Up CI/CD pipeline for ADF", "description": "h3. Objective\n\nImplement CI/CD pipeline to deploy Azure Data Factory assets across all environments via Infrastructure as Code. Eliminate manual publishing of ADF pipelines and ensure consistent, versioned deployments.\n\nh3. Steps \n\n* Configure Git integration with ADF.\n* Add automated validation and publish to target environment.\n* Apply service principal authentication.\n\nh3. Deliverables\n\n* CI/CD YAML pipeline for ADF.\n* Infrastructure templates under source control.\n* Environment-specific variable files.", "acceptance_criteria": "* -100% ADF deployments automated via CI/CD.-\n* -Environment parity verified via automated comparison.-\n* -No production changes made manually.-", "comments": "Hi @user let’s connect to review any remaining requirements.\n\nPlease find below the following ADF Pipelines:\n\n* CI - [edp-data-factory-ci - Pipelines|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_apps/hub/ms.vss-build-web.ci-designer-hub?pipelineId=638&branch=master] (Using ci.yaml)\n* \n\n* CD - [edp-data-factory-release - Pipelines|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_releaseDefinition?definitionId=5&_a=environments-editor-preview] (using classic Azure DevOps pipelines)\n*\n\nToDo: \n\n* changes in the CD process as soon as new environments for SIT and UAT are created.\n* further improvements on static security scans during CI (password checks, etc)\n\nAcceptance criteria:\n\n* ADF deployment across different environments are executed using the CD pipeline\n* The release pipeline does a comparison with the currently deployed resources and adds/updates existing resources if needed and deletes resources that are not in the deployment package.\n* Production changes can still be done manually depending on the rights of the users who have access to the data factory instance. Recommend that this is strictly limited and secured properly.\n\n@user can you please create a separate task to enhance the ADF CI pipeline to add static security scans?\n\n@user [https://sigmahealthcare.atlassian.net/browse/CSCI-632|https://sigmahealthcare.atlassian.net/browse/CSCI-632] \n\nplease update steps, acceptance criteria and estimated effort", "text": "Summary\nEDP Infra||Set Up CI/CD pipeline for ADF\n\n---\n\nDescription\nh3. Objective\n\nImplement CI/CD pipeline to deploy Azure Data Factory assets across all environments via Infrastructure as Code. Eliminate manual publishing of ADF pipelines and ensure consistent, versioned deployments.\n\nh3. Steps \n\n* Configure Git integration with ADF.\n* Add automated validation and publish to target environment.\n* Apply service principal authentication.\n\nh3. Deliverables\n\n* CI/CD YAML pipeline for ADF.\n* Infrastructure templates under source control.\n* Environment-specific variable files.\n\n---\n\nAcceptance Criteria\n* -100% ADF deployments automated via CI/CD.-\n* -Environment parity verified via automated comparison.-\n* -No production changes made manually.-\n\n---\n\nComments\nHi @user let’s connect to review any remaining requirements.\n\nPlease find below the following ADF Pipelines:\n\n* CI - [edp-data-factory-ci - Pipelines|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_apps/hub/ms.vss-build-web.ci-designer-hub?pipelineId=638&branch=master] (Using ci.yaml)\n* \n\n* CD - [edp-data-factory-release - Pipelines|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_releaseDefinition?definitionId=5&_a=environments-editor-preview] (using classic Azure DevOps pipelines)\n*\n\nToDo: \n\n* changes in the CD process as soon as new environments for SIT and UAT are created.\n* further improvements on static security scans during CI (password checks, etc)\n\nAcceptance criteria:\n\n* ADF deployment across different environments are executed using the CD pipeline\n* The release pipeline does a comparison with the currently deployed resources and adds/updates existing resources if needed and deletes resources that are not in the deployment package.\n* Production changes can still be done manually depending on the rights of the users who have access to the data factory instance. Recommend that this is strictly limited and secured properly.\n\n@user can you please create a separate task to enhance the ADF CI pipeline to add static security scans?\n\n@user [https://sigmahealthcare.atlassian.net/browse/CSCI-632|https://sigmahealthcare.atlassian.net/browse/CSCI-632] \n\nplease update steps, acceptance criteria and estimated effort", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 111}}
{"issue_key": "CSCI-574", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Oct/25 2:02 PM", "updated": "25/Nov/25 10:53 AM", "labels": [], "summary": "EDP Infra|| Prepare script for security permissions", "description": "h3. Objective\n\nAutomate the creation and management of roles, grants, and access privileges in Snowflake. Ensure RBAC is applied consistently across all environments with no manual SQL changes.\n\nh3. Steps \n\n* Create parameterized SQL or Terraform modules for RBAC.\n* Define standard roles: Platform Engineer, Data Engineer, Data Analyst etc.\n* Automate privilege assignment and auditing.\n\nh3. Deliverables\n\n* RBAC script/module in repo.", "acceptance_criteria": "* -Security permissions set up according to this design: Snowflake Security - Enterprise Data Platform - Confluence-\n* -Roles and grants created only via IaC.-\n* -Re-runable idempotent scripts.-\n* -No changes/deployments made manually-", "comments": "* Working on the RBAC documentation:\n** Object Roles Scripts\n** Function Roles Scripts\n** Assign Object Roles to Function Roles\n\nAdded Managed Access Schema strategy to enforce least privilege and enable IaC-driven future grants\n\n*Problem Statement*\n\nOur Snowflake Infrastructure-as-Code (IaC) setup, deployed via Azure DevOps, uses a service role ROLE_INFRA_ADMIN_SNOWFLAKE to create and manage objects (databases, schemas, RBAC grants) across environments: *DEV, SIT, UAT, PROD*.\n\nHowever, there’s a challenge:\n\n* Future grants on schemas and objects require either *MANAGE GRANTS* (a high-privilege, organisation-wide permission) or *schema ownership*.\n* Granting *MANAGE GRANTS* to the IaC role violates least-privilege principles.\n* We need a *fully automated IaC solution* with no manual steps, while maintaining strong access control.\n\n \n\n*Proposed Solution: Managed Access Schema + Role Ownership Pattern*\n\n* Use *Managed Access Schemas (MAS)* in all environments for consistent IaC behaviour and centralised grant control.\n* *Design Summary*:\n** Each schema is created as a Managed Access Schema, e.g.\nCREATE SCHEMA EDP_${ENV}.SILVER WITH MANAGED ACCESS;\n** Schema ownership is assigned to a dedicated role, e.g.\nOPS_${ENV}\n** The IaC service role (ROLE_INFRA_ADMIN_SNOWFLAKE) performs grants and object creation on behalf of the schema-owner role (via USE ROLE or delegated ownership).\n** Developers in *DEV* can assume the schema-owner role for flexibility.\n** In *SIT/UAT/PROD*, only the IaC pipeline holds this privilege — ensuring RBAC consistency, least privilege, and full automation.\n\n \n\n*Benefits*\n\n|*Area*|*Outcome*|\n|*Security*|No need for MANAGE GRANTS; privileges limited to schema level.|\n|*Consistency*|Identical IaC templates across all environments.|\n|*Governance*|Centralised privilege control via schema-owner roles.|\n|*Automation*|All schema creation and grants handled by IaC pipeline, zero manual steps.|\n|*Flexibility*|Developers retain freedom in DEV through delegated roles.|\n\n*In short:*\nStandardise on Managed Access Schemas across environments and assign schema ownership to environment-specific roles. The IaC role manages objects and future grants under these roles — delivering *full automation*, *least privilege*, and *end-to-end consistency*.\n\n{noformat}-- 1. Create the IAC service role and data warehouse\nUSE ROLE SECURITYADMIN;\n\nCREATE ROLE IF NOT EXISTS ROLE_INFRA_ADMIN_SNOWFLAKE COMMENT = 'Role used for IaC deployments';\nCREATE WAREHOUSE IF NOT EXISTS INFRA_ADMIN_WH COMMENT = 'WH used for IaC deployments';\n\n-- 2. Grant necessary privileges\nGRANT CREATE DATABASE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE INTEGRATION ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE ROLE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE SCHEMA TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT MANAGE GRANTS ON DATABASE EDP_DEV TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\n\n-- 3. Create the service account user\nCREATE USER IF NOT EXISTS SVC_IAC_SNOWFLAKE\n LOGIN_NAME = 'SVC_IAC_SNOWFLAKE'\n DISPLAY_NAME = 'SVC_IAC_SNOWFLAKE'\n DEFAULT_ROLE = ROLE_INFRA_ADMIN_SNOWFLAKE\n DEFAULT_WAREHOUSE = 'INFRA_ADMIN_WH'\n MUST_CHANGE_PASSWORD = FALSE\n DISABLED = FALSE\n RSA_PUBLIC_KEY = 'MIICIjANBgk..........AwEAAQ=='\n COMMENT = 'Service account for Snowflake IaC deployments';\n\n-- 4. Grant the role to the user\nGRANT ROLE ROLE_INFRA_ADMIN_SNOWFLAKE TO USER SVC_IAC_SNOWFLAKE;\n\n-- 5. Allow escalation only when approved\nGRANT ROLE ROLE_INFRA_ADMIN_SNOWFLAKE TO ROLE SECURITYADMIN;{noformat}", "text": "Summary\nEDP Infra|| Prepare script for security permissions\n\n---\n\nDescription\nh3. Objective\n\nAutomate the creation and management of roles, grants, and access privileges in Snowflake. Ensure RBAC is applied consistently across all environments with no manual SQL changes.\n\nh3. Steps \n\n* Create parameterized SQL or Terraform modules for RBAC.\n* Define standard roles: Platform Engineer, Data Engineer, Data Analyst etc.\n* Automate privilege assignment and auditing.\n\nh3. Deliverables\n\n* RBAC script/module in repo.\n\n---\n\nAcceptance Criteria\n* -Security permissions set up according to this design: Snowflake Security - Enterprise Data Platform - Confluence-\n* -Roles and grants created only via IaC.-\n* -Re-runable idempotent scripts.-\n* -No changes/deployments made manually-\n\n---\n\nComments\n* Working on the RBAC documentation:\n** Object Roles Scripts\n** Function Roles Scripts\n** Assign Object Roles to Function Roles\n\nAdded Managed Access Schema strategy to enforce least privilege and enable IaC-driven future grants\n\n*Problem Statement*\n\nOur Snowflake Infrastructure-as-Code (IaC) setup, deployed via Azure DevOps, uses a service role ROLE_INFRA_ADMIN_SNOWFLAKE to create and manage objects (databases, schemas, RBAC grants) across environments: *DEV, SIT, UAT, PROD*.\n\nHowever, there’s a challenge:\n\n* Future grants on schemas and objects require either *MANAGE GRANTS* (a high-privilege, organisation-wide permission) or *schema ownership*.\n* Granting *MANAGE GRANTS* to the IaC role violates least-privilege principles.\n* We need a *fully automated IaC solution* with no manual steps, while maintaining strong access control.\n\n \n\n*Proposed Solution: Managed Access Schema + Role Ownership Pattern*\n\n* Use *Managed Access Schemas (MAS)* in all environments for consistent IaC behaviour and centralised grant control.\n* *Design Summary*:\n** Each schema is created as a Managed Access Schema, e.g.\nCREATE SCHEMA EDP_${ENV}.SILVER WITH MANAGED ACCESS;\n** Schema ownership is assigned to a dedicated role, e.g.\nOPS_${ENV}\n** The IaC service role (ROLE_INFRA_ADMIN_SNOWFLAKE) performs grants and object creation on behalf of the schema-owner role (via USE ROLE or delegated ownership).\n** Developers in *DEV* can assume the schema-owner role for flexibility.\n** In *SIT/UAT/PROD*, only the IaC pipeline holds this privilege — ensuring RBAC consistency, least privilege, and full automation.\n\n \n\n*Benefits*\n\n|*Area*|*Outcome*|\n|*Security*|No need for MANAGE GRANTS; privileges limited to schema level.|\n|*Consistency*|Identical IaC templates across all environments.|\n|*Governance*|Centralised privilege control via schema-owner roles.|\n|*Automation*|All schema creation and grants handled by IaC pipeline, zero manual steps.|\n|*Flexibility*|Developers retain freedom in DEV through delegated roles.|\n\n*In short:*\nStandardise on Managed Access Schemas across environments and assign schema ownership to environment-specific roles. The IaC role manages objects and future grants under these roles — delivering *full automation*, *least privilege*, and *end-to-end consistency*.\n\n{noformat}-- 1. Create the IAC service role and data warehouse\nUSE ROLE SECURITYADMIN;\n\nCREATE ROLE IF NOT EXISTS ROLE_INFRA_ADMIN_SNOWFLAKE COMMENT = 'Role used for IaC deployments';\nCREATE WAREHOUSE IF NOT EXISTS INFRA_ADMIN_WH COMMENT = 'WH used for IaC deployments';\n\n-- 2. Grant necessary privileges\nGRANT CREATE DATABASE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE INTEGRATION ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE ROLE ON ACCOUNT TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT CREATE SCHEMA TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\nGRANT MANAGE GRANTS ON DATABASE EDP_DEV TO ROLE ROLE_INFRA_ADMIN_SNOWFLAKE;\n\n-- 3. Create the service account user\nCREATE USER IF NOT EXISTS SVC_IAC_SNOWFLAKE\n LOGIN_NAME = 'SVC_IAC_SNOWFLAKE'\n DISPLAY_NAME = 'SVC_IAC_SNOWFLAKE'\n DEFAULT_ROLE = ROLE_INFRA_ADMIN_SNOWFLAKE\n DEFAULT_WAREHOUSE = 'INFRA_ADMIN_WH'\n MUST_CHANGE_PASSWORD = FALSE\n DISABLED = FALSE\n RSA_PUBLIC_KEY = 'MIICIjANBgk..........AwEAAQ=='\n COMMENT = 'Service account for Snowflake IaC deployments';\n\n-- 4. Grant the role to the user\nGRANT ROLE ROLE_INFRA_ADMIN_SNOWFLAKE TO USER SVC_IAC_SNOWFLAKE;\n\n-- 5. Allow escalation only when approved\nGRANT ROLE ROLE_INFRA_ADMIN_SNOWFLAKE TO ROLE SECURITYADMIN;{noformat}", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 112}}
{"issue_key": "CSCI-573", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Oct/25 2:00 PM", "updated": "25/Nov/25 11:00 AM", "labels": [], "summary": "EDP Infra|| Create Snowflake Environment For IaC", "description": "h3. Objective\n\nCreate and configure Snowflake environments (DEV, SIT, UAT, PROD) using IaC, ensuring consistent configuration, compliance and zero manual intervention. All Snowflake environments must be fully automated via IaC to guarantee consistency, traceability and easy redeployment. No manual configuration.\n\nh3. Steps \n\n# Design Teraform or scripted templates for Snowflake (accounts, database, roles, warehouse, parameters).\n# Define environment variables for DEV, SIT, UAT, PROD (naming, warehouse sizes, privileges)\n# Execute initial deployment pipelines using Azure DevOps (GitHub Actions preferred if possible)\n# Validate resource creation and connectivity to each environment\n# Document E2E setup and add validation tests\n\nh3. Deliverables\n\n* Terraform module repository for Snowflake IaC\n* Separate environment configuration files\n* Successful deployment logs and state files\n* IaC documentation and validation checklist", "acceptance_criteria": "h3. Acceptance criteria\n\n* All Snowflake environments (DEV, SIT, UAT, PROD) provisioned automatically via IaC\n* Configuration parity validated between envrionments\n* No manual setup or post deployment SQL required\n* Terraform plan and apply steps run successfully in CI/CD pipeline\n* Approved security and configuration review completed", "comments": "I am working on the high level design for Infrastructures as Code for Snowflake. Once I have got the design I will discuss with Alan, Mike and Eugene.\n\nWorking on the repo structure, naming convention and branch strategy.\n\nHere’s a quick wrap-up:\n\n We’ll separate the ALTIDA framework into its own repository and set up a dedicated pipeline for deployment and maintenance.\n\n[@Mike Kazemi|mailto:Mike.Kazemi@sigmahealthcare.com.au] will assist in confirming the naming conventions for the following repositories:\n\n* Snowflake Azure Infrastructure\n* Snowflake Environment Setup\n* ALTIDA Framework\n* Snowflake Application (potentially including a dbt component)\n\n[@Eugene Paden|mailto:ra_ray_eugene.paden@mychemist.com.au] will update the repositories by country and functionality.\n\n[@Mike Kazemi|mailto:Mike.Kazemi@sigmahealthcare.com.au] will review and confirm the naming for the creation of the Service Account for IaC. I’ll send a separate email with the required details.\n\n[@Xavier Varalda|mailto:xavier.varalda@sigmahealthcare.com.au],\nWe can proceed straightaway with the first three layers to ensure version control of deployment artefacts. The final layer Snowflake Application CI/CD will require more detailed design, as it involves our data model architecture and dbt components. We’ve agreed to tackle the first three layers first, and then have an in-depth discussion around the Snowflake Application CI/CD layer at a later stage.\n\nService account to be created for Snowflake IaC with the appropriate permission. \n\n [https://sigmahealthcare.atlassian.net/browse/CSCI-620|https://sigmahealthcare.atlassian.net/browse/CSCI-620]", "text": "Summary\nEDP Infra|| Create Snowflake Environment For IaC\n\n---\n\nDescription\nh3. Objective\n\nCreate and configure Snowflake environments (DEV, SIT, UAT, PROD) using IaC, ensuring consistent configuration, compliance and zero manual intervention. All Snowflake environments must be fully automated via IaC to guarantee consistency, traceability and easy redeployment. No manual configuration.\n\nh3. Steps \n\n# Design Teraform or scripted templates for Snowflake (accounts, database, roles, warehouse, parameters).\n# Define environment variables for DEV, SIT, UAT, PROD (naming, warehouse sizes, privileges)\n# Execute initial deployment pipelines using Azure DevOps (GitHub Actions preferred if possible)\n# Validate resource creation and connectivity to each environment\n# Document E2E setup and add validation tests\n\nh3. Deliverables\n\n* Terraform module repository for Snowflake IaC\n* Separate environment configuration files\n* Successful deployment logs and state files\n* IaC documentation and validation checklist\n\n---\n\nAcceptance Criteria\nh3. Acceptance criteria\n\n* All Snowflake environments (DEV, SIT, UAT, PROD) provisioned automatically via IaC\n* Configuration parity validated between envrionments\n* No manual setup or post deployment SQL required\n* Terraform plan and apply steps run successfully in CI/CD pipeline\n* Approved security and configuration review completed\n\n---\n\nComments\nI am working on the high level design for Infrastructures as Code for Snowflake. Once I have got the design I will discuss with Alan, Mike and Eugene.\n\nWorking on the repo structure, naming convention and branch strategy.\n\nHere’s a quick wrap-up:\n\n We’ll separate the ALTIDA framework into its own repository and set up a dedicated pipeline for deployment and maintenance.\n\n[@Mike Kazemi|mailto:Mike.Kazemi@sigmahealthcare.com.au] will assist in confirming the naming conventions for the following repositories:\n\n* Snowflake Azure Infrastructure\n* Snowflake Environment Setup\n* ALTIDA Framework\n* Snowflake Application (potentially including a dbt component)\n\n[@Eugene Paden|mailto:ra_ray_eugene.paden@mychemist.com.au] will update the repositories by country and functionality.\n\n[@Mike Kazemi|mailto:Mike.Kazemi@sigmahealthcare.com.au] will review and confirm the naming for the creation of the Service Account for IaC. I’ll send a separate email with the required details.\n\n[@Xavier Varalda|mailto:xavier.varalda@sigmahealthcare.com.au],\nWe can proceed straightaway with the first three layers to ensure version control of deployment artefacts. The final layer Snowflake Application CI/CD will require more detailed design, as it involves our data model architecture and dbt components. We’ve agreed to tackle the first three layers first, and then have an in-depth discussion around the Snowflake Application CI/CD layer at a later stage.\n\nService account to be created for Snowflake IaC with the appropriate permission. \n\n [https://sigmahealthcare.atlassian.net/browse/CSCI-620|https://sigmahealthcare.atlassian.net/browse/CSCI-620]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 113}}
{"issue_key": "CSCI-571", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Oct/25 1:45 PM", "updated": "16/Dec/25 8:48 AM", "labels": [], "summary": "EDP Infra || Developer Access/Network Policy", "description": "h3. Objective\n\nDefine and implement network, access and connectivity policies for developers and service accounts. Ensure secure, controlled developer access and prevent accidental changes to production.\n\nh3. Steps \n\n# Create network policies per environment\n# Implement IP whitelisting, VPN or VNet Gateway\n# Enforce read-only access in production\n# Apply SSO/MFA via Entra ID\n\nh3. Deliverables\n\n* Network policies and role mapping\n* Access review documentation\n* Connectivity tests", "acceptance_criteria": "* Developers have modification privileges to DEV and sandbox only\n* SIT, UAT and PROD can be modified via service accounts only\n* MFA and leas privilege enforced", "comments": "I am able to SSO auth to our Snowflake private link using the jumpbox\n\nHere are the next steps in sequence: \n\n# Confirm all developers can SSO to privatelink url: [+cw-au.privatelink.snowflakecomputing.com+|http://cw-au.privatelink.snowflakecomputing.com]? Jess Coyle Eugene Paden Ashutosh Arya Mike Kazemi\n# We will add a network policy to block access to the public link: [+cw-au.snowflakecomputing.com+|http://cw-au.snowflakecomputing.com]\n# We will remove the Snowflake local users that were created before the SSO/ SCIM provision. Only keep local users for break glass accounts (ie. Alan, Xavier, Mike) \n\nTimeline: over the next 3 weeks - within the month of November\n\nHi Ashu can you pls confirm your access to the VDI still on and off? can you pls follow up to get it resolved with the relevant party pls @user\n\n@user Nah, no luck. Getting a new error. Please see attached. @user Could you take a look into this?\n\n@user to check on the availability of CWR jumpbox. When can we have access to the new jumpbox that are specifically requested for EDP/ Snowflake developers?\n\nSNOW raised to create a dedicated desktop pool for Data Team - RITM0179179\n\nCWR jumpbox is now accessible by all developer\n\nIssue\n\nNo VS code/Dbeaver in for dataops and can’t be installed as CWR is a jb for sysops and eucops\n\nNeville has chase up on jb creation for dataops\n\nEmailed Neville/John to install below software to Dataops jumpbox\n\nadditional software installation in progress\n\n20251204 - installation still in progress\n\nDone and pending testing from Jess & Mike", "text": "Summary\nEDP Infra || Developer Access/Network Policy\n\n---\n\nDescription\nh3. Objective\n\nDefine and implement network, access and connectivity policies for developers and service accounts. Ensure secure, controlled developer access and prevent accidental changes to production.\n\nh3. Steps \n\n# Create network policies per environment\n# Implement IP whitelisting, VPN or VNet Gateway\n# Enforce read-only access in production\n# Apply SSO/MFA via Entra ID\n\nh3. Deliverables\n\n* Network policies and role mapping\n* Access review documentation\n* Connectivity tests\n\n---\n\nAcceptance Criteria\n* Developers have modification privileges to DEV and sandbox only\n* SIT, UAT and PROD can be modified via service accounts only\n* MFA and leas privilege enforced\n\n---\n\nComments\nI am able to SSO auth to our Snowflake private link using the jumpbox\n\nHere are the next steps in sequence: \n\n# Confirm all developers can SSO to privatelink url: [+cw-au.privatelink.snowflakecomputing.com+|http://cw-au.privatelink.snowflakecomputing.com]? Jess Coyle Eugene Paden Ashutosh Arya Mike Kazemi\n# We will add a network policy to block access to the public link: [+cw-au.snowflakecomputing.com+|http://cw-au.snowflakecomputing.com]\n# We will remove the Snowflake local users that were created before the SSO/ SCIM provision. Only keep local users for break glass accounts (ie. Alan, Xavier, Mike) \n\nTimeline: over the next 3 weeks - within the month of November\n\nHi Ashu can you pls confirm your access to the VDI still on and off? can you pls follow up to get it resolved with the relevant party pls @user\n\n@user Nah, no luck. Getting a new error. Please see attached. @user Could you take a look into this?\n\n@user to check on the availability of CWR jumpbox. When can we have access to the new jumpbox that are specifically requested for EDP/ Snowflake developers?\n\nSNOW raised to create a dedicated desktop pool for Data Team - RITM0179179\n\nCWR jumpbox is now accessible by all developer\n\nIssue\n\nNo VS code/Dbeaver in for dataops and can’t be installed as CWR is a jb for sysops and eucops\n\nNeville has chase up on jb creation for dataops\n\nEmailed Neville/John to install below software to Dataops jumpbox\n\nadditional software installation in progress\n\n20251204 - installation still in progress\n\nDone and pending testing from Jess & Mike", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 114}}
{"issue_key": "CSCI-570", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Oct/25 1:37 PM", "updated": "07/Nov/25 10:12 AM", "labels": [], "summary": "EDP Infra || Infra & Security provision for SIT UAT PROD", "description": "h3. Objective\n\nProvision secure environments with consistent configurations across SIT, UAT and PROD. Achieve full environment parity and compliance alignment.\n\nh3. Steps \n\n# Replicate IaC templates for SIT, UAT, PROD\n# Validate VNet, Private Link, RBAC and logging setup\n\nh3. Deliverables\n\n* SIT, UAT, PROD environments deployed via IaC\n* Security approval sign-offs", "acceptance_criteria": "* 100% parity across SIT, UAT, PROD\n* Security posture matches PROD baseline.\n* All environments deployed via IaC (no manual setup).", "comments": "Discussed requirements with Anjali and Eugene.\n\nAnjali to raise a request to SysOps team.\n\n@user to work with @user to document the technical requirements for this stream of work.\n\nFYI @user\n\n@user Hey any update on this?\n\n@user Will look the details of the tickets raised by Anjali once have access to see other’s tickets on Service-Now .", "text": "Summary\nEDP Infra || Infra & Security provision for SIT UAT PROD\n\n---\n\nDescription\nh3. Objective\n\nProvision secure environments with consistent configurations across SIT, UAT and PROD. Achieve full environment parity and compliance alignment.\n\nh3. Steps \n\n# Replicate IaC templates for SIT, UAT, PROD\n# Validate VNet, Private Link, RBAC and logging setup\n\nh3. Deliverables\n\n* SIT, UAT, PROD environments deployed via IaC\n* Security approval sign-offs\n\n---\n\nAcceptance Criteria\n* 100% parity across SIT, UAT, PROD\n* Security posture matches PROD baseline.\n* All environments deployed via IaC (no manual setup).\n\n---\n\nComments\nDiscussed requirements with Anjali and Eugene.\n\nAnjali to raise a request to SysOps team.\n\n@user to work with @user to document the technical requirements for this stream of work.\n\nFYI @user\n\n@user Hey any update on this?\n\n@user Will look the details of the tickets raised by Anjali once have access to see other’s tickets on Service-Now .", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 115}}
{"issue_key": "CSCI-568", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Oct/25 2:26 PM", "updated": "24/Oct/25 9:55 AM", "labels": [], "summary": "EDP Infra || Local User Account Creation", "description": "h3. Context\n\n* Creating local user account with SF account for break glass use\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "h3. Acceptance criteria\n\n* Local account created with necessary access\n\nh3.", "comments": "Awaiting confirmation from Xavier and Alan\n\nLocal users have completed for all mentioned users. Permission assigned.", "text": "Summary\nEDP Infra || Local User Account Creation\n\n---\n\nDescription\nh3. Context\n\n* Creating local user account with SF account for break glass use\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nh3. Acceptance criteria\n\n* Local account created with necessary access\n\nh3.\n\n---\n\nComments\nAwaiting confirmation from Xavier and Alan\n\nLocal users have completed for all mentioned users. Permission assigned.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 116}}
{"issue_key": "CSCI-567", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Oct/25 10:00 AM", "updated": "24/Oct/25 11:03 AM", "labels": [], "summary": "MergeCo Reporting || Distribution Solution Monitor", "description": "h3. Context\n\n* To monitor Power automate distribution performance\n\nh3. Objective\n\n* Power automate working as expected\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "h3. Acceptance criteria\n\n* MergeCo report distributed as expected\n\nh3.", "comments": "", "text": "Summary\nMergeCo Reporting || Distribution Solution Monitor\n\n---\n\nDescription\nh3. Context\n\n* To monitor Power automate distribution performance\n\nh3. Objective\n\n* Power automate working as expected\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nh3. Acceptance criteria\n\n* MergeCo report distributed as expected\n\nh3.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 117}}
{"issue_key": "CSCI-566", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Oct/25 9:39 AM", "updated": "06/Nov/25 9:47 AM", "labels": [], "summary": "Firewall Whitelisting for new IP for SCAX2012 database server in ADF- RITM0177824", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Anjali talking to the security team to discuss\n\nThe whitelisting has been implemented. @user to test the connectivity.", "text": "Summary\nFirewall Whitelisting for new IP for SCAX2012 database server in ADF- RITM0177824\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nAnjali talking to the security team to discuss\n\nThe whitelisting has been implemented. @user to test the connectivity.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 118}}
{"issue_key": "CSCI-565", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "20/Oct/25 8:00 PM", "updated": "03/Nov/25 9:31 AM", "labels": [], "summary": "CW Platform - Internal Data Team Onboarding", "description": "Coordinate access and onboard Sigma admins (Alan, Mike, Emil) to CW platform ADF, Snowflake, Azure DevOps", "acceptance_criteria": "* Access granted in ADF, Snowflake, DevOps\n* Walk-through sessions\n* QnA, follow-ups", "comments": "Access has been granted in ADF, Snowflake, DevOps\n\nWalk-through sessions scheduled and being delivered\n\nNeed Anjali help to update access level from Stakeholder to Basic. I currently don't have this access.\n\n@user - please share the update in this card regarding devops access change, thanks\n\n@user @user @user your organisational access level in Azure DevOps have been updated. Can you pls check and confirm you have access to Repos?\n\nAccess confirmed for Mike & myself, @user can you check and let us know? Thx\n\n[https://dev.azure.com/MyChemist/Enterprise Data Platform Implementation/_git/edp-infrastructure|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infrastructure]", "text": "Summary\nCW Platform - Internal Data Team Onboarding\n\n---\n\nDescription\nCoordinate access and onboard Sigma admins (Alan, Mike, Emil) to CW platform ADF, Snowflake, Azure DevOps\n\n---\n\nAcceptance Criteria\n* Access granted in ADF, Snowflake, DevOps\n* Walk-through sessions\n* QnA, follow-ups\n\n---\n\nComments\nAccess has been granted in ADF, Snowflake, DevOps\n\nWalk-through sessions scheduled and being delivered\n\nNeed Anjali help to update access level from Stakeholder to Basic. I currently don't have this access.\n\n@user - please share the update in this card regarding devops access change, thanks\n\n@user @user @user your organisational access level in Azure DevOps have been updated. Can you pls check and confirm you have access to Repos?\n\nAccess confirmed for Mike & myself, @user can you check and let us know? Thx\n\n[https://dev.azure.com/MyChemist/Enterprise Data Platform Implementation/_git/edp-infrastructure|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infrastructure]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 119}}
{"issue_key": "CSCI-564", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "20/Oct/25 5:27 PM", "updated": "21/Oct/25 10:00 AM", "labels": [], "summary": "MergeCo Reporting || Bug Issue for Network Drive File Ingestion", "description": "h3. Context\n\n* As of Thursday 16th Oct, the DFIO_SKU.csv file located on the CWVault Network Drive stopped refreshing each day. Found the issue related to the fact Supply Chain team decommissioned the Boomi task to populate this file daily\n* This impacts the data source for the MergeCo Supply Chain Scorecard\n\nh3. Objective\n\n* Need to reinstate the DFIO_SKU.csv file to be generated on the CWVault network drive\n\nh3. Steps \n\n# Contact Boomi team to startup network drive file extract again\n# Create request with Boomi team\n# Validate that the extract is now back online and refreshing daily\n\nh3. Acceptance criteria\n\n* The DFIO_SKU.csv file is auto-generated by the Boomi extract into the Network Drive\n* The file is refreshed for at least 2 days consecutively", "acceptance_criteria": "* -The DFIO_SKU.csv file is auto-generated by the Boomi extract into the Network Drive-\n* -The file is refreshed for at least 2 days consecutively-", "comments": "", "text": "Summary\nMergeCo Reporting || Bug Issue for Network Drive File Ingestion\n\n---\n\nDescription\nh3. Context\n\n* As of Thursday 16th Oct, the DFIO_SKU.csv file located on the CWVault Network Drive stopped refreshing each day. Found the issue related to the fact Supply Chain team decommissioned the Boomi task to populate this file daily\n* This impacts the data source for the MergeCo Supply Chain Scorecard\n\nh3. Objective\n\n* Need to reinstate the DFIO_SKU.csv file to be generated on the CWVault network drive\n\nh3. Steps \n\n# Contact Boomi team to startup network drive file extract again\n# Create request with Boomi team\n# Validate that the extract is now back online and refreshing daily\n\nh3. Acceptance criteria\n\n* The DFIO_SKU.csv file is auto-generated by the Boomi extract into the Network Drive\n* The file is refreshed for at least 2 days consecutively\n\n---\n\nAcceptance Criteria\n* -The DFIO_SKU.csv file is auto-generated by the Boomi extract into the Network Drive-\n* -The file is refreshed for at least 2 days consecutively-", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 120}}
{"issue_key": "CSCI-563", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "20/Oct/25 10:51 AM", "updated": "21/Oct/25 9:47 AM", "labels": [], "summary": "Add Users to AD Groups-RITM0177612", "description": "h3. Groups : \n\nSnowflake-CW-AU-PlatformEngineers\nAzure-Permissions-dev-edp-aus-Contributor\nAzure-Permissions-qa-edp-aus-Contributor\nAzure-Permissions-prd-edp-aus-Contributor\nAzure-Permissions-dev-edp-aus-UAA\nAzure-Permissions-qa-edp-aus-UAA\nAzure-Permissions-prd-edp-aus-UAA\nAzure-Permissions-dev-edp-aus-Billing Reader\nAzure-Permissions-qa-edp-aus-Billing Reader\nAzure-Permissions-prd-edp-aus-Billing Reader\nAzure-Permissions-dev-edp-aus-Reader\nAzure-Permissions-qa-edp-aus-Reader\nAzure-Permissions-prd-edp-aus-Reader\nAzure-Permissions-dev-edp-aus-Owner\nAzure-Permissions-qa-edp-aus-Owner\nAzure-Permissions-prd-edp-aus-Owner\n\nUsers to Add : \n\n[sig_mike.kazemi@chemistwarehouse.com.au|mailto:sig_mike.kazemi@chemistwarehouse.com.au] & [sig_alan.yuen@chemistwarehouse.com.au|mailto:sig_alan.yuen@chemistwarehouse.com.au] & [sig_emil.julius@chemistwarehouse.com.au|mailto:sig_emil.julius@chemistwarehouse.com.au]\n\nGroup:\nAzure-Permissions-dev-edp-aus-Reader\nAzure-Permissions-qa-edp-aus-Reader\nAzure-Permissions-prd-edp-aus-Reader\n\nUser to add: [sig_han.li@chemistwarehouse.com.au|mailto:sig_han.li@chemistwarehouse.com.au]\n\nh3. Acceptance criteria\n\n* Users are successfully added to the above AD Groups\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Users have been added to the above mentioned AD Groups. \n\n@user @user", "text": "Summary\nAdd Users to AD Groups-RITM0177612\n\n---\n\nDescription\nh3. Groups : \n\nSnowflake-CW-AU-PlatformEngineers\nAzure-Permissions-dev-edp-aus-Contributor\nAzure-Permissions-qa-edp-aus-Contributor\nAzure-Permissions-prd-edp-aus-Contributor\nAzure-Permissions-dev-edp-aus-UAA\nAzure-Permissions-qa-edp-aus-UAA\nAzure-Permissions-prd-edp-aus-UAA\nAzure-Permissions-dev-edp-aus-Billing Reader\nAzure-Permissions-qa-edp-aus-Billing Reader\nAzure-Permissions-prd-edp-aus-Billing Reader\nAzure-Permissions-dev-edp-aus-Reader\nAzure-Permissions-qa-edp-aus-Reader\nAzure-Permissions-prd-edp-aus-Reader\nAzure-Permissions-dev-edp-aus-Owner\nAzure-Permissions-qa-edp-aus-Owner\nAzure-Permissions-prd-edp-aus-Owner\n\nUsers to Add : \n\n[sig_mike.kazemi@chemistwarehouse.com.au|mailto:sig_mike.kazemi@chemistwarehouse.com.au] & [sig_alan.yuen@chemistwarehouse.com.au|mailto:sig_alan.yuen@chemistwarehouse.com.au] & [sig_emil.julius@chemistwarehouse.com.au|mailto:sig_emil.julius@chemistwarehouse.com.au]\n\nGroup:\nAzure-Permissions-dev-edp-aus-Reader\nAzure-Permissions-qa-edp-aus-Reader\nAzure-Permissions-prd-edp-aus-Reader\n\nUser to add: [sig_han.li@chemistwarehouse.com.au|mailto:sig_han.li@chemistwarehouse.com.au]\n\nh3. Acceptance criteria\n\n* Users are successfully added to the above AD Groups\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nUsers have been added to the above mentioned AD Groups. \n\n@user @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 121}}
{"issue_key": "CSCI-562", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "17/Oct/25 9:51 AM", "updated": "20/Oct/25 9:59 AM", "labels": [], "summary": "Infra || Enterprise Observability Tool Review", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "TBC", "comments": "", "text": "Summary\nInfra || Enterprise Observability Tool Review\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nTBC", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 122}}
{"issue_key": "CSCI-561", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Oct/25 1:54 PM", "updated": "24/Oct/25 2:02 PM", "labels": [], "summary": "Azure DevOps access - PR review and approval", "description": "To transfer Review and Approval access to Alan, Mike, Emil, and Jagdish, as they are the designated platform owners from the Sigma side:\n\n@user could you please update the Pull Request settings for all EDP projects in Azure DevOps to include the following Sigma accounts as required reviewers?\n\n* Alan Yuen\n* Mike Kazemi\n* Emil Julius\n* Jagdish Jha\n\nAt least one approval from any of the individuals listed above is required to merge code into the repositories.\n\n@user @user @user @user @user @user", "acceptance_criteria": "Given, When, Then", "comments": "@user need clarification on the changes:\n\n* <repository name> \n** PR Review\n*** number of reviewers - ?\n*** required reviewers - ?\n*** optional reviewers - ?\n** Release Pipeline\n*** QA\n**** Approvers - ?\n**** Approval Mode - ? (Any Order, In Sequence, Any One User)\n*** Prod\n**** Approvers - ?\n**** Approval Mode - ? (Any Order, In Sequence, Any One User)\n\nwe are working on the following repositories\n\n* edp-infrastructure\n* edp-snowflake-infrastructure\n* edp-data-factory\n* edp-snowflake\n* edp-semantic-models\n* edp-powerbi-reports\n\nwill start working on them soon as I have the detailed breakdown.\n\n@user let me check with @user and @user on the process.\n\n@user based on our meeting yesterday, can we pls have a tentative list for each repo?\n\ncc: @user @user\n\nUpdated [edp-infrastructure|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infrastructure/branches]\n\n* PR Approver\n\n* Deployment to QA\n\n* Deployment to Prod\n\ncc: @user @user @user\n\nUpdated [edp-snowflake-infrastructure|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=4e016c43-1374-45eb-b26a-62da7bb94997]\n\n* PR Approver\n\n* Deploy to Prod\n\ncc: @user @user @user\n\nUpdated [edp-data-factory|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=a9d4deeb-ad55-465e-9425-438e29d991d5]\n\n* PR Approver\n\n* Deployment to QA\n\n* Deployment to Prod\n\ncc; @user @user @user\n\nUpdated [edp-snowflake|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=35a748cb-2300-47fc-a80d-206021e6c335]\n\n* PR Approver\n\n* Deployment to SIT\n\n* Deployment to UAT\n\n* Deployment to Prod\n\n@user @user @user\n\nUpdated [edp-semantic-models|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=f1342e50-703c-470f-91c7-fee3a7eff4f4&_a=policiesMid]\n\n* PR Approvers\n\ncc: @user @user @user\n\nUpdated [edp-powerbi-reports|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=adb2a221-e26d-407d-aa7b-b5972abbd947]\n\n* PR Approvers\n\ncc: @user @user @user", "text": "Summary\nAzure DevOps access - PR review and approval\n\n---\n\nDescription\nTo transfer Review and Approval access to Alan, Mike, Emil, and Jagdish, as they are the designated platform owners from the Sigma side:\n\n@user could you please update the Pull Request settings for all EDP projects in Azure DevOps to include the following Sigma accounts as required reviewers?\n\n* Alan Yuen\n* Mike Kazemi\n* Emil Julius\n* Jagdish Jha\n\nAt least one approval from any of the individuals listed above is required to merge code into the repositories.\n\n@user @user @user @user @user @user\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n@user need clarification on the changes:\n\n* <repository name> \n** PR Review\n*** number of reviewers - ?\n*** required reviewers - ?\n*** optional reviewers - ?\n** Release Pipeline\n*** QA\n**** Approvers - ?\n**** Approval Mode - ? (Any Order, In Sequence, Any One User)\n*** Prod\n**** Approvers - ?\n**** Approval Mode - ? (Any Order, In Sequence, Any One User)\n\nwe are working on the following repositories\n\n* edp-infrastructure\n* edp-snowflake-infrastructure\n* edp-data-factory\n* edp-snowflake\n* edp-semantic-models\n* edp-powerbi-reports\n\nwill start working on them soon as I have the detailed breakdown.\n\n@user let me check with @user and @user on the process.\n\n@user based on our meeting yesterday, can we pls have a tentative list for each repo?\n\ncc: @user @user\n\nUpdated [edp-infrastructure|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infrastructure/branches]\n\n* PR Approver\n\n* Deployment to QA\n\n* Deployment to Prod\n\ncc: @user @user @user\n\nUpdated [edp-snowflake-infrastructure|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=4e016c43-1374-45eb-b26a-62da7bb94997]\n\n* PR Approver\n\n* Deploy to Prod\n\ncc: @user @user @user\n\nUpdated [edp-data-factory|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=a9d4deeb-ad55-465e-9425-438e29d991d5]\n\n* PR Approver\n\n* Deployment to QA\n\n* Deployment to Prod\n\ncc; @user @user @user\n\nUpdated [edp-snowflake|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=35a748cb-2300-47fc-a80d-206021e6c335]\n\n* PR Approver\n\n* Deployment to SIT\n\n* Deployment to UAT\n\n* Deployment to Prod\n\n@user @user @user\n\nUpdated [edp-semantic-models|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=f1342e50-703c-470f-91c7-fee3a7eff4f4&_a=policiesMid]\n\n* PR Approvers\n\ncc: @user @user @user\n\nUpdated [edp-powerbi-reports|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_settings/repositories?repo=adb2a221-e26d-407d-aa7b-b5972abbd947]\n\n* PR Approvers\n\ncc: @user @user @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 123}}
{"issue_key": "CSCI-559", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Oct/25 10:28 AM", "updated": "24/Oct/25 11:04 AM", "labels": [], "summary": "MergeCo Reporting || Distribution Solution Discovery", "description": "h3. Context\n\n* To investigate on solution for report distribution\n\nh3. Objective\n\n* Understand options for automating report distribution\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "h3. Acceptance criteria\n\n* Document/understand what option is most suitable for MergeCo report\n\nh3.", "comments": "Have created a Power Automate flow to generate the MergeCo Supply Chain Scorecard.\nThe flow:\n\n* Extracts the PBI report as a .pdf\n* Validates 3 metrics\n** Ensures the latest date in the Semantic Model matches the latest date based on a TODAY() date function\n** Availability % is between 80% and 100%\n** SOH is between $500M and $1B\n* Emails the report to distribution list if validation is successful\n* Emails a notification alert to the Data Eng team if validation fails\n\n[https://make.powerautomate.com/environments/Default-8bc0280e-b3ad-438a-ba8a-2cf00d19af4a/flows/74c08131-3e07-477c-9442-7a88a2d67751/details|https://make.powerautomate.com/environments/Default-8bc0280e-b3ad-438a-ba8a-2cf00d19af4a/flows/74c08131-3e07-477c-9442-7a88a2d67751/details]", "text": "Summary\nMergeCo Reporting || Distribution Solution Discovery\n\n---\n\nDescription\nh3. Context\n\n* To investigate on solution for report distribution\n\nh3. Objective\n\n* Understand options for automating report distribution\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nh3. Acceptance criteria\n\n* Document/understand what option is most suitable for MergeCo report\n\nh3.\n\n---\n\nComments\nHave created a Power Automate flow to generate the MergeCo Supply Chain Scorecard.\nThe flow:\n\n* Extracts the PBI report as a .pdf\n* Validates 3 metrics\n** Ensures the latest date in the Semantic Model matches the latest date based on a TODAY() date function\n** Availability % is between 80% and 100%\n** SOH is between $500M and $1B\n* Emails the report to distribution list if validation is successful\n* Emails a notification alert to the Data Eng team if validation fails\n\n[https://make.powerautomate.com/environments/Default-8bc0280e-b3ad-438a-ba8a-2cf00d19af4a/flows/74c08131-3e07-477c-9442-7a88a2d67751/details|https://make.powerautomate.com/environments/Default-8bc0280e-b3ad-438a-ba8a-2cf00d19af4a/flows/74c08131-3e07-477c-9442-7a88a2d67751/details]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 124}}
{"issue_key": "CSCI-558", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Oct/25 10:19 AM", "updated": "10/Nov/25 9:48 AM", "labels": [], "summary": "MergeCo Reporting || Logic Update", "description": "To obtain enhance requirement for report update.\n\nh3. Acceptance criteria\n\n* Understand enhancement need\n* Report update\n* UAT\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nMergeCo Reporting || Logic Update\n\n---\n\nDescription\nTo obtain enhance requirement for report update.\n\nh3. Acceptance criteria\n\n* Understand enhancement need\n* Report update\n* UAT\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 125}}
{"issue_key": "CSCI-557", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Oct/25 10:03 AM", "updated": "23/Oct/25 9:49 AM", "labels": [], "summary": "Manhattan MAWM Data Save access || Service Account Review", "description": "h3. Context\n\n* Manhattan MAWM Data Save access require new service account as current *username \"readonlyuser\"* will be disable. We need to raise a request to have new CW service account email ID created so Manhattan team can assign access.\n* Expanding the question to 1. \n\nh3. Acceptance criteria\n\n* -Draw/Document/Determine +*all*+ the service accounts we will need for CW Snowflake solution upfront.-\n* Mike to review the above artefact\n* Once reviewed and we have a list of all the service accounts we will need, then we start raising and tracking the requests\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "* -Draw/Document/Determine +*all*+ the service accounts we will need for CW Snowflake solution upfront.-\n* -Mike/Alan to review the above artefact-\n* Once reviewed and we have a list of all the service accounts we will need, then we start raising and tracking the requests", "comments": "Have provided information to Alan and team via email.\n\n|*SOURCETYPE*|*DATABASENAME*|*SERVERNAME*|*Azure Keyvault User Name*|*Azure Keyvault Password Name*|*Environment*|\n|SqlServer|AXLINK|192.168.29.206|TDB19-Dev-SQL-LocalAccountName|TDB19-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|CWMgtStoreInvoices|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|General_Reference|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|ILS|192.168.29.78|TDB15-Dev-SQL-LocalAccountName|TDB15-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|SCAX2012|192.168.29.206|TDB19-Dev-SQL-LocalAccountName|TDB19-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|SKU|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|SpsWhsPurchase|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|StockDb|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|TransactionStorage|192.168.29.104|TDB14-Dev-SQL-LocalAccountName|TDB14-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|BI_Presentation|192.168.42.82|PBI05-Prod-SQL-LocalAccountName|PBI05-Prod-SQL-LocalAccountPassword|Prod|\n|SqlServer|SupplyChain|192.168.42.82|PBI05-Prod-SQL-LocalAccountName|PBI05-Prod-SQL-LocalAccountPassword|Prod|\n|SqlServer|TransactionsArchive|192.168.42.82|PBI05-Prod-SQL-LocalAccountName|PBI05-Prod-SQL-LocalAccountPassword|Prod|\n| | | | | | |\n|Nework Drive|NetworkDrive_CWVault|192.168.42.235|*pending (servac_edp_tactical)*|pending|Prod|\n|MySQL|default_dcinventory|172.19.232.11|*require a CW Service Account Email ID*| |Prod|\n\n \n\n* We haven’t yet had any EDP service account to access corporate systems (network drive, sharepoint etc.). We are waiting for the servac_edp_tactical to be created for MergeCo.\n* The MySQL Manhattan MAWM (high-lighted red) is pending a request.\n\n@user @user \nCan you please review and advise further\n\nHi @user @user I have documented the service accounts for source systems all in the section *5. Source System* of the design doc [CWR - EDP Solution Architecture - Detailed Design v.01.docx|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/05.%20Architecture/CWR%20-%20EDP%20Solution%20Architecture%20%20-%20Detailed%20Design%20v.01.docx?d=w2b1d2a9d799b4326b9cf52840fafa912&csf=1&web=1&e=X6HgI7]. Please review and advise if @user/@user can proceed to raise a request for a new service account ie.servac_edp_{name}.\n\n@user I’ve added a new page under *“Standards & Principles”* outlining the naming conventions for Service Accounts. When you have a moment, please take a look and let me know if you have any questions. [Service Accounts - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1688174594/Service+Accounts] (Still under dev)\n\n@user @user @user please review", "text": "Summary\nManhattan MAWM Data Save access || Service Account Review\n\n---\n\nDescription\nh3. Context\n\n* Manhattan MAWM Data Save access require new service account as current *username \"readonlyuser\"* will be disable. We need to raise a request to have new CW service account email ID created so Manhattan team can assign access.\n* Expanding the question to 1. \n\nh3. Acceptance criteria\n\n* -Draw/Document/Determine +*all*+ the service accounts we will need for CW Snowflake solution upfront.-\n* Mike to review the above artefact\n* Once reviewed and we have a list of all the service accounts we will need, then we start raising and tracking the requests\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\n* -Draw/Document/Determine +*all*+ the service accounts we will need for CW Snowflake solution upfront.-\n* -Mike/Alan to review the above artefact-\n* Once reviewed and we have a list of all the service accounts we will need, then we start raising and tracking the requests\n\n---\n\nComments\nHave provided information to Alan and team via email.\n\n|*SOURCETYPE*|*DATABASENAME*|*SERVERNAME*|*Azure Keyvault User Name*|*Azure Keyvault Password Name*|*Environment*|\n|SqlServer|AXLINK|192.168.29.206|TDB19-Dev-SQL-LocalAccountName|TDB19-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|CWMgtStoreInvoices|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|General_Reference|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|ILS|192.168.29.78|TDB15-Dev-SQL-LocalAccountName|TDB15-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|SCAX2012|192.168.29.206|TDB19-Dev-SQL-LocalAccountName|TDB19-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|SKU|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|SpsWhsPurchase|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|StockDb|192.168.29.105|TDB08AX2012-Dev-SQL-LocalAccountName|TDB08AX2012-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|TransactionStorage|192.168.29.104|TDB14-Dev-SQL-LocalAccountName|TDB14-Dev-SQL-LocalAccountPassword|Dev|\n|SqlServer|BI_Presentation|192.168.42.82|PBI05-Prod-SQL-LocalAccountName|PBI05-Prod-SQL-LocalAccountPassword|Prod|\n|SqlServer|SupplyChain|192.168.42.82|PBI05-Prod-SQL-LocalAccountName|PBI05-Prod-SQL-LocalAccountPassword|Prod|\n|SqlServer|TransactionsArchive|192.168.42.82|PBI05-Prod-SQL-LocalAccountName|PBI05-Prod-SQL-LocalAccountPassword|Prod|\n| | | | | | |\n|Nework Drive|NetworkDrive_CWVault|192.168.42.235|*pending (servac_edp_tactical)*|pending|Prod|\n|MySQL|default_dcinventory|172.19.232.11|*require a CW Service Account Email ID*| |Prod|\n\n \n\n* We haven’t yet had any EDP service account to access corporate systems (network drive, sharepoint etc.). We are waiting for the servac_edp_tactical to be created for MergeCo.\n* The MySQL Manhattan MAWM (high-lighted red) is pending a request.\n\n@user @user \nCan you please review and advise further\n\nHi @user @user I have documented the service accounts for source systems all in the section *5. Source System* of the design doc [CWR - EDP Solution Architecture - Detailed Design v.01.docx|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/05.%20Architecture/CWR%20-%20EDP%20Solution%20Architecture%20%20-%20Detailed%20Design%20v.01.docx?d=w2b1d2a9d799b4326b9cf52840fafa912&csf=1&web=1&e=X6HgI7]. Please review and advise if @user/@user can proceed to raise a request for a new service account ie.servac_edp_{name}.\n\n@user I’ve added a new page under *“Standards & Principles”* outlining the naming conventions for Service Accounts. When you have a moment, please take a look and let me know if you have any questions. [Service Accounts - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1688174594/Service+Accounts] (Still under dev)\n\n@user @user @user please review", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 126}}
{"issue_key": "CSCI-555", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Oct/25 2:02 PM", "updated": "05/Dec/25 11:32 AM", "labels": [], "summary": "Engagement with dbt and CW teams for dbt Cloud", "description": "# Initial discussions with dbt and CW infrastructure teams for onboarding and setup\n# Appropriate infrastructure tasks created and completed\n# Dbt to Alation\n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nEngagement with dbt and CW teams for dbt Cloud\n\n---\n\nDescription\n# Initial discussions with dbt and CW infrastructure teams for onboarding and setup\n# Appropriate infrastructure tasks created and completed\n# Dbt to Alation\n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 127}}
{"issue_key": "CSCI-544", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Oct/25 1:19 PM", "updated": "28/Nov/25 9:47 AM", "labels": [], "summary": "Dbt Implementation Business case", "description": "Create Business Case for Dbt Implementation in Data Platform\n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Completed Dbt Implementation Business Case", "comments": "", "text": "Summary\nDbt Implementation Business case\n\n---\n\nDescription\nCreate Business Case for Dbt Implementation in Data Platform\n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nCompleted Dbt Implementation Business Case", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 128}}
{"issue_key": "CSCI-543", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Oct/25 1:07 PM", "updated": "19/Nov/25 11:26 AM", "labels": [], "summary": "Document what tasks are still required to completed Developer Access/Network Policy for ADF and SF", "description": "h3. \n\nDocument remaining required tasks for Developer Access/Network Policy completion\n\nDEV → SIT → UAT → PROD\n\nh3. \n\n*Open related tickets*\n\nh3. [https://sigmahealthcare.atlassian.net/browse/CSCI-525|https://sigmahealthcare.atlassian.net/browse/CSCI-525]\n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nDocument what tasks are still required to completed Developer Access/Network Policy for ADF and SF\n\n---\n\nDescription\nh3. \n\nDocument remaining required tasks for Developer Access/Network Policy completion\n\nDEV → SIT → UAT → PROD\n\nh3. \n\n*Open related tickets*\n\nh3. [https://sigmahealthcare.atlassian.net/browse/CSCI-525|https://sigmahealthcare.atlassian.net/browse/CSCI-525]\n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 129}}
{"issue_key": "CSCI-542", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Oct/25 1:06 PM", "updated": "19/Nov/25 11:26 AM", "labels": [], "summary": "Placeholder", "description": "Document remaining required tasks for Infrastructure and security setup to enable e2e environment deployment.\n\nDEV → SIT → UAT → PROD\n\n* All environments (DEV, SIT, UAT, PROD)\n* Any security aspects outstanding? What are they?\n* Access and network Policies\n* Infrastructure (IaC)\n* CI/CD Pipelines\n* Ingestion framework\n* Dbt and transformation framework\n* All requires On-Prem Whitelisting on both sides for PROD environment, these should be called out now and raised now as we know this can take time\n* Completion of build of one Core Dim – Product or Store\n\n@user and @user to help define what is outstanding to meet there requirements.\n\n*Related Open Tickets*\n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nPlaceholder\n\n---\n\nDescription\nDocument remaining required tasks for Infrastructure and security setup to enable e2e environment deployment.\n\nDEV → SIT → UAT → PROD\n\n* All environments (DEV, SIT, UAT, PROD)\n* Any security aspects outstanding? What are they?\n* Access and network Policies\n* Infrastructure (IaC)\n* CI/CD Pipelines\n* Ingestion framework\n* Dbt and transformation framework\n* All requires On-Prem Whitelisting on both sides for PROD environment, these should be called out now and raised now as we know this can take time\n* Completion of build of one Core Dim – Product or Store\n\n@user and @user to help define what is outstanding to meet there requirements.\n\n*Related Open Tickets*\n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 130}}
{"issue_key": "CSCI-541", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Oct/25 10:56 AM", "updated": "17/Oct/25 9:10 AM", "labels": ["Design"], "summary": "Bulk loading Proc - Logging/Documentation of usage", "description": "h3. Context\n\n* For the end-to-end automation of the parquet based bulk uploads a procedure is created in the Snowflake CONTROL.PR_BULKLOAD_PARQUET(SOURCE_NAME VARCHAR,STAGE_TABLE_NAME VARCHAR). \n\nh3. Objective\n\n* Add logging at each step for better user experience.\n\nh3. Acceptance criteria\n\n* Add a logging table \n* Add logic in procedure to log every step’s start end and end time.\n* Document the logging\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Logging implemented. Have to document the details of the process in the operational documentation.", "text": "Summary\nBulk loading Proc - Logging/Documentation of usage\n\n---\n\nDescription\nh3. Context\n\n* For the end-to-end automation of the parquet based bulk uploads a procedure is created in the Snowflake CONTROL.PR_BULKLOAD_PARQUET(SOURCE_NAME VARCHAR,STAGE_TABLE_NAME VARCHAR). \n\nh3. Objective\n\n* Add logging at each step for better user experience.\n\nh3. Acceptance criteria\n\n* Add a logging table \n* Add logic in procedure to log every step’s start end and end time.\n* Document the logging\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nLogging implemented. Have to document the details of the process in the operational documentation.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 131}}
{"issue_key": "CSCI-540", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Oct/25 8:54 AM", "updated": "14/Oct/25 10:56 AM", "labels": [], "summary": "Parquet File load behaviour", "description": "h3. Context\n\n* While testing the performance the data ingestion from Parquet file was behaving unexpectedly.\n* The load configuration was set to ingest data for all null columns which is not behaving as expected.\n\nh3. Objective\n\n* Understand the behaviour of parquet data ingestion.\n\nh3. Acceptance criteria\n\n* Understand & document the behaviour of parquet data ingestion.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nParquet File load behaviour\n\n---\n\nDescription\nh3. Context\n\n* While testing the performance the data ingestion from Parquet file was behaving unexpectedly.\n* The load configuration was set to ingest data for all null columns which is not behaving as expected.\n\nh3. Objective\n\n* Understand the behaviour of parquet data ingestion.\n\nh3. Acceptance criteria\n\n* Understand & document the behaviour of parquet data ingestion.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 132}}
{"issue_key": "CSCI-539", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Oct/25 8:22 AM", "updated": "15/Oct/25 1:53 PM", "labels": [], "summary": "MergeCo Retail Data Ingestion || Sprint 10", "description": "h3. Context\n\ninitiate the process to convert the tables in to Parquet files for last 2 years of data for the fact tables mentioned here [PBI05 Objects for MergeCo Reporting.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BD5AD27C1-A51F-46E0-89EE-1B61847B2DAA%7D&file=PBI05%20Objects%20for%20MergeCo%20Reporting.xlsx&action=default&mobileredirect=true] (except FCT_STOCK_HISTORY)\n\nh3. Objective\n\n|Table Name|\n|FCT_SALES|\n|FCT_CUSTOMER_COUNT_HOURLY_AUS|\n|SubCategoryBudgetDaily|\n|FCT_SALES_NZL|\n|FCT_CUSTOMER_COUNT_HOURLY_NZL|\n\nh3. Acceptance criteria\n\n* Files being present in the Azure Storage", "acceptance_criteria": "Given, When, Then", "comments": "SubCategoryBudgetDaily and FCT_CUSTOMER_COUNT_HOURLY_AUS \n\nhave finished extracting \n\nThey are currently being uploaded intot he blob storage.\n\nNew card to be created in sprint 10 if ingestion failed.\n\nthe rest of the tables have been extracted and is being transfered into the Blobstorage\n\n@user , can you please create a split card or another card for the new sprint. Thanks\n\nHey @user \n\n[https://sigmahealthcare.atlassian.net/browse/CSCI-519|https://sigmahealthcare.atlassian.net/browse/CSCI-519] - This was the one for Sprint 9\n\n[https://sigmahealthcare.atlassian.net/browse/CSCI-539?sourceType=mention|https://sigmahealthcare.atlassian.net/browse/CSCI-539?sourceType=mention] - Which is this card is split from above and assigned to Sprint 10 for any follow up you need\n\nThx\n\n@user Thank you", "text": "Summary\nMergeCo Retail Data Ingestion || Sprint 10\n\n---\n\nDescription\nh3. Context\n\ninitiate the process to convert the tables in to Parquet files for last 2 years of data for the fact tables mentioned here [PBI05 Objects for MergeCo Reporting.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BD5AD27C1-A51F-46E0-89EE-1B61847B2DAA%7D&file=PBI05%20Objects%20for%20MergeCo%20Reporting.xlsx&action=default&mobileredirect=true] (except FCT_STOCK_HISTORY)\n\nh3. Objective\n\n|Table Name|\n|FCT_SALES|\n|FCT_CUSTOMER_COUNT_HOURLY_AUS|\n|SubCategoryBudgetDaily|\n|FCT_SALES_NZL|\n|FCT_CUSTOMER_COUNT_HOURLY_NZL|\n\nh3. Acceptance criteria\n\n* Files being present in the Azure Storage\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nSubCategoryBudgetDaily and FCT_CUSTOMER_COUNT_HOURLY_AUS \n\nhave finished extracting \n\nThey are currently being uploaded intot he blob storage.\n\nNew card to be created in sprint 10 if ingestion failed.\n\nthe rest of the tables have been extracted and is being transfered into the Blobstorage\n\n@user , can you please create a split card or another card for the new sprint. Thanks\n\nHey @user \n\n[https://sigmahealthcare.atlassian.net/browse/CSCI-519|https://sigmahealthcare.atlassian.net/browse/CSCI-519] - This was the one for Sprint 9\n\n[https://sigmahealthcare.atlassian.net/browse/CSCI-539?sourceType=mention|https://sigmahealthcare.atlassian.net/browse/CSCI-539?sourceType=mention] - Which is this card is split from above and assigned to Sprint 10 for any follow up you need\n\nThx\n\n@user Thank you", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 133}}
{"issue_key": "CSCI-538", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Oct/25 9:24 AM", "updated": "14/Oct/25 9:33 AM", "labels": ["Design"], "summary": "Bulk upload automation - performance testing", "description": "h3. Context\n\n* The automation is working for smaller table (61M records). \n\nh3. Objective\n\n* To test the performance of the pipeline with high number parquet file and records table. \n\nh3. \n\nh3. Acceptance criteria\n\n* We have loaded over 600M records in under 1 hour manually the automation should give similar results if not better.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Performance test complete and took around 1 hour to load 450M records.", "text": "Summary\nBulk upload automation - performance testing\n\n---\n\nDescription\nh3. Context\n\n* The automation is working for smaller table (61M records). \n\nh3. Objective\n\n* To test the performance of the pipeline with high number parquet file and records table. \n\nh3. \n\nh3. Acceptance criteria\n\n* We have loaded over 600M records in under 1 hour manually the automation should give similar results if not better.\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nPerformance test complete and took around 1 hour to load 450M records.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 134}}
{"issue_key": "CSCI-537", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "10/Oct/25 5:15 PM", "updated": "20/Oct/25 9:52 AM", "labels": [], "summary": "Creation of Service Account to access Network Drive- RITM0176342 Sprint 10", "description": "h3. Context\n\n* Creation of Service Account to access Network Drive- RITM0176342\n\nh3. We need a Service Account creation to access the below Network Drive for Ingestion of supply chain data into Snowflake for group level reporting.\n\nWill be retrieving data from this location (Folder path):\n \\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nCreation of Service Account to access Network Drive- RITM0176342 Sprint 10\n\n---\n\nDescription\nh3. Context\n\n* Creation of Service Account to access Network Drive- RITM0176342\n\nh3. We need a Service Account creation to access the below Network Drive for Ingestion of supply chain data into Snowflake for group level reporting.\n\nWill be retrieving data from this location (Folder path):\n \\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 135}}
{"issue_key": "CSCI-536", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "10/Oct/25 5:14 PM", "updated": "14/Oct/25 9:33 AM", "labels": [], "summary": "Creation of Service Account to access Network Drive- RITM0176342 Sprint 10", "description": "h3. Context\n\n* Creation of Service Account to access Network Drive- RITM0176342\n\nh3. We need a Service Account creation to access the below Network Drive for Ingestion of supply chain data into Snowflake for group level reporting.\n\nWill be retrieving data from this location (Folder path):\n \\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nCreation of Service Account to access Network Drive- RITM0176342 Sprint 10\n\n---\n\nDescription\nh3. Context\n\n* Creation of Service Account to access Network Drive- RITM0176342\n\nh3. We need a Service Account creation to access the below Network Drive for Ingestion of supply chain data into Snowflake for group level reporting.\n\nWill be retrieving data from this location (Folder path):\n \\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 136}}
{"issue_key": "CSCI-535", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "10/Oct/25 5:13 PM", "updated": "16/Oct/25 3:22 PM", "labels": [], "summary": "RITM0173487 - Implement Azure DNS Private Resolver - Sprint 10", "description": "Currently worked on by Shagun A.\n\nImplement Azure DNS Private Resolver to enable on-premises DNS resolution for existing Snowflake Private Link endpoint ([privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com]).\n\nThe Azure Private Link for Snowflake is already configured and operational. This DNS implementation will enable on-premises systems to resolve the private Snowflake endpoint, which is a prerequisite for subsequent firewall access configuration for Enterprise Data Platform and critical applications requiring Snowflake access.\n\nSpoke to Network team and they suggested we create AD security group for snowflake private link access control and add the relevant users who need access to this group as the user laptop ip address is dynamic so this way we can control this via global protect and firewall. \n\n \n\nCurrent status:", "acceptance_criteria": "* Azure DNS Private Resolver is deployed and configured\n* On-premises DNS resolution is verified\n* AD Security Group for Snowflake Access is created\n* Firewall and GlobalProtect integration is validated\n* Documentation is updated\n* -Stakeholder sign-off-", "comments": "New Change Request CHG0052343 has been scheduled for today \n\n[CHG0052343|https://cwretail.service-now.com/nav_to.do?uri=change_request.do?sys_id=61cc0f803360b25047764f945d5c7b23]\n\nPlanned start date: 13-10-2025 11:00:00 AM\nPlanned end date: 13-10-2025 12:00:00 PM\nImplementor: Louis Allsop\nActive Directory DNS - Configure Conditional Forwarding to Azure for Private Endpoint resolution ([http://snowflakecomputing.com|http://snowflakecomputing.com] )\n\nPlease use [https://sigmahealthcare.atlassian.net/browse/CSCI-294|https://sigmahealthcare.atlassian.net/browse/CSCI-294] for tracking\n\nthanks", "text": "Summary\nRITM0173487 - Implement Azure DNS Private Resolver - Sprint 10\n\n---\n\nDescription\nCurrently worked on by Shagun A.\n\nImplement Azure DNS Private Resolver to enable on-premises DNS resolution for existing Snowflake Private Link endpoint ([privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com]).\n\nThe Azure Private Link for Snowflake is already configured and operational. This DNS implementation will enable on-premises systems to resolve the private Snowflake endpoint, which is a prerequisite for subsequent firewall access configuration for Enterprise Data Platform and critical applications requiring Snowflake access.\n\nSpoke to Network team and they suggested we create AD security group for snowflake private link access control and add the relevant users who need access to this group as the user laptop ip address is dynamic so this way we can control this via global protect and firewall. \n\n \n\nCurrent status:\n\n---\n\nAcceptance Criteria\n* Azure DNS Private Resolver is deployed and configured\n* On-premises DNS resolution is verified\n* AD Security Group for Snowflake Access is created\n* Firewall and GlobalProtect integration is validated\n* Documentation is updated\n* -Stakeholder sign-off-\n\n---\n\nComments\nNew Change Request CHG0052343 has been scheduled for today \n\n[CHG0052343|https://cwretail.service-now.com/nav_to.do?uri=change_request.do?sys_id=61cc0f803360b25047764f945d5c7b23]\n\nPlanned start date: 13-10-2025 11:00:00 AM\nPlanned end date: 13-10-2025 12:00:00 PM\nImplementor: Louis Allsop\nActive Directory DNS - Configure Conditional Forwarding to Azure for Private Endpoint resolution ([http://snowflakecomputing.com|http://snowflakecomputing.com] )\n\nPlease use [https://sigmahealthcare.atlassian.net/browse/CSCI-294|https://sigmahealthcare.atlassian.net/browse/CSCI-294] for tracking\n\nthanks", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 137}}
{"issue_key": "CSCI-534", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "10/Oct/25 5:11 PM", "updated": "20/Oct/25 9:51 AM", "labels": [], "summary": "Creation of CW accts for sigma personnel- RITM0177239- Sprint 10", "description": "h3. Context\n\n* Sigma personnel need CW accounts to access CW snowflake\n** Mike Kazemi\n** Alan Yuen\n** Han Li\n** Emil Julius\n\nh3. Objective\n\n* CW snowflake access for sigma\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Creation of sig_firstname.lastname@chemistwarehouse.com.au\n* Access to snowflake\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nCreation of CW accts for sigma personnel- RITM0177239- Sprint 10\n\n---\n\nDescription\nh3. Context\n\n* Sigma personnel need CW accounts to access CW snowflake\n** Mike Kazemi\n** Alan Yuen\n** Han Li\n** Emil Julius\n\nh3. Objective\n\n* CW snowflake access for sigma\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Creation of sig_firstname.lastname@chemistwarehouse.com.au\n* Access to snowflake\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 138}}
{"issue_key": "CSCI-533", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "10/Oct/25 5:09 PM", "updated": "19/Oct/25 7:56 PM", "labels": [], "summary": "Review Source-to-Target Map for DimSupplier, BridgeProductSupplier - Sprint 10", "description": "h3. Context\n\n* There is a need to document the source-to-target mapping for the [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table , with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.", "acceptance_criteria": "h3. \n\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "comments": "review done", "text": "Summary\nReview Source-to-Target Map for DimSupplier, BridgeProductSupplier - Sprint 10\n\n---\n\nDescription\nh3. Context\n\n* There is a need to document the source-to-target mapping for the [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table , with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.\n\n---\n\nAcceptance Criteria\nh3. \n\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)\n\n---\n\nComments\nreview done", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 139}}
{"issue_key": "CSCI-532", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "10/Oct/25 5:07 PM", "updated": "24/Oct/25 11:08 AM", "labels": [], "summary": "MergeCo Conformed Reporting - Draft Solution Design - Sprint 10", "description": "Create draft of the solution design for the Merge Co Conformed reporting.\n\nJust needs to be a one-page document, outlining *how* we will build and ingest the data for this reporting.\n\nIncludes:\n\n* What grain of data required for each metric\n* How we will push data for each environment into a join environment\n* Merging them into a common dimension", "acceptance_criteria": "", "comments": "[https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design]", "text": "Summary\nMergeCo Conformed Reporting - Draft Solution Design - Sprint 10\n\n---\n\nDescription\nCreate draft of the solution design for the Merge Co Conformed reporting.\n\nJust needs to be a one-page document, outlining *how* we will build and ingest the data for this reporting.\n\nIncludes:\n\n* What grain of data required for each metric\n* How we will push data for each environment into a join environment\n* Merging them into a common dimension\n\n---\n\nComments\n[https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 140}}
{"issue_key": "CSCI-531", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "10/Oct/25 3:04 PM", "updated": "24/Oct/25 2:01 PM", "labels": [], "summary": "Architecture walk-through Eugene/ Mike", "description": "Eugene to walk Mike through Azure architecture, Infrastructure as Code (IaC), DevOps, and related components", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nArchitecture walk-through Eugene/ Mike\n\n---\n\nDescription\nEugene to walk Mike through Azure architecture, Infrastructure as Code (IaC), DevOps, and related components\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 141}}
{"issue_key": "CSCI-526", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "10/Oct/25 10:06 AM", "updated": "24/Oct/25 3:05 PM", "labels": [], "summary": "DBT Cloud -> SF network connection", "description": "h3. Context\n\n* Connect dbt Cloud to our Snowflake instance on Azure to enable secure, version-controlled data transformations, scheduled jobs, and CI for analytics. The setup will use least-privilege access, correct Azure region selection, and standardized dev/staging/prod environments.\n\nh3. Objective\n\n* Provision Snowflake user/role with least-privilege and a dedicated warehouse.\n* Configure dbt Cloud connection: account identifier, Azure region, user/role, warehouse, database, schema; select approved auth (key pair or OAuth/SSO).\n* Store credentials in dbt Cloud environment variables; avoid hardcoded secrets.\n* Set up dev and deployment environments/jobs; map targets to separate schemas.\n* Validate with dbt debug and a sample dbt run; confirm role, warehouse, and permissions.\n* Document egress IPs (for allowlisting/PrivateLink), connection details, and handoff steps.\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* TBC\n\nh3. @user - Can you ad detail - thx\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Following our introductory session with the dbt technical team\n\nand CWR Cloud Engineer, Brent Robinson, dbt proposed two hosting options:\n\n1) Australia (AU) without Azure Private Link\n\n2) Europe with Azure Private Link\n\nNeither option meets the Cloud team’s requirements. Brent, the Security team,\n\nand dbt will continue collaborating to identify a compliant solution. Updates to follow.\n\nSplit to [https://sigmahealthcare.atlassian.net/browse/CSCI-610|https://sigmahealthcare.atlassian.net/browse/CSCI-610]", "text": "Summary\nDBT Cloud -> SF network connection\n\n---\n\nDescription\nh3. Context\n\n* Connect dbt Cloud to our Snowflake instance on Azure to enable secure, version-controlled data transformations, scheduled jobs, and CI for analytics. The setup will use least-privilege access, correct Azure region selection, and standardized dev/staging/prod environments.\n\nh3. Objective\n\n* Provision Snowflake user/role with least-privilege and a dedicated warehouse.\n* Configure dbt Cloud connection: account identifier, Azure region, user/role, warehouse, database, schema; select approved auth (key pair or OAuth/SSO).\n* Store credentials in dbt Cloud environment variables; avoid hardcoded secrets.\n* Set up dev and deployment environments/jobs; map targets to separate schemas.\n* Validate with dbt debug and a sample dbt run; confirm role, warehouse, and permissions.\n* Document egress IPs (for allowlisting/PrivateLink), connection details, and handoff steps.\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* TBC\n\nh3. @user - Can you ad detail - thx\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nFollowing our introductory session with the dbt technical team\n\nand CWR Cloud Engineer, Brent Robinson, dbt proposed two hosting options:\n\n1) Australia (AU) without Azure Private Link\n\n2) Europe with Azure Private Link\n\nNeither option meets the Cloud team’s requirements. Brent, the Security team,\n\nand dbt will continue collaborating to identify a compliant solution. Updates to follow.\n\nSplit to [https://sigmahealthcare.atlassian.net/browse/CSCI-610|https://sigmahealthcare.atlassian.net/browse/CSCI-610]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 142}}
{"issue_key": "CSCI-525", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Oct/25 10:23 AM", "updated": "31/Oct/25 9:51 AM", "labels": [], "summary": "CLONE - Capture the requirements for creating the Jumpbox for the developers (data engineers)", "description": "Capture the requirements for creating the Jumpbox for the developers (data engineers).\n\n*Key action items*\n\n* -On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/] - On prem and Cloud Team- *RITM0173487 (for reference)*-\n\nWhat’s needed is for the DNS forwarding from on-premise DNS servers, to the Azure privatelink DNS. See: [Resolve Azure and on-premises domains. | Microsoft Learn|https://learn.microsoft.com/en-us/azure/dns/private-resolver-hybrid-dns] -CHG0052044 (change request created)\n\n* Collaborate with the EUC Team (CWR) to *create* Jumpbox\n* Give the Entra (AD) users access to the Jumpbox \n\n|*Snowflake_CW_AU_DataEngineers*|DEV_ROLE_DATA_ENG|\n\n* Configure firewalls (whitelisting) (On-Prem, Azure) as follows\n* VDI / Jumpbox network → Snowflake VNet *(servicenow tickets)*\n\n* *How many Jumpboxes* required? *no. of users 10*\n* Note: Ports to be allowed: 443, 80, 1433\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Configure VDI / Jumbox images to pre-install tools listed below: - The list has been reviewed and approved by EUC Team.\"}],\"attrs\":{\"localId\":\"a6a01a2f-604c-4540-b9f2-173ad2e43e57\",\"state\":\"DONE\"}}],\"attrs\":{\"localId\":\"0c33acf9-c722-4f42-8518-6b140130d8da\"}}\n{adf}\n* *Git for Windows* + *Git Credential Manager (GCM)* (Azure DevOps/GitHub SSO)\n* *Azure CLI* & *Azure PowerShell*\n* *AzCopy* (for ADLS/BLOB moves within Azure)\n* *VS Code* (primary IDE)\n** Extensions: _Snowflake SQL Tools_ (Snowflake), _SQLTools_ (optional), _Python_, _Pylance_, _Jupyter_, _YAML_, _GitLens_\n* *Snowflake CLIs/SDKs*\n** *Snow CLI* (preferred over SnowSQL)\n** _(Optional)_ *SnowSQL* legacy client (only if you still need it)\n* *Drivers*\n** *Snowflake ODBC* and *JDBC* drivers (for BI tools, dbt, notebooks)\n* *Languages / Runtimes*\n** *Python 3.11+* (via *Miniconda* or *pyenv-win*; standardize on conda envs)\n** *Java 17* (LTS) for Snowpark Java/Scala\n** *Node.js 20 LTS* (if you use Streamlit/Node helpers)\n* *Snowpark & data tooling (per environment)*\n** {{pipx}} and/or {{conda}}\n** {{snowflake-snowpark-python}}, {{pandas}}, {{pyarrow}}, {{jupyterlab}}, {{ipykernel}}\n\n* *Power BI Desktop* _(or Power BI Desktop – Optimized for Fabric if that’s your org standard)_\n* *Tabular Editor*\n** TE2 (free) for basic edits or *TE3* (licensed) for advanced modeling/CI\n* *DAX Studio* (performance tuning)\n* *ALM Toolkit* (schema compare/deployment)\n* *Power BI Report Builder* (if you produce paginated reports)\n* *Azure Data Studio* (lightweight SQL + notebooks)\n* *SSMS* (for on-prem SQL Server admin)\n* *Azure Storage Explorer* (browsing ADLS/BLOB via private endpoints)\n* *On-prem / misc drivers*\n** Microsoft SQL Server ODBC, Oracle/ODBC (if used), PostgreSQL ODBC (if used)", "acceptance_criteria": "* The data engineers are able to access Snowflake and other relevant applications via Jumpbox", "comments": "@user - please update on outcome of your discussion with the network architecture\n\nfyi - @user @user\n\n@user Moving this to sprint 10 as today is end of sprint and task haven’t start (last minute assignment)\n\nDNS resolver CR has been successfully completed. Now the traffic is completely private, doesn't go over internet\n\ncc- @user @user @user @user \n\n@user - can you please update us if there is any discussion/outcome from Network team yet.\n\n@user apologies if it is already covered, are the firewall rules (on-prem, azure) also done to connect from VDI network to the Snowflake VNet ?\n\n||Item||Status||Comment||\n|Azure → Snowflake (Private Link)|Completed|Private path verified by @user|\n|DNS Private Resolver|Completed| |\n|Jumpbox Creation|Pending|To be done by EUC team @user is working on this|\n|On-prem / VDI → Snowflake VNet firewall rules|Completed|Network Readiness|\n\ncc- @user @user @user\n\n@user Yes, this is done- VDI network to Snowflake Private Endpoint\n\nHey @user ,\n\nGiven Mike is looking after the jumpbox creation, can this ticket move to done or there is another pending task?\n\nThanks\n\nNeville would like a dedicated jumpbox for our team instead of using the shared CWR jumpbox (currently used by Sysops, EUCops, Netops, and Secops).\n\n@user , please create a project task to request EUCops to start building the dedicated jumpbox. Neville will guide them through the setup once the task is created.\n\nHe also prefers a single firewall request for the new jumpbox once network policies are in place. In the meantime, the team can continue using the existing CWR jumpbox.please make sure not to save huge files with in the jumppbox as it will fill up the back end file server.\n\nProject created under PRJ0116183\n\nNeville to confirm if below user can access existing CWR jumpbox, once confirmed, next steps is to create deliciated jumpbox for EDP \n\nMike K - \n\n[sig_mike.kazemi@chemistwarehouse.com.au|mailto:sig_mike.kazemi@chemistwarehouse.com.au]\n\n \n\nHan L -\n\n [sig_Han.Li@chemistwarehouse.com.au|mailto:sig_Han.Li@chemistwarehouse.com.au]\n\n \n\nChloe T - \n\n[chloe.tran@chemistwarehouse.com.au|mailto:chloe.tran@chemistwarehouse.com.au]\n\n \n\nAshu A - \n\n[ashutosh.arya@chemistwarehouse.com.au|mailto:ashutosh.arya@chemistwarehouse.com.au]\n\nEugene\n\nAswini\n\nEmil", "text": "Summary\nCLONE - Capture the requirements for creating the Jumpbox for the developers (data engineers)\n\n---\n\nDescription\nCapture the requirements for creating the Jumpbox for the developers (data engineers).\n\n*Key action items*\n\n* -On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/] - On prem and Cloud Team- *RITM0173487 (for reference)*-\n\nWhat’s needed is for the DNS forwarding from on-premise DNS servers, to the Azure privatelink DNS. See: [Resolve Azure and on-premises domains. | Microsoft Learn|https://learn.microsoft.com/en-us/azure/dns/private-resolver-hybrid-dns] -CHG0052044 (change request created)\n\n* Collaborate with the EUC Team (CWR) to *create* Jumpbox\n* Give the Entra (AD) users access to the Jumpbox \n\n|*Snowflake_CW_AU_DataEngineers*|DEV_ROLE_DATA_ENG|\n\n* Configure firewalls (whitelisting) (On-Prem, Azure) as follows\n* VDI / Jumpbox network → Snowflake VNet *(servicenow tickets)*\n\n* *How many Jumpboxes* required? *no. of users 10*\n* Note: Ports to be allowed: 443, 80, 1433\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Configure VDI / Jumbox images to pre-install tools listed below: - The list has been reviewed and approved by EUC Team.\"}],\"attrs\":{\"localId\":\"a6a01a2f-604c-4540-b9f2-173ad2e43e57\",\"state\":\"DONE\"}}],\"attrs\":{\"localId\":\"0c33acf9-c722-4f42-8518-6b140130d8da\"}}\n{adf}\n* *Git for Windows* + *Git Credential Manager (GCM)* (Azure DevOps/GitHub SSO)\n* *Azure CLI* & *Azure PowerShell*\n* *AzCopy* (for ADLS/BLOB moves within Azure)\n* *VS Code* (primary IDE)\n** Extensions: _Snowflake SQL Tools_ (Snowflake), _SQLTools_ (optional), _Python_, _Pylance_, _Jupyter_, _YAML_, _GitLens_\n* *Snowflake CLIs/SDKs*\n** *Snow CLI* (preferred over SnowSQL)\n** _(Optional)_ *SnowSQL* legacy client (only if you still need it)\n* *Drivers*\n** *Snowflake ODBC* and *JDBC* drivers (for BI tools, dbt, notebooks)\n* *Languages / Runtimes*\n** *Python 3.11+* (via *Miniconda* or *pyenv-win*; standardize on conda envs)\n** *Java 17* (LTS) for Snowpark Java/Scala\n** *Node.js 20 LTS* (if you use Streamlit/Node helpers)\n* *Snowpark & data tooling (per environment)*\n** {{pipx}} and/or {{conda}}\n** {{snowflake-snowpark-python}}, {{pandas}}, {{pyarrow}}, {{jupyterlab}}, {{ipykernel}}\n\n* *Power BI Desktop* _(or Power BI Desktop – Optimized for Fabric if that’s your org standard)_\n* *Tabular Editor*\n** TE2 (free) for basic edits or *TE3* (licensed) for advanced modeling/CI\n* *DAX Studio* (performance tuning)\n* *ALM Toolkit* (schema compare/deployment)\n* *Power BI Report Builder* (if you produce paginated reports)\n* *Azure Data Studio* (lightweight SQL + notebooks)\n* *SSMS* (for on-prem SQL Server admin)\n* *Azure Storage Explorer* (browsing ADLS/BLOB via private endpoints)\n* *On-prem / misc drivers*\n** Microsoft SQL Server ODBC, Oracle/ODBC (if used), PostgreSQL ODBC (if used)\n\n---\n\nAcceptance Criteria\n* The data engineers are able to access Snowflake and other relevant applications via Jumpbox\n\n---\n\nComments\n@user - please update on outcome of your discussion with the network architecture\n\nfyi - @user @user\n\n@user Moving this to sprint 10 as today is end of sprint and task haven’t start (last minute assignment)\n\nDNS resolver CR has been successfully completed. Now the traffic is completely private, doesn't go over internet\n\ncc- @user @user @user @user \n\n@user - can you please update us if there is any discussion/outcome from Network team yet.\n\n@user apologies if it is already covered, are the firewall rules (on-prem, azure) also done to connect from VDI network to the Snowflake VNet ?\n\n||Item||Status||Comment||\n|Azure → Snowflake (Private Link)|Completed|Private path verified by @user|\n|DNS Private Resolver|Completed| |\n|Jumpbox Creation|Pending|To be done by EUC team @user is working on this|\n|On-prem / VDI → Snowflake VNet firewall rules|Completed|Network Readiness|\n\ncc- @user @user @user\n\n@user Yes, this is done- VDI network to Snowflake Private Endpoint\n\nHey @user ,\n\nGiven Mike is looking after the jumpbox creation, can this ticket move to done or there is another pending task?\n\nThanks\n\nNeville would like a dedicated jumpbox for our team instead of using the shared CWR jumpbox (currently used by Sysops, EUCops, Netops, and Secops).\n\n@user , please create a project task to request EUCops to start building the dedicated jumpbox. Neville will guide them through the setup once the task is created.\n\nHe also prefers a single firewall request for the new jumpbox once network policies are in place. In the meantime, the team can continue using the existing CWR jumpbox.please make sure not to save huge files with in the jumppbox as it will fill up the back end file server.\n\nProject created under PRJ0116183\n\nNeville to confirm if below user can access existing CWR jumpbox, once confirmed, next steps is to create deliciated jumpbox for EDP \n\nMike K - \n\n[sig_mike.kazemi@chemistwarehouse.com.au|mailto:sig_mike.kazemi@chemistwarehouse.com.au]\n\n \n\nHan L -\n\n [sig_Han.Li@chemistwarehouse.com.au|mailto:sig_Han.Li@chemistwarehouse.com.au]\n\n \n\nChloe T - \n\n[chloe.tran@chemistwarehouse.com.au|mailto:chloe.tran@chemistwarehouse.com.au]\n\n \n\nAshu A - \n\n[ashutosh.arya@chemistwarehouse.com.au|mailto:ashutosh.arya@chemistwarehouse.com.au]\n\nEugene\n\nAswini\n\nEmil", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 143}}
{"issue_key": "CSCI-524", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Oct/25 9:35 AM", "updated": "09/Oct/25 2:43 PM", "labels": [], "summary": "Manhattan Active (MySQL) Integration - (Work on MySQL specific Procedure )", "description": "We are importing data from MYSQL, and we need to create a custom procedure to get the data type of the columns and keep the Snowflake objects in sync", "acceptance_criteria": "Definition of done:\n\n* The pipeline should execute successfully.\n* Staging view should have source data types", "comments": "The work is in progress. Procedure has been created but it is under review as we don’t have direct access to any MYSQL to debug the code.\n\nContinue in the next sprint in the CSCI-136", "text": "Summary\nManhattan Active (MySQL) Integration - (Work on MySQL specific Procedure )\n\n---\n\nDescription\nWe are importing data from MYSQL, and we need to create a custom procedure to get the data type of the columns and keep the Snowflake objects in sync\n\n---\n\nAcceptance Criteria\nDefinition of done:\n\n* The pipeline should execute successfully.\n* Staging view should have source data types\n\n---\n\nComments\nThe work is in progress. Procedure has been created but it is under review as we don’t have direct access to any MYSQL to debug the code.\n\nContinue in the next sprint in the CSCI-136", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 144}}
{"issue_key": "CSCI-523", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Oct/25 9:29 AM", "updated": "07/Nov/25 2:19 PM", "labels": [], "summary": "Optimize the Bulk load pipeline - end to end integration", "description": "h3. Context\n\n* Bulk upload process involves a few manual steps. \n\nh3. Objective\n\n* Optimize the bulk upload process to be as automated as well.\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Hey @user , please update then content for this one\n\nReviewed and pending PR", "text": "Summary\nOptimize the Bulk load pipeline - end to end integration\n\n---\n\nDescription\nh3. Context\n\n* Bulk upload process involves a few manual steps. \n\nh3. Objective\n\n* Optimize the bulk upload process to be as automated as well.\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nHey @user , please update then content for this one\n\nReviewed and pending PR", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 145}}
{"issue_key": "CSCI-520", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Oct/25 8:52 AM", "updated": "22/Oct/25 9:58 AM", "labels": [], "summary": "Creating Detailed documentation - Detailed Documentation of work - Sprint 10", "description": "h3. Context\n\n* Comprehensive documentation is required to centralise and standardise knowledge on data engineering assets, including ADF pipelines, Snowflake extract metadata, and related sources (e.g., SCAX2012). This will support onboarding, troubleshooting, and ongoing enhancements.\n\nh3. Objective\n\n* Summarise and document the architecture, processes, and metadata for ADF pipelines, Snowflake extract mechanisms, and SCAX2012, consolidating all sources and references in Confluence for easy access and governance.\n\nh3. Steps\n\n# Identify and catalogue all relevant ADF pipelines, Snowflake extract processes, and SCAX2012 assets.\n# Gather existing documentation, code, and metadata from source systems and repositories.\n# Summarise technical details, dependencies, and integration points for each asset.\n# Document best practices, naming conventions, and standards (referencing existing Snowflake and ADF standards).\n# Create or update Confluence pages with structured summaries, diagrams, and links to source artifacts.\n# Review documentation with stakeholders for completeness and accuracy.\n# Publish and communicate the documentation to the engineering and analytics teams.\n\nh3. Deliverables\n\n* Confluence summary pages for ADF pipelines, Snowflake extract metadata, and SCAX2012.\n* Linked references to source code, metadata tables, and configuration files.\n* Diagrams illustrating architecture and data flows.\n* Documented best practices and standards for each component.\n\nh3. Assumptions (Optional)\n\n* All relevant source systems and repositories are accessible.\n* Existing documentation is available and up to date for reference.\n* Stakeholders are available for review and feedback.", "acceptance_criteria": "* All ADF pipelines, Snowflake extract processes, and SCAX2012 assets are documented in Confluence.\n* Documentation includes architecture diagrams, metadata summaries, and integration points.\n* Best practices and naming conventions are clearly outlined.\n* Stakeholder review is completed and feedback incorporated.\n* Documentation is published and communicated to all relevant teams.", "comments": "Document framework is ready and now working on details.\n\nNeed help from DBA team to complete doc\n\nHey @user ,\n\nCan we ensure document is in centralised location? Is it confluence or sharepoint?\n\nThanks,\n\nHarrison\n\n@user attaching it and it is at in the MergeCo data team’s architecture folder. I will send it for @user review now. It is still an evolving document. [CWR - EDP Operational Details v0.1.docx|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BB4A4EEE0-67BB-4D2F-A05A-EE4929DAA15B%7D&file=CWR%20-%20EDP%20Operational%20Details%20v0.1.docx&action=default&mobileredirect=true]\n\n@user \n@user if this is TBD next week with @user et al, i’ll split this task for sprint 10 and then mark this task as done?\nLMK once you guys have had this discussion.", "text": "Summary\nCreating Detailed documentation - Detailed Documentation of work - Sprint 10\n\n---\n\nDescription\nh3. Context\n\n* Comprehensive documentation is required to centralise and standardise knowledge on data engineering assets, including ADF pipelines, Snowflake extract metadata, and related sources (e.g., SCAX2012). This will support onboarding, troubleshooting, and ongoing enhancements.\n\nh3. Objective\n\n* Summarise and document the architecture, processes, and metadata for ADF pipelines, Snowflake extract mechanisms, and SCAX2012, consolidating all sources and references in Confluence for easy access and governance.\n\nh3. Steps\n\n# Identify and catalogue all relevant ADF pipelines, Snowflake extract processes, and SCAX2012 assets.\n# Gather existing documentation, code, and metadata from source systems and repositories.\n# Summarise technical details, dependencies, and integration points for each asset.\n# Document best practices, naming conventions, and standards (referencing existing Snowflake and ADF standards).\n# Create or update Confluence pages with structured summaries, diagrams, and links to source artifacts.\n# Review documentation with stakeholders for completeness and accuracy.\n# Publish and communicate the documentation to the engineering and analytics teams.\n\nh3. Deliverables\n\n* Confluence summary pages for ADF pipelines, Snowflake extract metadata, and SCAX2012.\n* Linked references to source code, metadata tables, and configuration files.\n* Diagrams illustrating architecture and data flows.\n* Documented best practices and standards for each component.\n\nh3. Assumptions (Optional)\n\n* All relevant source systems and repositories are accessible.\n* Existing documentation is available and up to date for reference.\n* Stakeholders are available for review and feedback.\n\n---\n\nAcceptance Criteria\n* All ADF pipelines, Snowflake extract processes, and SCAX2012 assets are documented in Confluence.\n* Documentation includes architecture diagrams, metadata summaries, and integration points.\n* Best practices and naming conventions are clearly outlined.\n* Stakeholder review is completed and feedback incorporated.\n* Documentation is published and communicated to all relevant teams.\n\n---\n\nComments\nDocument framework is ready and now working on details.\n\nNeed help from DBA team to complete doc\n\nHey @user ,\n\nCan we ensure document is in centralised location? Is it confluence or sharepoint?\n\nThanks,\n\nHarrison\n\n@user attaching it and it is at in the MergeCo data team’s architecture folder. I will send it for @user review now. It is still an evolving document. [CWR - EDP Operational Details v0.1.docx|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BB4A4EEE0-67BB-4D2F-A05A-EE4929DAA15B%7D&file=CWR%20-%20EDP%20Operational%20Details%20v0.1.docx&action=default&mobileredirect=true]\n\n@user \n@user if this is TBD next week with @user et al, i’ll split this task for sprint 10 and then mark this task as done?\nLMK once you guys have had this discussion.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 146}}
{"issue_key": "CSCI-519", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "08/Oct/25 10:08 AM", "updated": "14/Oct/25 8:22 AM", "labels": [], "summary": "MergeCo Retail Data Ingestion", "description": "h3. Context\n\ninitiate the process to convert the tables in to Parquet files for last 2 years of data for the fact tables mentioned here [PBI05 Objects for MergeCo Reporting.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BD5AD27C1-A51F-46E0-89EE-1B61847B2DAA%7D&file=PBI05%20Objects%20for%20MergeCo%20Reporting.xlsx&action=default&mobileredirect=true] (except FCT_STOCK_HISTORY)\n\nh3. Objective\n\n|Table Name|\n|FCT_SALES|\n|FCT_CUSTOMER_COUNT_HOURLY_AUS|\n|SubCategoryBudgetDaily|\n|FCT_STOCK|\n|FCT_STOCK_HISTORY|\n|FCT_SALES_NZL|\n|FCT_CUSTOMER_COUNT_HOURLY_NZL|\n|FCT_STOCK_NZL|\n|FCT_STOCK_HISTORY_NZL|\n\nh3. Acceptance criteria\n\n* Files being present in the Azure Storage", "acceptance_criteria": "Given, When, Then", "comments": "SubCategoryBudgetDaily and FCT_CUSTOMER_COUNT_HOURLY_AUS \n\nhave finished extracting \n\nThey are currently being uploaded intot he blob storage.\n\nNew card to be created in sprint 10 if ingestion failed.", "text": "Summary\nMergeCo Retail Data Ingestion\n\n---\n\nDescription\nh3. Context\n\ninitiate the process to convert the tables in to Parquet files for last 2 years of data for the fact tables mentioned here [PBI05 Objects for MergeCo Reporting.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BD5AD27C1-A51F-46E0-89EE-1B61847B2DAA%7D&file=PBI05%20Objects%20for%20MergeCo%20Reporting.xlsx&action=default&mobileredirect=true] (except FCT_STOCK_HISTORY)\n\nh3. Objective\n\n|Table Name|\n|FCT_SALES|\n|FCT_CUSTOMER_COUNT_HOURLY_AUS|\n|SubCategoryBudgetDaily|\n|FCT_STOCK|\n|FCT_STOCK_HISTORY|\n|FCT_SALES_NZL|\n|FCT_CUSTOMER_COUNT_HOURLY_NZL|\n|FCT_STOCK_NZL|\n|FCT_STOCK_HISTORY_NZL|\n\nh3. Acceptance criteria\n\n* Files being present in the Azure Storage\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nSubCategoryBudgetDaily and FCT_CUSTOMER_COUNT_HOURLY_AUS \n\nhave finished extracting \n\nThey are currently being uploaded intot he blob storage.\n\nNew card to be created in sprint 10 if ingestion failed.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 147}}
{"issue_key": "CSCI-518", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "08/Oct/25 9:06 AM", "updated": "10/Oct/25 3:00 PM", "labels": [], "summary": "ADF Pull Requests - source config CI/CD setup", "description": "To update ADF Pipelines to use Global Parameters", "acceptance_criteria": "PRs approved and merged", "comments": "see [Task 209939 Update ADF Pipelines to use Global Parameters|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/209939]\n\nAdded powershell script to replace the dev datafactory json with the target environment datafactory json.\n\nUpdated Variable groups to add adf-identity:", "text": "Summary\nADF Pull Requests - source config CI/CD setup\n\n---\n\nDescription\nTo update ADF Pipelines to use Global Parameters\n\n---\n\nAcceptance Criteria\nPRs approved and merged\n\n---\n\nComments\nsee [Task 209939 Update ADF Pipelines to use Global Parameters|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/209939]\n\nAdded powershell script to replace the dev datafactory json with the target environment datafactory json.\n\nUpdated Variable groups to add adf-identity:", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 148}}
{"issue_key": "CSCI-517", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "08/Oct/25 8:23 AM", "updated": "10/Oct/25 9:55 AM", "labels": [], "summary": "SF- Creating Extract Meta Insert Scripts for all Sources completed", "description": "h3. Context\n\n* The first step to ingest data in any environment is to populate metadata into extract meta table.\n\nh3. Objective\n\n* Come up with one insert script per source to be used in higher environment.\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* One .sql file per source\n\nh3. Assumptions - (Optional)\n\n* Will generate the inserts from the existing entries present in test environment.\n\nh3. Acceptance criteria\n\n* Come up with a generic script that can produce these scripts\n* Include these scripts against each source\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Hi All, I have started working on the script that will generate the inserts when we pass the source name.\n\nHey @user I want to discuss this card with you separately.\n\nFYI @user", "text": "Summary\nSF- Creating Extract Meta Insert Scripts for all Sources completed\n\n---\n\nDescription\nh3. Context\n\n* The first step to ingest data in any environment is to populate metadata into extract meta table.\n\nh3. Objective\n\n* Come up with one insert script per source to be used in higher environment.\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* One .sql file per source\n\nh3. Assumptions - (Optional)\n\n* Will generate the inserts from the existing entries present in test environment.\n\nh3. Acceptance criteria\n\n* Come up with a generic script that can produce these scripts\n* Include these scripts against each source\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nHi All, I have started working on the script that will generate the inserts when we pass the source name.\n\nHey @user I want to discuss this card with you separately.\n\nFYI @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 149}}
{"issue_key": "CSCI-516", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "07/Oct/25 11:05 AM", "updated": "10/Oct/25 5:11 PM", "labels": [], "summary": "Creation of CW accts for sigma personnel- RITM0177057", "description": "h3. Context\n\n* Sigma personnel need CW accounts to access CW snowflake\n** Mike Kazemi\n** Alan Yuen\n** Han Li\n** Emil Julius\n\nh3. Objective\n\n* CW snowflake access for sigma\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Creation of sig_firstname.lastname@chemistwarehouse.com.au\n* Access to snowflake\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Followed up with Edward Jeffries to get the approval\n\ncc- @user @user @user\n\n@user thank you\n\nCc @user\n\n@user @user we’ll need to escalate this\n\nIssue split into:\n|CSCI-534|Creation of CW accts for sigma personnel- RITM0177057 - Sprint 10 |\n\nApproved by Eddy pending CW work \n\nTo continue monitor in sprint 10", "text": "Summary\nCreation of CW accts for sigma personnel- RITM0177057\n\n---\n\nDescription\nh3. Context\n\n* Sigma personnel need CW accounts to access CW snowflake\n** Mike Kazemi\n** Alan Yuen\n** Han Li\n** Emil Julius\n\nh3. Objective\n\n* CW snowflake access for sigma\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Creation of sig_firstname.lastname@chemistwarehouse.com.au\n* Access to snowflake\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nFollowed up with Edward Jeffries to get the approval\n\ncc- @user @user @user\n\n@user thank you\n\nCc @user\n\n@user @user we’ll need to escalate this\n\nIssue split into:\n|CSCI-534|Creation of CW accts for sigma personnel- RITM0177057 - Sprint 10 |\n\nApproved by Eddy pending CW work \n\nTo continue monitor in sprint 10", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 150}}
{"issue_key": "CSCI-515", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "07/Oct/25 9:46 AM", "updated": "07/Nov/25 2:15 PM", "labels": [], "summary": "Optimize the Bulk load pipeline", "description": "h3. Context\n\n* Bulk upload process involves a few manual steps. \n\nh3. Objective\n\n* Optimize the bulk upload process to be as automated as well.\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "ADF part is done\n\nReview scheduled for 8/10/2025\n\nProc Successfully loaded data in Snowflake table. Not integrating end to end process.\n\nIssue split into:\n|CSCI-523|Optimize the Bulk load pipeline - Sprint 10|\n\nEnd to end tested for one table and process worked fine without any manual intervention other than specifying the parameters.", "text": "Summary\nOptimize the Bulk load pipeline\n\n---\n\nDescription\nh3. Context\n\n* Bulk upload process involves a few manual steps. \n\nh3. Objective\n\n* Optimize the bulk upload process to be as automated as well.\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nADF part is done\n\nReview scheduled for 8/10/2025\n\nProc Successfully loaded data in Snowflake table. Not integrating end to end process.\n\nIssue split into:\n|CSCI-523|Optimize the Bulk load pipeline - Sprint 10|\n\nEnd to end tested for one table and process worked fine without any manual intervention other than specifying the parameters.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 151}}
{"issue_key": "CSCI-514", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "07/Oct/25 9:08 AM", "updated": "24/Oct/25 12:38 PM", "labels": [], "summary": "Review Source-to-Target Map for Fact_Warehouse_Location_Packing", "description": "Source to target mapping for Fact_Warehouse_Location_Packing - filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "as per the last conversation with @user the following tables need to be put into a single table “Fact_WH_Loc_Inventory_Adjustment“ and the below tables will be split in the gold layer\n\n|Fact_Warehouse_Location_Putaway|\n|Fact_Warehouse_Location_Picking|\n|Fact_Warehouse_Location_Replenishment|\n|Fact_Warehouse_Location_Packing|", "text": "Summary\nReview Source-to-Target Map for Fact_Warehouse_Location_Packing\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Packing - filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nas per the last conversation with @user the following tables need to be put into a single table “Fact_WH_Loc_Inventory_Adjustment“ and the below tables will be split in the gold layer\n\n|Fact_Warehouse_Location_Putaway|\n|Fact_Warehouse_Location_Picking|\n|Fact_Warehouse_Location_Replenishment|\n|Fact_Warehouse_Location_Packing|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 152}}
{"issue_key": "CSCI-513", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "07/Oct/25 9:08 AM", "updated": "24/Oct/25 12:38 PM", "labels": [], "summary": "Review Source-to-Target Map for Fact_Warehouse_Location_Replenishment", "description": "Source to target mapping for Fact_Warehouse_Location_Replenishment - filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "as per the last conversation with @user the following tables need to be put into a single table “Fact_WH_Loc_Inventory_Adjustment“ and the below tables will be split in the gold layer\n\n|Fact_Warehouse_Location_Putaway|\n|Fact_Warehouse_Location_Picking|\n|Fact_Warehouse_Location_Replenishment|\n|Fact_Warehouse_Location_Packing|\n\nif we pick up the values from transaction tables. \n\n||TRANSACTION_TYPE||WORK_TYPE||WORK_GROUP||\n|320|Z06 Cage Manual Move|Replenishment|\n|140|Z12 Gen Pool Replen|Replenishment|\n|100|Z04 General Wave Replen|Replenishment|\n|100|Z70 Gen Manual Replen|Replenishment|\n|120|Z10 Cage Manual Replen|Replenishment|\n|320|Z13 Manual Move|Replenishment|\n|130|Z51 Wave Replen|Replenishment|\n|130|Z02 General Wave Replen|Replenishment|\n|120|Z13 Cage Manual Replen|Replenishment|\n|330|Z77 Cage Pool Replen|Replenishment|\n|320|Z05 General Wave Replen|Replenishment|\n|130|Z07 Flamm Wave Replen|Replenishment|\n|130|Z10 Cage Manual Replen|Replenishment|\n|330|Fragrance Nested Putaway|Putaway|\n|140|Z10 Gen Pool Replen|Replenishment|\n|100|Z57  Pool Replen|Replenishment|\n|220|Cage Case Pick|Order Pick|\n|330|Z07 Flamm Manual Replen|Replenishment|\n|100|Z06 Manual Move|Replenishment|\n|120|Z07 Gen Manual Replen|Replenishment|\n|330|Z70 Gen Wave Replen|Replenishment|\n|130|Z13 Cage Manual Replen|Replenishment|\n|140|Z20  Manual Move|Replenishment|\n|140|Z57  Pool Replen|Replenishment|\n|320|Z10 Gen Manual Replen|Replenishment|\n|130|Z11 Manual Move|Replenishment|\n|130|Z05 Gen Pool Replen|Replenishment|\n|240|General Case Pick|Order Pick|\n|330|Z01 Gen Pool Replen|Replenishment|\n|330|Z22 Manual Move|Replenishment|\n|120|Fragrance Putaway|Putaway|\n|320|Z70 Gen Wave Replen|Replenishment|\n|140|Z77  Wave Replen|Replenishment|\n|120|Z04 Gen Pool Replen|Replenishment|\n|330|Z10 Gen Manual Replen|Replenishment|\n|330|Z03 General Wave Replen|Replenishment|\n|240|Z51  Manual Replen|Replenishment|\n|130|Z06 Frag Pool Replen|Replenishment|\n|140|Z05 General Manual Replen|Replenishment|\n|130|Z51  WCS Manual Replen|NULL|\n|320|Z51  WCS Manual Replen|Replenishment|\n|130|Z07 Mezz201 Manual Replen|Replenishment|\n|330|Cage Putaway|Putaway|\n|330|Z06 Cage Manual Move|Replenishment|\n|140|General Tote Pick|Order Pick|\n|100|Z01 General Wave Replen|Replenishment|\n|320|Z77  Cage Manual Move|Replenishment|\n|100|Z21  Manual Replen|Replenishment|\n|130|Z66  Wave Replen|Replenishment|\n|140|Z99 Manual Move|Replenishment|\n|240|Frag Tote Pick|Order Pick|\n|330|Z13 Manual Move|Replenishment|\n|140|Z07 Cage Manual Replen|Replenishment|\n|100|Z70 Gen Pool Replen|Replenishment|\n|140|Z01 General Manual Replen|Replenishment|\n|120|Flamm Case Pick|Order Pick|\n|330|Z11 Cage Manual Replen|Replenishment|\n|320|Z07 Flamm Manual Replen|Replenishment|\n|140|Z07 Manual Move|Replenishment|\n|140|Z75  Manual Move|Replenishment|\n|140|Z99 Gen Wave Replen|Replenishment|\n|140|Z66  Pool Replen|Replenishment|\n|140|Z06 General Wave Replen|Replenishment|\n|140|Z02 General Manual Replen|Replenishment|\n|120|Z04 Manual Move|Replenishment|\n|100|Z06 Gen Pool Replen|Replenishment|\n|140|Z51  WCS Manual Move|NULL|\n|140|Z09 Gen Manual Replen|Replenishment|\n|320|Z08 Gen Pool Replen|Replenishment|\n|330|Z03 General Manual Replen|Replenishment|\n|100|Z77  Manual Move|Replenishment|\n|320|Z11 Cage Pool Replen|Replenishment|\n|100|Z75  Manual Move|Replenishment|\n|140|Z75  Pool Replen|Replenishment|\n|140|Z20  Manual Replen|Replenishment|\n|100|Z70 Manual Move|Replenishment|\n|140|Z09 Gen Wave Replen|Replenishment|\n|330|Z57  Manual Move|Replenishment|\n|140|Z75  Manual Replen|Replenishment|\n|120|Ethical Case Pick|Order Pick|\n|320|Z02 General Manual Replen|Replenishment|\n|330|Z10 Gen Wave Replen|Replenishment|\n|330|Z06 Frag Manual Replen|Replenishment|\n|130|Z08 Gen Manual Replen|Replenishment|\n|100|Z02 Gen Pool Replen|Replenishment|\n|320|Z77 Cage Pool Replen|Replenishment|\n|330|Cage Case Pick|Order Pick|\n|320|Z57  Manual Move|Replenishment|\n|240|Z04 General Wave Replen|Replenishment|\n|320|Z99 Manual Move|Replenishment|\n|130|Z99  Gen Pool Replen|Replenishment|\n|330|Z57  Pool Replen|Replenishment|\n|130|Z07 Flamm Pool Replen|Replenishment|\n|120|Z51  Manual Move|Replenishment|\n|330|Z08 Gen Pool Replen|Replenishment|\n|130|General Nested Putaway|Putaway|\n|140|Z57  Manual Replen|Replenishment|\n|140|Z10 Gen Manual Replen|Replenishment|\n|330|Z12 Gen Pool Replen|Replenishment|\n|140|Z13 Manual Move|Replenishment|\n|120|Z06 Frag Manual Move|Replenishment|\n|120|Z10 Cage Pool Replen|Replenishment|\n|130|Z11 Cage Pool Replen|Replenishment|\n|140|Z77  Cage Manual Move|Replenishment|\n|330|Z06 General Wave Replen|Replenishment|\n|140|Z07 Cage Wave Replen|Replenishment|\n|140|Z07 Flamm Manual Move|Replenishment|\n|460|Loading|Dock Management|\n|120|Z04 General Manual Replen|Replenishment|\n|320|Z08 Ethical Manual Replen|Replenishment|\n|100|Z20  Pool Replen|Replenishment|\n|120|Z04 General Wave Replen|Replenishment|\n|260|Receipt|NULL|\n|130|Z12 Gen Wave Replen|Replenishment|\n|130|Z04 General Manual Replen|Replenishment|\n|220|Frag Case Pick|Order Pick|\n|320|Z07 Gen Wave Replen|Replenishment|\n|320|General Tote Pick|Order Pick|\n|130|Z06 Cage Manual Replen|Replenishment|\n|130|Z66  Manual Move|Replenishment|\n|120|Z06 Cage Manual Replen|Replenishment|\n|330|Z51  WCS Manual Replen|Replenishment|\n|320|General Putaway|Putaway|\n|320|Z57  Pool Replen|Replenishment|\n|330|Loading|Dock Management|\n|120|Z09 Ethical Pool Replen|Replenishment|\n|330|Z70 Gen Manual Replen|Replenishment|\n|140|Z06 Cage Manual Move|Replenishment|\n|330|Z05 General Wave Replen|Replenishment|\n|130|Z07 Gen Manual Replen|Replenishment|\n|120|Z09 Cage Manual Replen|Replenishment|\n|320|Z07 Manual Move|Replenishment|\n|130|Z11 Cage Manual Replen|Replenishment|\n|140|Z10 Gen Wave Replen|Replenishment|\n|130|Z12 Gen Manual Replen|Replenishment|\n|140|Z08 Gen Wave Replen|Replenishment|\n|320|Z03 General Wave Replen|Replenishment|\n|320|Z20  Manual Move|Replenishment|\n|140|Cage Case Pick|Order Pick|\n|120|Z77  Manual Replen|Replenishment|\n|320|Z21  Manual Move|Replenishment|\n|130|Z57  Manual Move|Replenishment|\n|330|Z03 Gen Pool Replen|Replenishment|\n|80|NULL|NULL|\n|320|Z11 Manual Move|Replenishment|\n|370|NULL|NULL|\n|320|Z22 Manual Move|Replenishment|\n|130|Fragrance Nested Putaway|Putaway|\n|130|Z06 Frag Manual Move|Replenishment|\n|240|Ethical Tote Pick|Order Pick|\n|330|Z75  Pool Replen|Replenishment|\n|120|Z12 Manual Move|Replenishment|\n|120|Ethical Nested Putaway|Putaway|\n|120|Z07 Gen Pool Replen|Replenishment|\n|320|Z06 Frag Manual Replen|Replenishment|\n|100|Z21  Manual Move|Replenishment|\n|240|Z09 Gen Manual Replen|Replenishment|\n|130|Z12 Manual Move|Replenishment|\n|320|Flamm Tote Pick|Order Pick|\n|320|Z66  Pool Replen|Replenishment|\n|140|Ethical Putaway|Putaway|\n|120|Z05 Gen Pool Replen|Replenishment|\n|320|Z05 General Manual Replen|Replenishment|\n|120|Z08 Gen Manual Replen|Replenishment|\n|120|Z99 Gen Manual Replen|Replenishment|\n|330|Z10 Cage Manual Replen|Replenishment|\n|130|Z20  Wave Replen|Replenishment|\n|330|Z01 Manual Move|Replenishment|\n|120|Z51 Wave Replen|Replenishment|\n|130|Z77 Cage Pool Replen|Replenishment|\n|320|Z10 Gen Pool Replen|Replenishment|\n|320|Z20  Manual Replen|Replenishment|\n|330|General Manual Replen|Replenishment|\n|320|Z02 General Wave Replen|Replenishment|\n|140|Z02 Manual Move|Replenishment|\n|240|Standard Tote Pick|Order Pick|\n|330|Z01 General Manual Replen|Replenishment|\n|130|Z08 Ethical Pool Replen|Replenishment|\n|320|Z07 Flamm Manual Move|Replenishment|\n|130|Z07 Mezz201 Pool Replen|Replenishment|\n|140|Z07 Gen Manual Replen|Replenishment|\n|330|Z08 Gen Manual Replen|Replenishment|\n|410|Dock Management|NULL|\n|460|Staging|Packing|\n|120|Z02 Manual Move|Replenishment|\n|100|Z57  Manual Move|Replenishment|\n|330|Z07 Gen Wave Replen|Replenishment|\n|320|Z10 Manual Move|Replenishment|\n|130|Cage Putaway|Putaway|\n|320|Z51 Wave Replen|Replenishment|\n|140|Flammable Putaway|Putaway|\n|100|Z08 Gen Pool Replen|Replenishment|\n|330|Z66  Wave Replen|Replenishment|\n|100|Z10 Gen Manual Replen|Replenishment|\n|130|Z77  Manual Replen|Replenishment|\n|130|Z07 Flamm Manual Replen|Replenishment|\n|320|Z06 Manual Move|Replenishment|\n|330|Z02 General Wave Replen|Replenishment|\n|140|Z10 Cage Pool Replen|Replenishment|\n|140|Z12 Gen Manual Replen|Replenishment|\n|140|Z04 General Wave Replen|Replenishment|\n|320|Z21  Manual Replen|Replenishment|\n|120|Z51  Manual Replen|Replenishment|\n|320|Z13 Cage Pool Replen|Replenishment|\n|130|Z51  WCS Manual Move|Replenishment|\n|330|Z12 Cage Manual Replen|Replenishment|\n|330|Z51  Gen Pool Replen|Replenishment|\n|330|Z06 Frag Pool Replen|Replenishment|\n|330|Z05 Gen Pool Replen|Replenishment|\n|320|Z66  Manual Replen|Replenishment|\n|120|Cage Case Pick|Order Pick|\n|240|Frag Case Pick|Order Pick|\n|130|Z01 Gen Pool Replen|Replenishment|\n|140|Z66  Manual Move|Replenishment|\n|330|Z09 Gen Pool Replen|Replenishment|\n|330|Z77  Cage Manual Replen|Replenishment|\n|320|Z04 General Wave Replen|Replenishment|\n|320|Activity Count|Cycle Counting|\n|120|Z75  Pool Replen|Replenishment|\n|120|Z75  Manual Replen|Replenishment|\n|140|Z07 Gen Pool Replen|Replenishment|\n|320|Z09 Gen Pool Replen|Replenishment|\n|130|Z04 Gen Pool Replen|Replenishment|\n|330|Z99 Gen Manual Replen|Replenishment|\n|330|Z20  Pool Replen|Replenishment|\n|140|Z07 Mezz201 Pool Replen|Replenishment|\n|330|Z51 Wave Replen|Replenishment|\n|320|Z70 Gen Manual Replen|Replenishment|\n|140|Z09 Ethical Pool Replen|Replenishment|\n|130|Z10 Cage Wave Replen|Replenishment|\n|140|Z11 Cage Wave Replen|Replenishment|\n|130|Flammable Putaway|Putaway|\n|130|Z70 Gen Wave Replen|Replenishment|\n|130|Z10 Gen Wave Replen|Replenishment|\n|240|Z02 General Manual Replen|Replenishment|\n|450|NULL|NULL|\n|330|Z13 Cage Wave Replen|Replenishment|\n|330|Flamm Case Pick|Order Pick|\n|140|Z21  Manual Move|Replenishment|\n|320|Z77  Pool Replen|Replenishment|\n|100|Z02 General Manual Replen|Replenishment|\n|140|Z09 Ethical Manual Replen|Replenishment|\n|320|Z09 Manual Move|Replenishment|\n|320|Z01 General Wave Replen|Replenishment|\n|130|Cage Case Pick|Order Pick|\n|130|Manual Move|Replenishment|\n|320|Z70 Manual Move|Replenishment|\n|330|Z12 Cage Pool Replen|Replenishment|\n|140|Z06 Cage Manual Replen|Replenishment|\n|320|Z06 General Manual Replen|Replenishment|\n|320|Z12 Manual Move|Replenishment|\n|140|Z07 Cage Pool Replen|Replenishment|\n|320|Z77  Manual Move|Replenishment|\n|330|Z03 Manual Move|Replenishment|\n|140|Z07 Flamm Pool Replen|Replenishment|\n|130|Z09 Cage Pool Replen|Replenishment|\n|130|Ethical Case Pick|Order Pick|\n|330|General Wave Replen|Replenishment|\n|140|Z75  Wave Replen|Replenishment|\n|70|Packing|NULL|\n|120|Z06 Cage Wave Replen|Replenishment|\n|320|Z99 Gen Manual Replen|Replenishment|\n|120|Z05 Manual Move|Replenishment|\n|320|Z51  Manual Move|Replenishment|\n|140|Z77  Manual Move|Replenishment|\n|140|Cage Nested Putaway|Putaway|\n|220|Ethical Case Pick|Order Pick|\n|320|Cage Tote Pick|Order Pick|\n|100|Z07 Cage Wave Replen|Replenishment|\n|330|Ethical Nested Putaway|Putaway|\n|140|Z22 General Manual Replen|Replenishment|\n|140|Z08 Manual Move|Replenishment|\n|130|Z07 Cage Manual Replen|Replenishment|\n|130|Z07 Gen Wave Replen|Replenishment|\n|140|Z06 Frag Wave Replen|Replenishment|\n|120|General Nested Putaway|Putaway|\n|360|NULL|NULL|\n|330|Z11 Manual Move|Replenishment|\n|130|Z09 Ethical Pool Replen|Replenishment|\n|140|Z05 Manual Move|Replenishment|\n|330|Staging|Packing|\n|130|Z03 General Manual Replen|Replenishment|\n|330|Z06 Frag Wave Replen|Replenishment|\n|130|Z07 Cage Pool Replen|Replenishment|\n|50|NULL|NULL|\n|140|Z08 Ethical Manual Replen|Replenishment|\n|120|Frag Tote Pick|Order Pick|\n|100|Z03 Gen Pool Replen|Replenishment|\n|100|Z12 Manual Move|Replenishment|\n|90|NULL|NULL|\n|330|Z77  Pool Replen|Replenishment|\n|140|Z10 Manual Move|Replenishment|\n|130|Z09 Ethical Manual Replen|Replenishment|\n|290|Replenishment|NULL|\n|130|Z05 General Manual Replen|Replenishment|\n|320|Z08 Ethical Pool Replen|Replenishment|\n|120|Z07 Gen Wave Replen|Replenishment|\n|320|Z77  Wave Replen|Replenishment|\n|330|Z07 Mezz201 Manual Replen|Replenishment|\n|130|Z77  Cage Manual Replen|Replenishment|\n|320|General Wave Replen|Replenishment|\n|130|Z06 Cage Manual Move|Replenishment|\n|140|General Putaway|Putaway|\n|320|Cycle Counting|Cycle Counting|\n|130|Z51  Gen Pool Replen|Replenishment|\n|130|Ethical Tote Pick|Order Pick|\n|120|Flamm Tote Pick|Order Pick|\n|20|NULL|NULL|\n|320|Z08 Gen Manual Replen|Replenishment|\n|320|Z22 General Manual Replen|Replenishment|\n|120|Z99 Gen Wave Replen|Replenishment|\n|130|General Tote Pick|Order Pick|\n|120|Z66  Pool Replen|Replenishment|\n|320|Z75  Pool Replen|Replenishment|\n|320|Z06 General Wave Replen|Replenishment|\n|320|Z01 General Manual Replen|Replenishment|\n|320|Z08 Gen Wave Replen|Replenishment|\n|120|General Tote Pick|Order Pick|\n|330|Z06 General Manual Replen|Replenishment|\n|130|Fragrance Putaway|Putaway|\n|140|Z06 Cage Wave Replen|Replenishment|\n|130|Z05 Manual Move|Replenishment|\n|140|Z01 Gen Pool Replen|Replenishment|\n|140|Z99  Gen Pool Replen|Replenishment|\n|130|Z03 Manual Move|Replenishment|\n|330|Z10 Cage Wave Replen|Replenishment|\n|320|Standard Tote Pick|Order Pick|\n|330|Z06 Gen Pool Replen|Replenishment|\n|330|Z07 Cage Manual Replen|Replenishment|\n|320|Z07 Cage Pool Replen|Replenishment|\n|330|Z10 Cage Pool Replen|Replenishment|\n|330|Z57  Manual Replen|Replenishment|\n|120|Z05 General Manual Replen|Replenishment|\n|120|Z11 Cage Wave Replen|Replenishment|\n|340|NULL|NULL|\n|130|Z08 Ethical Manual Replen|Replenishment|\n|120|Z02 General Manual Replen|Replenishment|\n|320|Z06 Gen Pool Replen|Replenishment|\n|140|Z08 Ethical Manual Move|Replenishment|\n|140|Z11 Cage Manual Replen|Replenishment|\n|130|Z66  Manual Replen|Replenishment|\n|240|Ethical Case Pick|Order Pick|\n|120|Z06 Manual Move|Replenishment|\n|140|Z99 Gen Manual Replen|Replenishment|\n|320|Z06 Cage Manual Replen|Replenishment|\n|100|Z05 General Wave Replen|Replenishment|\n|120|Z11 Manual Move|Replenishment|\n|130|General Putaway|Putaway|\n|140|Z70 Gen Manual Replen|Replenishment|\n|140|Z22 Manual Move|Replenishment|\n|330|Z77  Cage Manual Move|Replenishment|\n|120|Z77  Manual Move|Replenishment|\n|120|Z09 Ethical Manual Replen|Replenishment|\n|120|Z21  Manual Move|Replenishment|\n|120|Z20  Manual Move|Replenishment|\n|130|Z20  Manual Move|Replenishment|\n|140|Z12 Cage Pool Replen|Replenishment|\n|330|Z09 Manual Move|Replenishment|\n|130|Z06 Frag Manual Replen|Replenishment|\n|210|Packing|Packing|\n|320|Z01 Manual Move|Replenishment|\n|100|Z07 Gen Manual Replen|Replenishment|\n|330|Z08 Gen Wave Replen|Replenishment|\n|320|Z09 Cage Wave Replen|Replenishment|\n|330|Z08 Manual Move|Replenishment|\n|130|Z22 Manual Move|Replenishment|\n|320|Z04 General Manual Replen|Replenishment|\n|330|Z07 Manual Move|Replenishment|\n|130|Z99 Manual Move|Replenishment|\n|320|Z11 Cage Manual Replen|Replenishment|\n|130|Z12 Cage Manual Replen|Replenishment|\n|130|Z02 Manual Move|Replenishment|\n|140|Z57  Manual Move|Replenishment|\n|320|Z07 Gen Pool Replen|Replenishment|\n|320|Z75  Wave Replen|Replenishment|\n|120|General Case Pick|Order Pick|\n|120|Loading|Dock Management|\n|320|Z02 Gen Pool Replen|Replenishment|\n|130|Z20  Manual Replen|Replenishment|\n|330|Flammable Putaway|Putaway|\n|100|Z01 General Manual Replen|Replenishment|\n|120|Z08 Manual Move|Replenishment|\n|220|General Case Pick|Order Pick|\n|140|Z77 Cage Pool Replen|Replenishment|\n|130|Z06 General Manual Replen|Replenishment|\n|100|Z07 Gen Pool Replen|Replenishment|\n|100|Z01 Manual Move|Replenishment|\n|120|Z01 Gen Pool Replen|Replenishment|\n|140|Z20  Wave Replen|Replenishment|\n|100|Z22 General Manual Replen|Replenishment|\n|120|Z22 Manual Move|Replenishment|\n|130|Cage Nested Putaway|Putaway|\n|120|Z06 General Manual Replen|Replenishment|\n|130|Z08 Gen Wave Replen|Replenishment|\n|330|Activity Count|Cycle Counting|\n|330|Z13 Cage Manual Replen|Replenishment|\n|120|Z03 Manual Move|Replenishment|\n|100|Z08 Gen Manual Replen|Replenishment|\n|330|Fragrance Putaway|Putaway|\n|120|Z66  Manual Replen|Replenishment|\n|320|Staging|Packing|\n|130|Z09 Gen Pool Replen|Replenishment|\n|140|Z21  Manual Replen|Replenishment|\n|120|Z10 Manual Move|Replenishment|\n|190|NULL|NULL|\n|120|Manual Move|Replenishment|\n|320|Z09 Gen Manual Replen|Replenishment|\n|140|Z13 Cage Pool Replen|Replenishment|\n|330|Z05 General Manual Replen|Replenishment|\n|130|Z06 Gen Pool Replen|Replenishment|\n|220|Cage Tote Pick|Order Pick|\n|120|Z20  Manual Replen|Replenishment|\n|130|Z08 Manual Move|Replenishment|\n|120|Cage Putaway|Putaway|\n|330|Z07 Cage Wave Replen|Replenishment|\n|330|Z07 Flamm Wave Replen|Replenishment|\n|330|Ethical Tote Pick|Order Pick|\n|130|Z07 Flamm Manual Move|Replenishment|\n|100|Z12 Gen Pool Replen|Replenishment|\n|330|Z05 Manual Move|Replenishment|\n|270|Packing|Packing|\n|120|Z06 Cage Pool Replen|Replenishment|\n|130|Z10 Gen Pool Replen|Replenishment|\n|320|Z09 Ethical Pool Replen|Replenishment|\n|130|Z06 Cage Pool Replen|Replenishment|\n|320|Ethical Nested Putaway|Putaway|\n|140|Z09 Cage Manual Replen|Replenishment|\n|40|NULL|NULL|\n|330|Z51  WCS Manual Move|Replenishment|\n|140|Z01 General Wave Replen|Replenishment|\n|320|General Manual Replen|Replenishment|\n|320|Z06 Frag Manual Move|Replenishment|\n|140|Z66  Wave Replen|Replenishment|\n|140|Ethical Case Pick|Order Pick|\n|330|Z75  Manual Move|Replenishment|\n|140|Z70 Manual Move|Replenishment|\n|320|Z09 Gen Wave Replen|Replenishment|\n|100|Z04 General Manual Replen|Replenishment|\n|140|Z12 Gen Wave Replen|Replenishment|\n|320|Z75  Manual Move|Replenishment|\n|120|Z02 General Wave Replen|Replenishment|\n|130|Z21  Manual Move|Replenishment|\n|120|Z07 Manual Move|Replenishment|\n|140|Z51  Manual Replen|Replenishment|\n|320|Z66  Manual Move|Replenishment|\n|140|Fragrance Nested Putaway|Putaway|\n|320|Z07 Mezz201 Pool Replen|Replenishment|\n|320|Fragrance Nested Putaway|Putaway|\n|140|Z06 Manual Move|Replenishment|\n|320|Z03 General Manual Replen|Replenishment|\n|140|Z03 General Manual Replen|Replenishment|\n|330|Z09 Gen Manual Replen|Replenishment|\n|130|Z57  Manual Replen|Replenishment|\n|120|Z07 Flamm Manual Replen|Replenishment|\n|130|Z10 Gen Manual Replen|Replenishment|\n|320|Z07 Cage Wave Replen|Replenishment|\n|320|Ethical Tote Pick|Order Pick|\n|130|Z20  Pool Replen|Replenishment|\n|320|General Nested Putaway|Putaway|\n|330|Z04 General Wave Replen|Replenishment|\n|330|Z09 Cage Pool Replen|Replenishment|\n|120|Z07 Cage Manual Replen|Replenishment|\n|330|Z09 Ethical Manual Replen|Replenishment|\n|120|Z06 Cage Manual Move|Replenishment|\n|140|Z04 Gen Pool Replen|Replenishment|\n|120|Z77  Pool Replen|Replenishment|\n|130|Z13 Cage Wave Replen|Replenishment|\n|330|Cage Tote Pick|Order Pick|\n|320|Z03 Gen Pool Replen|Replenishment|\n|240|Z04 General Manual Replen|Replenishment|\n|320|Z12 Gen Manual Replen|Replenishment|\n|100|Z75  Pool Replen|Replenishment|\n|220|Frag Tote Pick|Order Pick|\n|320|Z75  Manual Replen|Replenishment|\n|130|Z07 Manual Move|Replenishment|\n|320|Z09 Cage Pool Replen|Replenishment|\n|320|Z20  Wave Replen|Replenishment|\n|120|Ethical Putaway|Putaway|\n|45|NULL|NULL|\n|130|Z04 Manual Move|Replenishment|\n|140|Z08 Gen Pool Replen|Replenishment|\n|140|Z51  Manual Move|Replenishment|\n|320|Z07 Flamm Pool Replen|Replenishment|\n|320|Z06 Frag Wave Replen|Replenishment|\n|240|Z01 General Manual Replen|Replenishment|\n|140|Z77  Manual Replen|Replenishment|\n|140|Z10 Cage Manual Replen|Replenishment|\n|220|Packing|Packing|\n|130|Z13 Manual Move|Replenishment|\n|140|Z11 Cage Pool Replen|Replenishment|\n|130|Ethical Putaway|Putaway|\n|330|Standard Tote Pick|Order Pick|\n|320|Z70 Gen Pool Replen|Replenishment|\n|120|Z99 Manual Move|Replenishment|\n|320|Z10 Cage Pool Replen|Replenishment|\n|100|Z75  Manual Replen|Replenishment|\n|330|Z06 Cage Pool Replen|Replenishment|\n|320|Z12 Gen Pool Replen|Replenishment|\n|100|Z03 General Manual Replen|Replenishment|\n|100|Z12 Gen Manual Replen|Replenishment|\n|130|Z12 Cage Pool Replen|Replenishment|\n|220|NULL|NULL|\n|330|Z66  Manual Replen|Replenishment|\n|130|Z03 General Wave Replen|Replenishment|\n|330|Z66  Manual Move|Replenishment|\n|100|Z01 Gen Pool Replen|Replenishment|\n|330|Frag Tote Pick|Order Pick|\n|130|Z09 Cage Wave Replen|Replenishment|\n|120|Z11 Cage Manual Replen|Replenishment|\n|320|Z51  Manual Replen|Replenishment|\n|120|Cage Tote Pick|Order Pick|\n|120|Z08 Ethical Manual Replen|Replenishment|\n|120|Z06 Gen Pool Replen|Replenishment|\n|130|Cage Tote Pick|Order Pick|\n|330|Z07 Flamm Manual Move|Replenishment|\n|140|Z09 Cage Pool Replen|Replenishment|\n|140|Z06 Frag Manual Move|Replenishment|\n|330|Z07 Mezz201 Pool Replen|Replenishment|\n|370|General Tote Pick|NULL|\n|240|Cage Case Pick|Order Pick|\n|320|Z07 Flamm Wave Replen|Replenishment|\n|320|Z10 Gen Wave Replen|Replenishment|\n|130|Z05 General Wave Replen|Replenishment|\n|320|Flammable Putaway|Putaway|\n|130|Z07 Cage Wave Replen|Replenishment|\n|240|Z03 General Manual Replen|Replenishment|\n|330|Z77  Manual Move|Replenishment|\n|120|Standard Tote Pick|Order Pick|\n|330|Z21  Manual Move|Replenishment|\n|330|Z12 Gen Manual Replen|Replenishment|\n|330|Z21  Manual Replen|Replenishment|\n|330|Z01 General Wave Replen|Replenishment|\n|120|Z11 Cage Pool Replen|Replenishment|\n|100|Z03 Manual Move|Replenishment|\n|140|General Manual Replen|Replenishment|\n|140|Z07 Gen Wave Replen|Replenishment|\n|320|Z04 Manual Move|Replenishment|\n|330|Cage Nested Putaway|Putaway|\n|165|NULL|NULL|\n|140|Z77  Cage Manual Replen|Replenishment|\n|120|Z70 Gen Manual Replen|Replenishment|\n|320|Z06 Frag Pool Replen|Replenishment|\n|330|Z09 Gen Wave Replen|Replenishment|\n|130|Z75  Manual Move|Replenishment|\n|330|General Case Pick|Order Pick|\n|320|Z09 Cage Manual Replen|Replenishment|\n|130|Z70 Gen Pool Replen|Replenishment|\n|330|Z11 Cage Wave Replen|Replenishment|\n|140|Z06 Frag Manual Replen|Replenishment|\n|140|Z12 Manual Move|Replenishment|\n|330|Z04 Manual Move|Replenishment|\n|320|Z66  Wave Replen|Replenishment|\n|330|Z99 Gen Wave Replen|Replenishment|\n|330|Z66  Pool Replen|Replenishment|\n|330|Z06 Manual Move|Replenishment|\n|120|Z10 Gen Manual Replen|Replenishment|\n|100|Z05 Gen Pool Replen|Replenishment|\n|120|Z57  Manual Move|Replenishment|\n|320|Z11 Cage Wave Replen|Replenishment|\n|320|Z51  WCS Manual Move|Replenishment|\n|330|Z02 General Manual Replen|Replenishment|\n|140|Z03 Gen Pool Replen|Replenishment|\n|120|General Putaway|Putaway|\n|320|Z13 Cage Manual Replen|Replenishment|\n|320|Z99 Gen Wave Replen|Replenishment|\n|130|Z04 General Wave Replen|Replenishment|\n|130|Z09 Gen Manual Replen|Replenishment|\n|130|Z10 Manual Move|Replenishment|\n|240|Z03 General Wave Replen|Replenishment|\n|140|Z51 Wave Replen|Replenishment|\n|100|Z22 Manual Move|Replenishment|\n|100|Z02 Manual Move|Replenishment|\n|140|Z03 General Wave Replen|Replenishment|\n|330|Z75  Wave Replen|Replenishment|\n|130|Z77  Wave Replen|Replenishment|\n|330|Z07 Flamm Pool Replen|Replenishment|\n|330|Z51  Manual Replen|Replenishment|\n|130|Z08 Ethical Manual Move|Replenishment|\n|140|Z51  Gen Pool Replen|Replenishment|\n|120|Z70 Gen Pool Replen|Replenishment|\n|330|Z20  Wave Replen|Replenishment|\n|100|Z77  Manual Replen|Replenishment|\n|330|Z75  Manual Replen|Replenishment|\n|45|Cycle Counting|Cycle Counting|\n|140|Z12 Cage Manual Replen|Replenishment|\n|330|Z07 Gen Pool Replen|Replenishment|\n|140|Z11 Manual Move|Replenishment|\n|140|General Nested Putaway|Putaway|\n|120|Z08 Gen Pool Replen|Replenishment|\n|330|Z70 Manual Move|Replenishment|\n|330|Z10 Gen Pool Replen|Replenishment|\n|140|Z51  WCS Manual Move|Replenishment|\n|330|Z20  Manual Replen|Replenishment|\n|330|Z02 Gen Pool Replen|Replenishment|\n|120|Frag Case Pick|Order Pick|\n|140|Z03 Manual Move|Replenishment|\n|330|Z12 Manual Move|Replenishment|\n|130|Z01 Manual Move|Replenishment|\n|140|Z70 Gen Pool Replen|Replenishment|\n|130|Z03 Gen Pool Replen|Replenishment|\n|330|Ethical Case Pick|Order Pick|\n|140|Z77  Pool Replen|Replenishment|\n|130|Z01 General Manual Replen|Replenishment|\n|130|Z75  Manual Replen|Replenishment|\n|320|Z08 Ethical Manual Move|Replenishment|\n|130|Z70 Manual Move|Replenishment|\n|320|Z13 Cage Wave Replen|Replenishment|\n|100|Z10 Gen Pool Replen|Replenishment|\n|60|NULL|NULL|\n|100|Z05 Manual Move|Replenishment|\n|100|Z05 General Manual Replen|Replenishment|\n|240|Cage Tote Pick|Order Pick|\n|100|Z66  Manual Replen|Replenishment|\n|240|Flamm Tote Pick|Order Pick|\n|100|Z02 General Wave Replen|Replenishment|\n|120|Z77  Cage Manual Move|Replenishment|\n|330|Z08 Ethical Pool Replen|Replenishment|\n|100|Z07 Manual Move|Replenishment|\n|330|Z04 General Manual Replen|Replenishment|\n|130|Z06 Cage Wave Replen|Replenishment|\n|130|Z51  WCS Manual Replen|Replenishment|\n|200|Shipment|NULL|\n|100|Z20  Manual Replen|Replenishment|\n|140|Z04 Manual Move|Replenishment|\n|320|Z09 Ethical Manual Replen|Replenishment|\n|120|Z09 Gen Wave Replen|Replenishment|\n|130|Z70 Gen Manual Replen|Replenishment|\n|320|General Case Pick|Order Pick|\n|330|General Nested Putaway|Putaway|\n|130|Z12 Gen Pool Replen|Replenishment|\n|100|Z03 General Wave Replen|Replenishment|\n|320|Cage Putaway|Putaway|\n|140|Fragrance Putaway|Putaway|\n|320|Frag Tote Pick|Order Pick|\n|130|Z06 Frag Wave Replen|Replenishment|\n|130|General Wave Replen|Replenishment|\n|140|Z06 Frag Pool Replen|Replenishment|\n|130|Z99 Gen Manual Replen|Replenishment|\n|120|General Wave Replen|Replenishment|\n|320|Ethical Case Pick|Order Pick|\n|130|Z51  Manual Replen|Replenishment|\n|120|Z09 Cage Pool Replen|Replenishment|\n|330|Z09 Cage Manual Replen|Replenishment|\n|320|Z20  Pool Replen|Replenishment|\n|330|Z10 Manual Move|Replenishment|\n|130|Z22 General Manual Replen|Replenishment|\n|330|Z07 Gen Manual Replen|Replenishment|\n|100|Z07 Gen Wave Replen|Replenishment|\n|330|Z77  Manual Replen|Replenishment|\n|130|General Manual Replen|Replenishment|\n|140|Z13 Cage Manual Replen|Replenishment|\n|140|Z07 Mezz201 Manual Replen|Replenishment|\n|320|Z12 Cage Pool Replen|Replenishment|\n|110|Packing|Packing|\n|100|Z77  Pool Replen|Replenishment|\n|330|Z11 Cage Pool Replen|Replenishment|\n|320|Z51  Gen Pool Replen|Replenishment|\n|130|Ethical Nested Putaway|Putaway|\n|120|Z03 Gen Pool Replen|Replenishment|\n|120|Z01 General Manual Replen|Replenishment|\n|320|Z03 Manual Move|Replenishment|\n|290|Receipt|NULL|\n|140|Ethical Tote Pick|Order Pick|\n|140|Z07 Flamm Wave Replen|Replenishment|\n|240|General Tote Pick|Order Pick|\n|120|Z01 Manual Move|Replenishment|\n|130|Z75  Pool Replen|Replenishment|\n|120|Z22 General Manual Replen|Replenishment|\n|100|Z07 Cage Manual Replen|Replenishment|\n|320|Z12 Cage Manual Replen|Replenishment|\n|100|Z06 General Manual Replen|Replenishment|\n|170|NULL|NULL|\n|320|Z10 Cage Wave Replen|Replenishment|\n|330|Manual Move|Replenishment|\n|150|NULL|NULL|\n|320|Z77  Cage Manual Replen|Replenishment|\n|330|Z13 Cage Pool Replen|Replenishment|\n|260|NULL|NULL|\n|130|Z51  WCS Manual Move|NULL|\n|130|Z06 General Wave Replen|Replenishment|\n|140|Z09 Manual Move|Replenishment|\n|330|Z51  Manual Move|Replenishment|\n|100|Z66  Pool Replen|Replenishment|\n|120|Staging|Packing|\n|320|Z06 Cage Pool Replen|Replenishment|\n|130|Z09 Cage Manual Replen|Replenishment|\n|120|Z12 Gen Pool Replen|Replenishment|\n|140|Z09 Cage Wave Replen|Replenishment|\n|320|Z04 Gen Pool Replen|Replenishment|\n|320|Z05 Manual Move|Replenishment|\n|130|Z57  Pool Replen|Replenishment|\n|140|Z05 General Wave Replen|Replenishment|\n|200|NULL|NULL|\n|320|Z57  Manual Replen|Replenishment|\n|140|Cage Tote Pick|Order Pick|\n|330|Z12 Gen Wave Replen|Replenishment|\n|120|Z66  Manual Move|Replenishment|\n|320|Z12 Gen Wave Replen|Replenishment|\n|140|Z66  Manual Replen|Replenishment|\n|140|Z02 Gen Pool Replen|Replenishment|\n|330|Z06 Frag Manual Move|Replenishment|\n|100|Z08 Manual Move|Replenishment|\n|330|Z04 Gen Pool Replen|Replenishment|\n|330|Z09 Ethical Pool Replen|Replenishment|\n|320|Cage Nested Putaway|Putaway|\n|140|Z09 Gen Pool Replen|Replenishment|\n|140|Z06 Gen Pool Replen|Replenishment|\n|330|Cycle Counting|Cycle Counting|\n|130|Z08 Gen Pool Replen|Replenishment|\n|330|Ethical Putaway|Putaway|\n|180|NULL|NULL|\n|140|Z05 Gen Pool Replen|Replenishment|\n|130|Z01 General Wave Replen|Replenishment|\n|130|Z99 Gen Wave Replen|Replenishment|\n|130|Z51  Manual Move|Replenishment|\n|330|Z08 Ethical Manual Move|Replenishment|\n|120|Z09 Manual Move|Replenishment|\n|330|Z99 Manual Move|Replenishment|\n|120|Z01 General Wave Replen|Replenishment|\n|140|Z08 Ethical Pool Replen|Replenishment|\n|120|Z03 General Manual Replen|Replenishment|\n|130|Z10 Cage Pool Replen|Replenishment|\n|240|General Wave Replen|Replenishment|\n|330|Z09 Cage Wave Replen|Replenishment|\n|320|Z99  Gen Pool Replen|Replenishment|\n|100|Z04 Manual Move|Replenishment|\n|140|Z08 Gen Manual Replen|Replenishment|\n|130|Z77  Manual Move|Replenishment|\n|130|Z09 Manual Move|Replenishment|\n|120|Z75  Manual Move|Replenishment|\n|140|Z02 General Wave Replen|Replenishment|\n|140|Z51  WCS Manual Replen|NULL|\n|320|Z06 Cage Wave Replen|Replenishment|\n|330|Flamm Tote Pick|Order Pick|\n|330|General Tote Pick|Order Pick|\n|120|Z21  Manual Replen|Replenishment|\n|330|General Putaway|Putaway|\n|320|Fragrance Putaway|Putaway|\n|320|Z07 Cage Manual Replen|Replenishment|\n|100|Z08 Gen Wave Replen|Replenishment|\n|410|NULL|NULL|\n|320|Loading|Dock Management|\n|130|Z75  Wave Replen|Replenishment|\n|320|Manual Move|Replenishment|\n|130|Z02 Gen Pool Replen|Replenishment|\n|130|Z77  Pool Replen|Replenishment|\n|130|Z77  Cage Manual Move|Replenishment|\n|120|Z10 Gen Pool Replen|Replenishment|\n|130|Z11 Cage Wave Replen|Replenishment|\n|320|Z77  Manual Replen|Replenishment|\n|130|Z66  Pool Replen|Replenishment|\n|120|Z12 Gen Manual Replen|Replenishment|\n|140|Z51  WCS Manual Replen|Replenishment|\n|320|Frag Case Pick|Order Pick|\n|320|Flamm Case Pick|Order Pick|\n|320|Z07 Mezz201 Manual Replen|Replenishment|\n|120|Z09 Gen Manual Replen|Replenishment|\n|130|Z09 Gen Wave Replen|Replenishment|\n|120|Z70 Manual Move|Replenishment|\n|140|Z10 Cage Wave Replen|Replenishment|\n|330|Z02 Manual Move|Replenishment|\n|120|Z02 Gen Pool Replen|Replenishment|\n|330|Z70 Gen Pool Replen|Replenishment|\n|140|General Wave Replen|Replenishment|\n|330|Z77  Wave Replen|Replenishment|\n|130|Z21  Manual Replen|Replenishment|\n|130|Z13 Cage Pool Replen|Replenishment|\n|140|Z06 Cage Pool Replen|Replenishment|\n|120|Cage Nested Putaway|Putaway|\n|140|Z20  Pool Replen|Replenishment|\n|320|Z08 Manual Move|Replenishment|\n|320|Z10 Cage Manual Replen|Replenishment|\n|140|Z13 Cage Wave Replen|Replenishment|\n|100|Z57  Manual Replen|Replenishment|\n|330|Z06 Cage Wave Replen|Replenishment|\n|320|Z07 Gen Manual Replen|Replenishment|\n|330|Z06 Cage Manual Replen|Replenishment|\n|330|Z99  Gen Pool Replen|Replenishment|\n|330|Z08 Ethical Manual Replen|Replenishment|\n|120|Z57  Pool Replen|Replenishment|\n|330|Z20  Manual Move|Replenishment|\n|120|Ethical Tote Pick|Order Pick|\n|140|Manual Move|Replenishment|\n|140|Z70 Gen Wave Replen|Replenishment|\n|130|Z07 Gen Pool Replen|Replenishment|\n|330|Z22 General Manual Replen|Replenishment|\n|140|Z04 General Manual Replen|Replenishment|\n|320|Z01 Gen Pool Replen|Replenishment|\n|140|Z01 Manual Move|Replenishment|\n|140|Z07 Flamm Manual Replen|Replenishment|\n|140|Z06 General Manual Replen|Replenishment|\n|130|Z06 Manual Move|Replenishment|\n|330|Frag Case Pick|Order Pick|\n|140|Cage Putaway|Putaway|\n|100|Z04 Gen Pool Replen|Replenishment|\n|320|Z02 Manual Move|Replenishment|\n|320|Ethical Putaway|Putaway|\n|130|Z02 General Manual Replen|Replenishment|\n|330|Z07 Cage Pool Replen|Replenishment|\n|220|Standard Tote Pick|Order Pick|\n|220|General Tote Pick|Order Pick|\n|320|Cage Case Pick|Order Pick|\n|320|Z05 Gen Pool Replen|Replenishment|\n|140|Ethical Nested Putaway|Putaway|\n|240|Z99 Gen Manual Replen|Replenishment|\n\n|The Diffrent Transaction Type will help us differciante between the type of transactions for it |\n|By filtering out the  '-scale%'  location we will remove the the duplicated processes |", "text": "Summary\nReview Source-to-Target Map for Fact_Warehouse_Location_Replenishment\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Replenishment - filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nas per the last conversation with @user the following tables need to be put into a single table “Fact_WH_Loc_Inventory_Adjustment“ and the below tables will be split in the gold layer\n\n|Fact_Warehouse_Location_Putaway|\n|Fact_Warehouse_Location_Picking|\n|Fact_Warehouse_Location_Replenishment|\n|Fact_Warehouse_Location_Packing|\n\nif we pick up the values from transaction tables. \n\n||TRANSACTION_TYPE||WORK_TYPE||WORK_GROUP||\n|320|Z06 Cage Manual Move|Replenishment|\n|140|Z12 Gen Pool Replen|Replenishment|\n|100|Z04 General Wave Replen|Replenishment|\n|100|Z70 Gen Manual Replen|Replenishment|\n|120|Z10 Cage Manual Replen|Replenishment|\n|320|Z13 Manual Move|Replenishment|\n|130|Z51 Wave Replen|Replenishment|\n|130|Z02 General Wave Replen|Replenishment|\n|120|Z13 Cage Manual Replen|Replenishment|\n|330|Z77 Cage Pool Replen|Replenishment|\n|320|Z05 General Wave Replen|Replenishment|\n|130|Z07 Flamm Wave Replen|Replenishment|\n|130|Z10 Cage Manual Replen|Replenishment|\n|330|Fragrance Nested Putaway|Putaway|\n|140|Z10 Gen Pool Replen|Replenishment|\n|100|Z57  Pool Replen|Replenishment|\n|220|Cage Case Pick|Order Pick|\n|330|Z07 Flamm Manual Replen|Replenishment|\n|100|Z06 Manual Move|Replenishment|\n|120|Z07 Gen Manual Replen|Replenishment|\n|330|Z70 Gen Wave Replen|Replenishment|\n|130|Z13 Cage Manual Replen|Replenishment|\n|140|Z20  Manual Move|Replenishment|\n|140|Z57  Pool Replen|Replenishment|\n|320|Z10 Gen Manual Replen|Replenishment|\n|130|Z11 Manual Move|Replenishment|\n|130|Z05 Gen Pool Replen|Replenishment|\n|240|General Case Pick|Order Pick|\n|330|Z01 Gen Pool Replen|Replenishment|\n|330|Z22 Manual Move|Replenishment|\n|120|Fragrance Putaway|Putaway|\n|320|Z70 Gen Wave Replen|Replenishment|\n|140|Z77  Wave Replen|Replenishment|\n|120|Z04 Gen Pool Replen|Replenishment|\n|330|Z10 Gen Manual Replen|Replenishment|\n|330|Z03 General Wave Replen|Replenishment|\n|240|Z51  Manual Replen|Replenishment|\n|130|Z06 Frag Pool Replen|Replenishment|\n|140|Z05 General Manual Replen|Replenishment|\n|130|Z51  WCS Manual Replen|NULL|\n|320|Z51  WCS Manual Replen|Replenishment|\n|130|Z07 Mezz201 Manual Replen|Replenishment|\n|330|Cage Putaway|Putaway|\n|330|Z06 Cage Manual Move|Replenishment|\n|140|General Tote Pick|Order Pick|\n|100|Z01 General Wave Replen|Replenishment|\n|320|Z77  Cage Manual Move|Replenishment|\n|100|Z21  Manual Replen|Replenishment|\n|130|Z66  Wave Replen|Replenishment|\n|140|Z99 Manual Move|Replenishment|\n|240|Frag Tote Pick|Order Pick|\n|330|Z13 Manual Move|Replenishment|\n|140|Z07 Cage Manual Replen|Replenishment|\n|100|Z70 Gen Pool Replen|Replenishment|\n|140|Z01 General Manual Replen|Replenishment|\n|120|Flamm Case Pick|Order Pick|\n|330|Z11 Cage Manual Replen|Replenishment|\n|320|Z07 Flamm Manual Replen|Replenishment|\n|140|Z07 Manual Move|Replenishment|\n|140|Z75  Manual Move|Replenishment|\n|140|Z99 Gen Wave Replen|Replenishment|\n|140|Z66  Pool Replen|Replenishment|\n|140|Z06 General Wave Replen|Replenishment|\n|140|Z02 General Manual Replen|Replenishment|\n|120|Z04 Manual Move|Replenishment|\n|100|Z06 Gen Pool Replen|Replenishment|\n|140|Z51  WCS Manual Move|NULL|\n|140|Z09 Gen Manual Replen|Replenishment|\n|320|Z08 Gen Pool Replen|Replenishment|\n|330|Z03 General Manual Replen|Replenishment|\n|100|Z77  Manual Move|Replenishment|\n|320|Z11 Cage Pool Replen|Replenishment|\n|100|Z75  Manual Move|Replenishment|\n|140|Z75  Pool Replen|Replenishment|\n|140|Z20  Manual Replen|Replenishment|\n|100|Z70 Manual Move|Replenishment|\n|140|Z09 Gen Wave Replen|Replenishment|\n|330|Z57  Manual Move|Replenishment|\n|140|Z75  Manual Replen|Replenishment|\n|120|Ethical Case Pick|Order Pick|\n|320|Z02 General Manual Replen|Replenishment|\n|330|Z10 Gen Wave Replen|Replenishment|\n|330|Z06 Frag Manual Replen|Replenishment|\n|130|Z08 Gen Manual Replen|Replenishment|\n|100|Z02 Gen Pool Replen|Replenishment|\n|320|Z77 Cage Pool Replen|Replenishment|\n|330|Cage Case Pick|Order Pick|\n|320|Z57  Manual Move|Replenishment|\n|240|Z04 General Wave Replen|Replenishment|\n|320|Z99 Manual Move|Replenishment|\n|130|Z99  Gen Pool Replen|Replenishment|\n|330|Z57  Pool Replen|Replenishment|\n|130|Z07 Flamm Pool Replen|Replenishment|\n|120|Z51  Manual Move|Replenishment|\n|330|Z08 Gen Pool Replen|Replenishment|\n|130|General Nested Putaway|Putaway|\n|140|Z57  Manual Replen|Replenishment|\n|140|Z10 Gen Manual Replen|Replenishment|\n|330|Z12 Gen Pool Replen|Replenishment|\n|140|Z13 Manual Move|Replenishment|\n|120|Z06 Frag Manual Move|Replenishment|\n|120|Z10 Cage Pool Replen|Replenishment|\n|130|Z11 Cage Pool Replen|Replenishment|\n|140|Z77  Cage Manual Move|Replenishment|\n|330|Z06 General Wave Replen|Replenishment|\n|140|Z07 Cage Wave Replen|Replenishment|\n|140|Z07 Flamm Manual Move|Replenishment|\n|460|Loading|Dock Management|\n|120|Z04 General Manual Replen|Replenishment|\n|320|Z08 Ethical Manual Replen|Replenishment|\n|100|Z20  Pool Replen|Replenishment|\n|120|Z04 General Wave Replen|Replenishment|\n|260|Receipt|NULL|\n|130|Z12 Gen Wave Replen|Replenishment|\n|130|Z04 General Manual Replen|Replenishment|\n|220|Frag Case Pick|Order Pick|\n|320|Z07 Gen Wave Replen|Replenishment|\n|320|General Tote Pick|Order Pick|\n|130|Z06 Cage Manual Replen|Replenishment|\n|130|Z66  Manual Move|Replenishment|\n|120|Z06 Cage Manual Replen|Replenishment|\n|330|Z51  WCS Manual Replen|Replenishment|\n|320|General Putaway|Putaway|\n|320|Z57  Pool Replen|Replenishment|\n|330|Loading|Dock Management|\n|120|Z09 Ethical Pool Replen|Replenishment|\n|330|Z70 Gen Manual Replen|Replenishment|\n|140|Z06 Cage Manual Move|Replenishment|\n|330|Z05 General Wave Replen|Replenishment|\n|130|Z07 Gen Manual Replen|Replenishment|\n|120|Z09 Cage Manual Replen|Replenishment|\n|320|Z07 Manual Move|Replenishment|\n|130|Z11 Cage Manual Replen|Replenishment|\n|140|Z10 Gen Wave Replen|Replenishment|\n|130|Z12 Gen Manual Replen|Replenishment|\n|140|Z08 Gen Wave Replen|Replenishment|\n|320|Z03 General Wave Replen|Replenishment|\n|320|Z20  Manual Move|Replenishment|\n|140|Cage Case Pick|Order Pick|\n|120|Z77  Manual Replen|Replenishment|\n|320|Z21  Manual Move|Replenishment|\n|130|Z57  Manual Move|Replenishment|\n|330|Z03 Gen Pool Replen|Replenishment|\n|80|NULL|NULL|\n|320|Z11 Manual Move|Replenishment|\n|370|NULL|NULL|\n|320|Z22 Manual Move|Replenishment|\n|130|Fragrance Nested Putaway|Putaway|\n|130|Z06 Frag Manual Move|Replenishment|\n|240|Ethical Tote Pick|Order Pick|\n|330|Z75  Pool Replen|Replenishment|\n|120|Z12 Manual Move|Replenishment|\n|120|Ethical Nested Putaway|Putaway|\n|120|Z07 Gen Pool Replen|Replenishment|\n|320|Z06 Frag Manual Replen|Replenishment|\n|100|Z21  Manual Move|Replenishment|\n|240|Z09 Gen Manual Replen|Replenishment|\n|130|Z12 Manual Move|Replenishment|\n|320|Flamm Tote Pick|Order Pick|\n|320|Z66  Pool Replen|Replenishment|\n|140|Ethical Putaway|Putaway|\n|120|Z05 Gen Pool Replen|Replenishment|\n|320|Z05 General Manual Replen|Replenishment|\n|120|Z08 Gen Manual Replen|Replenishment|\n|120|Z99 Gen Manual Replen|Replenishment|\n|330|Z10 Cage Manual Replen|Replenishment|\n|130|Z20  Wave Replen|Replenishment|\n|330|Z01 Manual Move|Replenishment|\n|120|Z51 Wave Replen|Replenishment|\n|130|Z77 Cage Pool Replen|Replenishment|\n|320|Z10 Gen Pool Replen|Replenishment|\n|320|Z20  Manual Replen|Replenishment|\n|330|General Manual Replen|Replenishment|\n|320|Z02 General Wave Replen|Replenishment|\n|140|Z02 Manual Move|Replenishment|\n|240|Standard Tote Pick|Order Pick|\n|330|Z01 General Manual Replen|Replenishment|\n|130|Z08 Ethical Pool Replen|Replenishment|\n|320|Z07 Flamm Manual Move|Replenishment|\n|130|Z07 Mezz201 Pool Replen|Replenishment|\n|140|Z07 Gen Manual Replen|Replenishment|\n|330|Z08 Gen Manual Replen|Replenishment|\n|410|Dock Management|NULL|\n|460|Staging|Packing|\n|120|Z02 Manual Move|Replenishment|\n|100|Z57  Manual Move|Replenishment|\n|330|Z07 Gen Wave Replen|Replenishment|\n|320|Z10 Manual Move|Replenishment|\n|130|Cage Putaway|Putaway|\n|320|Z51 Wave Replen|Replenishment|\n|140|Flammable Putaway|Putaway|\n|100|Z08 Gen Pool Replen|Replenishment|\n|330|Z66  Wave Replen|Replenishment|\n|100|Z10 Gen Manual Replen|Replenishment|\n|130|Z77  Manual Replen|Replenishment|\n|130|Z07 Flamm Manual Replen|Replenishment|\n|320|Z06 Manual Move|Replenishment|\n|330|Z02 General Wave Replen|Replenishment|\n|140|Z10 Cage Pool Replen|Replenishment|\n|140|Z12 Gen Manual Replen|Replenishment|\n|140|Z04 General Wave Replen|Replenishment|\n|320|Z21  Manual Replen|Replenishment|\n|120|Z51  Manual Replen|Replenishment|\n|320|Z13 Cage Pool Replen|Replenishment|\n|130|Z51  WCS Manual Move|Replenishment|\n|330|Z12 Cage Manual Replen|Replenishment|\n|330|Z51  Gen Pool Replen|Replenishment|\n|330|Z06 Frag Pool Replen|Replenishment|\n|330|Z05 Gen Pool Replen|Replenishment|\n|320|Z66  Manual Replen|Replenishment|\n|120|Cage Case Pick|Order Pick|\n|240|Frag Case Pick|Order Pick|\n|130|Z01 Gen Pool Replen|Replenishment|\n|140|Z66  Manual Move|Replenishment|\n|330|Z09 Gen Pool Replen|Replenishment|\n|330|Z77  Cage Manual Replen|Replenishment|\n|320|Z04 General Wave Replen|Replenishment|\n|320|Activity Count|Cycle Counting|\n|120|Z75  Pool Replen|Replenishment|\n|120|Z75  Manual Replen|Replenishment|\n|140|Z07 Gen Pool Replen|Replenishment|\n|320|Z09 Gen Pool Replen|Replenishment|\n|130|Z04 Gen Pool Replen|Replenishment|\n|330|Z99 Gen Manual Replen|Replenishment|\n|330|Z20  Pool Replen|Replenishment|\n|140|Z07 Mezz201 Pool Replen|Replenishment|\n|330|Z51 Wave Replen|Replenishment|\n|320|Z70 Gen Manual Replen|Replenishment|\n|140|Z09 Ethical Pool Replen|Replenishment|\n|130|Z10 Cage Wave Replen|Replenishment|\n|140|Z11 Cage Wave Replen|Replenishment|\n|130|Flammable Putaway|Putaway|\n|130|Z70 Gen Wave Replen|Replenishment|\n|130|Z10 Gen Wave Replen|Replenishment|\n|240|Z02 General Manual Replen|Replenishment|\n|450|NULL|NULL|\n|330|Z13 Cage Wave Replen|Replenishment|\n|330|Flamm Case Pick|Order Pick|\n|140|Z21  Manual Move|Replenishment|\n|320|Z77  Pool Replen|Replenishment|\n|100|Z02 General Manual Replen|Replenishment|\n|140|Z09 Ethical Manual Replen|Replenishment|\n|320|Z09 Manual Move|Replenishment|\n|320|Z01 General Wave Replen|Replenishment|\n|130|Cage Case Pick|Order Pick|\n|130|Manual Move|Replenishment|\n|320|Z70 Manual Move|Replenishment|\n|330|Z12 Cage Pool Replen|Replenishment|\n|140|Z06 Cage Manual Replen|Replenishment|\n|320|Z06 General Manual Replen|Replenishment|\n|320|Z12 Manual Move|Replenishment|\n|140|Z07 Cage Pool Replen|Replenishment|\n|320|Z77  Manual Move|Replenishment|\n|330|Z03 Manual Move|Replenishment|\n|140|Z07 Flamm Pool Replen|Replenishment|\n|130|Z09 Cage Pool Replen|Replenishment|\n|130|Ethical Case Pick|Order Pick|\n|330|General Wave Replen|Replenishment|\n|140|Z75  Wave Replen|Replenishment|\n|70|Packing|NULL|\n|120|Z06 Cage Wave Replen|Replenishment|\n|320|Z99 Gen Manual Replen|Replenishment|\n|120|Z05 Manual Move|Replenishment|\n|320|Z51  Manual Move|Replenishment|\n|140|Z77  Manual Move|Replenishment|\n|140|Cage Nested Putaway|Putaway|\n|220|Ethical Case Pick|Order Pick|\n|320|Cage Tote Pick|Order Pick|\n|100|Z07 Cage Wave Replen|Replenishment|\n|330|Ethical Nested Putaway|Putaway|\n|140|Z22 General Manual Replen|Replenishment|\n|140|Z08 Manual Move|Replenishment|\n|130|Z07 Cage Manual Replen|Replenishment|\n|130|Z07 Gen Wave Replen|Replenishment|\n|140|Z06 Frag Wave Replen|Replenishment|\n|120|General Nested Putaway|Putaway|\n|360|NULL|NULL|\n|330|Z11 Manual Move|Replenishment|\n|130|Z09 Ethical Pool Replen|Replenishment|\n|140|Z05 Manual Move|Replenishment|\n|330|Staging|Packing|\n|130|Z03 General Manual Replen|Replenishment|\n|330|Z06 Frag Wave Replen|Replenishment|\n|130|Z07 Cage Pool Replen|Replenishment|\n|50|NULL|NULL|\n|140|Z08 Ethical Manual Replen|Replenishment|\n|120|Frag Tote Pick|Order Pick|\n|100|Z03 Gen Pool Replen|Replenishment|\n|100|Z12 Manual Move|Replenishment|\n|90|NULL|NULL|\n|330|Z77  Pool Replen|Replenishment|\n|140|Z10 Manual Move|Replenishment|\n|130|Z09 Ethical Manual Replen|Replenishment|\n|290|Replenishment|NULL|\n|130|Z05 General Manual Replen|Replenishment|\n|320|Z08 Ethical Pool Replen|Replenishment|\n|120|Z07 Gen Wave Replen|Replenishment|\n|320|Z77  Wave Replen|Replenishment|\n|330|Z07 Mezz201 Manual Replen|Replenishment|\n|130|Z77  Cage Manual Replen|Replenishment|\n|320|General Wave Replen|Replenishment|\n|130|Z06 Cage Manual Move|Replenishment|\n|140|General Putaway|Putaway|\n|320|Cycle Counting|Cycle Counting|\n|130|Z51  Gen Pool Replen|Replenishment|\n|130|Ethical Tote Pick|Order Pick|\n|120|Flamm Tote Pick|Order Pick|\n|20|NULL|NULL|\n|320|Z08 Gen Manual Replen|Replenishment|\n|320|Z22 General Manual Replen|Replenishment|\n|120|Z99 Gen Wave Replen|Replenishment|\n|130|General Tote Pick|Order Pick|\n|120|Z66  Pool Replen|Replenishment|\n|320|Z75  Pool Replen|Replenishment|\n|320|Z06 General Wave Replen|Replenishment|\n|320|Z01 General Manual Replen|Replenishment|\n|320|Z08 Gen Wave Replen|Replenishment|\n|120|General Tote Pick|Order Pick|\n|330|Z06 General Manual Replen|Replenishment|\n|130|Fragrance Putaway|Putaway|\n|140|Z06 Cage Wave Replen|Replenishment|\n|130|Z05 Manual Move|Replenishment|\n|140|Z01 Gen Pool Replen|Replenishment|\n|140|Z99  Gen Pool Replen|Replenishment|\n|130|Z03 Manual Move|Replenishment|\n|330|Z10 Cage Wave Replen|Replenishment|\n|320|Standard Tote Pick|Order Pick|\n|330|Z06 Gen Pool Replen|Replenishment|\n|330|Z07 Cage Manual Replen|Replenishment|\n|320|Z07 Cage Pool Replen|Replenishment|\n|330|Z10 Cage Pool Replen|Replenishment|\n|330|Z57  Manual Replen|Replenishment|\n|120|Z05 General Manual Replen|Replenishment|\n|120|Z11 Cage Wave Replen|Replenishment|\n|340|NULL|NULL|\n|130|Z08 Ethical Manual Replen|Replenishment|\n|120|Z02 General Manual Replen|Replenishment|\n|320|Z06 Gen Pool Replen|Replenishment|\n|140|Z08 Ethical Manual Move|Replenishment|\n|140|Z11 Cage Manual Replen|Replenishment|\n|130|Z66  Manual Replen|Replenishment|\n|240|Ethical Case Pick|Order Pick|\n|120|Z06 Manual Move|Replenishment|\n|140|Z99 Gen Manual Replen|Replenishment|\n|320|Z06 Cage Manual Replen|Replenishment|\n|100|Z05 General Wave Replen|Replenishment|\n|120|Z11 Manual Move|Replenishment|\n|130|General Putaway|Putaway|\n|140|Z70 Gen Manual Replen|Replenishment|\n|140|Z22 Manual Move|Replenishment|\n|330|Z77  Cage Manual Move|Replenishment|\n|120|Z77  Manual Move|Replenishment|\n|120|Z09 Ethical Manual Replen|Replenishment|\n|120|Z21  Manual Move|Replenishment|\n|120|Z20  Manual Move|Replenishment|\n|130|Z20  Manual Move|Replenishment|\n|140|Z12 Cage Pool Replen|Replenishment|\n|330|Z09 Manual Move|Replenishment|\n|130|Z06 Frag Manual Replen|Replenishment|\n|210|Packing|Packing|\n|320|Z01 Manual Move|Replenishment|\n|100|Z07 Gen Manual Replen|Replenishment|\n|330|Z08 Gen Wave Replen|Replenishment|\n|320|Z09 Cage Wave Replen|Replenishment|\n|330|Z08 Manual Move|Replenishment|\n|130|Z22 Manual Move|Replenishment|\n|320|Z04 General Manual Replen|Replenishment|\n|330|Z07 Manual Move|Replenishment|\n|130|Z99 Manual Move|Replenishment|\n|320|Z11 Cage Manual Replen|Replenishment|\n|130|Z12 Cage Manual Replen|Replenishment|\n|130|Z02 Manual Move|Replenishment|\n|140|Z57  Manual Move|Replenishment|\n|320|Z07 Gen Pool Replen|Replenishment|\n|320|Z75  Wave Replen|Replenishment|\n|120|General Case Pick|Order Pick|\n|120|Loading|Dock Management|\n|320|Z02 Gen Pool Replen|Replenishment|\n|130|Z20  Manual Replen|Replenishment|\n|330|Flammable Putaway|Putaway|\n|100|Z01 General Manual Replen|Replenishment|\n|120|Z08 Manual Move|Replenishment|\n|220|General Case Pick|Order Pick|\n|140|Z77 Cage Pool Replen|Replenishment|\n|130|Z06 General Manual Replen|Replenishment|\n|100|Z07 Gen Pool Replen|Replenishment|\n|100|Z01 Manual Move|Replenishment|\n|120|Z01 Gen Pool Replen|Replenishment|\n|140|Z20  Wave Replen|Replenishment|\n|100|Z22 General Manual Replen|Replenishment|\n|120|Z22 Manual Move|Replenishment|\n|130|Cage Nested Putaway|Putaway|\n|120|Z06 General Manual Replen|Replenishment|\n|130|Z08 Gen Wave Replen|Replenishment|\n|330|Activity Count|Cycle Counting|\n|330|Z13 Cage Manual Replen|Replenishment|\n|120|Z03 Manual Move|Replenishment|\n|100|Z08 Gen Manual Replen|Replenishment|\n|330|Fragrance Putaway|Putaway|\n|120|Z66  Manual Replen|Replenishment|\n|320|Staging|Packing|\n|130|Z09 Gen Pool Replen|Replenishment|\n|140|Z21  Manual Replen|Replenishment|\n|120|Z10 Manual Move|Replenishment|\n|190|NULL|NULL|\n|120|Manual Move|Replenishment|\n|320|Z09 Gen Manual Replen|Replenishment|\n|140|Z13 Cage Pool Replen|Replenishment|\n|330|Z05 General Manual Replen|Replenishment|\n|130|Z06 Gen Pool Replen|Replenishment|\n|220|Cage Tote Pick|Order Pick|\n|120|Z20  Manual Replen|Replenishment|\n|130|Z08 Manual Move|Replenishment|\n|120|Cage Putaway|Putaway|\n|330|Z07 Cage Wave Replen|Replenishment|\n|330|Z07 Flamm Wave Replen|Replenishment|\n|330|Ethical Tote Pick|Order Pick|\n|130|Z07 Flamm Manual Move|Replenishment|\n|100|Z12 Gen Pool Replen|Replenishment|\n|330|Z05 Manual Move|Replenishment|\n|270|Packing|Packing|\n|120|Z06 Cage Pool Replen|Replenishment|\n|130|Z10 Gen Pool Replen|Replenishment|\n|320|Z09 Ethical Pool Replen|Replenishment|\n|130|Z06 Cage Pool Replen|Replenishment|\n|320|Ethical Nested Putaway|Putaway|\n|140|Z09 Cage Manual Replen|Replenishment|\n|40|NULL|NULL|\n|330|Z51  WCS Manual Move|Replenishment|\n|140|Z01 General Wave Replen|Replenishment|\n|320|General Manual Replen|Replenishment|\n|320|Z06 Frag Manual Move|Replenishment|\n|140|Z66  Wave Replen|Replenishment|\n|140|Ethical Case Pick|Order Pick|\n|330|Z75  Manual Move|Replenishment|\n|140|Z70 Manual Move|Replenishment|\n|320|Z09 Gen Wave Replen|Replenishment|\n|100|Z04 General Manual Replen|Replenishment|\n|140|Z12 Gen Wave Replen|Replenishment|\n|320|Z75  Manual Move|Replenishment|\n|120|Z02 General Wave Replen|Replenishment|\n|130|Z21  Manual Move|Replenishment|\n|120|Z07 Manual Move|Replenishment|\n|140|Z51  Manual Replen|Replenishment|\n|320|Z66  Manual Move|Replenishment|\n|140|Fragrance Nested Putaway|Putaway|\n|320|Z07 Mezz201 Pool Replen|Replenishment|\n|320|Fragrance Nested Putaway|Putaway|\n|140|Z06 Manual Move|Replenishment|\n|320|Z03 General Manual Replen|Replenishment|\n|140|Z03 General Manual Replen|Replenishment|\n|330|Z09 Gen Manual Replen|Replenishment|\n|130|Z57  Manual Replen|Replenishment|\n|120|Z07 Flamm Manual Replen|Replenishment|\n|130|Z10 Gen Manual Replen|Replenishment|\n|320|Z07 Cage Wave Replen|Replenishment|\n|320|Ethical Tote Pick|Order Pick|\n|130|Z20  Pool Replen|Replenishment|\n|320|General Nested Putaway|Putaway|\n|330|Z04 General Wave Replen|Replenishment|\n|330|Z09 Cage Pool Replen|Replenishment|\n|120|Z07 Cage Manual Replen|Replenishment|\n|330|Z09 Ethical Manual Replen|Replenishment|\n|120|Z06 Cage Manual Move|Replenishment|\n|140|Z04 Gen Pool Replen|Replenishment|\n|120|Z77  Pool Replen|Replenishment|\n|130|Z13 Cage Wave Replen|Replenishment|\n|330|Cage Tote Pick|Order Pick|\n|320|Z03 Gen Pool Replen|Replenishment|\n|240|Z04 General Manual Replen|Replenishment|\n|320|Z12 Gen Manual Replen|Replenishment|\n|100|Z75  Pool Replen|Replenishment|\n|220|Frag Tote Pick|Order Pick|\n|320|Z75  Manual Replen|Replenishment|\n|130|Z07 Manual Move|Replenishment|\n|320|Z09 Cage Pool Replen|Replenishment|\n|320|Z20  Wave Replen|Replenishment|\n|120|Ethical Putaway|Putaway|\n|45|NULL|NULL|\n|130|Z04 Manual Move|Replenishment|\n|140|Z08 Gen Pool Replen|Replenishment|\n|140|Z51  Manual Move|Replenishment|\n|320|Z07 Flamm Pool Replen|Replenishment|\n|320|Z06 Frag Wave Replen|Replenishment|\n|240|Z01 General Manual Replen|Replenishment|\n|140|Z77  Manual Replen|Replenishment|\n|140|Z10 Cage Manual Replen|Replenishment|\n|220|Packing|Packing|\n|130|Z13 Manual Move|Replenishment|\n|140|Z11 Cage Pool Replen|Replenishment|\n|130|Ethical Putaway|Putaway|\n|330|Standard Tote Pick|Order Pick|\n|320|Z70 Gen Pool Replen|Replenishment|\n|120|Z99 Manual Move|Replenishment|\n|320|Z10 Cage Pool Replen|Replenishment|\n|100|Z75  Manual Replen|Replenishment|\n|330|Z06 Cage Pool Replen|Replenishment|\n|320|Z12 Gen Pool Replen|Replenishment|\n|100|Z03 General Manual Replen|Replenishment|\n|100|Z12 Gen Manual Replen|Replenishment|\n|130|Z12 Cage Pool Replen|Replenishment|\n|220|NULL|NULL|\n|330|Z66  Manual Replen|Replenishment|\n|130|Z03 General Wave Replen|Replenishment|\n|330|Z66  Manual Move|Replenishment|\n|100|Z01 Gen Pool Replen|Replenishment|\n|330|Frag Tote Pick|Order Pick|\n|130|Z09 Cage Wave Replen|Replenishment|\n|120|Z11 Cage Manual Replen|Replenishment|\n|320|Z51  Manual Replen|Replenishment|\n|120|Cage Tote Pick|Order Pick|\n|120|Z08 Ethical Manual Replen|Replenishment|\n|120|Z06 Gen Pool Replen|Replenishment|\n|130|Cage Tote Pick|Order Pick|\n|330|Z07 Flamm Manual Move|Replenishment|\n|140|Z09 Cage Pool Replen|Replenishment|\n|140|Z06 Frag Manual Move|Replenishment|\n|330|Z07 Mezz201 Pool Replen|Replenishment|\n|370|General Tote Pick|NULL|\n|240|Cage Case Pick|Order Pick|\n|320|Z07 Flamm Wave Replen|Replenishment|\n|320|Z10 Gen Wave Replen|Replenishment|\n|130|Z05 General Wave Replen|Replenishment|\n|320|Flammable Putaway|Putaway|\n|130|Z07 Cage Wave Replen|Replenishment|\n|240|Z03 General Manual Replen|Replenishment|\n|330|Z77  Manual Move|Replenishment|\n|120|Standard Tote Pick|Order Pick|\n|330|Z21  Manual Move|Replenishment|\n|330|Z12 Gen Manual Replen|Replenishment|\n|330|Z21  Manual Replen|Replenishment|\n|330|Z01 General Wave Replen|Replenishment|\n|120|Z11 Cage Pool Replen|Replenishment|\n|100|Z03 Manual Move|Replenishment|\n|140|General Manual Replen|Replenishment|\n|140|Z07 Gen Wave Replen|Replenishment|\n|320|Z04 Manual Move|Replenishment|\n|330|Cage Nested Putaway|Putaway|\n|165|NULL|NULL|\n|140|Z77  Cage Manual Replen|Replenishment|\n|120|Z70 Gen Manual Replen|Replenishment|\n|320|Z06 Frag Pool Replen|Replenishment|\n|330|Z09 Gen Wave Replen|Replenishment|\n|130|Z75  Manual Move|Replenishment|\n|330|General Case Pick|Order Pick|\n|320|Z09 Cage Manual Replen|Replenishment|\n|130|Z70 Gen Pool Replen|Replenishment|\n|330|Z11 Cage Wave Replen|Replenishment|\n|140|Z06 Frag Manual Replen|Replenishment|\n|140|Z12 Manual Move|Replenishment|\n|330|Z04 Manual Move|Replenishment|\n|320|Z66  Wave Replen|Replenishment|\n|330|Z99 Gen Wave Replen|Replenishment|\n|330|Z66  Pool Replen|Replenishment|\n|330|Z06 Manual Move|Replenishment|\n|120|Z10 Gen Manual Replen|Replenishment|\n|100|Z05 Gen Pool Replen|Replenishment|\n|120|Z57  Manual Move|Replenishment|\n|320|Z11 Cage Wave Replen|Replenishment|\n|320|Z51  WCS Manual Move|Replenishment|\n|330|Z02 General Manual Replen|Replenishment|\n|140|Z03 Gen Pool Replen|Replenishment|\n|120|General Putaway|Putaway|\n|320|Z13 Cage Manual Replen|Replenishment|\n|320|Z99 Gen Wave Replen|Replenishment|\n|130|Z04 General Wave Replen|Replenishment|\n|130|Z09 Gen Manual Replen|Replenishment|\n|130|Z10 Manual Move|Replenishment|\n|240|Z03 General Wave Replen|Replenishment|\n|140|Z51 Wave Replen|Replenishment|\n|100|Z22 Manual Move|Replenishment|\n|100|Z02 Manual Move|Replenishment|\n|140|Z03 General Wave Replen|Replenishment|\n|330|Z75  Wave Replen|Replenishment|\n|130|Z77  Wave Replen|Replenishment|\n|330|Z07 Flamm Pool Replen|Replenishment|\n|330|Z51  Manual Replen|Replenishment|\n|130|Z08 Ethical Manual Move|Replenishment|\n|140|Z51  Gen Pool Replen|Replenishment|\n|120|Z70 Gen Pool Replen|Replenishment|\n|330|Z20  Wave Replen|Replenishment|\n|100|Z77  Manual Replen|Replenishment|\n|330|Z75  Manual Replen|Replenishment|\n|45|Cycle Counting|Cycle Counting|\n|140|Z12 Cage Manual Replen|Replenishment|\n|330|Z07 Gen Pool Replen|Replenishment|\n|140|Z11 Manual Move|Replenishment|\n|140|General Nested Putaway|Putaway|\n|120|Z08 Gen Pool Replen|Replenishment|\n|330|Z70 Manual Move|Replenishment|\n|330|Z10 Gen Pool Replen|Replenishment|\n|140|Z51  WCS Manual Move|Replenishment|\n|330|Z20  Manual Replen|Replenishment|\n|330|Z02 Gen Pool Replen|Replenishment|\n|120|Frag Case Pick|Order Pick|\n|140|Z03 Manual Move|Replenishment|\n|330|Z12 Manual Move|Replenishment|\n|130|Z01 Manual Move|Replenishment|\n|140|Z70 Gen Pool Replen|Replenishment|\n|130|Z03 Gen Pool Replen|Replenishment|\n|330|Ethical Case Pick|Order Pick|\n|140|Z77  Pool Replen|Replenishment|\n|130|Z01 General Manual Replen|Replenishment|\n|130|Z75  Manual Replen|Replenishment|\n|320|Z08 Ethical Manual Move|Replenishment|\n|130|Z70 Manual Move|Replenishment|\n|320|Z13 Cage Wave Replen|Replenishment|\n|100|Z10 Gen Pool Replen|Replenishment|\n|60|NULL|NULL|\n|100|Z05 Manual Move|Replenishment|\n|100|Z05 General Manual Replen|Replenishment|\n|240|Cage Tote Pick|Order Pick|\n|100|Z66  Manual Replen|Replenishment|\n|240|Flamm Tote Pick|Order Pick|\n|100|Z02 General Wave Replen|Replenishment|\n|120|Z77  Cage Manual Move|Replenishment|\n|330|Z08 Ethical Pool Replen|Replenishment|\n|100|Z07 Manual Move|Replenishment|\n|330|Z04 General Manual Replen|Replenishment|\n|130|Z06 Cage Wave Replen|Replenishment|\n|130|Z51  WCS Manual Replen|Replenishment|\n|200|Shipment|NULL|\n|100|Z20  Manual Replen|Replenishment|\n|140|Z04 Manual Move|Replenishment|\n|320|Z09 Ethical Manual Replen|Replenishment|\n|120|Z09 Gen Wave Replen|Replenishment|\n|130|Z70 Gen Manual Replen|Replenishment|\n|320|General Case Pick|Order Pick|\n|330|General Nested Putaway|Putaway|\n|130|Z12 Gen Pool Replen|Replenishment|\n|100|Z03 General Wave Replen|Replenishment|\n|320|Cage Putaway|Putaway|\n|140|Fragrance Putaway|Putaway|\n|320|Frag Tote Pick|Order Pick|\n|130|Z06 Frag Wave Replen|Replenishment|\n|130|General Wave Replen|Replenishment|\n|140|Z06 Frag Pool Replen|Replenishment|\n|130|Z99 Gen Manual Replen|Replenishment|\n|120|General Wave Replen|Replenishment|\n|320|Ethical Case Pick|Order Pick|\n|130|Z51  Manual Replen|Replenishment|\n|120|Z09 Cage Pool Replen|Replenishment|\n|330|Z09 Cage Manual Replen|Replenishment|\n|320|Z20  Pool Replen|Replenishment|\n|330|Z10 Manual Move|Replenishment|\n|130|Z22 General Manual Replen|Replenishment|\n|330|Z07 Gen Manual Replen|Replenishment|\n|100|Z07 Gen Wave Replen|Replenishment|\n|330|Z77  Manual Replen|Replenishment|\n|130|General Manual Replen|Replenishment|\n|140|Z13 Cage Manual Replen|Replenishment|\n|140|Z07 Mezz201 Manual Replen|Replenishment|\n|320|Z12 Cage Pool Replen|Replenishment|\n|110|Packing|Packing|\n|100|Z77  Pool Replen|Replenishment|\n|330|Z11 Cage Pool Replen|Replenishment|\n|320|Z51  Gen Pool Replen|Replenishment|\n|130|Ethical Nested Putaway|Putaway|\n|120|Z03 Gen Pool Replen|Replenishment|\n|120|Z01 General Manual Replen|Replenishment|\n|320|Z03 Manual Move|Replenishment|\n|290|Receipt|NULL|\n|140|Ethical Tote Pick|Order Pick|\n|140|Z07 Flamm Wave Replen|Replenishment|\n|240|General Tote Pick|Order Pick|\n|120|Z01 Manual Move|Replenishment|\n|130|Z75  Pool Replen|Replenishment|\n|120|Z22 General Manual Replen|Replenishment|\n|100|Z07 Cage Manual Replen|Replenishment|\n|320|Z12 Cage Manual Replen|Replenishment|\n|100|Z06 General Manual Replen|Replenishment|\n|170|NULL|NULL|\n|320|Z10 Cage Wave Replen|Replenishment|\n|330|Manual Move|Replenishment|\n|150|NULL|NULL|\n|320|Z77  Cage Manual Replen|Replenishment|\n|330|Z13 Cage Pool Replen|Replenishment|\n|260|NULL|NULL|\n|130|Z51  WCS Manual Move|NULL|\n|130|Z06 General Wave Replen|Replenishment|\n|140|Z09 Manual Move|Replenishment|\n|330|Z51  Manual Move|Replenishment|\n|100|Z66  Pool Replen|Replenishment|\n|120|Staging|Packing|\n|320|Z06 Cage Pool Replen|Replenishment|\n|130|Z09 Cage Manual Replen|Replenishment|\n|120|Z12 Gen Pool Replen|Replenishment|\n|140|Z09 Cage Wave Replen|Replenishment|\n|320|Z04 Gen Pool Replen|Replenishment|\n|320|Z05 Manual Move|Replenishment|\n|130|Z57  Pool Replen|Replenishment|\n|140|Z05 General Wave Replen|Replenishment|\n|200|NULL|NULL|\n|320|Z57  Manual Replen|Replenishment|\n|140|Cage Tote Pick|Order Pick|\n|330|Z12 Gen Wave Replen|Replenishment|\n|120|Z66  Manual Move|Replenishment|\n|320|Z12 Gen Wave Replen|Replenishment|\n|140|Z66  Manual Replen|Replenishment|\n|140|Z02 Gen Pool Replen|Replenishment|\n|330|Z06 Frag Manual Move|Replenishment|\n|100|Z08 Manual Move|Replenishment|\n|330|Z04 Gen Pool Replen|Replenishment|\n|330|Z09 Ethical Pool Replen|Replenishment|\n|320|Cage Nested Putaway|Putaway|\n|140|Z09 Gen Pool Replen|Replenishment|\n|140|Z06 Gen Pool Replen|Replenishment|\n|330|Cycle Counting|Cycle Counting|\n|130|Z08 Gen Pool Replen|Replenishment|\n|330|Ethical Putaway|Putaway|\n|180|NULL|NULL|\n|140|Z05 Gen Pool Replen|Replenishment|\n|130|Z01 General Wave Replen|Replenishment|\n|130|Z99 Gen Wave Replen|Replenishment|\n|130|Z51  Manual Move|Replenishment|\n|330|Z08 Ethical Manual Move|Replenishment|\n|120|Z09 Manual Move|Replenishment|\n|330|Z99 Manual Move|Replenishment|\n|120|Z01 General Wave Replen|Replenishment|\n|140|Z08 Ethical Pool Replen|Replenishment|\n|120|Z03 General Manual Replen|Replenishment|\n|130|Z10 Cage Pool Replen|Replenishment|\n|240|General Wave Replen|Replenishment|\n|330|Z09 Cage Wave Replen|Replenishment|\n|320|Z99  Gen Pool Replen|Replenishment|\n|100|Z04 Manual Move|Replenishment|\n|140|Z08 Gen Manual Replen|Replenishment|\n|130|Z77  Manual Move|Replenishment|\n|130|Z09 Manual Move|Replenishment|\n|120|Z75  Manual Move|Replenishment|\n|140|Z02 General Wave Replen|Replenishment|\n|140|Z51  WCS Manual Replen|NULL|\n|320|Z06 Cage Wave Replen|Replenishment|\n|330|Flamm Tote Pick|Order Pick|\n|330|General Tote Pick|Order Pick|\n|120|Z21  Manual Replen|Replenishment|\n|330|General Putaway|Putaway|\n|320|Fragrance Putaway|Putaway|\n|320|Z07 Cage Manual Replen|Replenishment|\n|100|Z08 Gen Wave Replen|Replenishment|\n|410|NULL|NULL|\n|320|Loading|Dock Management|\n|130|Z75  Wave Replen|Replenishment|\n|320|Manual Move|Replenishment|\n|130|Z02 Gen Pool Replen|Replenishment|\n|130|Z77  Pool Replen|Replenishment|\n|130|Z77  Cage Manual Move|Replenishment|\n|120|Z10 Gen Pool Replen|Replenishment|\n|130|Z11 Cage Wave Replen|Replenishment|\n|320|Z77  Manual Replen|Replenishment|\n|130|Z66  Pool Replen|Replenishment|\n|120|Z12 Gen Manual Replen|Replenishment|\n|140|Z51  WCS Manual Replen|Replenishment|\n|320|Frag Case Pick|Order Pick|\n|320|Flamm Case Pick|Order Pick|\n|320|Z07 Mezz201 Manual Replen|Replenishment|\n|120|Z09 Gen Manual Replen|Replenishment|\n|130|Z09 Gen Wave Replen|Replenishment|\n|120|Z70 Manual Move|Replenishment|\n|140|Z10 Cage Wave Replen|Replenishment|\n|330|Z02 Manual Move|Replenishment|\n|120|Z02 Gen Pool Replen|Replenishment|\n|330|Z70 Gen Pool Replen|Replenishment|\n|140|General Wave Replen|Replenishment|\n|330|Z77  Wave Replen|Replenishment|\n|130|Z21  Manual Replen|Replenishment|\n|130|Z13 Cage Pool Replen|Replenishment|\n|140|Z06 Cage Pool Replen|Replenishment|\n|120|Cage Nested Putaway|Putaway|\n|140|Z20  Pool Replen|Replenishment|\n|320|Z08 Manual Move|Replenishment|\n|320|Z10 Cage Manual Replen|Replenishment|\n|140|Z13 Cage Wave Replen|Replenishment|\n|100|Z57  Manual Replen|Replenishment|\n|330|Z06 Cage Wave Replen|Replenishment|\n|320|Z07 Gen Manual Replen|Replenishment|\n|330|Z06 Cage Manual Replen|Replenishment|\n|330|Z99  Gen Pool Replen|Replenishment|\n|330|Z08 Ethical Manual Replen|Replenishment|\n|120|Z57  Pool Replen|Replenishment|\n|330|Z20  Manual Move|Replenishment|\n|120|Ethical Tote Pick|Order Pick|\n|140|Manual Move|Replenishment|\n|140|Z70 Gen Wave Replen|Replenishment|\n|130|Z07 Gen Pool Replen|Replenishment|\n|330|Z22 General Manual Replen|Replenishment|\n|140|Z04 General Manual Replen|Replenishment|\n|320|Z01 Gen Pool Replen|Replenishment|\n|140|Z01 Manual Move|Replenishment|\n|140|Z07 Flamm Manual Replen|Replenishment|\n|140|Z06 General Manual Replen|Replenishment|\n|130|Z06 Manual Move|Replenishment|\n|330|Frag Case Pick|Order Pick|\n|140|Cage Putaway|Putaway|\n|100|Z04 Gen Pool Replen|Replenishment|\n|320|Z02 Manual Move|Replenishment|\n|320|Ethical Putaway|Putaway|\n|130|Z02 General Manual Replen|Replenishment|\n|330|Z07 Cage Pool Replen|Replenishment|\n|220|Standard Tote Pick|Order Pick|\n|220|General Tote Pick|Order Pick|\n|320|Cage Case Pick|Order Pick|\n|320|Z05 Gen Pool Replen|Replenishment|\n|140|Ethical Nested Putaway|Putaway|\n|240|Z99 Gen Manual Replen|Replenishment|\n\n|The Diffrent Transaction Type will help us differciante between the type of transactions for it |\n|By filtering out the  '-scale%'  location we will remove the the duplicated processes |", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 153}}
{"issue_key": "CSCI-511", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "03/Oct/25 5:40 PM", "updated": "14/Oct/25 10:00 AM", "labels": [], "summary": "MergeCo Supply Chain Reporting - Create Availability Historical Table from Network Drive File", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nMergeCo Supply Chain Reporting - Create Availability Historical Table from Network Drive File\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 154}}
{"issue_key": "CSCI-510", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "03/Oct/25 5:40 PM", "updated": "16/Oct/25 9:52 AM", "labels": [], "summary": "MergeCo Supply Chain Reporting - Create ADF Pipeline to refresh Availability Network Drive file daily", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nMergeCo Supply Chain Reporting - Create ADF Pipeline to refresh Availability Network Drive file daily\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 155}}
{"issue_key": "CSCI-509", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "03/Oct/25 5:39 PM", "updated": "09/Oct/25 10:06 AM", "labels": [], "summary": "MergeCo Supply Chain Reporting - Schedule ADF Pipelines to refresh daily", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Pipelines are created - just awaiting on approval for the PR by @user", "text": "Summary\nMergeCo Supply Chain Reporting - Schedule ADF Pipelines to refresh daily\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nPipelines are created - just awaiting on approval for the PR by @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 156}}
{"issue_key": "CSCI-507", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "03/Oct/25 5:34 PM", "updated": "07/Oct/25 9:59 AM", "labels": [], "summary": "MergeCo Supply Chain Reporting - Re-Engineer YTD Availability Metric", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nMergeCo Supply Chain Reporting - Re-Engineer YTD Availability Metric\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 157}}
{"issue_key": "CSCI-506", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "02/Oct/25 9:56 AM", "updated": "25/Oct/25 4:13 AM", "labels": [], "summary": "CI/CD documentation - document CI/CD repos in documentation", "description": "Azure DevOps Linked tickets:\n\n* [Feature 209127 Document Environment IAC|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/209127]\n* [User Story 210948 Create ADF Documentation|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/210948]\n\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Can you please share doc link thx\n\nDocuments created:\n\n* Infrastructure\n** [Architecture - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5277/Architecture]\n** [Configuration - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5279/Configuration]\n** all docs in source control: [docs - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infrastructure?version=GBmaster&path=/docs]\n* Snowflake Infrastructure\n** [Architecture - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5265/Architecture]\n** [Configuration - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5267/Configuration]\n** all docs in source control: [docs - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-snowflake-infrastructure?path=/docs]\n\nDocuments for edp-data-factory repository are created for review: [docs - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory?version=GBfeature/210950-create-adf-architecture-documentation&path=/docs]\n\n* README\n* Architecture\n* Configuration\n* CI\n* Deployment\n* Security\n* Developer Guide\n* Troubleshooting\n\nPR created for ed-data-factory documentation at; [Pull request 22988: Created documentation for edp-adf - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22988?path=%2Fdocs%2Farchitecture.md]\n\n@user @user can you please review?\n\nThanks.\n\n@user @user @user can you please revie this PR [Pull request 22988: Created documentation for edp-adf - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22988]. Thanks.\n\ncc: @user\n\nHi Eugene, I have scanned through the docs. My only comment is: at the moment we are referencing QA but we will need to go back and add/ update details on SIT/UAT.\n\n@user once we have the tickets for the additional environments, we need to update the docs accordingly as soon as they are configured.", "text": "Summary\nCI/CD documentation - document CI/CD repos in documentation\n\n---\n\nDescription\nAzure DevOps Linked tickets:\n\n* [Feature 209127 Document Environment IAC|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/209127]\n* [User Story 210948 Create ADF Documentation|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/210948]\n\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nCan you please share doc link thx\n\nDocuments created:\n\n* Infrastructure\n** [Architecture - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5277/Architecture]\n** [Configuration - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5279/Configuration]\n** all docs in source control: [docs - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-infrastructure?version=GBmaster&path=/docs]\n* Snowflake Infrastructure\n** [Architecture - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5265/Architecture]\n** [Configuration - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5267/Configuration]\n** all docs in source control: [docs - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-snowflake-infrastructure?path=/docs]\n\nDocuments for edp-data-factory repository are created for review: [docs - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory?version=GBfeature/210950-create-adf-architecture-documentation&path=/docs]\n\n* README\n* Architecture\n* Configuration\n* CI\n* Deployment\n* Security\n* Developer Guide\n* Troubleshooting\n\nPR created for ed-data-factory documentation at; [Pull request 22988: Created documentation for edp-adf - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22988?path=%2Fdocs%2Farchitecture.md]\n\n@user @user can you please review?\n\nThanks.\n\n@user @user @user can you please revie this PR [Pull request 22988: Created documentation for edp-adf - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22988]. Thanks.\n\ncc: @user\n\nHi Eugene, I have scanned through the docs. My only comment is: at the moment we are referencing QA but we will need to go back and add/ update details on SIT/UAT.\n\n@user once we have the tickets for the additional environments, we need to update the docs accordingly as soon as they are configured.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 158}}
{"issue_key": "CSCI-504", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Oct/25 11:38 AM", "updated": "05/Nov/25 3:51 PM", "labels": [], "summary": "SCAX2012 test server availability", "description": "The SCAX2012 DB on TDB19b is not available anymore. Adeel has set up access on the TDB16MB test server. The IP of this server is 172.21.10.33. \n\nThe username and password are the same as the old server. \n\nNext steps: \n\n* Confirm the same tables and data history on the new server\n* To turn off the trigger on SCAX2012 in ADF\n* To get the new server whitelisted\n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Adeel has confirmed that there are the same tables and data history on the new server. Existing trigger in ADF was also removed.\n\nNow we need to raise a request to get the new servers whitelisted and update ADF linked service to point to it.\n\n@user will help take over from here.\n\nRequest has been sent to Anjali to create whitelisting of IPs from ADF. Once done we will check the ingestion process running with the new server.", "text": "Summary\nSCAX2012 test server availability\n\n---\n\nDescription\nThe SCAX2012 DB on TDB19b is not available anymore. Adeel has set up access on the TDB16MB test server. The IP of this server is 172.21.10.33. \n\nThe username and password are the same as the old server. \n\nNext steps: \n\n* Confirm the same tables and data history on the new server\n* To turn off the trigger on SCAX2012 in ADF\n* To get the new server whitelisted\n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nAdeel has confirmed that there are the same tables and data history on the new server. Existing trigger in ADF was also removed.\n\nNow we need to raise a request to get the new servers whitelisted and update ADF linked service to point to it.\n\n@user will help take over from here.\n\nRequest has been sent to Anjali to create whitelisting of IPs from ADF. Once done we will check the ingestion process running with the new server.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 159}}
{"issue_key": "CSCI-503", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Oct/25 8:34 AM", "updated": "08/Oct/25 11:35 AM", "labels": [], "summary": "ADF- Generic Dataset, Linked Service POC & Raise PR", "description": "h3. Context\n\n* As per Review comments on ADF PR, Eugene suggested to do POC for generic pipeline to take care of all ingestion instead of creating different pipelines for different sources.\n\nh3. Objective\n\n* To come up with a generic pipeline strategy to take care of majority of ingestion.\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* -Create Parameterized LinkedService-\n* -Create Parameterized DataSets-\n* -Create Generic ADF Pipeline for SQL Server-\n* Create Generic ADF Pipeline for MY SQL\n* Work On to raise first clean PR to push these generic components\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "@user \nCan you note what @user has got you to try and then provide your approach?\n\nCan update this in description\n\nInitial PR created\n\nHi @user / @user ,\n\nI have implemented the review comments provided by @user .\n\nI also need to test the concurrent load testing on the shared pipeline. Looks like to do that we first need to publish the pipeline & triggers into master first so that the triggers can be tested to run parallelly.\n\nHi @user ,\n\nThe PR is merged into Master. We are able to trigger multiple instances of the pipeline for various sources at the same time. I will try to do say 5 concurrent loads at the same time to test how it goes and post the results here today.\n\n@user , @user .. FYI as discussed", "text": "Summary\nADF- Generic Dataset, Linked Service POC & Raise PR\n\n---\n\nDescription\nh3. Context\n\n* As per Review comments on ADF PR, Eugene suggested to do POC for generic pipeline to take care of all ingestion instead of creating different pipelines for different sources.\n\nh3. Objective\n\n* To come up with a generic pipeline strategy to take care of majority of ingestion.\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* -Create Parameterized LinkedService-\n* -Create Parameterized DataSets-\n* -Create Generic ADF Pipeline for SQL Server-\n* Create Generic ADF Pipeline for MY SQL\n* Work On to raise first clean PR to push these generic components\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n@user \nCan you note what @user has got you to try and then provide your approach?\n\nCan update this in description\n\nInitial PR created\n\nHi @user / @user ,\n\nI have implemented the review comments provided by @user .\n\nI also need to test the concurrent load testing on the shared pipeline. Looks like to do that we first need to publish the pipeline & triggers into master first so that the triggers can be tested to run parallelly.\n\nHi @user ,\n\nThe PR is merged into Master. We are able to trigger multiple instances of the pipeline for various sources at the same time. I will try to do say 5 concurrent loads at the same time to test how it goes and post the results here today.\n\n@user , @user .. FYI as discussed", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 160}}
{"issue_key": "CSCI-502", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Sep/25 2:55 PM", "updated": "10/Oct/25 5:15 PM", "labels": [], "summary": "Creation of Service Account to access Network Drive- RITM0176342", "description": "h3. Context\n\n* Creation of Service Account to access Network Drive- RITM0176342\n\nh3. We need a Service Account creation to access the below Network Drive for Ingestion of supply chain data into Snowflake for group level reporting.\n\nWill be retrieving data from this location (Folder path):\n \\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "assigned to Ops teams.\n\nIT SysOps Team is looking into this, I followed up today.\n\ncc- @user @user @user @user\n\nReached out to a SysOps team member to help as it hadn’t been picked by anyone from their team.\n\ncc- @user @user @user\n\nPending ServiceNow respond - WIP\n\ncurrently WIP cc @user @user\n\nIssue split into:\n|CSCI-536|Creation of Service Account to access Network Drive- RITM0176342 Sprint 10|\n\nTo continue monitor in sprint 10\n\nIssue split into:\n|CSCI-537|Creation of Service Account to access Network Drive- RITM0176342 Sprint 10|", "text": "Summary\nCreation of Service Account to access Network Drive- RITM0176342\n\n---\n\nDescription\nh3. Context\n\n* Creation of Service Account to access Network Drive- RITM0176342\n\nh3. We need a Service Account creation to access the below Network Drive for Ingestion of supply chain data into Snowflake for group level reporting.\n\nWill be retrieving data from this location (Folder path):\n \\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nassigned to Ops teams.\n\nIT SysOps Team is looking into this, I followed up today.\n\ncc- @user @user @user @user\n\nReached out to a SysOps team member to help as it hadn’t been picked by anyone from their team.\n\ncc- @user @user @user\n\nPending ServiceNow respond - WIP\n\ncurrently WIP cc @user @user\n\nIssue split into:\n|CSCI-536|Creation of Service Account to access Network Drive- RITM0176342 Sprint 10|\n\nTo continue monitor in sprint 10\n\nIssue split into:\n|CSCI-537|Creation of Service Account to access Network Drive- RITM0176342 Sprint 10|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 161}}
{"issue_key": "CSCI-501", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Sep/25 1:13 PM", "updated": "09/Oct/25 2:05 AM", "labels": [], "summary": "ADF Pull Requests - Review and Approve", "description": "Eugene to review and approve the below PRs:\n\n \n\n#1 Ashu: [https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22791|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22791] \n\n#2 Aswini: [https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22802|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22802] \n\n#3 Jess: [https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22748|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22748]", "acceptance_criteria": "PRs approved and merged", "comments": "Eugene has provided feedback to the team. \n\n@user @user are exploring an option to create a generalised data pipelines where we can parameterise the Datasets.\n\nIssue split into:\n|CSCI-518|ADF Pull Requests - source config CI/CD setup|", "text": "Summary\nADF Pull Requests - Review and Approve\n\n---\n\nDescription\nEugene to review and approve the below PRs:\n\n \n\n#1 Ashu: [https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22791|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22791] \n\n#2 Aswini: [https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22802|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22802] \n\n#3 Jess: [https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22748|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory/pullrequest/22748]\n\n---\n\nAcceptance Criteria\nPRs approved and merged\n\n---\n\nComments\nEugene has provided feedback to the team. \n\n@user @user are exploring an option to create a generalised data pipelines where we can parameterise the Datasets.\n\nIssue split into:\n|CSCI-518|ADF Pull Requests - source config CI/CD setup|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 162}}
{"issue_key": "CSCI-500", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Sep/25 2:03 PM", "updated": "30/Sep/25 8:52 AM", "labels": [], "summary": "ADF Naming standard documentation", "description": "Documentation of ADF naming standard.", "acceptance_criteria": "* Create a Wiki page in azure DevOps", "comments": "@user to review it. [Azure Data Factory - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5259/Azure-Data-Factory]\n\n@user Awesome! Thanks Ashu!", "text": "Summary\nADF Naming standard documentation\n\n---\n\nDescription\nDocumentation of ADF naming standard.\n\n---\n\nAcceptance Criteria\n* Create a Wiki page in azure DevOps\n\n---\n\nComments\n@user to review it. [Azure Data Factory - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5259/Azure-Data-Factory]\n\n@user Awesome! Thanks Ashu!", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 163}}
{"issue_key": "CSCI-499", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Sep/25 9:45 AM", "updated": "18/Dec/25 11:09 AM", "labels": [], "summary": "Snowflake - PowerBI network policy", "description": "h3. Context\n\n* To configure network policies for PowerBI to only allow access from trusted IP addresses\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Hi @user we need to add a network policy to the Snowflake PowerBI user being used for MergeCo reports. Can you pls help me confirm if the PowerBI is accessing Snowflake using the privatelink? and can you provide the IP addresses for this user?\n\nMany thanks!\n\nChloe\n\nFYI\n\n@user @user @user @user\n\n@user Hi Chloe,\n\nYes currently the PBI user is connecting with the privatelink ([cw-au.privatelink.snowflakecomputing.com|http://cw-au.privatelink.snowflakecomputing.com]). See below details of the current setup:\n\nWe are connecting via the *EDP_ANALYSIS_DEV* warehouse, and using the *S_POWERBI_DEV* user account (basic authentication).\n\nThe current VNet Gateway we are connecting with is *cwr-ase-edp-dev-vdgw*:\n\nThe virtual network this sits on is *cwr-ase-nonprod-dpdev-vnet*:\n\nAnd the IP Address under the Subnet for Fabric is *172.29.84.96/27*.\n\nAs confirmed by Eugene the IP range below is fixed for Vnet Data Gateway in Dev environment. @user @user @user\n\nWe do not need a Snowflake Network Policy when we are using PrivateLink-only.\n\nOnce we enable the PrivateLink-only mode via the below scripts:\n\n{noformat}SELECT SYSTEM$ENFORCE_PRIVATELINK_ACCESS_ONLY();{noformat}\n\nSnowflake enforces:\n\n* All traffic must come through Azure PrivateLink\n* Public endpoint → fully blocked\n* TLS termination happens only on the PrivateLink endpoint\n* Source traffic will always appear as your private VNet IPs (10.x.x.x)\n\nThis alone gives you a network boundary.\n\nSo even without any Snowflake Network Policy: \n\n* Public access is impossible\n* Internet access is impossible\n* Malicious external traffic cannot reach your account\n* Only PrivateLink-connected Azure VNets can reach Snowflake\n\n@user @user @user @user @user @user\n\nI am looking to enable the Snowflake Privatelink-Only mode however @user tested again this morning and is still having issues with connecting to Snowflake's privatelink via Power BI Desktop. Suggesting delaying enabling privatelink-only mode until we get this resolved. \n\nPending updates from Raveen and Eugene on MS response for the above issue.\n\nEnabled the Snowflake Privatelink-Only mode today after confirming SSO authentication from PowerBI to Snowflake successful.", "text": "Summary\nSnowflake - PowerBI network policy\n\n---\n\nDescription\nh3. Context\n\n* To configure network policies for PowerBI to only allow access from trusted IP addresses\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nHi @user we need to add a network policy to the Snowflake PowerBI user being used for MergeCo reports. Can you pls help me confirm if the PowerBI is accessing Snowflake using the privatelink? and can you provide the IP addresses for this user?\n\nMany thanks!\n\nChloe\n\nFYI\n\n@user @user @user @user\n\n@user Hi Chloe,\n\nYes currently the PBI user is connecting with the privatelink ([cw-au.privatelink.snowflakecomputing.com|http://cw-au.privatelink.snowflakecomputing.com]). See below details of the current setup:\n\nWe are connecting via the *EDP_ANALYSIS_DEV* warehouse, and using the *S_POWERBI_DEV* user account (basic authentication).\n\nThe current VNet Gateway we are connecting with is *cwr-ase-edp-dev-vdgw*:\n\nThe virtual network this sits on is *cwr-ase-nonprod-dpdev-vnet*:\n\nAnd the IP Address under the Subnet for Fabric is *172.29.84.96/27*.\n\nAs confirmed by Eugene the IP range below is fixed for Vnet Data Gateway in Dev environment. @user @user @user\n\nWe do not need a Snowflake Network Policy when we are using PrivateLink-only.\n\nOnce we enable the PrivateLink-only mode via the below scripts:\n\n{noformat}SELECT SYSTEM$ENFORCE_PRIVATELINK_ACCESS_ONLY();{noformat}\n\nSnowflake enforces:\n\n* All traffic must come through Azure PrivateLink\n* Public endpoint → fully blocked\n* TLS termination happens only on the PrivateLink endpoint\n* Source traffic will always appear as your private VNet IPs (10.x.x.x)\n\nThis alone gives you a network boundary.\n\nSo even without any Snowflake Network Policy: \n\n* Public access is impossible\n* Internet access is impossible\n* Malicious external traffic cannot reach your account\n* Only PrivateLink-connected Azure VNets can reach Snowflake\n\n@user @user @user @user @user @user\n\nI am looking to enable the Snowflake Privatelink-Only mode however @user tested again this morning and is still having issues with connecting to Snowflake's privatelink via Power BI Desktop. Suggesting delaying enabling privatelink-only mode until we get this resolved. \n\nPending updates from Raveen and Eugene on MS response for the above issue.\n\nEnabled the Snowflake Privatelink-Only mode today after confirming SSO authentication from PowerBI to Snowflake successful.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 164}}
{"issue_key": "CSCI-496", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Sep/25 8:22 AM", "updated": "10/Oct/25 5:07 PM", "labels": [], "summary": "MergeCo Conformed Reporting - Draft Solution Design - Sprint 9", "description": "Create draft of the solution design for the Merge Co Conformed reporting.\n\nJust needs to be a one-page document, outlining *how* we will build and ingest the data for this reporting.\n\nIncludes:\n\n* What grain of data required for each metric\n* How we will push data for each environment into a join environment\n* Merging them into a common dimension", "acceptance_criteria": "", "comments": "Have started populating page:\n\n[https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design]\n\nIssue split into:\n|CSCI-532|MergeCo Conformed Reporting - Draft Solution Design - Sprint 10|\n\nTo be continue in sprint 10", "text": "Summary\nMergeCo Conformed Reporting - Draft Solution Design - Sprint 9\n\n---\n\nDescription\nCreate draft of the solution design for the Merge Co Conformed reporting.\n\nJust needs to be a one-page document, outlining *how* we will build and ingest the data for this reporting.\n\nIncludes:\n\n* What grain of data required for each metric\n* How we will push data for each environment into a join environment\n* Merging them into a common dimension\n\n---\n\nComments\nHave started populating page:\n\n[https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1655504898/MergeCo+Tactical+Reporting+-+Solution+Design]\n\nIssue split into:\n|CSCI-532|MergeCo Conformed Reporting - Draft Solution Design - Sprint 10|\n\nTo be continue in sprint 10", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 165}}
{"issue_key": "CSCI-495", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Sep/25 8:18 AM", "updated": "06/Oct/25 5:30 PM", "labels": [], "summary": "Access Manhattan Active- Whitelisting- RITM0176041 - sprint 9", "description": "h3. Context\n\nThis is for Snowflake to access Manhattan Active. They have already been able to action the whitelisting on their end, but we need to enable whitelisting on our firewall also.\nWe will be querying their database and ingesting into Snowflake via ADF (same pattern as our network drive). Manhattan won't be accessing anything our end - we just need to enable our firewall so that we can query their DB\n\nh3. More Details:\n\n# Port is 3306 (default) \n# We are wanting to enable the firewall rule for Azure (so our SHIR - which is hosted in Azure can access Manhattan)\n# The data flow is: Manhattan Active --> SHIR + ADF --> Blob Storage --> Snowflake\n#* We use ADF as the ETL tool to move data from Manhattan to Blob Storage, and the SHIR is the machine which powers this\n# The requirement is to have the ability to move data *from Manhattan* *to our Azure* Environment - that's what we need enabled\n#* From here, we then use ADF + SHIR to move the data to Snowflake\n# SHIR is on an Azure VM Scale Set - cwr-ase-edp-shir-dev-vmss. It is all in the cloud and is sitting in our private network\n# Warehouse Management/Stock Data - no PII\n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "assigned to Cloud team - Brent R - @user\n\n@user - working on this (whitelisting) - ETA today.\n\nPretty sure this is done 🙂 \n @user\n\nThis has been whitelisted, Team confirmed via testing (screenshot below)\n\ncc- @user @user @user @user\n\n@user I’ll put it as in-review", "text": "Summary\nAccess Manhattan Active- Whitelisting- RITM0176041 - sprint 9\n\n---\n\nDescription\nh3. Context\n\nThis is for Snowflake to access Manhattan Active. They have already been able to action the whitelisting on their end, but we need to enable whitelisting on our firewall also.\nWe will be querying their database and ingesting into Snowflake via ADF (same pattern as our network drive). Manhattan won't be accessing anything our end - we just need to enable our firewall so that we can query their DB\n\nh3. More Details:\n\n# Port is 3306 (default) \n# We are wanting to enable the firewall rule for Azure (so our SHIR - which is hosted in Azure can access Manhattan)\n# The data flow is: Manhattan Active --> SHIR + ADF --> Blob Storage --> Snowflake\n#* We use ADF as the ETL tool to move data from Manhattan to Blob Storage, and the SHIR is the machine which powers this\n# The requirement is to have the ability to move data *from Manhattan* *to our Azure* Environment - that's what we need enabled\n#* From here, we then use ADF + SHIR to move the data to Snowflake\n# SHIR is on an Azure VM Scale Set - cwr-ase-edp-shir-dev-vmss. It is all in the cloud and is sitting in our private network\n# Warehouse Management/Stock Data - no PII\n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nassigned to Cloud team - Brent R - @user\n\n@user - working on this (whitelisting) - ETA today.\n\nPretty sure this is done 🙂 \n @user\n\nThis has been whitelisted, Team confirmed via testing (screenshot below)\n\ncc- @user @user @user @user\n\n@user I’ll put it as in-review", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 166}}
{"issue_key": "CSCI-494", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Sep/25 8:16 AM", "updated": "30/Sep/25 9:48 AM", "labels": [], "summary": "Follow up on - RITM0175340- Azure Blob storage access from office - sprint 9", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Hey @user - i’ll at least need a copy-paste from the original RITM ticket to here. \nThanks.\n\nwith @user @user to test.\n\nThe ticket number is RITM0175340 as mentioned in the header of this ticket.\n\ncc- @user", "text": "Summary\nFollow up on - RITM0175340- Azure Blob storage access from office - sprint 9\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nHey @user - i’ll at least need a copy-paste from the original RITM ticket to here. \nThanks.\n\nwith @user @user to test.\n\nThe ticket number is RITM0175340 as mentioned in the header of this ticket.\n\ncc- @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 167}}
{"issue_key": "CSCI-493", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Sep/25 8:09 AM", "updated": "06/Oct/25 2:08 PM", "labels": [], "summary": "Handover From Adeel to next person from DB team - finalisation", "description": "Confluence doc to cover:\n\n* -where things are up to-\n** -Modelling-\n** -STTM-\n* -Parquet extract:-\n** -Extract-\n** -Upload-\n* What we need from this project\n* Catchup session", "acceptance_criteria": "", "comments": "@user I’ve uploaded the handover docs in Mergco folder\n\n[MergeCo Data Team - Adeel Handover - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F01%2E%20SC%20Interim%20Data%20Platform%2F03%2E%20Test%20and%20Deploy%2FAdeel%20Handover&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx]\n\nhandover with sam complete", "text": "Summary\nHandover From Adeel to next person from DB team - finalisation\n\n---\n\nDescription\nConfluence doc to cover:\n\n* -where things are up to-\n** -Modelling-\n** -STTM-\n* -Parquet extract:-\n** -Extract-\n** -Upload-\n* What we need from this project\n* Catchup session\n\n---\n\nComments\n@user I’ve uploaded the handover docs in Mergco folder\n\n[MergeCo Data Team - Adeel Handover - All Documents|https://mychemist.sharepoint.com/sites/TheLanding/MergeCoData/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2F01%2E%20Projects%2F01%2E%20SC%20Interim%20Data%20Platform%2F03%2E%20Test%20and%20Deploy%2FAdeel%20Handover&newTargetListUrl=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents&viewpath=%2Fsites%2FTheLanding%2FMergeCoData%2FShared%20Documents%2FForms%2FAllItems%2Easpx]\n\nhandover with sam complete", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 168}}
{"issue_key": "CSCI-492", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Sep/25 8:08 AM", "updated": "10/Oct/25 3:44 PM", "labels": [], "summary": "Review Source-to-Target Map for Fact_Warehouse_Location_Inbound_Transactions - sprint 9", "description": "Source to target mapping for Fact_Warehouse_Location_Inbound_Transactions - filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "review complete. But we might have to revist this as we progress to clearly get the business logic in place", "text": "Summary\nReview Source-to-Target Map for Fact_Warehouse_Location_Inbound_Transactions - sprint 9\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Inbound_Transactions - filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nreview complete. But we might have to revist this as we progress to clearly get the business logic in place", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 169}}
{"issue_key": "CSCI-490", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Sep/25 8:07 AM", "updated": "10/Oct/25 10:25 AM", "labels": [], "summary": "Create Source-to-Target Map for DimBatch", "description": "h3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Batch table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Batch table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.", "acceptance_criteria": "* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "comments": "found the source table \n@user FYI, this mapping is only for on Prem ILS\nlocation: PDB15.ILS.dbo.LOCATION_INVENTORY\n\n{noformat}SELECT Top 100 li.BATCH, li.BATCH_EXPIRY_DATE,li.item, li.warehouse, li.LOCATION, li.RECEIVED_DATE,ON_HAND_QTY,AGING_DATE,INTERNAL_LOCATION_INV,*\nfrom ILS.dbo.item i (nolock)\nleft join ILS.dbo.LOCATION_INVENTORY li (nolock) on li.item=i.item AND LI.COMPANY =I.COMPANY\nwhere i.COMPANY = 'CW01' --Excluding EPH\nAND i.ITEM_CATEGORY10 ='Y' --Batch and Expiry\nAND li.LOCATION_TEMPLATE !='MISC'\n\nand batch = 'ZL01Y'\n\nOrder by 1 desc {noformat}", "text": "Summary\nCreate Source-to-Target Map for DimBatch\n\n---\n\nDescription\nh3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Batch table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Batch table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.\n\n---\n\nAcceptance Criteria\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)\n\n---\n\nComments\nfound the source table \n@user FYI, this mapping is only for on Prem ILS\nlocation: PDB15.ILS.dbo.LOCATION_INVENTORY\n\n{noformat}SELECT Top 100 li.BATCH, li.BATCH_EXPIRY_DATE,li.item, li.warehouse, li.LOCATION, li.RECEIVED_DATE,ON_HAND_QTY,AGING_DATE,INTERNAL_LOCATION_INV,*\nfrom ILS.dbo.item i (nolock)\nleft join ILS.dbo.LOCATION_INVENTORY li (nolock) on li.item=i.item AND LI.COMPANY =I.COMPANY\nwhere i.COMPANY = 'CW01' --Excluding EPH\nAND i.ITEM_CATEGORY10 ='Y' --Batch and Expiry\nAND li.LOCATION_TEMPLATE !='MISC'\n\nand batch = 'ZL01Y'\n\nOrder by 1 desc {noformat}", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 170}}
{"issue_key": "CSCI-489", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 10:53 AM", "updated": "29/Sep/25 8:18 AM", "labels": [], "summary": "Access Manhattan Active- Whitelisting- RITM0176041", "description": "h3. Context\n\nThis is for Snowflake to access Manhattan Active. They have already been able to action the whitelisting on their end, but we need to enable whitelisting on our firewall also.\nWe will be querying their database and ingesting into Snowflake via ADF (same pattern as our network drive). Manhattan won't be accessing anything our end - we just need to enable our firewall so that we can query their DB\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAccess Manhattan Active- Whitelisting- RITM0176041\n\n---\n\nDescription\nh3. Context\n\nThis is for Snowflake to access Manhattan Active. They have already been able to action the whitelisting on their end, but we need to enable whitelisting on our firewall also.\nWe will be querying their database and ingesting into Snowflake via ADF (same pattern as our network drive). Manhattan won't be accessing anything our end - we just need to enable our firewall so that we can query their DB\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 171}}
{"issue_key": "CSCI-488", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 10:06 AM", "updated": "10/Oct/25 9:49 AM", "labels": [], "summary": "Capture the requirements for creating the Jumpbox for the developers (data engineers)", "description": "Capture the requirements for creating the Jumpbox for the developers (data engineers).\n\n*Key action items*\n\n* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/] - On prem and Cloud Team- *RITM0173487 (for reference)*\n\nWhat’s needed is for the DNS forwarding from on-premise DNS servers, to the Azure privatelink DNS. See: [Resolve Azure and on-premises domains. | Microsoft Learn|https://learn.microsoft.com/en-us/azure/dns/private-resolver-hybrid-dns] -CHG0052044 (change request created)\n\n* Collaborate with the EUC Team (CWR) to *create* Jumpbox\n* Give the Entra (AD) users access to the Jumpbox \n\n|*Snowflake_CW_AU_DataEngineers*|DEV_ROLE_DATA_ENG|\n\n* Configure firewalls (whitelisting) (On-Prem, Azure) as follows\n* VDI / Jumpbox network → Snowflake VNet *(servicenow tickets)*\n\n* *how many jumpboxes* we need? *no. of users 8*\n* Note: Ports to be allowed: 443, 80, 1433\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Configure VDI / Jumbox images to pre-install tools listed below: - The list has been reviewed and approved by EUC Team.\"}],\"attrs\":{\"localId\":\"a6a01a2f-604c-4540-b9f2-173ad2e43e57\",\"state\":\"DONE\"}}],\"attrs\":{\"localId\":\"0c33acf9-c722-4f42-8518-6b140130d8da\"}}\n{adf}\n* *Git for Windows* + *Git Credential Manager (GCM)* (Azure DevOps/GitHub SSO)\n* *Azure CLI* & *Azure PowerShell*\n* *AzCopy* (for ADLS/BLOB moves within Azure)\n* *VS Code* (primary IDE)\n** Extensions: _Snowflake SQL Tools_ (Snowflake), _SQLTools_ (optional), _Python_, _Pylance_, _Jupyter_, _YAML_, _GitLens_\n* *Snowflake CLIs/SDKs*\n** *Snow CLI* (preferred over SnowSQL)\n** _(Optional)_ *SnowSQL* legacy client (only if you still need it)\n* *Drivers*\n** *Snowflake ODBC* and *JDBC* drivers (for BI tools, dbt, notebooks)\n* *Languages / Runtimes*\n** *Python 3.11+* (via *Miniconda* or *pyenv-win*; standardize on conda envs)\n** *Java 17* (LTS) for Snowpark Java/Scala\n** *Node.js 20 LTS* (if you use Streamlit/Node helpers)\n* *Snowpark & data tooling (per environment)*\n** {{pipx}} and/or {{conda}}\n** {{snowflake-snowpark-python}}, {{pandas}}, {{pyarrow}}, {{jupyterlab}}, {{ipykernel}}\n\n* *Power BI Desktop* _(or Power BI Desktop – Optimized for Fabric if that’s your org standard)_\n* *Tabular Editor*\n** TE2 (free) for basic edits or *TE3* (licensed) for advanced modeling/CI\n* *DAX Studio* (performance tuning)\n* *ALM Toolkit* (schema compare/deployment)\n* *Power BI Report Builder* (if you produce paginated reports)\n* *Azure Data Studio* (lightweight SQL + notebooks)\n* *SSMS* (for on-prem SQL Server admin)\n* *Azure Storage Explorer* (browsing ADLS/BLOB via private endpoints)\n* *On-prem / misc drivers*\n** Microsoft SQL Server ODBC, Oracle/ODBC (if used), PostgreSQL ODBC (if used)", "acceptance_criteria": "* The data engineers are able to access Snowflake and other relevant applications via Jumpbox", "comments": "The change request has been raised for DNS forwarding from on-premise DNS servers to the Azure Privatelink DNS.\n\n[CHG0052044|https://cwretail.service-now.com/nav_to.do?uri=sysapproval_approver.do?sys_id=97c987ff33c83e9047764f945d5c7bde]\n\nIf we implement this change, people will be locked out of snowflake and rely on VDI. We are awaiting Harrison’s and Alan’s architecture review as mentioned by Frank to proceed with this request.\n\ncc- @user @user @user\n\n@user to follow up - will create card", "text": "Summary\nCapture the requirements for creating the Jumpbox for the developers (data engineers)\n\n---\n\nDescription\nCapture the requirements for creating the Jumpbox for the developers (data engineers).\n\n*Key action items*\n\n* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/] - On prem and Cloud Team- *RITM0173487 (for reference)*\n\nWhat’s needed is for the DNS forwarding from on-premise DNS servers, to the Azure privatelink DNS. See: [Resolve Azure and on-premises domains. | Microsoft Learn|https://learn.microsoft.com/en-us/azure/dns/private-resolver-hybrid-dns] -CHG0052044 (change request created)\n\n* Collaborate with the EUC Team (CWR) to *create* Jumpbox\n* Give the Entra (AD) users access to the Jumpbox \n\n|*Snowflake_CW_AU_DataEngineers*|DEV_ROLE_DATA_ENG|\n\n* Configure firewalls (whitelisting) (On-Prem, Azure) as follows\n* VDI / Jumpbox network → Snowflake VNet *(servicenow tickets)*\n\n* *how many jumpboxes* we need? *no. of users 8*\n* Note: Ports to be allowed: 443, 80, 1433\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Configure VDI / Jumbox images to pre-install tools listed below: - The list has been reviewed and approved by EUC Team.\"}],\"attrs\":{\"localId\":\"a6a01a2f-604c-4540-b9f2-173ad2e43e57\",\"state\":\"DONE\"}}],\"attrs\":{\"localId\":\"0c33acf9-c722-4f42-8518-6b140130d8da\"}}\n{adf}\n* *Git for Windows* + *Git Credential Manager (GCM)* (Azure DevOps/GitHub SSO)\n* *Azure CLI* & *Azure PowerShell*\n* *AzCopy* (for ADLS/BLOB moves within Azure)\n* *VS Code* (primary IDE)\n** Extensions: _Snowflake SQL Tools_ (Snowflake), _SQLTools_ (optional), _Python_, _Pylance_, _Jupyter_, _YAML_, _GitLens_\n* *Snowflake CLIs/SDKs*\n** *Snow CLI* (preferred over SnowSQL)\n** _(Optional)_ *SnowSQL* legacy client (only if you still need it)\n* *Drivers*\n** *Snowflake ODBC* and *JDBC* drivers (for BI tools, dbt, notebooks)\n* *Languages / Runtimes*\n** *Python 3.11+* (via *Miniconda* or *pyenv-win*; standardize on conda envs)\n** *Java 17* (LTS) for Snowpark Java/Scala\n** *Node.js 20 LTS* (if you use Streamlit/Node helpers)\n* *Snowpark & data tooling (per environment)*\n** {{pipx}} and/or {{conda}}\n** {{snowflake-snowpark-python}}, {{pandas}}, {{pyarrow}}, {{jupyterlab}}, {{ipykernel}}\n\n* *Power BI Desktop* _(or Power BI Desktop – Optimized for Fabric if that’s your org standard)_\n* *Tabular Editor*\n** TE2 (free) for basic edits or *TE3* (licensed) for advanced modeling/CI\n* *DAX Studio* (performance tuning)\n* *ALM Toolkit* (schema compare/deployment)\n* *Power BI Report Builder* (if you produce paginated reports)\n* *Azure Data Studio* (lightweight SQL + notebooks)\n* *SSMS* (for on-prem SQL Server admin)\n* *Azure Storage Explorer* (browsing ADLS/BLOB via private endpoints)\n* *On-prem / misc drivers*\n** Microsoft SQL Server ODBC, Oracle/ODBC (if used), PostgreSQL ODBC (if used)\n\n---\n\nAcceptance Criteria\n* The data engineers are able to access Snowflake and other relevant applications via Jumpbox\n\n---\n\nComments\nThe change request has been raised for DNS forwarding from on-premise DNS servers to the Azure Privatelink DNS.\n\n[CHG0052044|https://cwretail.service-now.com/nav_to.do?uri=sysapproval_approver.do?sys_id=97c987ff33c83e9047764f945d5c7bde]\n\nIf we implement this change, people will be locked out of snowflake and rely on VDI. We are awaiting Harrison’s and Alan’s architecture review as mentioned by Frank to proceed with this request.\n\ncc- @user @user @user\n\n@user to follow up - will create card", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 172}}
{"issue_key": "CSCI-487", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 9:58 AM", "updated": "03/Oct/25 9:55 AM", "labels": [], "summary": "MergeCo Conformed Reporting - Draft Dashboard Testing", "description": "Once dashboard requirements are set and confirmed by the business, need to create a dashboard wireframe.\n\nThe wireframe will outline how the dashboard will look, what functionality will be available (eg: what filters available and any other interactive features).\n\nWireframe will also include definition of metrics within the dashboard and any cadences around dashboard refresh, and email subscriptions.", "acceptance_criteria": "", "comments": "", "text": "Summary\nMergeCo Conformed Reporting - Draft Dashboard Testing\n\n---\n\nDescription\nOnce dashboard requirements are set and confirmed by the business, need to create a dashboard wireframe.\n\nThe wireframe will outline how the dashboard will look, what functionality will be available (eg: what filters available and any other interactive features).\n\nWireframe will also include definition of metrics within the dashboard and any cadences around dashboard refresh, and email subscriptions.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 173}}
{"issue_key": "CSCI-486", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 9:57 AM", "updated": "02/Oct/25 9:59 AM", "labels": [], "summary": "MergeCo Conformed Reporting - Draft Dashboard Development", "description": "Once dashboard requirements are set and confirmed by the business, need to create a dashboard wireframe.\n\nThe wireframe will outline how the dashboard will look, what functionality will be available (eg: what filters available and any other interactive features).\n\nWireframe will also include definition of metrics within the dashboard and any cadences around dashboard refresh, and email subscriptions.", "acceptance_criteria": "", "comments": "Initial doc for wireframe done", "text": "Summary\nMergeCo Conformed Reporting - Draft Dashboard Development\n\n---\n\nDescription\nOnce dashboard requirements are set and confirmed by the business, need to create a dashboard wireframe.\n\nThe wireframe will outline how the dashboard will look, what functionality will be available (eg: what filters available and any other interactive features).\n\nWireframe will also include definition of metrics within the dashboard and any cadences around dashboard refresh, and email subscriptions.\n\n---\n\nComments\nInitial doc for wireframe done", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 174}}
{"issue_key": "CSCI-485", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 9:35 AM", "updated": "10/Oct/25 5:06 PM", "labels": [], "summary": "Handover - PO's manual.", "description": "h3. Context\n\n* As @user is leaving D&A, he will be writing steps to ensure next person is well equipped with context/objective of what the role is about, what’s the steps to get there, and what are the deliverables.\n\nh3. Objective\n\n* To create a clear, empowering, and energising handover for the next Product owenr\n\nh3. Steps \n\n# Create coverage map for handover\n# Create induction checklist\n# Create instruction manual for\n## Scrum\n## Prioritisation\n## What a day/Week loooks like\n## What happens if someone’s away\n## \n\nh3. Deliverables\n\n* PO’s manual - confluence doc\n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Aligned with @user / @user \n* Doc accessible via confluence/equivalent\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nHandover - PO's manual.\n\n---\n\nDescription\nh3. Context\n\n* As @user is leaving D&A, he will be writing steps to ensure next person is well equipped with context/objective of what the role is about, what’s the steps to get there, and what are the deliverables.\n\nh3. Objective\n\n* To create a clear, empowering, and energising handover for the next Product owenr\n\nh3. Steps \n\n# Create coverage map for handover\n# Create induction checklist\n# Create instruction manual for\n## Scrum\n## Prioritisation\n## What a day/Week loooks like\n## What happens if someone’s away\n## \n\nh3. Deliverables\n\n* PO’s manual - confluence doc\n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Aligned with @user / @user \n* Doc accessible via confluence/equivalent\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 175}}
{"issue_key": "CSCI-484", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 9:26 AM", "updated": "19/Oct/25 7:57 PM", "labels": [], "summary": "Review Source-to-Target Map for Dim_Party, BridgeStorePartyRole", "description": "h3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Party table and BridgeStorePartyRole table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Party table and BridgeStorePartyRole table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.", "acceptance_criteria": "* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "comments": "review done", "text": "Summary\nReview Source-to-Target Map for Dim_Party, BridgeStorePartyRole\n\n---\n\nDescription\nh3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Party table and BridgeStorePartyRole table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Party table and BridgeStorePartyRole table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.\n\n---\n\nAcceptance Criteria\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)\n\n---\n\nComments\nreview done", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 176}}
{"issue_key": "CSCI-483", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 9:25 AM", "updated": "19/Oct/25 7:56 PM", "labels": [], "summary": "Review Source-to-Target Map for DimBuyer, BridgeProductBuyer", "description": "h3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Buyer table and BridgeProductBuyer table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Buyer table and BridgeProductBuyer table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.", "acceptance_criteria": "* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "comments": "review done", "text": "Summary\nReview Source-to-Target Map for DimBuyer, BridgeProductBuyer\n\n---\n\nDescription\nh3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Buyer table and BridgeProductBuyer table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Buyer table and BridgeProductBuyer table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\n---\n\nAcceptance Criteria\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)\n\n---\n\nComments\nreview done", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 177}}
{"issue_key": "CSCI-481", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 9:22 AM", "updated": "10/Oct/25 10:25 AM", "labels": [], "summary": "Review Source-to-Target Map for DimBatch", "description": "h3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Batch table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Batch table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.", "acceptance_criteria": "* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "comments": "", "text": "Summary\nReview Source-to-Target Map for DimBatch\n\n---\n\nDescription\nh3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Batch table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Batch table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.\n\n---\n\nAcceptance Criteria\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 178}}
{"issue_key": "CSCI-480", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 9:21 AM", "updated": "10/Oct/25 10:25 AM", "labels": [], "summary": "Review Source-to-Target Map for DimManufacturer, BridgeProductManufacturer", "description": "h3. Context\n\n* There is a need to document the source-to-target mapping for the [Dim_Manufacturer|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer] table and [BridgeProductManufacturer |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for [Dim_Manufacturer|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer] table and [BridgeProductManufacturer |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.", "acceptance_criteria": "h3. Acceptance criteria\n\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "comments": "The script had to be changed to include isManufacture Flag.", "text": "Summary\nReview Source-to-Target Map for DimManufacturer, BridgeProductManufacturer\n\n---\n\nDescription\nh3. Context\n\n* There is a need to document the source-to-target mapping for the [Dim_Manufacturer|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer] table and [BridgeProductManufacturer |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for [Dim_Manufacturer|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer] table and [BridgeProductManufacturer |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.\n\n---\n\nAcceptance Criteria\nh3. Acceptance criteria\n\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)\n\n---\n\nComments\nThe script had to be changed to include isManufacture Flag.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 179}}
{"issue_key": "CSCI-479", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 9:20 AM", "updated": "19/Oct/25 7:55 PM", "labels": [], "summary": "Review Source-to-Target Map for Fact_Warehouse_Location_Space_Utilisation", "description": "Source to target mapping for Fact_Warehouse_Location_Space_Utilisation - filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "review done", "text": "Summary\nReview Source-to-Target Map for Fact_Warehouse_Location_Space_Utilisation\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Space_Utilisation - filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nreview done", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 180}}
{"issue_key": "CSCI-478", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 9:20 AM", "updated": "19/Oct/25 7:55 PM", "labels": [], "summary": "Review Source-to-Target Map for Fact_Warehouse_Location_Cycle_Count", "description": "Source to target mapping for Fact_Warehouse_Location_Cycle_Count- filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "Review Done\n\nBut there is a duplicate Sheet called Fact_warehouse_cycle_count i think we can delete this one as this doesnt have enough details. I have coloured the tab red. once @user confirms i will delete it.", "text": "Summary\nReview Source-to-Target Map for Fact_Warehouse_Location_Cycle_Count\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Cycle_Count- filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nReview Done\n\nBut there is a duplicate Sheet called Fact_warehouse_cycle_count i think we can delete this one as this doesnt have enough details. I have coloured the tab red. once @user confirms i will delete it.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 181}}
{"issue_key": "CSCI-477", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 9:18 AM", "updated": "24/Oct/25 4:36 PM", "labels": [], "summary": "MergeCo - REtail tactical reporting", "description": "h3. Context\n\n* The mergeco report is a report that is used by Execs to track the health of the business. Vikesh would like a series of reports that will help with this so he can make the correct decisions on the business.\n\nh3. Objective\n\n* To help with the availability of data that is used for producing Mergeco reports\n* Tables to ingest\n\n|DIM_COUNTRY|\n|DIM_STORE -Done|\n|DIM_LIKE_FOR_LIKE -Done|\n|DATE - Done|\n|DIM_PRODUCT -Done|\n|DIM_STORE_NZL -Done|\n|DIM_PRODUCT_NZL - Done|\n|FCT_SALES|\n|FCT_CUSTOMER_COUNT_HOURLY_AUS - Done|\n|SubCategoryBudgetDaily -|\n|FCT_STOCK - Done|\n|FCT_STOCK_HISTORY|\n|FCT_SALES_NZL|\n|FCT_CUSTOMER_COUNT_HOURLY_NZL -Done|\n|FCT_STOCK_NZL -DOne|\n|FCT_STOCK_HISTORY_NZL|\n\n|| || || ||\n| | | |\n| | | |\n\nh3. Steps \n\n# Ingest Dim tables \n# Ingest Fact tables \n# Ingest Historical Data\n\nh3. Deliverables\n\n* \n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Create Extract Meta for delta & historical data \"}],\"attrs\":{\"localId\":\"50df6b4c-d7a0-435c-badb-b1db13ed0f02\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Create Pipelines \"}],\"attrs\":{\"localId\":\"0be7bc70-2c7e-46d0-b93b-f03e7ee404ba\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Load historical data for big tables\"}],\"attrs\":{\"localId\":\"8ccec245-dca7-4ae9-95a5-0d86a402aed5\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"3af7df84-c13f-4a98-9ae1-d189349ee0a2\"}}\n{adf}\n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Data available for Mergeco reporting\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Dim Ingestion completed\n\nHistorical Load in progress. It got hampered due to Space is column names! The is now identified and rectified.\n\nData loaded. Pending for SubCategoryBudgetDaily", "text": "Summary\nMergeCo - REtail tactical reporting\n\n---\n\nDescription\nh3. Context\n\n* The mergeco report is a report that is used by Execs to track the health of the business. Vikesh would like a series of reports that will help with this so he can make the correct decisions on the business.\n\nh3. Objective\n\n* To help with the availability of data that is used for producing Mergeco reports\n* Tables to ingest\n\n|DIM_COUNTRY|\n|DIM_STORE -Done|\n|DIM_LIKE_FOR_LIKE -Done|\n|DATE - Done|\n|DIM_PRODUCT -Done|\n|DIM_STORE_NZL -Done|\n|DIM_PRODUCT_NZL - Done|\n|FCT_SALES|\n|FCT_CUSTOMER_COUNT_HOURLY_AUS - Done|\n|SubCategoryBudgetDaily -|\n|FCT_STOCK - Done|\n|FCT_STOCK_HISTORY|\n|FCT_SALES_NZL|\n|FCT_CUSTOMER_COUNT_HOURLY_NZL -Done|\n|FCT_STOCK_NZL -DOne|\n|FCT_STOCK_HISTORY_NZL|\n\n|| || || ||\n| | | |\n| | | |\n\nh3. Steps \n\n# Ingest Dim tables \n# Ingest Fact tables \n# Ingest Historical Data\n\nh3. Deliverables\n\n* \n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Create Extract Meta for delta & historical data \"}],\"attrs\":{\"localId\":\"50df6b4c-d7a0-435c-badb-b1db13ed0f02\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Create Pipelines \"}],\"attrs\":{\"localId\":\"0be7bc70-2c7e-46d0-b93b-f03e7ee404ba\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Load historical data for big tables\"}],\"attrs\":{\"localId\":\"8ccec245-dca7-4ae9-95a5-0d86a402aed5\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"3af7df84-c13f-4a98-9ae1-d189349ee0a2\"}}\n{adf}\n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Data available for Mergeco reporting\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nDim Ingestion completed\n\nHistorical Load in progress. It got hampered due to Space is column names! The is now identified and rectified.\n\nData loaded. Pending for SubCategoryBudgetDaily", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 182}}
{"issue_key": "CSCI-476", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 9:18 AM", "updated": "14/Oct/25 1:30 PM", "labels": [], "summary": "MergeCo - Supply chain tactical report", "description": "h3. Context\n\n* The mergeco report is a report that is used by Execs to track the health of the business. Vikesh would like a series of reports that will help with this so he can make the correct decisions on the business.\n\nh3. Objective\n\n* To help with the availability of data that is used for producing Mergeco reports\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Data available for Mergeco reporting\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Importing the template pipeline to ADF and test the import.\n\nHey @user ,\n\nI believe this is duplicate of work @user has already completed, can we make sure we validate that and if so then not spend effort on this.\n\nI believe duplicate of [https://sigmahealthcare.atlassian.net/browse/CSCI-511|https://sigmahealthcare.atlassian.net/browse/CSCI-511] ?\n\ncc @user\n\n@user you are right. @user Please close /remove this.", "text": "Summary\nMergeCo - Supply chain tactical report\n\n---\n\nDescription\nh3. Context\n\n* The mergeco report is a report that is used by Execs to track the health of the business. Vikesh would like a series of reports that will help with this so he can make the correct decisions on the business.\n\nh3. Objective\n\n* To help with the availability of data that is used for producing Mergeco reports\n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Data available for Mergeco reporting\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nImporting the template pipeline to ADF and test the import.\n\nHey @user ,\n\nI believe this is duplicate of work @user has already completed, can we make sure we validate that and if so then not spend effort on this.\n\nI believe duplicate of [https://sigmahealthcare.atlassian.net/browse/CSCI-511|https://sigmahealthcare.atlassian.net/browse/CSCI-511] ?\n\ncc @user\n\n@user you are right. @user Please close /remove this.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 183}}
{"issue_key": "CSCI-475", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 8:43 AM", "updated": "09/Oct/25 9:02 AM", "labels": [], "summary": "AA - Creating Detailed documentation - Detailed Documentation of work - Sprint 9", "description": "h3. Context\n\n* Comprehensive documentation is required to centralise and standardise knowledge on data engineering assets, including ADF pipelines, Snowflake extract metadata, and related sources (e.g., SCAX2012). This will support onboarding, troubleshooting, and ongoing enhancements.\n\nh3. Objective\n\n* Summarise and document the architecture, processes, and metadata for ADF pipelines, Snowflake extract mechanisms, and SCAX2012, consolidating all sources and references in Confluence for easy access and governance.\n\nh3. Steps\n\n# Identify and catalogue all relevant ADF pipelines, Snowflake extract processes, and SCAX2012 assets.\n# Gather existing documentation, code, and metadata from source systems and repositories.\n# Summarise technical details, dependencies, and integration points for each asset.\n# Document best practices, naming conventions, and standards (referencing existing Snowflake and ADF standards).\n# Create or update Confluence pages with structured summaries, diagrams, and links to source artifacts.\n# Review documentation with stakeholders for completeness and accuracy.\n# Publish and communicate the documentation to the engineering and analytics teams.\n\nh3. Deliverables\n\n* Confluence summary pages for ADF pipelines, Snowflake extract metadata, and SCAX2012.\n* Linked references to source code, metadata tables, and configuration files.\n* Diagrams illustrating architecture and data flows.\n* Documented best practices and standards for each component.\n\nh3. Assumptions (Optional)\n\n* All relevant source systems and repositories are accessible.\n* Existing documentation is available and up to date for reference.\n* Stakeholders are available for review and feedback.", "acceptance_criteria": "* All ADF pipelines, Snowflake extract processes, and SCAX2012 assets are documented in Confluence.\n* Documentation includes architecture diagrams, metadata summaries, and integration points.\n* Best practices and naming conventions are clearly outlined.\n* Stakeholder review is completed and feedback incorporated.\n* Documentation is published and communicated to all relevant teams.", "comments": "Document framework is ready and now working on details.\n\nNeed help from DBA team to complete doc\n\n@user \n@user if this is TBD next week with @user et al, i’ll split this task for sprint 10 and then mark this task as done?\nLMK once you guys have had this discussion.\n\nHey @user ,\n\nCan we ensure document is in centralised location? Is it confluence or sharepoint?\n\nThanks,\n\nHarrison\n\n@user attaching it and it is at in the MergeCo data team’s architecture folder. I will send it for @user review now. It is still an evolving document. [CWR - EDP Operational Details v0.1.docx|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BB4A4EEE0-67BB-4D2F-A05A-EE4929DAA15B%7D&file=CWR%20-%20EDP%20Operational%20Details%20v0.1.docx&action=default&mobileredirect=true]", "text": "Summary\nAA - Creating Detailed documentation - Detailed Documentation of work - Sprint 9\n\n---\n\nDescription\nh3. Context\n\n* Comprehensive documentation is required to centralise and standardise knowledge on data engineering assets, including ADF pipelines, Snowflake extract metadata, and related sources (e.g., SCAX2012). This will support onboarding, troubleshooting, and ongoing enhancements.\n\nh3. Objective\n\n* Summarise and document the architecture, processes, and metadata for ADF pipelines, Snowflake extract mechanisms, and SCAX2012, consolidating all sources and references in Confluence for easy access and governance.\n\nh3. Steps\n\n# Identify and catalogue all relevant ADF pipelines, Snowflake extract processes, and SCAX2012 assets.\n# Gather existing documentation, code, and metadata from source systems and repositories.\n# Summarise technical details, dependencies, and integration points for each asset.\n# Document best practices, naming conventions, and standards (referencing existing Snowflake and ADF standards).\n# Create or update Confluence pages with structured summaries, diagrams, and links to source artifacts.\n# Review documentation with stakeholders for completeness and accuracy.\n# Publish and communicate the documentation to the engineering and analytics teams.\n\nh3. Deliverables\n\n* Confluence summary pages for ADF pipelines, Snowflake extract metadata, and SCAX2012.\n* Linked references to source code, metadata tables, and configuration files.\n* Diagrams illustrating architecture and data flows.\n* Documented best practices and standards for each component.\n\nh3. Assumptions (Optional)\n\n* All relevant source systems and repositories are accessible.\n* Existing documentation is available and up to date for reference.\n* Stakeholders are available for review and feedback.\n\n---\n\nAcceptance Criteria\n* All ADF pipelines, Snowflake extract processes, and SCAX2012 assets are documented in Confluence.\n* Documentation includes architecture diagrams, metadata summaries, and integration points.\n* Best practices and naming conventions are clearly outlined.\n* Stakeholder review is completed and feedback incorporated.\n* Documentation is published and communicated to all relevant teams.\n\n---\n\nComments\nDocument framework is ready and now working on details.\n\nNeed help from DBA team to complete doc\n\n@user \n@user if this is TBD next week with @user et al, i’ll split this task for sprint 10 and then mark this task as done?\nLMK once you guys have had this discussion.\n\nHey @user ,\n\nCan we ensure document is in centralised location? Is it confluence or sharepoint?\n\nThanks,\n\nHarrison\n\n@user attaching it and it is at in the MergeCo data team’s architecture folder. I will send it for @user review now. It is still an evolving document. [CWR - EDP Operational Details v0.1.docx|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BB4A4EEE0-67BB-4D2F-A05A-EE4929DAA15B%7D&file=CWR%20-%20EDP%20Operational%20Details%20v0.1.docx&action=default&mobileredirect=true]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 184}}
{"issue_key": "CSCI-472", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 8:41 AM", "updated": "09/Oct/25 9:39 AM", "labels": [], "summary": "Remaining ingestion - (APPRISS, MergeCo Retail)", "description": "h3. Context\n\n* The ingestion of data from APPRISS and MergeCo Retail is pending completion. These integrations are critical to ensure comprehensive data coverage and enable downstream analytics and reporting.\n\nh3. Objective\n\n* Complete the end-to-end ingestion processes for APPRISS and MergeCo Retail, ensuring data is reliably extracted, transformed, loaded, and validated within the enterprise data platform.\n\nh3. Steps\n\n# Confirm data source access and connectivity for APPRISS and MergeCo Retail.\n# Gather and review source data structures, formats, and update frequencies.\n# Design and document ingestion pipelines, including extraction, transformation, and loading logic.\n# Implement pipelines using standard naming conventions and best practices *1*.\n# Populate and maintain extract/load metadata tables for orchestration and monitoring.\n# Conduct system, integration, and performance testing of ingestion processes *2* *3*.\n# Validate ingested data with business stakeholders and resolve discrepancies.\n# Finalise documentation and update Confluence with process details and data dictionaries.\n# Transition pipelines to production and monitor initial loads.\n\nh3. Deliverables\n\n* Fully operational ingestion pipelines for APPRISS and MergeCo Retail.\n* Updated extract/load metadata and monitoring dashboards.\n* Technical documentation and data dictionaries in Confluence.\n* Test results and sign-off from stakeholders.\n\nh3. Assumptions (Optional)\n\n* Source systems for APPRISS and MergeCo Retail are accessible and stable.\n* Required credentials and permissions are available.\n* Business stakeholders are available for validation and sign-off.", "acceptance_criteria": "* Data from APPRISS and MergeCo Retail is ingested and available in the enterprise data platform.\n* Ingestion pipelines follow established naming conventions and best practices.\n* Metadata and monitoring are in place for all new ingestion processes.\n* Successful completion of system, integration, and performance testing.\n* Documentation is complete and published in Confluence.\n* Stakeholder validation and sign-off are obtained.", "comments": "cc @user", "text": "Summary\nRemaining ingestion - (APPRISS, MergeCo Retail)\n\n---\n\nDescription\nh3. Context\n\n* The ingestion of data from APPRISS and MergeCo Retail is pending completion. These integrations are critical to ensure comprehensive data coverage and enable downstream analytics and reporting.\n\nh3. Objective\n\n* Complete the end-to-end ingestion processes for APPRISS and MergeCo Retail, ensuring data is reliably extracted, transformed, loaded, and validated within the enterprise data platform.\n\nh3. Steps\n\n# Confirm data source access and connectivity for APPRISS and MergeCo Retail.\n# Gather and review source data structures, formats, and update frequencies.\n# Design and document ingestion pipelines, including extraction, transformation, and loading logic.\n# Implement pipelines using standard naming conventions and best practices *1*.\n# Populate and maintain extract/load metadata tables for orchestration and monitoring.\n# Conduct system, integration, and performance testing of ingestion processes *2* *3*.\n# Validate ingested data with business stakeholders and resolve discrepancies.\n# Finalise documentation and update Confluence with process details and data dictionaries.\n# Transition pipelines to production and monitor initial loads.\n\nh3. Deliverables\n\n* Fully operational ingestion pipelines for APPRISS and MergeCo Retail.\n* Updated extract/load metadata and monitoring dashboards.\n* Technical documentation and data dictionaries in Confluence.\n* Test results and sign-off from stakeholders.\n\nh3. Assumptions (Optional)\n\n* Source systems for APPRISS and MergeCo Retail are accessible and stable.\n* Required credentials and permissions are available.\n* Business stakeholders are available for validation and sign-off.\n\n---\n\nAcceptance Criteria\n* Data from APPRISS and MergeCo Retail is ingested and available in the enterprise data platform.\n* Ingestion pipelines follow established naming conventions and best practices.\n* Metadata and monitoring are in place for all new ingestion processes.\n* Successful completion of system, integration, and performance testing.\n* Documentation is complete and published in Confluence.\n* Stakeholder validation and sign-off are obtained.\n\n---\n\nComments\ncc @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 185}}
{"issue_key": "CSCI-471", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 8:33 AM", "updated": "10/Oct/25 10:31 AM", "labels": [], "summary": "AP - Creating Detailed documentation - Detailed Documentation of work", "description": "h3. Context\n\n* Comprehensive documentation is required to centralise and standardise knowledge on data engineering assets, including ADF pipelines, Snowflake extract metadata, and related sources (e.g., SCAX2012). This will support onboarding, troubleshooting, and ongoing enhancements.\n\nh3. Objective\n\n* Summarise and document the architecture, processes, and metadata for ADF pipelines, Snowflake extract mechanisms, and SCAX2012, consolidating all sources and references in Confluence for easy access and governance.\n\nh3. Steps\n\n# Identify and catalogue all relevant ADF pipelines, Snowflake extract processes, and SCAX2012 assets.\n# Gather existing documentation, code, and metadata from source systems and repositories.\n# Summarise technical details, dependencies, and integration points for each asset.\n# Document best practices, naming conventions, and standards (referencing existing Snowflake and ADF standards).\n# Create or update Confluence pages with structured summaries, diagrams, and links to source artifacts.\n# Review documentation with stakeholders for completeness and accuracy.\n# Publish and communicate the documentation to the engineering and analytics teams.\n\nh3. Deliverables\n\n* Confluence summary pages for ADF pipelines, Snowflake extract metadata, and SCAX2012.\n* Linked references to source code, metadata tables, and configuration files.\n* Diagrams illustrating architecture and data flows.\n* Documented best practices and standards for each component.\n\nh3. Assumptions (Optional)\n\n* All relevant source systems and repositories are accessible.\n* Existing documentation is available and up to date for reference.\n* Stakeholders are available for review and feedback.", "acceptance_criteria": "* All ADF pipelines, Snowflake extract processes, and SCAX2012 assets are documented in Confluence.\n* Documentation includes architecture diagrams, metadata summaries, and integration points.\n* Best practices and naming conventions are clearly outlined.\n* Stakeholder review is completed and feedback incorporated.\n* Documentation is published and communicated to all relevant teams.", "comments": "", "text": "Summary\nAP - Creating Detailed documentation - Detailed Documentation of work\n\n---\n\nDescription\nh3. Context\n\n* Comprehensive documentation is required to centralise and standardise knowledge on data engineering assets, including ADF pipelines, Snowflake extract metadata, and related sources (e.g., SCAX2012). This will support onboarding, troubleshooting, and ongoing enhancements.\n\nh3. Objective\n\n* Summarise and document the architecture, processes, and metadata for ADF pipelines, Snowflake extract mechanisms, and SCAX2012, consolidating all sources and references in Confluence for easy access and governance.\n\nh3. Steps\n\n# Identify and catalogue all relevant ADF pipelines, Snowflake extract processes, and SCAX2012 assets.\n# Gather existing documentation, code, and metadata from source systems and repositories.\n# Summarise technical details, dependencies, and integration points for each asset.\n# Document best practices, naming conventions, and standards (referencing existing Snowflake and ADF standards).\n# Create or update Confluence pages with structured summaries, diagrams, and links to source artifacts.\n# Review documentation with stakeholders for completeness and accuracy.\n# Publish and communicate the documentation to the engineering and analytics teams.\n\nh3. Deliverables\n\n* Confluence summary pages for ADF pipelines, Snowflake extract metadata, and SCAX2012.\n* Linked references to source code, metadata tables, and configuration files.\n* Diagrams illustrating architecture and data flows.\n* Documented best practices and standards for each component.\n\nh3. Assumptions (Optional)\n\n* All relevant source systems and repositories are accessible.\n* Existing documentation is available and up to date for reference.\n* Stakeholders are available for review and feedback.\n\n---\n\nAcceptance Criteria\n* All ADF pipelines, Snowflake extract processes, and SCAX2012 assets are documented in Confluence.\n* Documentation includes architecture diagrams, metadata summaries, and integration points.\n* Best practices and naming conventions are clearly outlined.\n* Stakeholder review is completed and feedback incorporated.\n* Documentation is published and communicated to all relevant teams.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 186}}
{"issue_key": "CSCI-470", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Sep/25 8:19 AM", "updated": "10/Oct/25 10:31 AM", "labels": [], "summary": "AP - Creating Documentation (Summary and scaffolding)", "description": "h3. Context\n\n* Comprehensive documentation is required to centralise and standardise knowledge on data engineering assets, including ADF pipelines, Snowflake extract metadata, and related sources (e.g., SCAX2012). This will support onboarding, troubleshooting, and ongoing enhancements.\n\nh3. Objective\n\n* Summarise and document the architecture, processes, and metadata for ADF pipelines, Snowflake extract mechanisms, and SCAX2012, consolidating all sources and references in Confluence for easy access and governance.\n\nh3. Steps\n\n# Identify and catalogue all relevant ADF pipelines, Snowflake extract processes, and SCAX2012 assets.\n# Gather existing documentation, code, and metadata from source systems and repositories.\n# Summarise technical details, dependencies, and integration points for each asset.\n# Document best practices, naming conventions, and standards (referencing existing Snowflake and ADF standards).\n# Create or update Confluence pages with structured summaries, diagrams, and links to source artifacts.\n# Review documentation with stakeholders for completeness and accuracy.\n# Publish and communicate the documentation to the engineering and analytics teams.\n\nh3. Deliverables\n\n* Confluence summary pages for ADF pipelines, Snowflake extract metadata, and SCAX2012.\n* Linked references to source code, metadata tables, and configuration files.\n* Diagrams illustrating architecture and data flows.\n* Documented best practices and standards for each component.\n\nh3. Assumptions (Optional)\n\n* All relevant source systems and repositories are accessible.\n* Existing documentation is available and up to date for reference.\n* Stakeholders are available for review and feedback.\n\nh3. Acceptance criteria\n\n* [ ] All ADF pipelines, Snowflake extract processes, and SCAX2012 assets are documented in Confluence.\n* [ ] Documentation includes architecture diagrams, metadata summaries, and integration points.\n* [ ] Best practices and naming conventions are clearly outlined.\n* [ ] Stakeholder review is completed and feedback incorporated.\n* [ ] Documentation is published and communicated to all relevant teams.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nAP - Creating Documentation (Summary and scaffolding)\n\n---\n\nDescription\nh3. Context\n\n* Comprehensive documentation is required to centralise and standardise knowledge on data engineering assets, including ADF pipelines, Snowflake extract metadata, and related sources (e.g., SCAX2012). This will support onboarding, troubleshooting, and ongoing enhancements.\n\nh3. Objective\n\n* Summarise and document the architecture, processes, and metadata for ADF pipelines, Snowflake extract mechanisms, and SCAX2012, consolidating all sources and references in Confluence for easy access and governance.\n\nh3. Steps\n\n# Identify and catalogue all relevant ADF pipelines, Snowflake extract processes, and SCAX2012 assets.\n# Gather existing documentation, code, and metadata from source systems and repositories.\n# Summarise technical details, dependencies, and integration points for each asset.\n# Document best practices, naming conventions, and standards (referencing existing Snowflake and ADF standards).\n# Create or update Confluence pages with structured summaries, diagrams, and links to source artifacts.\n# Review documentation with stakeholders for completeness and accuracy.\n# Publish and communicate the documentation to the engineering and analytics teams.\n\nh3. Deliverables\n\n* Confluence summary pages for ADF pipelines, Snowflake extract metadata, and SCAX2012.\n* Linked references to source code, metadata tables, and configuration files.\n* Diagrams illustrating architecture and data flows.\n* Documented best practices and standards for each component.\n\nh3. Assumptions (Optional)\n\n* All relevant source systems and repositories are accessible.\n* Existing documentation is available and up to date for reference.\n* Stakeholders are available for review and feedback.\n\nh3. Acceptance criteria\n\n* [ ] All ADF pipelines, Snowflake extract processes, and SCAX2012 assets are documented in Confluence.\n* [ ] Documentation includes architecture diagrams, metadata summaries, and integration points.\n* [ ] Best practices and naming conventions are clearly outlined.\n* [ ] Stakeholder review is completed and feedback incorporated.\n* [ ] Documentation is published and communicated to all relevant teams.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 187}}
{"issue_key": "CSCI-468", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "24/Sep/25 11:27 AM", "updated": "10/Oct/25 5:09 PM", "labels": [], "summary": "Review Source-to-Target Map for DimSupplier, BridgeProductSupplier", "description": "h3. Context\n\n* There is a need to document the source-to-target mapping for the [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table , with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.", "acceptance_criteria": "h3. \n\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "comments": "To resume in sprint 10 if needed", "text": "Summary\nReview Source-to-Target Map for DimSupplier, BridgeProductSupplier\n\n---\n\nDescription\nh3. Context\n\n* There is a need to document the source-to-target mapping for the [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table , with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.\n\n---\n\nAcceptance Criteria\nh3. \n\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)\n\n---\n\nComments\nTo resume in sprint 10 if needed", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 188}}
{"issue_key": "CSCI-467", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "24/Sep/25 10:28 AM", "updated": "25/Sep/25 3:47 PM", "labels": [], "summary": "Data Model Review", "description": "Fact tables review:\n\nInitial review for the fact tables built by Amit. These facts are built in line with the strategic solution but at this point will be used for the APPRISS delivery. \n\n* -Validated all the underlying objects are ingested.-\n* -Confirm the joins-\n* -Confirm the calculations-\n\nBelow are the fact tables:\n\n* FCT_Sales_Retail\n* FCT_Sales_Retail_Electronic_Pay\n* FCT_Transactions_Audit_History\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Review complete. We scheduled a call for today with Amit.\n\nWe identified a missing table from the ingestion table list", "text": "Summary\nData Model Review\n\n---\n\nDescription\nFact tables review:\n\nInitial review for the fact tables built by Amit. These facts are built in line with the strategic solution but at this point will be used for the APPRISS delivery. \n\n* -Validated all the underlying objects are ingested.-\n* -Confirm the joins-\n* -Confirm the calculations-\n\nBelow are the fact tables:\n\n* FCT_Sales_Retail\n* FCT_Sales_Retail_Electronic_Pay\n* FCT_Transactions_Audit_History\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nReview complete. We scheduled a call for today with Amit.\n\nWe identified a missing table from the ingestion table list", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 189}}
{"issue_key": "CSCI-466", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "24/Sep/25 10:21 AM", "updated": "25/Sep/25 2:09 PM", "labels": [], "summary": "Review catalogue ingestion data for Retail", "description": "h3. Context\n\n* The project aims to ensure that the correct and complete set of retail catalogue data is ingested into Snowflake to support downstream retail data modelling, analytics, and reporting. This includes aligning source data structures, applying data quality checks, and following Snowflake development standards for consistency and performance.\n\nh3. Objective\n\n* To validate and document the ingestion process for retail catalogue data into Snowflake, ensuring data completeness, accuracy, and readiness for use in retail data models and Power BI reporting.\n\nh3. Steps\n\n* TBC @user \n\nh3. Deliverables\n\n* TBC \n\nh3. Assumptions - (Optional)\n\n* All required source systems are accessible and up-to-date.\n* Existing Snowflake environments follow established development and security standards.\n* Stakeholders are available for clarification and validation.\n\nh3.", "acceptance_criteria": "* [ ] All relevant catalogue data sources are identified and documented.\n* [ ] Data ingestion pipelines are reviewed and validated for completeness and accuracy.\n* [ ] Data mapping and transformation logic is documented and aligns with Snowflake standards.\n* [ ] Data quality checks are performed and results are satisfactory.\n* [ ] Any issues or gaps are documented with recommendations.\n* [ ] Stakeholder sign-off confirming data readiness for retail modelling and reporting.", "comments": "", "text": "Summary\nReview catalogue ingestion data for Retail\n\n---\n\nDescription\nh3. Context\n\n* The project aims to ensure that the correct and complete set of retail catalogue data is ingested into Snowflake to support downstream retail data modelling, analytics, and reporting. This includes aligning source data structures, applying data quality checks, and following Snowflake development standards for consistency and performance.\n\nh3. Objective\n\n* To validate and document the ingestion process for retail catalogue data into Snowflake, ensuring data completeness, accuracy, and readiness for use in retail data models and Power BI reporting.\n\nh3. Steps\n\n* TBC @user \n\nh3. Deliverables\n\n* TBC \n\nh3. Assumptions - (Optional)\n\n* All required source systems are accessible and up-to-date.\n* Existing Snowflake environments follow established development and security standards.\n* Stakeholders are available for clarification and validation.\n\nh3.\n\n---\n\nAcceptance Criteria\n* [ ] All relevant catalogue data sources are identified and documented.\n* [ ] Data ingestion pipelines are reviewed and validated for completeness and accuracy.\n* [ ] Data mapping and transformation logic is documented and aligns with Snowflake standards.\n* [ ] Data quality checks are performed and results are satisfactory.\n* [ ] Any issues or gaps are documented with recommendations.\n* [ ] Stakeholder sign-off confirming data readiness for retail modelling and reporting.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 190}}
{"issue_key": "CSCI-465", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "24/Sep/25 9:23 AM", "updated": "10/Oct/25 9:55 AM", "labels": [], "summary": "APPRISS Documentation : Progress so far & going forward", "description": "h2. Summary\n\nCome up with a documentation to track progress on APPRISS and approach going forward.\n\nh2. Context\n\nMost tables required for APPRISS have already been ingested. This issue focuses on ensuring that we are capturing all what we have done so far APPRISS ( like sharing of sample data share from on prem, ingesting all necessary tables in snowflake EDP) and how are we going to move forward. \n\nh2. Other information\n\nN/A\n\nDatabases covered:\n\n* transactionstorage\n* stockdb\n* general_reference", "acceptance_criteria": "We need to create documentation for APPRISS\n\n* -Added Information on sample data share-\n* -Queries used to create sample data share added-\n* Other documentation links available are provided\n* Current Progress of data Ingestion into EDP\n* Progress on any additional table ingestion required\n* Future data share solution from snowflake (through Query/tables/views etc)", "comments": "cc @user\n\nHi All, \n\nThis is link to the currently in progress Confluence documentation for APPRISS:\n\n[APPRISS Documentation: Progress so far & going forward - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1654292481/APPRISS+Documentation+Progress+so+far+going+forward]\n\nFYI.. @user , @user , @user , @user\n\nHi All,\n\nI have added most of the details in the confluence. Will try to add any further links/recordings of meetings we had in past as well to mark this complete.\n\n@user - is this something you can validate whether we have capture what’s needed here? -[+APPRISS Documentation: Progress so far & going forward - Enterprise Data Platform - Confluence+|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1654292481/APPRISS+Documentation+Progress+so+far+going+forward]\n\nFYI @user \n\nThanks\n\nHan", "text": "Summary\nAPPRISS Documentation : Progress so far & going forward\n\n---\n\nDescription\nh2. Summary\n\nCome up with a documentation to track progress on APPRISS and approach going forward.\n\nh2. Context\n\nMost tables required for APPRISS have already been ingested. This issue focuses on ensuring that we are capturing all what we have done so far APPRISS ( like sharing of sample data share from on prem, ingesting all necessary tables in snowflake EDP) and how are we going to move forward. \n\nh2. Other information\n\nN/A\n\nDatabases covered:\n\n* transactionstorage\n* stockdb\n* general_reference\n\n---\n\nAcceptance Criteria\nWe need to create documentation for APPRISS\n\n* -Added Information on sample data share-\n* -Queries used to create sample data share added-\n* Other documentation links available are provided\n* Current Progress of data Ingestion into EDP\n* Progress on any additional table ingestion required\n* Future data share solution from snowflake (through Query/tables/views etc)\n\n---\n\nComments\ncc @user\n\nHi All, \n\nThis is link to the currently in progress Confluence documentation for APPRISS:\n\n[APPRISS Documentation: Progress so far & going forward - Enterprise Data Platform - Confluence|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1654292481/APPRISS+Documentation+Progress+so+far+going+forward]\n\nFYI.. @user , @user , @user , @user\n\nHi All,\n\nI have added most of the details in the confluence. Will try to add any further links/recordings of meetings we had in past as well to mark this complete.\n\n@user - is this something you can validate whether we have capture what’s needed here? -[+APPRISS Documentation: Progress so far & going forward - Enterprise Data Platform - Confluence+|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1654292481/APPRISS+Documentation+Progress+so+far+going+forward]\n\nFYI @user \n\nThanks\n\nHan", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 191}}
{"issue_key": "CSCI-464", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Sep/25 4:21 PM", "updated": "10/Oct/25 10:12 AM", "labels": [], "summary": "EDP - Detailed Work Breakdown Plan", "description": "h3. Context\n\n* Based on the EDP Solution Design Doc [https://sigmahealthcare.atlassian.net/browse/CSCI-415|https://sigmahealthcare.atlassian.net/browse/CSCI-415] , Chloe to work out the Work Breakdown Plan with Alan and Harrison\n\nh3. Objective\n\n* To produce an work breakdown plan that can be executed with coordination\n\nh3. Steps \n\n# Write work breakdown plan document\n\nh3. Deliverables\n\n* Work breakdown plan in confluence\n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Reviewed by Alan\n* Available for access in confluence\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "Working with Alan and Mike on the Detailed Work Breakdown Plan. \n\n* Interim Solution - Done - Pending Review\n* Interim Solution (with dbt)\n* Future State Solution\n\nReviewed and responded to Alan’s architecture doco on Confluence [https://sigmahealthcare.atlassian.net/wiki/x/BYBAYw|https://sigmahealthcare.atlassian.net/wiki/x/BYBAYw]", "text": "Summary\nEDP - Detailed Work Breakdown Plan\n\n---\n\nDescription\nh3. Context\n\n* Based on the EDP Solution Design Doc [https://sigmahealthcare.atlassian.net/browse/CSCI-415|https://sigmahealthcare.atlassian.net/browse/CSCI-415] , Chloe to work out the Work Breakdown Plan with Alan and Harrison\n\nh3. Objective\n\n* To produce an work breakdown plan that can be executed with coordination\n\nh3. Steps \n\n# Write work breakdown plan document\n\nh3. Deliverables\n\n* Work breakdown plan in confluence\n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* Reviewed by Alan\n* Available for access in confluence\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nWorking with Alan and Mike on the Detailed Work Breakdown Plan. \n\n* Interim Solution - Done - Pending Review\n* Interim Solution (with dbt)\n* Future State Solution\n\nReviewed and responded to Alan’s architecture doco on Confluence [https://sigmahealthcare.atlassian.net/wiki/x/BYBAYw|https://sigmahealthcare.atlassian.net/wiki/x/BYBAYw]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 192}}
{"issue_key": "CSCI-463", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Sep/25 4:00 PM", "updated": "09/Oct/25 9:39 AM", "labels": [], "summary": "contributing to Detail design documentation", "description": "h3. Context\n\n* to help @user with detail design doc\n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\ncontributing to Detail design documentation\n\n---\n\nDescription\nh3. Context\n\n* to help @user with detail design doc\n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 193}}
{"issue_key": "CSCI-462", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Sep/25 3:40 PM", "updated": "03/Oct/25 9:49 AM", "labels": [], "summary": "Enable SSO login to snowflake - SCIM", "description": "Consdering the options between \n\n* MS Entra ID\n* Okta\n\nCK to have a follow-up call with Brent from Cloud to firm up the plan for SSO implementation\n\n* -book next tuesday to follow up-\n\nMeeting completed on Tues:\n\nImplementation of Microsoft Entra ID SSO for Snowflake access across the entire data platform, enabling unified authentication for all user personas and tools including PowerBI, Tabular Editor 3, Azure Data Factory, DBT, and direct Snowflake access.", "acceptance_criteria": "* --booking made for tuesday meeting--\n* --To follow-up from the results of the call on Tuesday--\n* --Decision made between MS Entra ID vs Okta- ✅ *MS Entra ID Selected*-\n\n* -Meeting with Brent from Cloud team scheduled- ✅ *Completed*\n* *Entra ID SSO configured and functional for Snowflake*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Work with Raveendran from our cloud team \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84\",\"accessLevel\":\"\",\"localId\":\"5a778d84-3fcf-46a6-8eed-a75d321d3ac5\"}},{\"type\":\"text\",\"text\":\" and \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"712020:a3bb398f-73df-472d-a226-f67ed86b49ee\",\"accessLevel\":\"\",\"localId\":\"542bb25d-3701-49bc-be49-7b5f9c0a5bea\"}},{\"type\":\"text\",\"text\":\" \"}],\"attrs\":{\"localId\":\"0a6a01ee-0890-4b42-99a3-17e68a53d8bb\",\"state\":\"DONE\"}}],\"attrs\":{\"localId\":\"340fd36c-fd9c-494d-93b5-65b83210118a\"}}\n{adf}\n* *Service Principal OAuth setup for PowerBI Service scheduled refresh*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Work with Raveendran from our cloud team \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84\",\"accessLevel\":\"\",\"localId\":\"29da3589-921e-4bd8-9020-7c2027ba319a\"}},{\"type\":\"text\",\"text\":\" and \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"712020:a3bb398f-73df-472d-a226-f67ed86b49ee\",\"accessLevel\":\"\",\"localId\":\"7cfbbe27-380e-4949-a2c5-ab2e425ff05a\"}},{\"type\":\"text\",\"text\":\" \"}],\"attrs\":{\"localId\":\"7d2f681f-168c-4a83-8bc1-c56634d2d967\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"340fd36c-fd9c-494d-93b5-65b83210118a\"}}\n{adf}\n* *Network security implemented (VNet Gateway)*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\" \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84\",\"accessLevel\":\"\",\"localId\":\"4d11860e-95ff-49b1-bcd3-89eb13c3af94\"}},{\"type\":\"text\",\"text\":\" - as part of the SSO can we also collaborate on this\"}],\"attrs\":{\"localId\":\"736cca4b-dae9-4882-9b4e-de06b4790373\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"819123db-3a96-41ca-8543-00cb537f3b64\"}}\n{adf}\n* *All user personas can authenticate successfully*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Testing post entra ID sso dependencies completed\"}],\"attrs\":{\"localId\":\"c9b6f8b8-7224-4cd6-9ba4-1b8c34b4d47d\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"db194595-b904-41c9-8026-19c645d2aaac\"}}\n{adf}\n* *Conditional access policies and MFA enforced*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Testing post entra ID sso dependencies completed\"}],\"attrs\":{\"localId\":\"b541fd8a-1751-4451-945b-4cd78a6a3069\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"db194595-b904-41c9-8026-19c645d2aaac\"}}\n{adf}\n* *Production rollout completed*", "comments": "@user Will be working with @user tomorrow", "text": "Summary\nEnable SSO login to snowflake - SCIM\n\n---\n\nDescription\nConsdering the options between \n\n* MS Entra ID\n* Okta\n\nCK to have a follow-up call with Brent from Cloud to firm up the plan for SSO implementation\n\n* -book next tuesday to follow up-\n\nMeeting completed on Tues:\n\nImplementation of Microsoft Entra ID SSO for Snowflake access across the entire data platform, enabling unified authentication for all user personas and tools including PowerBI, Tabular Editor 3, Azure Data Factory, DBT, and direct Snowflake access.\n\n---\n\nAcceptance Criteria\n* --booking made for tuesday meeting--\n* --To follow-up from the results of the call on Tuesday--\n* --Decision made between MS Entra ID vs Okta- ✅ *MS Entra ID Selected*-\n\n* -Meeting with Brent from Cloud team scheduled- ✅ *Completed*\n* *Entra ID SSO configured and functional for Snowflake*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Work with Raveendran from our cloud team \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84\",\"accessLevel\":\"\",\"localId\":\"5a778d84-3fcf-46a6-8eed-a75d321d3ac5\"}},{\"type\":\"text\",\"text\":\" and \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"712020:a3bb398f-73df-472d-a226-f67ed86b49ee\",\"accessLevel\":\"\",\"localId\":\"542bb25d-3701-49bc-be49-7b5f9c0a5bea\"}},{\"type\":\"text\",\"text\":\" \"}],\"attrs\":{\"localId\":\"0a6a01ee-0890-4b42-99a3-17e68a53d8bb\",\"state\":\"DONE\"}}],\"attrs\":{\"localId\":\"340fd36c-fd9c-494d-93b5-65b83210118a\"}}\n{adf}\n* *Service Principal OAuth setup for PowerBI Service scheduled refresh*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Work with Raveendran from our cloud team \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84\",\"accessLevel\":\"\",\"localId\":\"29da3589-921e-4bd8-9020-7c2027ba319a\"}},{\"type\":\"text\",\"text\":\" and \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"712020:a3bb398f-73df-472d-a226-f67ed86b49ee\",\"accessLevel\":\"\",\"localId\":\"7cfbbe27-380e-4949-a2c5-ab2e425ff05a\"}},{\"type\":\"text\",\"text\":\" \"}],\"attrs\":{\"localId\":\"7d2f681f-168c-4a83-8bc1-c56634d2d967\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"340fd36c-fd9c-494d-93b5-65b83210118a\"}}\n{adf}\n* *Network security implemented (VNet Gateway)*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\" \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84\",\"accessLevel\":\"\",\"localId\":\"4d11860e-95ff-49b1-bcd3-89eb13c3af94\"}},{\"type\":\"text\",\"text\":\" - as part of the SSO can we also collaborate on this\"}],\"attrs\":{\"localId\":\"736cca4b-dae9-4882-9b4e-de06b4790373\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"819123db-3a96-41ca-8543-00cb537f3b64\"}}\n{adf}\n* *All user personas can authenticate successfully*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Testing post entra ID sso dependencies completed\"}],\"attrs\":{\"localId\":\"c9b6f8b8-7224-4cd6-9ba4-1b8c34b4d47d\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"db194595-b904-41c9-8026-19c645d2aaac\"}}\n{adf}\n* *Conditional access policies and MFA enforced*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Testing post entra ID sso dependencies completed\"}],\"attrs\":{\"localId\":\"b541fd8a-1751-4451-945b-4cd78a6a3069\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"db194595-b904-41c9-8026-19c645d2aaac\"}}\n{adf}\n* *Production rollout completed*\n\n---\n\nComments\n@user Will be working with @user tomorrow", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 194}}
{"issue_key": "CSCI-461", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Sep/25 3:20 PM", "updated": "27/Nov/25 2:38 PM", "labels": [], "summary": "Manhattan Active (MySQL) Integration part 2", "description": "DATA IN for critical identified objects from Manhattan Active (MySQL) db\n\n* confirm with RFoong decision for data lake extraction rather than API\n* Whitelinsting\n* Credential needed for GCP - login and testing connection\n* Create pipeline\n\n@user", "acceptance_criteria": "Definition of done:\n\n* Extract meta created\n* Linked services created\n* Pipeline run end to end", "comments": "To resume on unfinish parts for CSCI-136\n\nInitial work is done. We need to additional work with the business to identify the load pattern and might be additional tables.", "text": "Summary\nManhattan Active (MySQL) Integration part 2\n\n---\n\nDescription\nDATA IN for critical identified objects from Manhattan Active (MySQL) db\n\n* confirm with RFoong decision for data lake extraction rather than API\n* Whitelinsting\n* Credential needed for GCP - login and testing connection\n* Create pipeline\n\n@user\n\n---\n\nAcceptance Criteria\nDefinition of done:\n\n* Extract meta created\n* Linked services created\n* Pipeline run end to end\n\n---\n\nComments\nTo resume on unfinish parts for CSCI-136\n\nInitial work is done. We need to additional work with the business to identify the load pattern and might be additional tables.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 195}}
{"issue_key": "CSCI-460", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Sep/25 11:03 AM", "updated": "03/Oct/25 2:36 PM", "labels": [], "summary": "Manhattan Active - Discovery", "description": "Identify how we can connect to the Manhattan Active MY SQL Database\n\n# We have MYSQL login, but it we are unable to connect with Dbeaver\n# Supply chain team is connecting the database using Cogonos to get table details.", "acceptance_criteria": "|Table|Record count|\n|default_dcinventory.DCI_INVENTORY|37,311 |\n|default_dcinventory.DCI_LOCATION|19,775 |\n|default_dcinventory.DCI_LOCATION_CAPACITY_USAGE|17,388 |\n|default_dcinventory.DCI_LOCATION_ITEM_ASSIGNMENT|15,076 |\n|default_dcorder.DCO_ORDER_FAILURE|50,736 |\n|default_dcorder.DCO_ORDER_FAILURE_DETAIL|267,805 |\n|default_dcorder.DCO_ORDER_PLAN_RUN_STRATEGY|12,338 |\n|default_dcorder.DCO_PLANNING_RUN_STATUS | 8 |\n|default_item_master.ITE_ITEM|68,546 |\n|default_item_master.ITE_ITEM_PACKAGE|119,436 |\n|default_pickpack.PPK_OLPN|807,382 |\n\n|| || || ||\n| | | |\n| | | |", "comments": "Have sent email to Chandan from Boomi team to raise whitelist request for us with the Manhattan Active team\n\nThis is an ongoing process. CK helped us understand the current access method and Jess is communicating with Candan and we are waiting for the response from them.\n\nConnection has been made. Continue with the finding the number of records of the table identified by Jess\n\nInitial tables have been identified. Need to contact Supply change team another ticket will be created for that", "text": "Summary\nManhattan Active - Discovery\n\n---\n\nDescription\nIdentify how we can connect to the Manhattan Active MY SQL Database\n\n# We have MYSQL login, but it we are unable to connect with Dbeaver\n# Supply chain team is connecting the database using Cogonos to get table details.\n\n---\n\nAcceptance Criteria\n|Table|Record count|\n|default_dcinventory.DCI_INVENTORY|37,311 |\n|default_dcinventory.DCI_LOCATION|19,775 |\n|default_dcinventory.DCI_LOCATION_CAPACITY_USAGE|17,388 |\n|default_dcinventory.DCI_LOCATION_ITEM_ASSIGNMENT|15,076 |\n|default_dcorder.DCO_ORDER_FAILURE|50,736 |\n|default_dcorder.DCO_ORDER_FAILURE_DETAIL|267,805 |\n|default_dcorder.DCO_ORDER_PLAN_RUN_STRATEGY|12,338 |\n|default_dcorder.DCO_PLANNING_RUN_STATUS | 8 |\n|default_item_master.ITE_ITEM|68,546 |\n|default_item_master.ITE_ITEM_PACKAGE|119,436 |\n|default_pickpack.PPK_OLPN|807,382 |\n\n|| || || ||\n| | | |\n| | | |\n\n---\n\nComments\nHave sent email to Chandan from Boomi team to raise whitelist request for us with the Manhattan Active team\n\nThis is an ongoing process. CK helped us understand the current access method and Jess is communicating with Candan and we are waiting for the response from them.\n\nConnection has been made. Continue with the finding the number of records of the table identified by Jess\n\nInitial tables have been identified. Need to contact Supply change team another ticket will be created for that", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 196}}
{"issue_key": "CSCI-459", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Sep/25 9:36 AM", "updated": "23/Sep/25 9:36 AM", "labels": [], "summary": "Review - DimWarehouse, DimWarehouseLocation, DimZone", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nReview - DimWarehouse, DimWarehouseLocation, DimZone\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 197}}
{"issue_key": "CSCI-458", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Sep/25 1:33 PM", "updated": "10/Oct/25 5:06 PM", "labels": [], "summary": "Network Drive Whitelisting- RITM0176046", "description": "h3. Context\n\n* In order to ingest supply chain data into snowflake for group level reporting, network drive has to be whitelisted\n* \n\nh3. Objective\n\n* network drive to be whitelisted\n\nh3. Steps \n\n# ServiceNow ticket RITM0176046 submitted (Screenshot below)\n\n# track progress\n\nh3. Deliverables\n\n* network drive whitelisted\n\nh3. Rules Implemented for the whitelisting:\n\nRULE1\n\n***************************************************************\nName:\nVDI and AlbertSt To Azure_Snowflake Priv-Endpoints\n\n \n\nSource: \n10.15.0.0/19  albert_st_10.15.0.0_19_subnet_1\n172.25.128.0-172.25.150.62  AR_m1_vdi_desktop_full_range\n172.27.128.0-172.27.150.62  \n\n \n\nDestination:\n172.29.100.4 (cwraseedp-snowflake-prd-pep)\n\n \n\n \n\nService  443  TCP  \n***************************************************************\n\n \n\n \n\nRULE 2\n\n***************************************************************\nName: Snowflake to CW Vault\n\n \n\nSource:\n172.29.84.76 - cwr-ase-edp-shir-dev-vmss_0\n\n \n\nDestination:\n192.168.42.235 - H_m2phofil01_fileshare_srv\n\n \n\nService: \nSMB - TCP 445 \n***************************************************************\n\nh3. Acceptance criteria\n\n* network drive accessible - \\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "@user-- will get ticket number if possible-\nThanks @user\n\nNow asssigned to the network team - reaching out to Brent to work on this @user\n\nDiafy is working on this task\n\nFollowed up with network team- Diafy will be implementing this today\n\ncc-@user @user @user\n\n@user thank you\n\nRule has been implemented: TESTING IS PENDING\n\nRULE1\n\n***************************************************************\nName:\nVDI and AlbertSt To Azure_Snowflake Priv-Endpoints\n\n \n\nSource: \n10.15.0.0/19  albert_st_10.15.0.0_19_subnet_1\n172.25.128.0-172.25.150.62  AR_m1_vdi_desktop_full_range\n172.27.128.0-172.27.150.62  \n\n \n\nDestination:\n172.29.100.4 (cwraseedp-snowflake-prd-pep)\n\n \n\n \n\nService  443  TCP  \n***************************************************************\n\n \n\n \n\nRULE 2\n\n***************************************************************\nName: Snowflake to CW Vault\n\n \n\nSource:\n172.29.84.76 - cwr-ase-edp-shir-dev-vmss_0\n\n \n\nDestination:\n192.168.42.235 - H_m2phofil01_fileshare_srv\n\n \n\nService: \nSMB - TCP 445 \n***************************************************************\n\ncc- @user @user @user @user\n\n@user - Please update testing result\n\n@user Hi Han - as of today (8/10/25) the testing is still failing (error message received is ‘network drive not found’)\n\nThanks Jess .\n\n@user - can we forward the feedback to the team for further troubleshoot?\n\nThanks\n\nThis has been completed. Test result awaited from @user \n\n@user @user\n\nCan confirm as of yesterday we can now connect ADF to the network drive successfully 🙂 \n\nThis whitelisting task is now done.", "text": "Summary\nNetwork Drive Whitelisting- RITM0176046\n\n---\n\nDescription\nh3. Context\n\n* In order to ingest supply chain data into snowflake for group level reporting, network drive has to be whitelisted\n* \n\nh3. Objective\n\n* network drive to be whitelisted\n\nh3. Steps \n\n# ServiceNow ticket RITM0176046 submitted (Screenshot below)\n\n# track progress\n\nh3. Deliverables\n\n* network drive whitelisted\n\nh3. Rules Implemented for the whitelisting:\n\nRULE1\n\n***************************************************************\nName:\nVDI and AlbertSt To Azure_Snowflake Priv-Endpoints\n\n \n\nSource: \n10.15.0.0/19  albert_st_10.15.0.0_19_subnet_1\n172.25.128.0-172.25.150.62  AR_m1_vdi_desktop_full_range\n172.27.128.0-172.27.150.62  \n\n \n\nDestination:\n172.29.100.4 (cwraseedp-snowflake-prd-pep)\n\n \n\n \n\nService  443  TCP  \n***************************************************************\n\n \n\n \n\nRULE 2\n\n***************************************************************\nName: Snowflake to CW Vault\n\n \n\nSource:\n172.29.84.76 - cwr-ase-edp-shir-dev-vmss_0\n\n \n\nDestination:\n192.168.42.235 - H_m2phofil01_fileshare_srv\n\n \n\nService: \nSMB - TCP 445 \n***************************************************************\n\nh3. Acceptance criteria\n\n* network drive accessible - \\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n@user-- will get ticket number if possible-\nThanks @user\n\nNow asssigned to the network team - reaching out to Brent to work on this @user\n\nDiafy is working on this task\n\nFollowed up with network team- Diafy will be implementing this today\n\ncc-@user @user @user\n\n@user thank you\n\nRule has been implemented: TESTING IS PENDING\n\nRULE1\n\n***************************************************************\nName:\nVDI and AlbertSt To Azure_Snowflake Priv-Endpoints\n\n \n\nSource: \n10.15.0.0/19  albert_st_10.15.0.0_19_subnet_1\n172.25.128.0-172.25.150.62  AR_m1_vdi_desktop_full_range\n172.27.128.0-172.27.150.62  \n\n \n\nDestination:\n172.29.100.4 (cwraseedp-snowflake-prd-pep)\n\n \n\n \n\nService  443  TCP  \n***************************************************************\n\n \n\n \n\nRULE 2\n\n***************************************************************\nName: Snowflake to CW Vault\n\n \n\nSource:\n172.29.84.76 - cwr-ase-edp-shir-dev-vmss_0\n\n \n\nDestination:\n192.168.42.235 - H_m2phofil01_fileshare_srv\n\n \n\nService: \nSMB - TCP 445 \n***************************************************************\n\ncc- @user @user @user @user\n\n@user - Please update testing result\n\n@user Hi Han - as of today (8/10/25) the testing is still failing (error message received is ‘network drive not found’)\n\nThanks Jess .\n\n@user - can we forward the feedback to the team for further troubleshoot?\n\nThanks\n\nThis has been completed. Test result awaited from @user \n\n@user @user\n\nCan confirm as of yesterday we can now connect ADF to the network drive successfully 🙂 \n\nThis whitelisting task is now done.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 198}}
{"issue_key": "CSCI-457", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Sep/25 9:53 AM", "updated": "29/Sep/25 8:15 AM", "labels": [], "summary": "Network Drive Whitelisting- Cloud/Network Team- RITM0176046", "description": "h3. +*Context:*+ \n\n*Location of Network Drive is:* \n\n\\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\\DFIO_SCI_ZZ\\\n\nh3. +*Steps:*+\n\nWe're going to need:\n\n* Our Azure Subscription to be whitelisted to access the file drive and,\n* an account created to use to connect to the file drive\n\nh3. Objective\n\n* \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "@user awaiting for a design (diagram) from @user \n\ncc- @user @user\n\n@user has already given design \nwill loop @user in\n\nRITM0175995 created@user @user @user @user\n\nFirewall ticket for whitelisting Snowflake SHIR from Manhattan will be raised after meeting with Brent on 25/09/2025 at 10:30 am@user @user @user @user\n\n[https://sigmahealthcare.atlassian.net/browse/CSCI-458|https://sigmahealthcare.atlassian.net/browse/CSCI-458] \n\nticket has been split to here.", "text": "Summary\nNetwork Drive Whitelisting- Cloud/Network Team- RITM0176046\n\n---\n\nDescription\nh3. +*Context:*+ \n\n*Location of Network Drive is:* \n\n\\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\\DFIO_SCI_ZZ\\\n\nh3. +*Steps:*+\n\nWe're going to need:\n\n* Our Azure Subscription to be whitelisted to access the file drive and,\n* an account created to use to connect to the file drive\n\nh3. Objective\n\n* \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\n@user awaiting for a design (diagram) from @user \n\ncc- @user @user\n\n@user has already given design \nwill loop @user in\n\nRITM0175995 created@user @user @user @user\n\nFirewall ticket for whitelisting Snowflake SHIR from Manhattan will be raised after meeting with Brent on 25/09/2025 at 10:30 am@user @user @user @user\n\n[https://sigmahealthcare.atlassian.net/browse/CSCI-458|https://sigmahealthcare.atlassian.net/browse/CSCI-458] \n\nticket has been split to here.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 199}}
{"issue_key": "CSCI-455", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Sep/25 1:19 PM", "updated": "22/Sep/25 9:49 AM", "labels": ["Ingestion"], "summary": "Data Ingestion - PBI05", "description": "Import data from PBI05 for MergeCo tactical reporting.\n\n* Train Jess on ALTIDA\n** -Initial Session-\n** Follow up Session\n* -Identify the list of tables we want to import-\n*", "acceptance_criteria": "Given, When, Then", "comments": "Had initial session with Jess and 10 tables has been identified in the first list.\n\n@userCan you list the tables here thanks", "text": "Summary\nData Ingestion - PBI05\n\n---\n\nDescription\nImport data from PBI05 for MergeCo tactical reporting.\n\n* Train Jess on ALTIDA\n** -Initial Session-\n** Follow up Session\n* -Identify the list of tables we want to import-\n*\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nHad initial session with Jess and 10 tables has been identified in the first list.\n\n@userCan you list the tables here thanks", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 200}}
{"issue_key": "CSCI-451", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "17/Sep/25 2:24 PM", "updated": "25/Sep/25 9:21 AM", "labels": [], "summary": "Create Source-to-Target Map for DimSupplier, BridgeProductSupplier", "description": "h3. Context\n\n* There is a need to document the source-to-target mapping for the [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table , with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.", "acceptance_criteria": "h3. \n\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "comments": "@user Completed the mapping for DimSupplier, BridgeProductSupplier,\n[https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=xXfNPJ&nav=MTVfezMwMERDQ0Q5LUI2QkQtNDlFRC1BNTZBLTFGQzNGMzZFQjY1M30|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=xXfNPJ&nav=MTVfezMwMERDQ0Q5LUI2QkQtNDlFRC1BNTZBLTFGQzNGMzZFQjY1M30]\n\nCC:- @user\n\nIssue split into:\n|CSCI-468|Review Source-to-Target Map for DimSupplier, BridgeProductSupplier|", "text": "Summary\nCreate Source-to-Target Map for DimSupplier, BridgeProductSupplier\n\n---\n\nDescription\nh3. Context\n\n* There is a need to document the source-to-target mapping for the [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for [Dim_Supplier |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier]table and [BridgeProductSupplier|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimSupplier---BridgeProductSupplier][ |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table , with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.\n\n---\n\nAcceptance Criteria\nh3. \n\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)\n\n---\n\nComments\n@user Completed the mapping for DimSupplier, BridgeProductSupplier,\n[https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=xXfNPJ&nav=MTVfezMwMERDQ0Q5LUI2QkQtNDlFRC1BNTZBLTFGQzNGMzZFQjY1M30|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=xXfNPJ&nav=MTVfezMwMERDQ0Q5LUI2QkQtNDlFRC1BNTZBLTFGQzNGMzZFQjY1M30]\n\nCC:- @user\n\nIssue split into:\n|CSCI-468|Review Source-to-Target Map for DimSupplier, BridgeProductSupplier|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 201}}
{"issue_key": "CSCI-450", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "17/Sep/25 2:24 PM", "updated": "25/Sep/25 9:21 AM", "labels": [], "summary": "Create Source-to-Target Map for DimManufacturer, BridgeProductManufacturer", "description": "h3. Context\n\n* There is a need to document the source-to-target mapping for the [Dim_Manufacturer|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer] table and [BridgeProductManufacturer |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for [Dim_Manufacturer|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer] table and [BridgeProductManufacturer |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.", "acceptance_criteria": "h3. Acceptance criteria\n\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "comments": "", "text": "Summary\nCreate Source-to-Target Map for DimManufacturer, BridgeProductManufacturer\n\n---\n\nDescription\nh3. Context\n\n* There is a need to document the source-to-target mapping for the [Dim_Manufacturer|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer] table and [BridgeProductManufacturer |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for [Dim_Manufacturer|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer] table and [BridgeProductManufacturer |https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#DimManufacturer---BridgeProductManufacturer]table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.\n\n---\n\nAcceptance Criteria\nh3. Acceptance criteria\n\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 202}}
{"issue_key": "CSCI-449", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "17/Sep/25 2:23 PM", "updated": "29/Sep/25 8:10 AM", "labels": [], "summary": "Create Source-to-Target Map for DimBatch", "description": "h3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Batch table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Batch table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.", "acceptance_criteria": "* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "comments": "", "text": "Summary\nCreate Source-to-Target Map for DimBatch\n\n---\n\nDescription\nh3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Batch table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Batch table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.\n\n---\n\nAcceptance Criteria\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 203}}
{"issue_key": "CSCI-448", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "17/Sep/25 2:22 PM", "updated": "29/Sep/25 8:04 AM", "labels": [], "summary": "Create Source-to-Target Map for DimBuyer, BridgeProductBuyer", "description": "h3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Buyer table and BridgeProductBuyer table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Buyer table and BridgeProductBuyer table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.", "acceptance_criteria": "* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "comments": "", "text": "Summary\nCreate Source-to-Target Map for DimBuyer, BridgeProductBuyer\n\n---\n\nDescription\nh3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Buyer table and BridgeProductBuyer table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Buyer table and BridgeProductBuyer table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\n---\n\nAcceptance Criteria\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 204}}
{"issue_key": "CSCI-447", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "17/Sep/25 2:19 PM", "updated": "29/Sep/25 8:04 AM", "labels": [], "summary": "Create Source-to-Target Map for Dim_Party, BridgeStorePartyRole", "description": "h3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Party table and BridgeStorePartyRole table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Party table and BridgeStorePartyRole table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.", "acceptance_criteria": "* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)", "comments": "@user Its completed, I’ve moved it to review", "text": "Summary\nCreate Source-to-Target Map for Dim_Party, BridgeStorePartyRole\n\n---\n\nDescription\nh3. Context\n\n* There is a need to document the source-to-target mapping for the Dim_Party table and BridgeStorePartyRole table as part of the CW Cloud Data Platform Interim Solution project.\n\nh3. Objective\n\n* To complete the source-to-target mapping sheet for Fact_Warehouse_Location_Packing, ensuring all required columns and metadata are accurately filled out as per the supplied template.\n\nh3. Steps\n\n# Access the [STTM |https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]document\n# For each column in Fact_Warehouse_Location_Packing, fill in:\n#* Pres Column name\n#* Data type\n#* Description\n#* Source details (Server, DB, Schema, Table, Column)\n#* Indicate if derived (Yes/No)\n# Review the completed mapping for accuracy and completeness.\n\nh3. Deliverables\n\n* A fully completed source-to-target mapping sheet for Dim_Party table and BridgeStorePartyRole table, with all required fields filled as per the template.\n\nh3. Assumptions (Optional)\n\n* Access to all necessary source systems and metadata is available.\n* The supplied mapping sheet template is up to date and reflects current requirements.\n\nh3.\n\n---\n\nAcceptance Criteria\n* Columns as per the supplied sheet are filled:\n** Pres Column name\n** Data type\n** Description\n** Source (Server, DB, Schema, Table, Column)\n** Derived? (Yes/No)\n\n---\n\nComments\n@user Its completed, I’ve moved it to review", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 205}}
{"issue_key": "CSCI-443", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Sep/25 3:45 PM", "updated": "17/Sep/25 12:58 PM", "labels": [], "summary": "CW BAU Tasks", "description": "* Needed to investigate issue with the PBI On-prem gateway\n* SC Team recently receiving a “gateway datasource” error on a few of their SM’s intermittently:\n** \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "", "text": "Summary\nCW BAU Tasks\n\n---\n\nDescription\n* Needed to investigate issue with the PBI On-prem gateway\n* SC Team recently receiving a “gateway datasource” error on a few of their SM’s intermittently:\n** \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 206}}
{"issue_key": "CSCI-442", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Sep/25 1:05 PM", "updated": "29/Sep/25 8:17 AM", "labels": [], "summary": "Follow up on - RITM0175340- Azure Blob storage access from office", "description": "h3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "Given, When, Then", "comments": "In touch with Systems and Cloud team to get this sorted.\n\nAs Per @user the systems team has advised to let them know when @user and him are in the head office so that they can run somethings is the background to check the connections.\n\ncc- @user\n\nThe firewall rules are now in place from IT side for Aswini and Ashutosh to access the *BLOB Storage*. They will test this week when they are in office. \n\ncc- @user @user @user\n\nIssue split into:\n|CSCI-494|Follow up on - RITM0175340- Azure Blob storage access from office - sprint 9 |", "text": "Summary\nFollow up on - RITM0175340- Azure Blob storage access from office\n\n---\n\nDescription\nh3. Context\n\n* \n\nh3. Objective\n\n* \n\nh3. Steps \n\n# \n\nh3. Deliverables\n\n* \n\nh3. Assumptions - (Optional)\n\n* \n\nh3. Acceptance criteria\n\n* \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\nGiven, When, Then\n\n---\n\nComments\nIn touch with Systems and Cloud team to get this sorted.\n\nAs Per @user the systems team has advised to let them know when @user and him are in the head office so that they can run somethings is the background to check the connections.\n\ncc- @user\n\nThe firewall rules are now in place from IT side for Aswini and Ashutosh to access the *BLOB Storage*. They will test this week when they are in office. \n\ncc- @user @user @user\n\nIssue split into:\n|CSCI-494|Follow up on - RITM0175340- Azure Blob storage access from office - sprint 9 |", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 207}}
{"issue_key": "CSCI-441", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Sep/25 9:04 AM", "updated": "19/Sep/25 9:47 AM", "labels": [], "summary": "Create the prod access for PDB14 and PDB19 reader servers", "description": "h3. Create the SQL authenticated logins for PDB14 and PDB19 readonly servers and grant access\n\nh3. \n\nh3. \n\nh3. \n\nh3.", "acceptance_criteria": "* Create required SQL logins\n* Create SQL user for SCAX2012 and TransactionStorage databases.\n* Grant SELECT access on tables used in the data platform\n* Add secrets to Key Vault", "comments": "@user Created the SQL users and added the passwords to the DEV key vault. These databases need to be accessed with application intent set to read-only\n\ncc: @user\n\naswini to test connection to prod \n\nneed:\n- whitelisting\n\nPHil to split task", "text": "Summary\nCreate the prod access for PDB14 and PDB19 reader servers\n\n---\n\nDescription\nh3. Create the SQL authenticated logins for PDB14 and PDB19 readonly servers and grant access\n\nh3. \n\nh3. \n\nh3. \n\nh3.\n\n---\n\nAcceptance Criteria\n* Create required SQL logins\n* Create SQL user for SCAX2012 and TransactionStorage databases.\n* Grant SELECT access on tables used in the data platform\n* Add secrets to Key Vault\n\n---\n\nComments\n@user Created the SQL users and added the passwords to the DEV key vault. These databases need to be accessed with application intent set to read-only\n\ncc: @user\n\naswini to test connection to prod \n\nneed:\n- whitelisting\n\nPHil to split task", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 208}}
{"issue_key": "CSCI-440", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 6:32 PM", "updated": "23/Sep/25 9:52 AM", "labels": [], "summary": "MergeCo Conformed Reporting - Identify Source for Availability Metrics - part 2", "description": "Identify which specific tables/views to use for Merge Co reporting from a CW domain for the below availability metrics:\n\n* Availability %\n* TOS %\n** Temporarily Out of Stock\n* MCS %\n** Manufacturer Can’t Supply\n* SOH $\n** Stock on Hand\n* DOI\n** Days of Inventory - within CW also known as DIH (Days In Hand)\n\nIdeal grain is at the: \n\nper day, per DC, per product level\n\nNeed to work out:\n\n* Which tables best to extract from\n* If tables can’t be identified, why? Is it not available at all? Or is there an issue with the grain of data? Or access to data?", "acceptance_criteria": "", "comments": "||*Metric*||*Current SC Source*||*Proposed Tactic Source*||*Detail*||\n|Availability %|DFIO Extract\n(stored on-prem network drive)|SKU? (PDB08)|\\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\\DFIO_SCI_ZZ\\|\n|TOS%|DFIO Extract\n(stored on-prem network drive)|SKU? (PDB08)|\\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\\DFIO_SCI_ZZ\\|\n|MCS%|DFIO Extract\n(stored on-prem network drive)|SKU? (PDB08)|\\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\\DFIO_SCI_ZZ\\|\n|SOH $/Units|PBI05 AX Table|PBI05 AX Table|PBI05.SupplyChain.SCAX.DCsInventoryHistory.InventoryAmount|\n|DOI|PBI05 AX Tables|PBI05 AX Tables|[BI_Presentation].[dbo].[AX_FCT_DC_STOCK].[STOCK_VALUE] && [BI_Presentation].[dbo].[AX_FCT_DC_SALES].[ExtendedCostExGST]|\n\nHave collated all info - need to talk with @user re: what to do about the metrics where DFIO extracts are its source\n\nAX_FCT_DC_STOCK - only displays grain for per DC, per product (does not distinguish per day, only displays current data)", "text": "Summary\nMergeCo Conformed Reporting - Identify Source for Availability Metrics - part 2\n\n---\n\nDescription\nIdentify which specific tables/views to use for Merge Co reporting from a CW domain for the below availability metrics:\n\n* Availability %\n* TOS %\n** Temporarily Out of Stock\n* MCS %\n** Manufacturer Can’t Supply\n* SOH $\n** Stock on Hand\n* DOI\n** Days of Inventory - within CW also known as DIH (Days In Hand)\n\nIdeal grain is at the: \n\nper day, per DC, per product level\n\nNeed to work out:\n\n* Which tables best to extract from\n* If tables can’t be identified, why? Is it not available at all? Or is there an issue with the grain of data? Or access to data?\n\n---\n\nComments\n||*Metric*||*Current SC Source*||*Proposed Tactic Source*||*Detail*||\n|Availability %|DFIO Extract\n(stored on-prem network drive)|SKU? (PDB08)|\\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\\DFIO_SCI_ZZ\\|\n|TOS%|DFIO Extract\n(stored on-prem network drive)|SKU? (PDB08)|\\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\\DFIO_SCI_ZZ\\|\n|MCS%|DFIO Extract\n(stored on-prem network drive)|SKU? (PDB08)|\\\\cwvault\\Everyone\\Supply Chain\\Administration\\Report and Dashboard\\ZZ_DataSource\\DFIO_SCI_ZZ\\|\n|SOH $/Units|PBI05 AX Table|PBI05 AX Table|PBI05.SupplyChain.SCAX.DCsInventoryHistory.InventoryAmount|\n|DOI|PBI05 AX Tables|PBI05 AX Tables|[BI_Presentation].[dbo].[AX_FCT_DC_STOCK].[STOCK_VALUE] && [BI_Presentation].[dbo].[AX_FCT_DC_SALES].[ExtendedCostExGST]|\n\nHave collated all info - need to talk with @user re: what to do about the metrics where DFIO extracts are its source\n\nAX_FCT_DC_STOCK - only displays grain for per DC, per product (does not distinguish per day, only displays current data)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 209}}
{"issue_key": "CSCI-439", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 6:32 PM", "updated": "18/Sep/25 4:53 PM", "labels": [], "summary": "MergeCo Conformed Reporting - Pull Data from CW to SF - part 2", "description": "Copy data from CW on-prem SQL Server (PBI05) to SF via ADF.\n\nGrain:\n\n* High-level: Per DC, Per Day", "acceptance_criteria": "", "comments": "Had meeting with network team - they advised they have whitelisted “their side” of PBI05 - however we now require the cloud team to enable the whitelisting on their side within Azure.\n\n@user to follow up\n\nCloud team performed whitelisting action on their end.\n\nRan test in ADF to test connection to PBI05 and test ran successful.\n\nOnce a table has been loaded into SF - will mark task as complete.", "text": "Summary\nMergeCo Conformed Reporting - Pull Data from CW to SF - part 2\n\n---\n\nDescription\nCopy data from CW on-prem SQL Server (PBI05) to SF via ADF.\n\nGrain:\n\n* High-level: Per DC, Per Day\n\n---\n\nComments\nHad meeting with network team - they advised they have whitelisted “their side” of PBI05 - however we now require the cloud team to enable the whitelisting on their side within Azure.\n\n@user to follow up\n\nCloud team performed whitelisting action on their end.\n\nRan test in ADF to test connection to PBI05 and test ran successful.\n\nOnce a table has been loaded into SF - will mark task as complete.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 210}}
{"issue_key": "CSCI-438", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 6:28 PM", "updated": "19/Sep/25 9:21 AM", "labels": [], "summary": "APPRISS - next steps on data modelling - For Alan/Harrison to review", "description": "This card is for Alan/Harrison to review Amit’s work\n\n* -Identify business process for APPRISS-\n\n{quote}Identify source table(s) for APPRISS - partially done (review)\n\nCreate structure for FActs and Dims{quote}\n\n{quote}\n\n* Finalizing identification of business processes relevant to APPRISS.\n* Reviewing and confirming source tables for APPRISS data ingestion.\n* Designing and documenting the structure for Fact and Dimension tables to support robust analytics and reporting.\nThis task aims to ensure a clear, scalable, and auditable data model foundation for APPRISS integration within the CW Cloud Data Platform Interim Solution.\n\n{quote}", "acceptance_criteria": "* -Business Process Identification-\n* -Source Table Confirmation-\n* -Review Initial Data Extract Queries-\n* - Dimension Table Design-\n* -Fact Tables Design-\n* -Sample Data Validation-\n* -Documentation-\n* Stakeholder Review\n* -Readiness for Next Steps-", "comments": "", "text": "Summary\nAPPRISS - next steps on data modelling - For Alan/Harrison to review\n\n---\n\nDescription\nThis card is for Alan/Harrison to review Amit’s work\n\n* -Identify business process for APPRISS-\n\n{quote}Identify source table(s) for APPRISS - partially done (review)\n\nCreate structure for FActs and Dims{quote}\n\n{quote}\n\n* Finalizing identification of business processes relevant to APPRISS.\n* Reviewing and confirming source tables for APPRISS data ingestion.\n* Designing and documenting the structure for Fact and Dimension tables to support robust analytics and reporting.\nThis task aims to ensure a clear, scalable, and auditable data model foundation for APPRISS integration within the CW Cloud Data Platform Interim Solution.\n\n{quote}\n\n---\n\nAcceptance Criteria\n* -Business Process Identification-\n* -Source Table Confirmation-\n* -Review Initial Data Extract Queries-\n* - Dimension Table Design-\n* -Fact Tables Design-\n* -Sample Data Validation-\n* -Documentation-\n* Stakeholder Review\n* -Readiness for Next Steps-", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 211}}
{"issue_key": "CSCI-437", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 6:27 PM", "updated": "19/Sep/25 9:21 AM", "labels": [], "summary": "Silver model for facts - review for Alan/Harrison", "description": "This card is for Alan/Harrison to review Amit’s work\n\n{quote}Develop a Silver data model for Facts, transforming and integrating data from the Bronze layer to provide a cleansed, structured, and business-ready dataset. {quote}\n\n{quote}The Silver model should address data quality issues, apply necessary business logic, and ensure consistency for downstream analytics and reporting.{quote}", "acceptance_criteria": "* -Silver (Kimball Medallion model) for Facts to be constructed.-\n* -Diagram in tool i.e. [http://draw.io|http://draw.io] -\n* peer reviewed\n* Documentation\n* Stakeholder Sign-off", "comments": "", "text": "Summary\nSilver model for facts - review for Alan/Harrison\n\n---\n\nDescription\nThis card is for Alan/Harrison to review Amit’s work\n\n{quote}Develop a Silver data model for Facts, transforming and integrating data from the Bronze layer to provide a cleansed, structured, and business-ready dataset. {quote}\n\n{quote}The Silver model should address data quality issues, apply necessary business logic, and ensure consistency for downstream analytics and reporting.{quote}\n\n---\n\nAcceptance Criteria\n* -Silver (Kimball Medallion model) for Facts to be constructed.-\n* -Diagram in tool i.e. [http://draw.io|http://draw.io] -\n* peer reviewed\n* Documentation\n* Stakeholder Sign-off", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 212}}
{"issue_key": "CSCI-436", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 6:21 PM", "updated": "25/Sep/25 9:21 AM", "labels": [], "summary": "Create Source-to-Target Map for Fact_Warehouse_Location_Space_Utilisation - part 2", "description": "Source to target mapping for Fact_Warehouse_Location_Space_Utilisation - filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "", "text": "Summary\nCreate Source-to-Target Map for Fact_Warehouse_Location_Space_Utilisation - part 2\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Space_Utilisation - filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 213}}
{"issue_key": "CSCI-435", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 6:21 PM", "updated": "25/Sep/25 9:21 AM", "labels": [], "summary": "Create Source-to-Target Map for Fact_Warehouse_Location_Cycle_Count - part 2", "description": "Source to target mapping for Fact_Warehouse_Location_Cycle_Count- filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "", "text": "Summary\nCreate Source-to-Target Map for Fact_Warehouse_Location_Cycle_Count - part 2\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Cycle_Count- filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 214}}
{"issue_key": "CSCI-434", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 6:18 PM", "updated": "23/Sep/25 9:31 AM", "labels": [], "summary": "Review Source-to-Target Map for Dim_Product - Alan - part 2", "description": "to have a discussion with you @user about this task.", "acceptance_criteria": "* Finalise that the work on Source-to-Target Map for Dim_product is correct.", "comments": "", "text": "Summary\nReview Source-to-Target Map for Dim_Product - Alan - part 2\n\n---\n\nDescription\nto have a discussion with you @user about this task.\n\n---\n\nAcceptance Criteria\n* Finalise that the work on Source-to-Target Map for Dim_product is correct.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 215}}
{"issue_key": "CSCI-433", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 6:18 PM", "updated": "23/Sep/25 9:30 AM", "labels": [], "summary": "Review Source-to-Target Map for Dim_Product_Pack (Previously Dim_Product_UOM)", "description": "Review the mapping of DIM_PRODICT_UOM Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=67yqFK&nav=MTVfe0RDNzkwNDhDLTg0NDAtNEREOS05NkE3LUM2ODRFOTNBNjkzQX0]", "acceptance_criteria": "* Finalize that the work on Source-to-Target Map for DIM_PRODICT_UOM is correct.\n** Review the Source-to-Target Map for DIM_PRODICT_UOM against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** -Communicate any changes to relevant business stakeholders.-\n** Communicate changes of mapping file to the Jira/standup for traceability.", "comments": "@user @user - please rearrange spreadsheet as per Alan’s [data model|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#Silver.20]\n\nLMK if there’s any issues cc @user\n\n@user @user / @user best to split this to an additional card linked to this card.", "text": "Summary\nReview Source-to-Target Map for Dim_Product_Pack (Previously Dim_Product_UOM)\n\n---\n\nDescription\nReview the mapping of DIM_PRODICT_UOM Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=67yqFK&nav=MTVfe0RDNzkwNDhDLTg0NDAtNEREOS05NkE3LUM2ODRFOTNBNjkzQX0]\n\n---\n\nAcceptance Criteria\n* Finalize that the work on Source-to-Target Map for DIM_PRODICT_UOM is correct.\n** Review the Source-to-Target Map for DIM_PRODICT_UOM against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** -Communicate any changes to relevant business stakeholders.-\n** Communicate changes of mapping file to the Jira/standup for traceability.\n\n---\n\nComments\n@user @user - please rearrange spreadsheet as per Alan’s [data model|https://sigmahealthcare.atlassian.net/wiki/spaces/EDP/pages/1622769666/Enterprise+Data+Model#Silver.20]\n\nLMK if there’s any issues cc @user\n\n@user @user / @user best to split this to an additional card linked to this card.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 216}}
{"issue_key": "CSCI-432", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 6:18 PM", "updated": "23/Sep/25 9:30 AM", "labels": [], "summary": "Review Source-to-Target Map for Dim_Store - Alan part 2", "description": "Reviewing Source-to-Target Map for Dim_Store in Supply chain", "acceptance_criteria": "* Finalise that the work on Source-to-Target Map for Dim_Store is correct.", "comments": "@user @user\n\nadd Dim_address, StoreHours, StoreHoursSpecial\n\ncc @ @user", "text": "Summary\nReview Source-to-Target Map for Dim_Store - Alan part 2\n\n---\n\nDescription\nReviewing Source-to-Target Map for Dim_Store in Supply chain\n\n---\n\nAcceptance Criteria\n* Finalise that the work on Source-to-Target Map for Dim_Store is correct.\n\n---\n\nComments\n@user @user\n\nadd Dim_address, StoreHours, StoreHoursSpecial\n\ncc @ @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 217}}
{"issue_key": "CSCI-431", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Duplicate", "priority": "Medium", "created": "12/Sep/25 6:14 PM", "updated": "22/Dec/25 8:27 AM", "labels": [], "summary": "Enable developer access to snowflake via Privatelink", "description": "Enable developer access to snowflake via Privatelink - via browser \n\nUser story:\n\n* As a snowflake person (not service) user, I want to access snowflake via Privatelink so that it is accessible and secure (confirming to security requirements).\n\n@user - Below are the key action items as identified by Eugene for us as next steps.\n\nDetails are [here|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5145/Developer-Access?anchor=**developer-access-to-snowflake-via-azure-privatelink**]\n\nCan you please help us engage with Cloud team and raise a servicenow ticket.\n\ncc @user \n\n*Key action items* \n\n* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com]\n* Configure firewalls (On-Prem, Azure) as follows\n\n* On-Prem Network for developers → Snowflake VNet *(servnicenow ticket)*\n* VDI / Jumpbox network → Snowflake VNet *(servicenow tickets)*\n** *how many jumpboxes* we need?\n** *What are the things we need?*\n* VPN Network for developers → Snowflake VNet (servicenow ticket)\n** QoS? - traffic prioritisation\n** What tools are involved.\n** CW - Palo Alto GlobalProtect.\n* Note:\n** Ports to be allowed: 443, 80, 1433\n* Configure VDI / Jumbox images to pre-install tools defined above\n* Create Development VMs in Snowflake Subscription", "acceptance_criteria": "* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/]\n* Access snowflake via [https://app.snowflake.com/cw/au/#/homepage|https://app.snowflake.com/cw/au/#/homepage] after infrastructure changes.\n* Test cases\n** Access on premises (office network)\n** Access off premises\n*** (fail path) access with no VPN\n*** Access via GlobalProtect (CW)\n*** Access via Sigma VPN\n** Access via PBI\n*", "comments": "yes since @user has been onboarded, she will be working on the setting up the DE jumpbox, once this is set up, I already explained to her this is the next step. So this should be aligned to the list of tasks to what Anjali will be working on.\n\nI believe she set up meeting for Monday and we can discuss this.\n\n @user @user@user\n\nFor clarity, approach\n\n# Jumpbox will be created\n# Create VMs\n# VMs will have access to privatelink\n\n@user this task is a duplicate of existing - [https://sigmahealthcare.atlassian.net/browse/CSCI-422|https://sigmahealthcare.atlassian.net/browse/CSCI-422] \n\nCan we please close this? To avoid confusion and duplication.\n\n@user sure I”ll close this one", "text": "Summary\nEnable developer access to snowflake via Privatelink\n\n---\n\nDescription\nEnable developer access to snowflake via Privatelink - via browser \n\nUser story:\n\n* As a snowflake person (not service) user, I want to access snowflake via Privatelink so that it is accessible and secure (confirming to security requirements).\n\n@user - Below are the key action items as identified by Eugene for us as next steps.\n\nDetails are [here|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5145/Developer-Access?anchor=**developer-access-to-snowflake-via-azure-privatelink**]\n\nCan you please help us engage with Cloud team and raise a servicenow ticket.\n\ncc @user \n\n*Key action items* \n\n* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com]\n* Configure firewalls (On-Prem, Azure) as follows\n\n* On-Prem Network for developers → Snowflake VNet *(servnicenow ticket)*\n* VDI / Jumpbox network → Snowflake VNet *(servicenow tickets)*\n** *how many jumpboxes* we need?\n** *What are the things we need?*\n* VPN Network for developers → Snowflake VNet (servicenow ticket)\n** QoS? - traffic prioritisation\n** What tools are involved.\n** CW - Palo Alto GlobalProtect.\n* Note:\n** Ports to be allowed: 443, 80, 1433\n* Configure VDI / Jumbox images to pre-install tools defined above\n* Create Development VMs in Snowflake Subscription\n\n---\n\nAcceptance Criteria\n* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/]\n* Access snowflake via [https://app.snowflake.com/cw/au/#/homepage|https://app.snowflake.com/cw/au/#/homepage] after infrastructure changes.\n* Test cases\n** Access on premises (office network)\n** Access off premises\n*** (fail path) access with no VPN\n*** Access via GlobalProtect (CW)\n*** Access via Sigma VPN\n** Access via PBI\n*\n\n---\n\nComments\nyes since @user has been onboarded, she will be working on the setting up the DE jumpbox, once this is set up, I already explained to her this is the next step. So this should be aligned to the list of tasks to what Anjali will be working on.\n\nI believe she set up meeting for Monday and we can discuss this.\n\n @user @user@user\n\nFor clarity, approach\n\n# Jumpbox will be created\n# Create VMs\n# VMs will have access to privatelink\n\n@user this task is a duplicate of existing - [https://sigmahealthcare.atlassian.net/browse/CSCI-422|https://sigmahealthcare.atlassian.net/browse/CSCI-422] \n\nCan we please close this? To avoid confusion and duplication.\n\n@user sure I”ll close this one", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 218}}
{"issue_key": "CSCI-430", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Duplicate", "priority": "Medium", "created": "12/Sep/25 6:13 PM", "updated": "22/Dec/25 8:27 AM", "labels": [], "summary": "RITM0174658 - Aswini and Ashutosh access to CW BLOB storage", "description": "PHil to raise in Servicenow direct access to CW BLOB storage for Aswini", "acceptance_criteria": "Jumpbox → Blob Access availble for Aswini", "comments": "maybe duplicate ticket - will take down if it is so\n\nHi @user this is a duplicate ticket, there is already existing-RITM0175340 on which the Systems team is working on. You may please take this one down.\n\n@user marked as duplicate", "text": "Summary\nRITM0174658 - Aswini and Ashutosh access to CW BLOB storage\n\n---\n\nDescription\nPHil to raise in Servicenow direct access to CW BLOB storage for Aswini\n\n---\n\nAcceptance Criteria\nJumpbox → Blob Access availble for Aswini\n\n---\n\nComments\nmaybe duplicate ticket - will take down if it is so\n\nHi @user this is a duplicate ticket, there is already existing-RITM0175340 on which the Systems team is working on. You may please take this one down.\n\n@user marked as duplicate", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 219}}
{"issue_key": "CSCI-429", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 2:56 PM", "updated": "18/Sep/25 9:56 AM", "labels": [], "summary": "PBI05 whitelisting Servicenow Ticket - MergeCo Conformed Reporting - Pull Data from CW to SF", "description": "Copy data from CW on-prem SQL Server (PBI05) to SF via ADF.\n\nGrain:\n\n* High-level: Per DC, Per Day", "acceptance_criteria": "", "comments": "Have scheduled a meeting with the CWR Network team to walk them through the Snowflake data flow (related to ticket _RITM0173895_) to ensure clarity on how the flow operates within our network.\n\nThe goal is to provide the context required for the Network team to review and approve the ticket so we can move forward.\n\ncc- @user @user @user\n\nThe network team has whitelisted the firewall from their end. Awaiting cloud team to whitelist from Azure end.\n\nThe firewall rule has been applied from both, Cloud and On-prem side. And jess has tested this.", "text": "Summary\nPBI05 whitelisting Servicenow Ticket - MergeCo Conformed Reporting - Pull Data from CW to SF\n\n---\n\nDescription\nCopy data from CW on-prem SQL Server (PBI05) to SF via ADF.\n\nGrain:\n\n* High-level: Per DC, Per Day\n\n---\n\nComments\nHave scheduled a meeting with the CWR Network team to walk them through the Snowflake data flow (related to ticket _RITM0173895_) to ensure clarity on how the flow operates within our network.\n\nThe goal is to provide the context required for the Network team to review and approve the ticket so we can move forward.\n\ncc- @user @user @user\n\nThe network team has whitelisted the firewall from their end. Awaiting cloud team to whitelist from Azure end.\n\nThe firewall rule has been applied from both, Cloud and On-prem side. And jess has tested this.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 220}}
{"issue_key": "CSCI-428", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 2:51 PM", "updated": "24/Sep/25 9:46 AM", "labels": [], "summary": "Documentation for transformation logic for fact tables for APPRISS", "description": "Create mapping & transformation document for Silver Layer of Fact tables needed for Appriss Data Delivery \n\n[https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/02.%20Commercial%20Data%20Platform/Retail%20Data%20Model%20-%20Silver/Silver%20Fact%20Tables%20Mapping%20Info.xlsx?d=w8de2c221e0014e0da95ac90496078562&csf=1&web=1&e=nqkmvj|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/02.%20Commercial%20Data%20Platform/Retail%20Data%20Model%20-%20Silver/Silver%20Fact%20Tables%20Mapping%20Info.xlsx?d=w8de2c221e0014e0da95ac90496078562&csf=1&web=1&e=nqkmvj]", "acceptance_criteria": "* -Document link-\n* Engineering team should be able to populate silver layer fact tables based on Mapping document \n\n----\n\nColumns:\n\n|Column Name|Description|Source Table|Mapping|Logic If any|", "comments": "Started with FCT_Sales_Retail. Work in progress\n\nTwo more to be done afterwards\n\nFCT_Transactions_Audit_History\nFCT_Sales_Retail_Electronic_Payments\n\nCan we fill description and AC\n\nThanks\n\nadded them @user\n\n@user can we have a bit more details e.g. what the AC entailis?\nWhat are we expecting as a deliverable at the end of this task?\n\nThanks\n\nI”ll get you to place link to document here @user and go from there.\n\n[Silver Fact Table Mapping info|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/02.%20Commercial%20Data%20Platform/Retail%20Data%20Model%20-%20Silver/Silver%20Fact%20Tables%20Mapping%20Info.xlsx?d=w8de2c221e0014e0da95ac90496078562&csf=1&web=1&e=nqkmvj] \nHi @user and @user here is the document link .@user\n\nCompleted doc for \n FCT_Sales_Retail\nFCT_Sales_Retail_Electronic_Payment\n\nWill complete FCT_Transactions_Audit_History by today\n\nCompleted FCT_Transactions_Audit_History mapping. This task is now completed", "text": "Summary\nDocumentation for transformation logic for fact tables for APPRISS\n\n---\n\nDescription\nCreate mapping & transformation document for Silver Layer of Fact tables needed for Appriss Data Delivery \n\n[https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/02.%20Commercial%20Data%20Platform/Retail%20Data%20Model%20-%20Silver/Silver%20Fact%20Tables%20Mapping%20Info.xlsx?d=w8de2c221e0014e0da95ac90496078562&csf=1&web=1&e=nqkmvj|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/02.%20Commercial%20Data%20Platform/Retail%20Data%20Model%20-%20Silver/Silver%20Fact%20Tables%20Mapping%20Info.xlsx?d=w8de2c221e0014e0da95ac90496078562&csf=1&web=1&e=nqkmvj]\n\n---\n\nAcceptance Criteria\n* -Document link-\n* Engineering team should be able to populate silver layer fact tables based on Mapping document \n\n----\n\nColumns:\n\n|Column Name|Description|Source Table|Mapping|Logic If any|\n\n---\n\nComments\nStarted with FCT_Sales_Retail. Work in progress\n\nTwo more to be done afterwards\n\nFCT_Transactions_Audit_History\nFCT_Sales_Retail_Electronic_Payments\n\nCan we fill description and AC\n\nThanks\n\nadded them @user\n\n@user can we have a bit more details e.g. what the AC entailis?\nWhat are we expecting as a deliverable at the end of this task?\n\nThanks\n\nI”ll get you to place link to document here @user and go from there.\n\n[Silver Fact Table Mapping info|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/02.%20Commercial%20Data%20Platform/Retail%20Data%20Model%20-%20Silver/Silver%20Fact%20Tables%20Mapping%20Info.xlsx?d=w8de2c221e0014e0da95ac90496078562&csf=1&web=1&e=nqkmvj] \nHi @user and @user here is the document link .@user\n\nCompleted doc for \n FCT_Sales_Retail\nFCT_Sales_Retail_Electronic_Payment\n\nWill complete FCT_Transactions_Audit_History by today\n\nCompleted FCT_Transactions_Audit_History mapping. This task is now completed", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 221}}
{"issue_key": "CSCI-427", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 2:42 PM", "updated": "25/Sep/25 11:51 AM", "labels": [], "summary": "Issue in parquet file load - part 2", "description": "Getting variable columns in Parquet file load for historical data for ILS / Apriss tables", "acceptance_criteria": "", "comments": "", "text": "Summary\nIssue in parquet file load - part 2\n\n---\n\nDescription\nGetting variable columns in Parquet file load for historical data for ILS / Apriss tables", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 222}}
{"issue_key": "CSCI-426", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 2:41 PM", "updated": "18/Sep/25 3:37 PM", "labels": ["Ingestion"], "summary": "Manhattan Scale data ingestion - part 2", "description": "Import the data for ILS tables\n\nlocation_inventory\n\nAR_Transaction_History\n\nTransaction_History\n\n* Create extract meta records\n* Create & Run end to end pipelines in ADF", "acceptance_criteria": "", "comments": "assigning to @user \n\n@user when you’re back, if you can let Aswini know what’s covered, that’ll be great.\n\nThanks!\n\nHi @user ,\n\nI have started on AR_TRANSACTION_HISTORY. I can still see the missing column issue. Let’s connect to discuss more about this.", "text": "Summary\nManhattan Scale data ingestion - part 2\n\n---\n\nDescription\nImport the data for ILS tables\n\nlocation_inventory\n\nAR_Transaction_History\n\nTransaction_History\n\n* Create extract meta records\n* Create & Run end to end pipelines in ADF\n\n---\n\nComments\nassigning to @user \n\n@user when you’re back, if you can let Aswini know what’s covered, that’ll be great.\n\nThanks!\n\nHi @user ,\n\nI have started on AR_TRANSACTION_HISTORY. I can still see the missing column issue. Let’s connect to discuss more about this.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 223}}
{"issue_key": "CSCI-425", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 2:36 PM", "updated": "29/Sep/25 8:10 AM", "labels": [], "summary": "Handover From Adeel to next person from DB team", "description": "Confluence doc to cover:\n\n* where things are up to\n** Modelling\n** STTM\n* -Parquet extract:-\n** -Extract-\n** -Upload-\n* What we need from this project", "acceptance_criteria": "", "comments": "Added doco to extract parquet files\n\nCC:- @user\n\nAdded steps to upload parquet extracts\n\nIssue split into:\n|CSCI-493|Handover From Adeel to next person from DB team - finalisation|", "text": "Summary\nHandover From Adeel to next person from DB team\n\n---\n\nDescription\nConfluence doc to cover:\n\n* where things are up to\n** Modelling\n** STTM\n* -Parquet extract:-\n** -Extract-\n** -Upload-\n* What we need from this project\n\n---\n\nComments\nAdded doco to extract parquet files\n\nCC:- @user\n\nAdded steps to upload parquet extracts\n\nIssue split into:\n|CSCI-493|Handover From Adeel to next person from DB team - finalisation|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 224}}
{"issue_key": "CSCI-424", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 2:27 PM", "updated": "29/Sep/25 8:10 AM", "labels": [], "summary": "Review Source-to-Target Map for Fact_Warehouse_Location_Inbound_Transactions", "description": "Source to target mapping for Fact_Warehouse_Location_Inbound_Transactions - filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "", "text": "Summary\nReview Source-to-Target Map for Fact_Warehouse_Location_Inbound_Transactions\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Inbound_Transactions - filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 225}}
{"issue_key": "CSCI-422", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 2:16 PM", "updated": "29/Sep/25 8:14 AM", "labels": [], "summary": "Capture the requirements for creating the Jumpbox for the developers (data engineers)", "description": "Capture the requirements for creating the Jumpbox for the developers (data engineers).\n\n*Key action items*\n\n* -On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/] - On prem and Cloud Team- *RITM0173487 (for reference)*-\n\nWhat’s needed is for the DNS forwarding from on-premise DNS servers, to the Azure privatelink DNS. See: [Resolve Azure and on-premises domains. | Microsoft Learn|https://learn.microsoft.com/en-us/azure/dns/private-resolver-hybrid-dns] IN PROGRESS- CR Created\n\n* -Collaborate with the EUC Team (CWR) to *create* Jumpbox IN PROGRESS- Onboarding of resources-\n* Give the Entra (AD) users access to the Jumpbox \n\n|*Snowflake_CW_AU_DataEngineers*|DEV_ROLE_DATA_ENG|\n\n* Configure firewalls (whitelisting) (On-Prem, Azure) as follows\n* VDI / Jumpbox network → Snowflake VNet *(servicenow tickets)*\n\n* *how many jumpboxes* we need? *no. of users 8*\n* Note: Ports to be allowed: 443, 80, 1433\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Configure VDI / Jumbox images to pre-install tools listed below: - The list has been reviewed and approved by EUC Team.\"}],\"attrs\":{\"localId\":\"a6a01a2f-604c-4540-b9f2-173ad2e43e57\",\"state\":\"DONE\"}}],\"attrs\":{\"localId\":\"0c33acf9-c722-4f42-8518-6b140130d8da\"}}\n{adf}\n* *Git for Windows* + *Git Credential Manager (GCM)* (Azure DevOps/GitHub SSO)\n* *Azure CLI* & *Azure PowerShell*\n* *AzCopy* (for ADLS/BLOB moves within Azure)\n* *VS Code* (primary IDE)\n** Extensions: _Snowflake SQL Tools_ (Snowflake), _SQLTools_ (optional), _Python_, _Pylance_, _Jupyter_, _YAML_, _GitLens_\n* *Snowflake CLIs/SDKs*\n** *Snow CLI* (preferred over SnowSQL)\n** _(Optional)_ *SnowSQL* legacy client (only if you still need it)\n* *Drivers*\n** *Snowflake ODBC* and *JDBC* drivers (for BI tools, dbt, notebooks)\n* *Languages / Runtimes*\n** *Python 3.11+* (via *Miniconda* or *pyenv-win*; standardize on conda envs)\n** *Java 17* (LTS) for Snowpark Java/Scala\n** *Node.js 20 LTS* (if you use Streamlit/Node helpers)\n* *Snowpark & data tooling (per environment)*\n** {{pipx}} and/or {{conda}}\n** {{snowflake-snowpark-python}}, {{pandas}}, {{pyarrow}}, {{jupyterlab}}, {{ipykernel}}\n\n* *Power BI Desktop* _(or Power BI Desktop – Optimized for Fabric if that’s your org standard)_\n* *Tabular Editor*\n** TE2 (free) for basic edits or *TE3* (licensed) for advanced modeling/CI\n* *DAX Studio* (performance tuning)\n* *ALM Toolkit* (schema compare/deployment)\n* *Power BI Report Builder* (if you produce paginated reports)\n* *Azure Data Studio* (lightweight SQL + notebooks)\n* *SSMS* (for on-prem SQL Server admin)\n* *Azure Storage Explorer* (browsing ADLS/BLOB via private endpoints)\n* *On-prem / misc drivers*\n** Microsoft SQL Server ODBC, Oracle/ODBC (if used), PostgreSQL ODBC (if used)", "acceptance_criteria": "* The data engineers are able to access Snowflake and other relevant applications via Jumpbox", "comments": "Have scheduled a meeting with @user and @user to discuss the requirements for setting up the Jumpbox. \n\ncc- @user\n\nHad a meeting with EUC Team and Network Team to discuss the requirements. Awaiting EUC Team resource to officially get allocated to the project to start the creation of Jumpbox.\n\nMeeting Minutes as per today’s meeting 18/09/2025:\n\n# [@Frank Perez|mailto:frank.perez@chemistwarehouse.com.au] will engage [@Jai Bagga|mailto:jai.bagga@chemistwarehouse.com.au] & IT Architecture Team (Neville) officially, to work on this requirement.\n# [@Jai Bagga|mailto:jai.bagga@chemistwarehouse.com.au] will start working on creation of Jumpbox, we have a Teams channel to discuss everything related to this requirement once officially allocated.\n# The team confirmed that number of VMs created will be *8* (depending on the number of users).\n# The AD Groups that will require access are as follows:\n\n|*Snowflake-CW-AU-Consultants*|DEV_ROLE_CONSULTANT|\n|*Snowflake-CW-AU-DataAnalysts*|UAT_ROLE_ANALYST|\n|*Snowflake-CW-AU-DataEngineers*|DEV_ROLE_DATA_ENG|\n|*Snowflake-CW-AU-PlatformEngineers*|DEV_ROLE_PLATFORM_ENG|\n\n \n\n# [@Diafy Gerardo|mailto:diafeked.gerardo@chemistwarehouse.com.au] from network team will be sharing the NIC ( Network Interface card) details to [@Jai Bagga|mailto:jai.bagga@chemistwarehouse.com.au]\n# [@Anjali Verma|mailto:anjali.verma@chemistwarehouse.com.au] to work with [@Diafy Gerardo|mailto:diafeked.gerardo@chemistwarehouse.com.au] to create Service Now Ticket for the DNS resolution and Firewall Whitelisting. CC- [@Chloe Tran|mailto:chloe.tran@chemistwarehouse.com.au] [@Eugene Paden|mailto:eugene@raybiztech.com] [@Raveendran Kumaravelu|mailto:raveendran.kumaravel@chemistwarehouse.com.au]\n# [@Jai Bagga|mailto:jai.bagga@chemistwarehouse.com.au] also confirmed and reviewed the tools/applications to be installed in the Jumpbox.\n\ncc- @user @user @user\n\nDependency on getting the EUC Team officially onboarded. Frank is working on this already.\n\ncc- @user @user @user @user @user\n\nThe change request has been raised for DNS forwarding from on-premise DNS servers to the Azure Privatelink DNS. \n\nWe have change freeze this week, so this CR will be actioned next week.\n\n@user @user @user @user\n\n@user Project Task Created PRJTASK0192572. David Strah is aware and actively on it.\n\nIssue split into:\n|CSCI-488|Capture the requirements for creating the Jumpbox for the developers (data engineers)|", "text": "Summary\nCapture the requirements for creating the Jumpbox for the developers (data engineers)\n\n---\n\nDescription\nCapture the requirements for creating the Jumpbox for the developers (data engineers).\n\n*Key action items*\n\n* -On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/] - On prem and Cloud Team- *RITM0173487 (for reference)*-\n\nWhat’s needed is for the DNS forwarding from on-premise DNS servers, to the Azure privatelink DNS. See: [Resolve Azure and on-premises domains. | Microsoft Learn|https://learn.microsoft.com/en-us/azure/dns/private-resolver-hybrid-dns] IN PROGRESS- CR Created\n\n* -Collaborate with the EUC Team (CWR) to *create* Jumpbox IN PROGRESS- Onboarding of resources-\n* Give the Entra (AD) users access to the Jumpbox \n\n|*Snowflake_CW_AU_DataEngineers*|DEV_ROLE_DATA_ENG|\n\n* Configure firewalls (whitelisting) (On-Prem, Azure) as follows\n* VDI / Jumpbox network → Snowflake VNet *(servicenow tickets)*\n\n* *how many jumpboxes* we need? *no. of users 8*\n* Note: Ports to be allowed: 443, 80, 1433\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Configure VDI / Jumbox images to pre-install tools listed below: - The list has been reviewed and approved by EUC Team.\"}],\"attrs\":{\"localId\":\"a6a01a2f-604c-4540-b9f2-173ad2e43e57\",\"state\":\"DONE\"}}],\"attrs\":{\"localId\":\"0c33acf9-c722-4f42-8518-6b140130d8da\"}}\n{adf}\n* *Git for Windows* + *Git Credential Manager (GCM)* (Azure DevOps/GitHub SSO)\n* *Azure CLI* & *Azure PowerShell*\n* *AzCopy* (for ADLS/BLOB moves within Azure)\n* *VS Code* (primary IDE)\n** Extensions: _Snowflake SQL Tools_ (Snowflake), _SQLTools_ (optional), _Python_, _Pylance_, _Jupyter_, _YAML_, _GitLens_\n* *Snowflake CLIs/SDKs*\n** *Snow CLI* (preferred over SnowSQL)\n** _(Optional)_ *SnowSQL* legacy client (only if you still need it)\n* *Drivers*\n** *Snowflake ODBC* and *JDBC* drivers (for BI tools, dbt, notebooks)\n* *Languages / Runtimes*\n** *Python 3.11+* (via *Miniconda* or *pyenv-win*; standardize on conda envs)\n** *Java 17* (LTS) for Snowpark Java/Scala\n** *Node.js 20 LTS* (if you use Streamlit/Node helpers)\n* *Snowpark & data tooling (per environment)*\n** {{pipx}} and/or {{conda}}\n** {{snowflake-snowpark-python}}, {{pandas}}, {{pyarrow}}, {{jupyterlab}}, {{ipykernel}}\n\n* *Power BI Desktop* _(or Power BI Desktop – Optimized for Fabric if that’s your org standard)_\n* *Tabular Editor*\n** TE2 (free) for basic edits or *TE3* (licensed) for advanced modeling/CI\n* *DAX Studio* (performance tuning)\n* *ALM Toolkit* (schema compare/deployment)\n* *Power BI Report Builder* (if you produce paginated reports)\n* *Azure Data Studio* (lightweight SQL + notebooks)\n* *SSMS* (for on-prem SQL Server admin)\n* *Azure Storage Explorer* (browsing ADLS/BLOB via private endpoints)\n* *On-prem / misc drivers*\n** Microsoft SQL Server ODBC, Oracle/ODBC (if used), PostgreSQL ODBC (if used)\n\n---\n\nAcceptance Criteria\n* The data engineers are able to access Snowflake and other relevant applications via Jumpbox\n\n---\n\nComments\nHave scheduled a meeting with @user and @user to discuss the requirements for setting up the Jumpbox. \n\ncc- @user\n\nHad a meeting with EUC Team and Network Team to discuss the requirements. Awaiting EUC Team resource to officially get allocated to the project to start the creation of Jumpbox.\n\nMeeting Minutes as per today’s meeting 18/09/2025:\n\n# [@Frank Perez|mailto:frank.perez@chemistwarehouse.com.au] will engage [@Jai Bagga|mailto:jai.bagga@chemistwarehouse.com.au] & IT Architecture Team (Neville) officially, to work on this requirement.\n# [@Jai Bagga|mailto:jai.bagga@chemistwarehouse.com.au] will start working on creation of Jumpbox, we have a Teams channel to discuss everything related to this requirement once officially allocated.\n# The team confirmed that number of VMs created will be *8* (depending on the number of users).\n# The AD Groups that will require access are as follows:\n\n|*Snowflake-CW-AU-Consultants*|DEV_ROLE_CONSULTANT|\n|*Snowflake-CW-AU-DataAnalysts*|UAT_ROLE_ANALYST|\n|*Snowflake-CW-AU-DataEngineers*|DEV_ROLE_DATA_ENG|\n|*Snowflake-CW-AU-PlatformEngineers*|DEV_ROLE_PLATFORM_ENG|\n\n \n\n# [@Diafy Gerardo|mailto:diafeked.gerardo@chemistwarehouse.com.au] from network team will be sharing the NIC ( Network Interface card) details to [@Jai Bagga|mailto:jai.bagga@chemistwarehouse.com.au]\n# [@Anjali Verma|mailto:anjali.verma@chemistwarehouse.com.au] to work with [@Diafy Gerardo|mailto:diafeked.gerardo@chemistwarehouse.com.au] to create Service Now Ticket for the DNS resolution and Firewall Whitelisting. CC- [@Chloe Tran|mailto:chloe.tran@chemistwarehouse.com.au] [@Eugene Paden|mailto:eugene@raybiztech.com] [@Raveendran Kumaravelu|mailto:raveendran.kumaravel@chemistwarehouse.com.au]\n# [@Jai Bagga|mailto:jai.bagga@chemistwarehouse.com.au] also confirmed and reviewed the tools/applications to be installed in the Jumpbox.\n\ncc- @user @user @user\n\nDependency on getting the EUC Team officially onboarded. Frank is working on this already.\n\ncc- @user @user @user @user @user\n\nThe change request has been raised for DNS forwarding from on-premise DNS servers to the Azure Privatelink DNS. \n\nWe have change freeze this week, so this CR will be actioned next week.\n\n@user @user @user @user\n\n@user Project Task Created PRJTASK0192572. David Strah is aware and actively on it.\n\nIssue split into:\n|CSCI-488|Capture the requirements for creating the Jumpbox for the developers (data engineers)|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 226}}
{"issue_key": "CSCI-421", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Sep/25 9:22 AM", "updated": "12/Sep/25 6:36 PM", "labels": [], "summary": "Issue in parquet file load", "description": "Getting variable columns in Parquet file load for historical data for ILS / Apriss tables", "acceptance_criteria": "", "comments": "", "text": "Summary\nIssue in parquet file load\n\n---\n\nDescription\nGetting variable columns in Parquet file load for historical data for ILS / Apriss tables", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 227}}
{"issue_key": "CSCI-420", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 6:31 PM", "updated": "07/Oct/25 2:11 PM", "labels": [], "summary": "APPRISS Modelling/Transformation", "description": "h2. Summary\n\nWe need to identify any additional objects required for APPRISS that have not yet been ingested into EDP Snowflake.\n\nh2. Context\n\nMost tables from the StockDb and TransactionStorage databases have already been ingested. This issue focuses on ensuring that all necessary objects for APPRISS are accounted for.\n\nh2. Other information\n\nN/A\n\nDatabases covered:\n\n* transactionstorage\n* stockdb\n* general_reference", "acceptance_criteria": "We need to set up pipelines for initial and delta loads for the following APPRISS tables\n\n* Extract meta created\n* Linked services created\n* pipelines set up\n* initial load tested\n* delta load tested\n* pipeline run from end to end", "comments": "cc @user\n\nHi @user , \n\nCurrently I am awaiting access to confluence documentation created by Amit. Thanks for raising the access request to get started.\n\nHi @user, @user \n\nWe need the source to target transformation mapping document (which is worked by Amit) to get started. Also, Amit is currently creating the APPRISS data mart (subset of enterprise data model) if APPRISS needs to be prioritized.\n\n FYI.. @user , @user @user\n\nHey @user , @user ,\n\nWhat is the actual purpose of this ticket.\n\nMy understanding is @user has completed data modelling work, is this correct? This ticket is to just review with Amit and NOT to do any actual DATA OUT build yeh?\n\nThanks,\n\nHarrison\n\nHi @user,\n\nYour understanding is correct. This is to do initial review and start putting initial questions. No build work at the moment. At the moment Amit has provided the mapping sheet for facts ( Fact Sales, electronic payments etc) but not the dimensions yet. Also, he is yet to provide APPRISS data model for us to get started.\n\nFYI.. @user", "text": "Summary\nAPPRISS Modelling/Transformation\n\n---\n\nDescription\nh2. Summary\n\nWe need to identify any additional objects required for APPRISS that have not yet been ingested into EDP Snowflake.\n\nh2. Context\n\nMost tables from the StockDb and TransactionStorage databases have already been ingested. This issue focuses on ensuring that all necessary objects for APPRISS are accounted for.\n\nh2. Other information\n\nN/A\n\nDatabases covered:\n\n* transactionstorage\n* stockdb\n* general_reference\n\n---\n\nAcceptance Criteria\nWe need to set up pipelines for initial and delta loads for the following APPRISS tables\n\n* Extract meta created\n* Linked services created\n* pipelines set up\n* initial load tested\n* delta load tested\n* pipeline run from end to end\n\n---\n\nComments\ncc @user\n\nHi @user , \n\nCurrently I am awaiting access to confluence documentation created by Amit. Thanks for raising the access request to get started.\n\nHi @user, @user \n\nWe need the source to target transformation mapping document (which is worked by Amit) to get started. Also, Amit is currently creating the APPRISS data mart (subset of enterprise data model) if APPRISS needs to be prioritized.\n\n FYI.. @user , @user @user\n\nHey @user , @user ,\n\nWhat is the actual purpose of this ticket.\n\nMy understanding is @user has completed data modelling work, is this correct? This ticket is to just review with Amit and NOT to do any actual DATA OUT build yeh?\n\nThanks,\n\nHarrison\n\nHi @user,\n\nYour understanding is correct. This is to do initial review and start putting initial questions. No build work at the moment. At the moment Amit has provided the mapping sheet for facts ( Fact Sales, electronic payments etc) but not the dimensions yet. Also, he is yet to provide APPRISS data model for us to get started.\n\nFYI.. @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 228}}
{"issue_key": "CSCI-419", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 6:28 PM", "updated": "07/Oct/25 9:54 AM", "labels": [], "summary": "APPRISS Objects ingestion - Additional tables Ingestion Requirement", "description": "h2. Summary\n\nWe need to identify any additional objects required for APPRISS that have not yet been ingested into EDP Snowflake.\n\nh2. Context\n\nMost tables from the StockDb and TransactionStorage databases have already been ingested. This issue focuses on ensuring that all necessary objects for APPRISS are accounted for.\n\nh2. Other information\n\nN/A\n\nDatabases covered:\n\n* transactionstorage\n* stockdb\n* generalstorage", "acceptance_criteria": "We need to set up pipelines for initial and delta loads for the following APPRISS tables\n\n* -Extract meta created-\n* -Linked services created-\n* pipelines set up\n* initial load tested\n* delta load tested\n* pipeline run from end to end", "comments": "transactionstorage\nstockdb\ngeneralstorage\n\nHi @user ,\n\nThe historic data ingestion for below 2 tables are done through parquet files. However, There is an ongoing issue with ingestion framework where some of the columns getting missed if they contain null in first row. So, at the moment we have added the missing columns manually. Hence there is a need to test these columns getting populated properly when we manually add these in snowflake tables and views. \n\n|TDB14|TransactionStorage|TRANSACTIONS_ELECTRONICPAYMENTS|\n\n|TDB14|TransactionStorage|TransactionAuditSaleActivityLogHistory|\n\n@user following up with @user about details for additional tables today.\n\n@user cc @user\n\nHi @user ,\n\nAs discussed yesterday, please provide the list of additional tables required after your review with Bhavya and team.\n\nFYI. @user, @user\n\nHi @user,\n\nI tested the end to end load for TRANSACTIONS_ELECTRONICPAYMENTS & TransactionAuditSaleActivityLogHistory after the issue of missing column got fixed. \n\n@user FYI.. There was an ongoing issue where in the historic data load, some of the columns were getting missed in snowflake tables as these columns were empty in the first row. Now the issue is resolved and hence the need for end to end testing.\n\n@user , Hopefully the final remaining list of tables have been finalized for me to get started today.\n\n@user - have these tables been finalised?\n\nThanks\n\nHi @user , @user\n\nAmit had indicated that there are some more tables to be ingested while he was doing the mapping documents. However, he is yet to provide the details if any table is missing and needs to be ingested.\n\nHi @user,\n\nI have received the additional table names from stockDb for ingestion from Amit. However, he advised us to double verify the server details with Bhavya/Adeel.\n\nHi @user / @user ,\n\nCurrently we are ingesting the stock DB data from following server. Amit asked us to verify if this is the right server as he he is not very sure if this is the right one or not. FYI, the below one was identified much earlier (Adeel is aware of). Please double confirm this information.\n\n|Server|Database|Table|Total Row Count | Table Size(MB) | Table Size(GB) |\n|TDB08AX2012|StockDb|BranchInfoHierarchy|899|*|*|\n|TDB08AX2012|StockDb|BuyByPg|275|*|*|\n|TDB08AX2012|StockDb|Buyers|46|*|*|\n|TDB08AX2012|StockDb|Catalogues|12,483|1|*|\n|TDB08AX2012|StockDb|DimensionProductView|6|*|*|\n|TDB08AX2012|StockDb|Dimensions|407,993|55|*|\n|TDB08AX2012|StockDb|DimensionType|61|*|*|\n|TDB08AX2012|StockDb|DimensionValues|12,807|1|*|\n|TDB08AX2012|StockDb|LockieMainGroups|56|*|*|\n|TDB08AX2012|StockDb|MultiBuy|18,066|1|*|\n|TDB08AX2012|StockDb|MultiBuyTriggers|18,359|1|*|\n|TDB08AX2012|StockDb|ProductGroup|168| | |\n|TDB08AX2012|StockDb|ProductNetworkCosts|126,223|23|*|\n|TDB08AX2012|StockDb|ProductNetworkCostsHistory|136,546,828|16,037|15|\n|TDB08AX2012|StockDb|Products|196,344|103|*|\n|TDB08AX2012|StockDb|ProductsDrugMatch|1,535|*|*|\n|TDB08AX2012|StockDb|SOHAdjustmentTypes|22| | |\n|TDB08AX2012|StockDb|SubGroup|275|*|*|\n|TDB08AX2012|StockDb|Supplier|294,671|71|*|\n|TDB08AX2012|StockDb|SupplierDetails|2,837|*|*|\n|TDB08AX2012|StockDb|StoreStaffDetails|97,048| | |\n|TDB08AX2012|StockDb|BranchInfoGlobal|760| | |\n|TDB08AX2012|StockDb|CatalogueItems| | | |\n|TDB08AX2012|StockDb|CatalogueMeta| | | |\n|TDB08AX2012|StockDb|CatGroup| | | |\n|TDB08AX2012|StockDb|CatalogueTypeDefinition| | | |\n\nFYI .. @user\n\nHi @user/ @user,\n\nWe got confirmation from @user yesterday saying the server details are correct mentioned earlier in this ticket. So we will be adding these new tables from this server only.\n\n @user FYI..", "text": "Summary\nAPPRISS Objects ingestion - Additional tables Ingestion Requirement\n\n---\n\nDescription\nh2. Summary\n\nWe need to identify any additional objects required for APPRISS that have not yet been ingested into EDP Snowflake.\n\nh2. Context\n\nMost tables from the StockDb and TransactionStorage databases have already been ingested. This issue focuses on ensuring that all necessary objects for APPRISS are accounted for.\n\nh2. Other information\n\nN/A\n\nDatabases covered:\n\n* transactionstorage\n* stockdb\n* generalstorage\n\n---\n\nAcceptance Criteria\nWe need to set up pipelines for initial and delta loads for the following APPRISS tables\n\n* -Extract meta created-\n* -Linked services created-\n* pipelines set up\n* initial load tested\n* delta load tested\n* pipeline run from end to end\n\n---\n\nComments\ntransactionstorage\nstockdb\ngeneralstorage\n\nHi @user ,\n\nThe historic data ingestion for below 2 tables are done through parquet files. However, There is an ongoing issue with ingestion framework where some of the columns getting missed if they contain null in first row. So, at the moment we have added the missing columns manually. Hence there is a need to test these columns getting populated properly when we manually add these in snowflake tables and views. \n\n|TDB14|TransactionStorage|TRANSACTIONS_ELECTRONICPAYMENTS|\n\n|TDB14|TransactionStorage|TransactionAuditSaleActivityLogHistory|\n\n@user following up with @user about details for additional tables today.\n\n@user cc @user\n\nHi @user ,\n\nAs discussed yesterday, please provide the list of additional tables required after your review with Bhavya and team.\n\nFYI. @user, @user\n\nHi @user,\n\nI tested the end to end load for TRANSACTIONS_ELECTRONICPAYMENTS & TransactionAuditSaleActivityLogHistory after the issue of missing column got fixed. \n\n@user FYI.. There was an ongoing issue where in the historic data load, some of the columns were getting missed in snowflake tables as these columns were empty in the first row. Now the issue is resolved and hence the need for end to end testing.\n\n@user , Hopefully the final remaining list of tables have been finalized for me to get started today.\n\n@user - have these tables been finalised?\n\nThanks\n\nHi @user , @user\n\nAmit had indicated that there are some more tables to be ingested while he was doing the mapping documents. However, he is yet to provide the details if any table is missing and needs to be ingested.\n\nHi @user,\n\nI have received the additional table names from stockDb for ingestion from Amit. However, he advised us to double verify the server details with Bhavya/Adeel.\n\nHi @user / @user ,\n\nCurrently we are ingesting the stock DB data from following server. Amit asked us to verify if this is the right server as he he is not very sure if this is the right one or not. FYI, the below one was identified much earlier (Adeel is aware of). Please double confirm this information.\n\n|Server|Database|Table|Total Row Count | Table Size(MB) | Table Size(GB) |\n|TDB08AX2012|StockDb|BranchInfoHierarchy|899|*|*|\n|TDB08AX2012|StockDb|BuyByPg|275|*|*|\n|TDB08AX2012|StockDb|Buyers|46|*|*|\n|TDB08AX2012|StockDb|Catalogues|12,483|1|*|\n|TDB08AX2012|StockDb|DimensionProductView|6|*|*|\n|TDB08AX2012|StockDb|Dimensions|407,993|55|*|\n|TDB08AX2012|StockDb|DimensionType|61|*|*|\n|TDB08AX2012|StockDb|DimensionValues|12,807|1|*|\n|TDB08AX2012|StockDb|LockieMainGroups|56|*|*|\n|TDB08AX2012|StockDb|MultiBuy|18,066|1|*|\n|TDB08AX2012|StockDb|MultiBuyTriggers|18,359|1|*|\n|TDB08AX2012|StockDb|ProductGroup|168| | |\n|TDB08AX2012|StockDb|ProductNetworkCosts|126,223|23|*|\n|TDB08AX2012|StockDb|ProductNetworkCostsHistory|136,546,828|16,037|15|\n|TDB08AX2012|StockDb|Products|196,344|103|*|\n|TDB08AX2012|StockDb|ProductsDrugMatch|1,535|*|*|\n|TDB08AX2012|StockDb|SOHAdjustmentTypes|22| | |\n|TDB08AX2012|StockDb|SubGroup|275|*|*|\n|TDB08AX2012|StockDb|Supplier|294,671|71|*|\n|TDB08AX2012|StockDb|SupplierDetails|2,837|*|*|\n|TDB08AX2012|StockDb|StoreStaffDetails|97,048| | |\n|TDB08AX2012|StockDb|BranchInfoGlobal|760| | |\n|TDB08AX2012|StockDb|CatalogueItems| | | |\n|TDB08AX2012|StockDb|CatalogueMeta| | | |\n|TDB08AX2012|StockDb|CatGroup| | | |\n|TDB08AX2012|StockDb|CatalogueTypeDefinition| | | |\n\nFYI .. @user\n\nHi @user/ @user,\n\nWe got confirmation from @user yesterday saying the server details are correct mentioned earlier in this ticket. So we will be adding these new tables from this server only.\n\n @user FYI..", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 229}}
{"issue_key": "CSCI-418", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 6:25 PM", "updated": "20/Oct/25 7:56 PM", "labels": [], "summary": "CWR Data Ingestion Architecture walk through", "description": "Take Mike through the architecture in CWR (Snowflake, ADF, Altida)", "acceptance_criteria": "", "comments": "", "text": "Summary\nCWR Data Ingestion Architecture walk through\n\n---\n\nDescription\nTake Mike through the architecture in CWR (Snowflake, ADF, Altida)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 230}}
{"issue_key": "CSCI-417", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 6:25 PM", "updated": "23/Sep/25 9:48 AM", "labels": [], "summary": "AA - Detailed Design Documentation", "description": "", "acceptance_criteria": "", "comments": "with @user at the moment.\n\n[https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B2B1D2A9D-799B-4326-B9CF-52840FAFA912%7D&file=CWR%20-%20EDP%20Solution%20Architecture%20%20-%20Detailed%20Design%20v.01.docx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B2B1D2A9D-799B-4326-B9CF-52840FAFA912%7D&file=CWR%20-%20EDP%20Solution%20Architecture%20%20-%20Detailed%20Design%20v.01.docx&action=default&mobileredirect=true]", "text": "Summary\nAA - Detailed Design Documentation\n\n---\n\nComments\nwith @user at the moment.\n\n[https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B2B1D2A9D-799B-4326-B9CF-52840FAFA912%7D&file=CWR%20-%20EDP%20Solution%20Architecture%20%20-%20Detailed%20Design%20v.01.docx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B2B1D2A9D-799B-4326-B9CF-52840FAFA912%7D&file=CWR%20-%20EDP%20Solution%20Architecture%20%20-%20Detailed%20Design%20v.01.docx&action=default&mobileredirect=true]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 231}}
{"issue_key": "CSCI-416", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 6:23 PM", "updated": "25/Sep/25 9:49 AM", "labels": [], "summary": "EDP Architecture - Detailed Solution Design doco", "description": "Provide a document of the EDP:\n\n* High-level logical architecture (Interim Solution)\n* High-level logical architecture (Future State Solution)\n* Detailed design components of Azure, Snowflake across environments. \n\n*Agreed Timeline:*\n\n* *Completed Detailed Design:* 19/09 (this Friday)\n* *Alan Review + CT & HW Discussions:* 26/09 (end of next week)\n* *Detailed Work Breakdown Plan:* 03/10", "acceptance_criteria": "* -link to documentation-", "comments": "Walk-through meeting with Alan, Harrison and Xavier yesterday. We are largely aligned on the architecture and agreed on the timeline. \n\nChloe to add the below information: \n\n* High level transition plan and effort estimation from DLA Transformation to dbt Data Tranformation.\n* Design Snowflake’s Organisation and Account Structure for Chemist Warehouse International.\n* Azure DevOps GIT Repos (how many repos and what are they?)\n* Data Engineering best practices (incl. naming convention in Snowflake, ADF, Azure DevOps)\n* Attach Altis DLA handbook.\n\nThe doc is being reviewed by Alan, Xavier and Harrison. \n\nInitial feedback by Xavier:\n\n* To include Snowflake OpenFlow and Snowflake Intelligence as part of the platform solution design for Future State", "text": "Summary\nEDP Architecture - Detailed Solution Design doco\n\n---\n\nDescription\nProvide a document of the EDP:\n\n* High-level logical architecture (Interim Solution)\n* High-level logical architecture (Future State Solution)\n* Detailed design components of Azure, Snowflake across environments. \n\n*Agreed Timeline:*\n\n* *Completed Detailed Design:* 19/09 (this Friday)\n* *Alan Review + CT & HW Discussions:* 26/09 (end of next week)\n* *Detailed Work Breakdown Plan:* 03/10\n\n---\n\nAcceptance Criteria\n* -link to documentation-\n\n---\n\nComments\nWalk-through meeting with Alan, Harrison and Xavier yesterday. We are largely aligned on the architecture and agreed on the timeline. \n\nChloe to add the below information: \n\n* High level transition plan and effort estimation from DLA Transformation to dbt Data Tranformation.\n* Design Snowflake’s Organisation and Account Structure for Chemist Warehouse International.\n* Azure DevOps GIT Repos (how many repos and what are they?)\n* Data Engineering best practices (incl. naming convention in Snowflake, ADF, Azure DevOps)\n* Attach Altis DLA handbook.\n\nThe doc is being reviewed by Alan, Xavier and Harrison. \n\nInitial feedback by Xavier:\n\n* To include Snowflake OpenFlow and Snowflake Intelligence as part of the platform solution design for Future State", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 232}}
{"issue_key": "CSCI-415", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 6:23 PM", "updated": "12/Sep/25 6:25 PM", "labels": [], "summary": "CT - Detailed Design Documentation", "description": "", "acceptance_criteria": "", "comments": "hey @user i think this would be the same as [https://sigmahealthcare.atlassian.net/browse/CSCI-148|https://sigmahealthcare.atlassian.net/browse/CSCI-148] but just wanted to double check", "text": "Summary\nCT - Detailed Design Documentation\n\n---\n\nComments\nhey @user i think this would be the same as [https://sigmahealthcare.atlassian.net/browse/CSCI-148|https://sigmahealthcare.atlassian.net/browse/CSCI-148] but just wanted to double check", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 233}}
{"issue_key": "CSCI-414", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 6:10 PM", "updated": "10/Oct/25 3:00 PM", "labels": [], "summary": "Infrastructures As Code in EDP - Walk-through and Documentation", "description": "h2. Summary\n\nEugene will conduct a walk-through of the Infrastructure As Code (IAC) implementation with Alan. This is scheduled for Tuesday, 17th September. Documentation of the deployment process for Azure services and environments is also required.\n\nh2. Context\n\nThe issue involves the implementation of Infrastructure As Code within the EDP. It includes a scheduled meeting for a detailed walk-through and the need for comprehensive documentation.\n\nh2. Acceptance criteria\n\n* Eugene to walk through IAC implementation with Alan on Tuesday, 17th September.\n* Document the deployment of Azure services and environments.\n\nh2. Other information\n\nThe documentation has been created at [https://dev.azure.com/MyChemist/Enterprise Data Platform Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5263/Snowflake-Subscription|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5263/Snowflake-Subscription]", "acceptance_criteria": "Infrastructure As Code implementation is documented by Eugene and reviewed by Alan and Mike", "comments": "@user\n\n@user i don’t have access to the Sharepoint. Also, i think it would be best to document the IAC in the code itself and create a separate readme file. Same readme file can be stored in Wiki.\n\nlinking this to [Feature 209127 Document Environment IAC|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/209127]\n\nThanks @user\n\n----\n\nh3. 🔧 *Architecture Overview*\n\n* Snowflake deployed securely on Microsoft Azure using a hub-spoke network model.\n* Supports Dev, UAT, and Prod environments with private connectivity and centralised monitoring.\n* Designed for scalability, compliance, and high availability.\n\n----\n\nh3. 🛡️ *Security Principles*\n\n* Zero Trust Network: All traffic via private endpoints.\n* Network isolation with dedicated VNets and controlled peering.\n* TLS 1.2+ encryption for data at rest and in transit.\n* Managed identities for secure service-to-service authentication.\n\n----\n\nh3. ⚙️ *High Availability & Governance*\n\n* Geo-redundant storage (RAGRS) across regions.\n* Hub-spoke design enables resilient failover.\n* Resource tagging, management locks, and Azure Policy for governance.\n* Centralised monitoring via Log Analytics at the management group level.\n\n----\n\nh3. 🌐 *Network Architecture*\n\n* Hub subscription connects to Dev, UAT, Prod, and Snowflake via VNet peering.\n* Snowflake subscription includes dedicated subnets for endpoints, storage, and VMs.\n* Cross-subscription private endpoints enable secure access to Snowflake from all environments.\n\n----\n\nh3. 🗂️ *Subnet & Resource Group Design*\n\n* Subnets allocated for Snowflake, storage, VMs, and future expansion.\n* Separate resource groups for application, network, and security components.\n* Key Vault integrated for secrets and certificates.\n\n----\n\nh3. 🔄 *Data Flow & Integration*\n\n* Data ingested into environment-specific Data Lakes (Dev/UAT/Prod).\n* Processed via Azure Data Factory and Self-hosted Integration Runtime (SHIR).\n* Integrated into shared Snowflake warehouse for analytics and reporting.\n* Exported to local storage for specific use cases.\n\n----\n\nh3. 🔐 *Network Security Controls*\n\n* NSGs enforce strict traffic rules (allow trusted sources, deny all else).\n* Private endpoints for Snowflake, storage, and Key Vault.\n* Hub-managed private DNS zones with automatic registration and conditional forwarding.\n\n----\n\nh3. 👤 *Identity & Access Management*\n\n* System- and user-assigned managed identities for secure access.\n* RBAC roles assigned for contributors, network admins, and security teams.\n* Key Vault access managed via identities and private endpoints.\n\n----\n\nh3. 📦 *Storage Architecture*\n\n* Local Data Lake Storage Gen2 for staging and exports.\n* Environment-specific Data Lakes for ingestion and transformation.\n* RAGRS replication and private endpoint-only access.\n* Managed identities enable cross-subscription access to Snowflake.\n\n----\n\nh3. 📊 *Monitoring & Observability*\n\n* Azure Policy enforces diagnostic settings and Log Analytics across all resources.\n* Monitors network flow logs, storage operations, Key Vault access, and Snowflake connectivity.\n* Centralised logging reduces complexity and ensures compliance.", "text": "Summary\nInfrastructures As Code in EDP - Walk-through and Documentation\n\n---\n\nDescription\nh2. Summary\n\nEugene will conduct a walk-through of the Infrastructure As Code (IAC) implementation with Alan. This is scheduled for Tuesday, 17th September. Documentation of the deployment process for Azure services and environments is also required.\n\nh2. Context\n\nThe issue involves the implementation of Infrastructure As Code within the EDP. It includes a scheduled meeting for a detailed walk-through and the need for comprehensive documentation.\n\nh2. Acceptance criteria\n\n* Eugene to walk through IAC implementation with Alan on Tuesday, 17th September.\n* Document the deployment of Azure services and environments.\n\nh2. Other information\n\nThe documentation has been created at [https://dev.azure.com/MyChemist/Enterprise Data Platform Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5263/Snowflake-Subscription|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5263/Snowflake-Subscription]\n\n---\n\nAcceptance Criteria\nInfrastructure As Code implementation is documented by Eugene and reviewed by Alan and Mike\n\n---\n\nComments\n@user\n\n@user i don’t have access to the Sharepoint. Also, i think it would be best to document the IAC in the code itself and create a separate readme file. Same readme file can be stored in Wiki.\n\nlinking this to [Feature 209127 Document Environment IAC|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/209127]\n\nThanks @user\n\n----\n\nh3. 🔧 *Architecture Overview*\n\n* Snowflake deployed securely on Microsoft Azure using a hub-spoke network model.\n* Supports Dev, UAT, and Prod environments with private connectivity and centralised monitoring.\n* Designed for scalability, compliance, and high availability.\n\n----\n\nh3. 🛡️ *Security Principles*\n\n* Zero Trust Network: All traffic via private endpoints.\n* Network isolation with dedicated VNets and controlled peering.\n* TLS 1.2+ encryption for data at rest and in transit.\n* Managed identities for secure service-to-service authentication.\n\n----\n\nh3. ⚙️ *High Availability & Governance*\n\n* Geo-redundant storage (RAGRS) across regions.\n* Hub-spoke design enables resilient failover.\n* Resource tagging, management locks, and Azure Policy for governance.\n* Centralised monitoring via Log Analytics at the management group level.\n\n----\n\nh3. 🌐 *Network Architecture*\n\n* Hub subscription connects to Dev, UAT, Prod, and Snowflake via VNet peering.\n* Snowflake subscription includes dedicated subnets for endpoints, storage, and VMs.\n* Cross-subscription private endpoints enable secure access to Snowflake from all environments.\n\n----\n\nh3. 🗂️ *Subnet & Resource Group Design*\n\n* Subnets allocated for Snowflake, storage, VMs, and future expansion.\n* Separate resource groups for application, network, and security components.\n* Key Vault integrated for secrets and certificates.\n\n----\n\nh3. 🔄 *Data Flow & Integration*\n\n* Data ingested into environment-specific Data Lakes (Dev/UAT/Prod).\n* Processed via Azure Data Factory and Self-hosted Integration Runtime (SHIR).\n* Integrated into shared Snowflake warehouse for analytics and reporting.\n* Exported to local storage for specific use cases.\n\n----\n\nh3. 🔐 *Network Security Controls*\n\n* NSGs enforce strict traffic rules (allow trusted sources, deny all else).\n* Private endpoints for Snowflake, storage, and Key Vault.\n* Hub-managed private DNS zones with automatic registration and conditional forwarding.\n\n----\n\nh3. 👤 *Identity & Access Management*\n\n* System- and user-assigned managed identities for secure access.\n* RBAC roles assigned for contributors, network admins, and security teams.\n* Key Vault access managed via identities and private endpoints.\n\n----\n\nh3. 📦 *Storage Architecture*\n\n* Local Data Lake Storage Gen2 for staging and exports.\n* Environment-specific Data Lakes for ingestion and transformation.\n* RAGRS replication and private endpoint-only access.\n* Managed identities enable cross-subscription access to Snowflake.\n\n----\n\nh3. 📊 *Monitoring & Observability*\n\n* Azure Policy enforces diagnostic settings and Log Analytics across all resources.\n* Monitors network flow logs, storage operations, Key Vault access, and Snowflake connectivity.\n* Centralised logging reduces complexity and ensures compliance.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 234}}
{"issue_key": "CSCI-413", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 6:09 PM", "updated": "12/Sep/25 2:46 PM", "labels": [], "summary": "ExpressRoute Monitoring - (15 Sep - 29th Sep)", "description": "Monitor ExpressRoute throughput utilisation", "acceptance_criteria": "# *Metric Visibility:*\n#* ExpressRoute connection metrics (e.g., Ingress/Egress throughput) are visible in Azure Monitor or Network Insights.\n#* Bandwidth usage can be correlated with ADF pipeline execution timelines.\n# *Alerting Rules:*\n#* Alerts are configured to trigger when bandwidth usage exceeds a defined threshold (e.g., 70% of provisioned bandwidth).\n#* Alerts include pipeline metadata (e.g., pipeline name, run ID) when possible, to trace high-usage executions.\n# *Log Collection:*\n#* Network bandwidth logs (e.g., from Network Performance Monitor or Azure Network Watcher) are retained for a minimum of 30 days.\n#* ADF pipeline run logs include integration runtime IPs or endpoints for cross-reference.\n# *Validation Testing:*\n#* At least one high-throughput ADF pipeline is executed to validate that bandwidth spikes are captured and logged as expected.\n#* Test scenarios simulate concurrent pipeline runs to verify system responsiveness and alert thresholds.\n# *Governance & Review:*\n#* A monthly review is conducted to assess if bandwidth consumption patterns require scaling up/down of ExpressRoute or ADF optimization.\n#* A weekly review to be initially done on until Oct 2025.", "comments": "", "text": "Summary\nExpressRoute Monitoring - (15 Sep - 29th Sep)\n\n---\n\nDescription\nMonitor ExpressRoute throughput utilisation\n\n---\n\nAcceptance Criteria\n# *Metric Visibility:*\n#* ExpressRoute connection metrics (e.g., Ingress/Egress throughput) are visible in Azure Monitor or Network Insights.\n#* Bandwidth usage can be correlated with ADF pipeline execution timelines.\n# *Alerting Rules:*\n#* Alerts are configured to trigger when bandwidth usage exceeds a defined threshold (e.g., 70% of provisioned bandwidth).\n#* Alerts include pipeline metadata (e.g., pipeline name, run ID) when possible, to trace high-usage executions.\n# *Log Collection:*\n#* Network bandwidth logs (e.g., from Network Performance Monitor or Azure Network Watcher) are retained for a minimum of 30 days.\n#* ADF pipeline run logs include integration runtime IPs or endpoints for cross-reference.\n# *Validation Testing:*\n#* At least one high-throughput ADF pipeline is executed to validate that bandwidth spikes are captured and logged as expected.\n#* Test scenarios simulate concurrent pipeline runs to verify system responsiveness and alert thresholds.\n# *Governance & Review:*\n#* A monthly review is conducted to assess if bandwidth consumption patterns require scaling up/down of ExpressRoute or ADF optimization.\n#* A weekly review to be initially done on until Oct 2025.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 235}}
{"issue_key": "CSCI-412", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 4:00 PM", "updated": "24/Oct/25 1:02 PM", "labels": [], "summary": "Create User Guide for using ServiceNow to create tickets", "description": "Create user guide that will alllow staff to submit servicenow tickets", "acceptance_criteria": "* confluence page for Servicenow access\n* Peer reviewed with @user", "comments": "hi @user - I recommend to do this asap to avoid further communication issues amongst teams re: networking drive whitelisting re [https://sigmahealthcare.atlassian.net/browse/CSCI-458|https://sigmahealthcare.atlassian.net/browse/CSCI-458]\n\nHey @user \nCan you please prioritize this? I would suggest you note down the step when you are raising any new serviceNow ticket\n\nThanks", "text": "Summary\nCreate User Guide for using ServiceNow to create tickets\n\n---\n\nDescription\nCreate user guide that will alllow staff to submit servicenow tickets\n\n---\n\nAcceptance Criteria\n* confluence page for Servicenow access\n* Peer reviewed with @user\n\n---\n\nComments\nhi @user - I recommend to do this asap to avoid further communication issues amongst teams re: networking drive whitelisting re [https://sigmahealthcare.atlassian.net/browse/CSCI-458|https://sigmahealthcare.atlassian.net/browse/CSCI-458]\n\nHey @user \nCan you please prioritize this? I would suggest you note down the step when you are raising any new serviceNow ticket\n\nThanks", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 236}}
{"issue_key": "CSCI-411", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 3:58 PM", "updated": "12/Sep/25 2:24 PM", "labels": [], "summary": "Test task 1", "description": "test text", "acceptance_criteria": "", "comments": "", "text": "Summary\nTest task 1\n\n---\n\nDescription\ntest text", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 237}}
{"issue_key": "CSCI-409", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 11:51 AM", "updated": "02/Oct/25 10:03 AM", "labels": [], "summary": "MergeCo Conformed Reporting - Draft Dashboard Wireframe", "description": "Once dashboard requirements are set and confirmed by the business, need to create a dashboard wireframe.\n\nThe wireframe will outline how the dashboard will look, what functionality will be available (eg: what filters available and any other interactive features).\n\nWireframe will also include definition of metrics within the dashboard and any cadences around dashboard refresh, and email subscriptions.", "acceptance_criteria": "", "comments": "@user for availability wireframe initially\n\nIssue split into:\n|CSCI-486|MergeCo Conformed Reporting - Draft Dashboard Development|\n|CSCI-487|MergeCo Conformed Reporting - Draft Dashboard Testing|\n\nMarked as done.", "text": "Summary\nMergeCo Conformed Reporting - Draft Dashboard Wireframe\n\n---\n\nDescription\nOnce dashboard requirements are set and confirmed by the business, need to create a dashboard wireframe.\n\nThe wireframe will outline how the dashboard will look, what functionality will be available (eg: what filters available and any other interactive features).\n\nWireframe will also include definition of metrics within the dashboard and any cadences around dashboard refresh, and email subscriptions.\n\n---\n\nComments\n@user for availability wireframe initially\n\nIssue split into:\n|CSCI-486|MergeCo Conformed Reporting - Draft Dashboard Development|\n|CSCI-487|MergeCo Conformed Reporting - Draft Dashboard Testing|\n\nMarked as done.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 238}}
{"issue_key": "CSCI-408", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 11:49 AM", "updated": "29/Sep/25 8:23 AM", "labels": [], "summary": "MergeCo Conformed Reporting - Draft Solution Design", "description": "Create draft of the solution design for the Merge Co Conformed reporting.\n\nJust needs to be a one-page document, outlining *how* we will build and ingest the data for this reporting.\n\nIncludes:\n\n* What grain of data required for each metric\n* How we will push data for each environment into a join environment\n* Merging them into a common dimension", "acceptance_criteria": "", "comments": "", "text": "Summary\nMergeCo Conformed Reporting - Draft Solution Design\n\n---\n\nDescription\nCreate draft of the solution design for the Merge Co Conformed reporting.\n\nJust needs to be a one-page document, outlining *how* we will build and ingest the data for this reporting.\n\nIncludes:\n\n* What grain of data required for each metric\n* How we will push data for each environment into a join environment\n* Merging them into a common dimension", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 239}}
{"issue_key": "CSCI-407", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 11:45 AM", "updated": "29/Sep/25 8:23 AM", "labels": [], "summary": "MergeCo Conformed Reporting - Identify Source for SC Operational Metrics", "description": "Identify which specific tables/views to use for Merge Co reporting from a CW domain for the below Operational metrics:\n\n* Order Qty\n* Confirmed Qty\n* Invoiced Qty\n* ATP %\n* DIF %\n* Cust Impact %\n* Availability Opportunity Qty/$\n* DC Opportunity Qty\n* DOT %\n\nIdeal grain is at the: \n\nper day, per DC, per product level\n\nNeed to work out:\n\n* Which tables best to extract from\n* If tables can’t be identified, why? Is it not available at all? Or is there an issue with the grain of data? Or access to data?", "acceptance_criteria": "", "comments": "|Sigma Metrics|CW Equivalent Available?|PBI Report|Notes|Table|Notes 2|\n|Order Qty|Y*| |no clear definition of order qty - some dashboards (SPS KPIs) use PDB08 as definition, other use Scale as definition (KPI Dashboard)| | |\n|Confirmed Qty|N*| |currently not defined by business| | |\n|Invoiced Qty|Y*| |no clear definition of order qty - some dashboards (SPS KPIs) use AX as definition, other use Scale as definition (KPI Dashboard)| | |\n|ATP %|N| | |-| |\n|DIF %|Y*|DC KPI Dashboard|within CW known also as FillRate|[SupplyChain].[Scale].[shipment_detail].[TOTAL_QTY] (delivered Qty)\n/\n[SupplyChain].[Scale].[shipment_detail].[REQUESTED_QTY]|**only contains SCALE data - no active (therefore no QLD DC data) - current source references the MySQL DB|\n|DIF %|Y*|Complicance & Governance|is based on supplier + PO information - assume this refers to inbound orders (not outbound)|Refers to PDB08 objects**| |\n|DIF %|Y*|SPS KPIs|is based on supplier + PO information - assume this refers to inbound orders (not outbound)|Refers to PDB08 objects**| |\n|Cust Impact|N| | |-| |\n|Availability Opp $|N| | |-| |\n|DC Opportunity Qty|N| | |-| |\n|DOT %|Y*|DC KPI Dashboard|current definition different to Sigma (within 2 days CW vs 1 day Sigma)|On Time Qty = where diff between [ACTUAL_SHIP_DATE_TIME] and [SCHEDULED_SHIP_DATE] on table [SupplyChain].[Scale].[shipment_header] is <= 2 days (factors in excluding weekends)\n/\n[SupplyChain].[Scale].[shipment_detail].[TOTAL_QTY] (delivered Qty)|**only contains SCALE data - no active (therefore no QLD DC data) - current source references the MySQL DB|\n|DOT %|Y*|Complicance & Governance|is based on supplier + PO information - assume this refers to inbound orders (not outbound)|Refers to PDB08 objects**| |\n|DOT %|Y*|SPS KPIs|is based on supplier + PO information - assume this refers to inbound orders (not outbound)|Refers to PDB08 objects**| |\n\nIssue split into:\n|CSCI-497|MergeCo Conformed Reporting - Identify Source for SC Operational Metrics - Sprint 9|", "text": "Summary\nMergeCo Conformed Reporting - Identify Source for SC Operational Metrics\n\n---\n\nDescription\nIdentify which specific tables/views to use for Merge Co reporting from a CW domain for the below Operational metrics:\n\n* Order Qty\n* Confirmed Qty\n* Invoiced Qty\n* ATP %\n* DIF %\n* Cust Impact %\n* Availability Opportunity Qty/$\n* DC Opportunity Qty\n* DOT %\n\nIdeal grain is at the: \n\nper day, per DC, per product level\n\nNeed to work out:\n\n* Which tables best to extract from\n* If tables can’t be identified, why? Is it not available at all? Or is there an issue with the grain of data? Or access to data?\n\n---\n\nComments\n|Sigma Metrics|CW Equivalent Available?|PBI Report|Notes|Table|Notes 2|\n|Order Qty|Y*| |no clear definition of order qty - some dashboards (SPS KPIs) use PDB08 as definition, other use Scale as definition (KPI Dashboard)| | |\n|Confirmed Qty|N*| |currently not defined by business| | |\n|Invoiced Qty|Y*| |no clear definition of order qty - some dashboards (SPS KPIs) use AX as definition, other use Scale as definition (KPI Dashboard)| | |\n|ATP %|N| | |-| |\n|DIF %|Y*|DC KPI Dashboard|within CW known also as FillRate|[SupplyChain].[Scale].[shipment_detail].[TOTAL_QTY] (delivered Qty)\n/\n[SupplyChain].[Scale].[shipment_detail].[REQUESTED_QTY]|**only contains SCALE data - no active (therefore no QLD DC data) - current source references the MySQL DB|\n|DIF %|Y*|Complicance & Governance|is based on supplier + PO information - assume this refers to inbound orders (not outbound)|Refers to PDB08 objects**| |\n|DIF %|Y*|SPS KPIs|is based on supplier + PO information - assume this refers to inbound orders (not outbound)|Refers to PDB08 objects**| |\n|Cust Impact|N| | |-| |\n|Availability Opp $|N| | |-| |\n|DC Opportunity Qty|N| | |-| |\n|DOT %|Y*|DC KPI Dashboard|current definition different to Sigma (within 2 days CW vs 1 day Sigma)|On Time Qty = where diff between [ACTUAL_SHIP_DATE_TIME] and [SCHEDULED_SHIP_DATE] on table [SupplyChain].[Scale].[shipment_header] is <= 2 days (factors in excluding weekends)\n/\n[SupplyChain].[Scale].[shipment_detail].[TOTAL_QTY] (delivered Qty)|**only contains SCALE data - no active (therefore no QLD DC data) - current source references the MySQL DB|\n|DOT %|Y*|Complicance & Governance|is based on supplier + PO information - assume this refers to inbound orders (not outbound)|Refers to PDB08 objects**| |\n|DOT %|Y*|SPS KPIs|is based on supplier + PO information - assume this refers to inbound orders (not outbound)|Refers to PDB08 objects**| |\n\nIssue split into:\n|CSCI-497|MergeCo Conformed Reporting - Identify Source for SC Operational Metrics - Sprint 9|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 240}}
{"issue_key": "CSCI-406", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 11:42 AM", "updated": "12/Sep/25 6:32 PM", "labels": [], "summary": "MergeCo Conformed Reporting - Identify Source for Availability Metrics", "description": "Identify which specific tables/views to use for Merge Co reporting from a CW domain for the below availability metrics:\n\n* Availability %\n* TOS %\n** Temporarily Out of Stock\n* MCS %\n** Manufacturer Can’t Supply\n* SOH $\n** Stock on Hand\n* DOI\n** Days of Inventory - within CW also known as DIH (Days In Hand)\n\nIdeal grain is at the: \n\nper day, per DC, per product level\n\nNeed to work out:\n\n* Which tables best to extract from\n* If tables can’t be identified, why? Is it not available at all? Or is there an issue with the grain of data? Or access to data?", "acceptance_criteria": "", "comments": "", "text": "Summary\nMergeCo Conformed Reporting - Identify Source for Availability Metrics\n\n---\n\nDescription\nIdentify which specific tables/views to use for Merge Co reporting from a CW domain for the below availability metrics:\n\n* Availability %\n* TOS %\n** Temporarily Out of Stock\n* MCS %\n** Manufacturer Can’t Supply\n* SOH $\n** Stock on Hand\n* DOI\n** Days of Inventory - within CW also known as DIH (Days In Hand)\n\nIdeal grain is at the: \n\nper day, per DC, per product level\n\nNeed to work out:\n\n* Which tables best to extract from\n* If tables can’t be identified, why? Is it not available at all? Or is there an issue with the grain of data? Or access to data?", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 241}}
{"issue_key": "CSCI-405", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 11:38 AM", "updated": "12/Sep/25 5:12 PM", "labels": [], "summary": "MergeCo Conformed Reporting - Update Notes to Tactical Reporting Slides", "description": "After scoping what KPI metrics are available within the CW domain - need to add my notes to the “Tactical Reporting” slides created by Harrison:\n\n[https://sigmacompanylimited.sharepoint.com/:p:/r/sites/EnterpriseDataReportingPlatformproject/_layouts/15/doc2.aspx?sourcedoc=%7B11de107f-f7a5-4028-8bf6-c52457bec338%7D&action=edit&wdPreviousSession=7d624487-be28-0f92-a297-ea52b70d0d8a&previoussessionid=cd8ab3a8-4459-49b6-4d8b-e08e28bec6ea|https://sigmacompanylimited.sharepoint.com/:p:/r/sites/EnterpriseDataReportingPlatformproject/_layouts/15/doc2.aspx?sourcedoc=%7B11de107f-f7a5-4028-8bf6-c52457bec338%7D&action=edit&wdPreviousSession=7d624487-be28-0f92-a297-ea52b70d0d8a&previoussessionid=cd8ab3a8-4459-49b6-4d8b-e08e28bec6ea]", "acceptance_criteria": "", "comments": "", "text": "Summary\nMergeCo Conformed Reporting - Update Notes to Tactical Reporting Slides\n\n---\n\nDescription\nAfter scoping what KPI metrics are available within the CW domain - need to add my notes to the “Tactical Reporting” slides created by Harrison:\n\n[https://sigmacompanylimited.sharepoint.com/:p:/r/sites/EnterpriseDataReportingPlatformproject/_layouts/15/doc2.aspx?sourcedoc=%7B11de107f-f7a5-4028-8bf6-c52457bec338%7D&action=edit&wdPreviousSession=7d624487-be28-0f92-a297-ea52b70d0d8a&previoussessionid=cd8ab3a8-4459-49b6-4d8b-e08e28bec6ea|https://sigmacompanylimited.sharepoint.com/:p:/r/sites/EnterpriseDataReportingPlatformproject/_layouts/15/doc2.aspx?sourcedoc=%7B11de107f-f7a5-4028-8bf6-c52457bec338%7D&action=edit&wdPreviousSession=7d624487-be28-0f92-a297-ea52b70d0d8a&previoussessionid=cd8ab3a8-4459-49b6-4d8b-e08e28bec6ea]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 242}}
{"issue_key": "CSCI-404", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 9:50 AM", "updated": "11/Sep/25 10:00 AM", "labels": [], "summary": "SSO/ SCIM - Snowflake AD Groups definition", "description": "To design and confirm the AD Groups for SSO/SCIM mapping with Snowflake purpose.", "acceptance_criteria": "", "comments": "Confirmed with Alan/ Raveen the below AD Groups and mapping is to be implemented:\n\n|*AD Groups*|*Default Role*|\n|*Snowflake_CW_AU_Consultants*|DEV_ROLE_CONSULTANT|\n|*Snowflake_CW_AU_BusinessUsers*|UAT_ROLE_BUSINESS_READONLY|\n|*Snowflake_CW_AU_DataAnalysts*|UAT_ROLE_ANALYST|\n|*Snowflake_CW_AU_DataEngineers*|DEV_ROLE_DATA_ENG|\n|*Snowflake_CW_AU_PlatformEngineers*|DEV_ROLE_PLATFORM_ENG|\n|*Snowflake_CW_AU_IT_Audit*|IT_AUDIT|", "text": "Summary\nSSO/ SCIM - Snowflake AD Groups definition\n\n---\n\nDescription\nTo design and confirm the AD Groups for SSO/SCIM mapping with Snowflake purpose.\n\n---\n\nComments\nConfirmed with Alan/ Raveen the below AD Groups and mapping is to be implemented:\n\n|*AD Groups*|*Default Role*|\n|*Snowflake_CW_AU_Consultants*|DEV_ROLE_CONSULTANT|\n|*Snowflake_CW_AU_BusinessUsers*|UAT_ROLE_BUSINESS_READONLY|\n|*Snowflake_CW_AU_DataAnalysts*|UAT_ROLE_ANALYST|\n|*Snowflake_CW_AU_DataEngineers*|DEV_ROLE_DATA_ENG|\n|*Snowflake_CW_AU_PlatformEngineers*|DEV_ROLE_PLATFORM_ENG|\n|*Snowflake_CW_AU_IT_Audit*|IT_AUDIT|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 243}}
{"issue_key": "CSCI-403", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 9:44 AM", "updated": "11/Sep/25 9:58 AM", "labels": [], "summary": "Snowflake - Single vs Multiple Account design", "description": "Get alignment with Alan on having Single vs Multiple Account design for Snowflake environments", "acceptance_criteria": "Decision to be made on which design to be implemented for EDP", "comments": "", "text": "Summary\nSnowflake - Single vs Multiple Account design\n\n---\n\nDescription\nGet alignment with Alan on having Single vs Multiple Account design for Snowflake environments\n\n---\n\nAcceptance Criteria\nDecision to be made on which design to be implemented for EDP", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 244}}
{"issue_key": "CSCI-402", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 9:27 AM", "updated": "10/Oct/25 12:10 PM", "labels": [], "summary": "Review Source-to-Target Map for Dim_DC part 2 - DimWarehouse DimWarehouseLocation, DimZone & BridgeWarehouseLocationZone", "description": "Review the mapping of DIM_DC Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=XQdnyO&nav=MTVfe0Q4MDZEQUY3LTJCQzUtNDRBMy1CNDEwLUNBMEE4RjVFQkYwNX0]", "acceptance_criteria": "* Finalize that the work on Source-to-Target Map for DIM_DC is correct.\n** Review the Source-to-Target Map for DIM_DC against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.", "comments": "@user This has been broken down to: DimWarehouse, DimWarehouseLocation, DiimZone & BridgeWarehouseLocationZone. Need @user to review my model designs in Confluence.\n\n@user would you like me to create this (create Source to target Mapping) card?\n\n{quote}DimWarehouse, DimWarehouseLocation, DiimZone & BridgeWarehouseLocationZone.{quote}\n\n@user Up to you, personally I would just leave it as one card with multiple tasks. But no preference either way for me.\n\n@user OK i’ll leave it, will just change title for now.", "text": "Summary\nReview Source-to-Target Map for Dim_DC part 2 - DimWarehouse DimWarehouseLocation, DimZone & BridgeWarehouseLocationZone\n\n---\n\nDescription\nReview the mapping of DIM_DC Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=XQdnyO&nav=MTVfe0Q4MDZEQUY3LTJCQzUtNDRBMy1CNDEwLUNBMEE4RjVFQkYwNX0]\n\n---\n\nAcceptance Criteria\n* Finalize that the work on Source-to-Target Map for DIM_DC is correct.\n** Review the Source-to-Target Map for DIM_DC against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.\n\n---\n\nComments\n@user This has been broken down to: DimWarehouse, DimWarehouseLocation, DiimZone & BridgeWarehouseLocationZone. Need @user to review my model designs in Confluence.\n\n@user would you like me to create this (create Source to target Mapping) card?\n\n{quote}DimWarehouse, DimWarehouseLocation, DiimZone & BridgeWarehouseLocationZone.{quote}\n\n@user Up to you, personally I would just leave it as one card with multiple tasks. But no preference either way for me.\n\n@user OK i’ll leave it, will just change title for now.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 245}}
{"issue_key": "CSCI-401", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 9:26 AM", "updated": "20/Nov/25 9:50 AM", "labels": [], "summary": "Review Source-to-Target Map for Dim_PO part 2", "description": "Review the mapping of DIM_PO Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=KnX8aA&nav=MTVfezA1OUMyOEU2LTI4QTQtNDY2Ny1CODMzLUIwQUM0NDVFODBDNH0]", "acceptance_criteria": "* Finalize that the work on Source-to-Target Map for DIM_PO is correct.\n** Review the Source-to-Target Map for DIM_PO against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.", "comments": "@user I have changed this to FactPOLineItem, need @user to review my design in Confluence.\n\nTo Review after Meeting (20251014)\n\nDim PO is not required for our Supply Chain Data Model.\nThis Dim can be removed as part of phase 1 delivery - if there’s a specific requirement that requires a Dim PO in future we can look to reinstate this.", "text": "Summary\nReview Source-to-Target Map for Dim_PO part 2\n\n---\n\nDescription\nReview the mapping of DIM_PO Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=KnX8aA&nav=MTVfezA1OUMyOEU2LTI4QTQtNDY2Ny1CODMzLUIwQUM0NDVFODBDNH0]\n\n---\n\nAcceptance Criteria\n* Finalize that the work on Source-to-Target Map for DIM_PO is correct.\n** Review the Source-to-Target Map for DIM_PO against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.\n\n---\n\nComments\n@user I have changed this to FactPOLineItem, need @user to review my design in Confluence.\n\nTo Review after Meeting (20251014)\n\nDim PO is not required for our Supply Chain Data Model.\nThis Dim can be removed as part of phase 1 delivery - if there’s a specific requirement that requires a Dim PO in future we can look to reinstate this.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 246}}
{"issue_key": "CSCI-400", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 9:26 AM", "updated": "23/Sep/25 9:29 AM", "labels": [], "summary": "Review Source-to-Target Map for Dim_location part 2", "description": "Review the mapping of DIM_LOCATION Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=XQdnyO&nav=MTVfe0Q4MDZEQUY3LTJCQzUtNDRBMy1CNDEwLUNBMEE4RjVFQkYwNX0]", "acceptance_criteria": "* Finalize that the work on Source-to-Target Map for DIM_LOCATIONis correct.\n** Review the Source-to-Target Map for DIM_LOCATION against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.", "comments": "Renamed to Dim_address\nAdded Dim_Country and Dim_currency\n\n @user @usercc @user", "text": "Summary\nReview Source-to-Target Map for Dim_location part 2\n\n---\n\nDescription\nReview the mapping of DIM_LOCATION Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=XQdnyO&nav=MTVfe0Q4MDZEQUY3LTJCQzUtNDRBMy1CNDEwLUNBMEE4RjVFQkYwNX0]\n\n---\n\nAcceptance Criteria\n* Finalize that the work on Source-to-Target Map for DIM_LOCATIONis correct.\n** Review the Source-to-Target Map for DIM_LOCATION against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.\n\n---\n\nComments\nRenamed to Dim_address\nAdded Dim_Country and Dim_currency\n\n @user @usercc @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 247}}
{"issue_key": "CSCI-399", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 9:26 AM", "updated": "10/Oct/25 12:10 PM", "labels": [], "summary": "Review Source-to-Target Map for Dim_Date part 2", "description": "to have a discussion with you @user about this task.", "acceptance_criteria": "* Finalize that the work on Source-to-Target Map for DIM_DATE is correct.", "comments": "", "text": "Summary\nReview Source-to-Target Map for Dim_Date part 2\n\n---\n\nDescription\nto have a discussion with you @user about this task.\n\n---\n\nAcceptance Criteria\n* Finalize that the work on Source-to-Target Map for DIM_DATE is correct.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 248}}
{"issue_key": "CSCI-398", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Sep/25 9:25 AM", "updated": "29/Sep/25 9:12 AM", "labels": [], "summary": "Review Source-to-Target Map for Dim_Employee part 2", "description": "Review the mapping of DIM_Employee Dimension: DIM_Employee", "acceptance_criteria": "* Finalize that the work on Source-to-Target Map for DIM_Employee correct.\n** Review the Source-to-Target Map for DIM_Employee against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.", "comments": "@user to review cc @user\n\n@user I have completed model design for this.", "text": "Summary\nReview Source-to-Target Map for Dim_Employee part 2\n\n---\n\nDescription\nReview the mapping of DIM_Employee Dimension: DIM_Employee\n\n---\n\nAcceptance Criteria\n* Finalize that the work on Source-to-Target Map for DIM_Employee correct.\n** Review the Source-to-Target Map for DIM_Employee against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.\n\n---\n\nComments\n@user to review cc @user\n\n@user I have completed model design for this.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 249}}
{"issue_key": "CSCI-394", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "09/Sep/25 3:20 PM", "updated": "11/Sep/25 11:38 AM", "labels": [], "summary": "Summary of KPIs from businesses that are available on the platform.", "description": "Looking at KPIs and what’s already available in Snowflake", "acceptance_criteria": "To pick available KPIs available on Presentation Layer, and eventually structured in the Gold layer when in medallion structure.", "comments": "", "text": "Summary\nSummary of KPIs from businesses that are available on the platform.\n\n---\n\nDescription\nLooking at KPIs and what’s already available in Snowflake\n\n---\n\nAcceptance Criteria\nTo pick available KPIs available on Presentation Layer, and eventually structured in the Gold layer when in medallion structure.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 250}}
{"issue_key": "CSCI-391", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "08/Sep/25 8:49 AM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Extract the parquet for AR_Transaction_History", "description": "*Description*\n\nExtract the parquets files for these tables\n\n* TDB15.ILS.dbo.AR_Transaction_History\n\nh2.", "acceptance_criteria": "* -Extract the parquet files-\n* -File validation-\n* -Upload it in Azure blob storage-\n* -Post-upload verification-", "comments": "@user @user \n\nParquet files have been uploaded to Azure Storage.\n\n[parquetfiles - Microsoft Azure|https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Fdc516e92-8716-44f9-b09c-fc5ca9cdd01a%2FresourceGroups%2Fcwr-ase-dpdev-rg%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Fcwraseedpdevst/path/parquetfiles/etag/%220x8DD4FE2D7009ECA%22/defaultId//publicAccessVal/None]", "text": "Summary\nExtract the parquet for AR_Transaction_History\n\n---\n\nDescription\n*Description*\n\nExtract the parquets files for these tables\n\n* TDB15.ILS.dbo.AR_Transaction_History\n\nh2.\n\n---\n\nAcceptance Criteria\n* -Extract the parquet files-\n* -File validation-\n* -Upload it in Azure blob storage-\n* -Post-upload verification-\n\n---\n\nComments\n@user @user \n\nParquet files have been uploaded to Azure Storage.\n\n[parquetfiles - Microsoft Azure|https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Fdc516e92-8716-44f9-b09c-fc5ca9cdd01a%2FresourceGroups%2Fcwr-ase-dpdev-rg%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Fcwraseedpdevst/path/parquetfiles/etag/%220x8DD4FE2D7009ECA%22/defaultId//publicAccessVal/None]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 251}}
{"issue_key": "CSCI-390", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "08/Sep/25 8:29 AM", "updated": "12/Sep/25 6:13 PM", "labels": [], "summary": "RITM0174658 - - Ashutosh Jumpbox access to CW BLOB storage", "description": "Phil to raise servnicenow ticket for Ashutosh Jumpbox access to CW BLOB storage", "acceptance_criteria": "Jumpbox → Blob Access availble for Ashutosh", "comments": "still in progress", "text": "Summary\nRITM0174658 - - Ashutosh Jumpbox access to CW BLOB storage\n\n---\n\nDescription\nPhil to raise servnicenow ticket for Ashutosh Jumpbox access to CW BLOB storage\n\n---\n\nAcceptance Criteria\nJumpbox → Blob Access availble for Ashutosh\n\n---\n\nComments\nstill in progress", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 252}}
{"issue_key": "CSCI-389", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "08/Sep/25 8:29 AM", "updated": "12/Sep/25 6:13 PM", "labels": [], "summary": "RITM0174658 - Aswini access to CW BLOB storage", "description": "PHil to raise in Servicenow direct access to CW BLOB storage for Aswini", "acceptance_criteria": "Jumpbox → Blob Access availble for Aswini", "comments": "still in progress\n\nIssue split into:\n|CSCI-430|RITM0174658 - Aswini and Ashutosh access to CW BLOB storage|", "text": "Summary\nRITM0174658 - Aswini access to CW BLOB storage\n\n---\n\nDescription\nPHil to raise in Servicenow direct access to CW BLOB storage for Aswini\n\n---\n\nAcceptance Criteria\nJumpbox → Blob Access availble for Aswini\n\n---\n\nComments\nstill in progress\n\nIssue split into:\n|CSCI-430|RITM0174658 - Aswini and Ashutosh access to CW BLOB storage|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 253}}
{"issue_key": "CSCI-388", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "02/Sep/25 12:41 PM", "updated": "12/Sep/25 6:36 PM", "labels": ["Ingestion"], "summary": "Manhattan Scale data ingestion", "description": "Import the data for ILS tables\n\nlocation_inventory\n\nAR_Transaction_History\n\nTransaction_History\n\n* Create extract meta records\n* Create & Run end to end pipelines in ADF", "acceptance_criteria": "", "comments": "Delta in progress. \n\nWaiting for @Adeel to get access to start historical upload.\n\nHistorical upload in progress\n\nIssue split into:\n|CSCI-426|Manhattan Scale data ingestion - part 2|", "text": "Summary\nManhattan Scale data ingestion\n\n---\n\nDescription\nImport the data for ILS tables\n\nlocation_inventory\n\nAR_Transaction_History\n\nTransaction_History\n\n* Create extract meta records\n* Create & Run end to end pipelines in ADF\n\n---\n\nComments\nDelta in progress. \n\nWaiting for @Adeel to get access to start historical upload.\n\nHistorical upload in progress\n\nIssue split into:\n|CSCI-426|Manhattan Scale data ingestion - part 2|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 254}}
{"issue_key": "CSCI-387", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "02/Sep/25 11:40 AM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Review Source-to-Target Map for Dim_PO", "description": "Review the mapping of DIM_PO Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=KnX8aA&nav=MTVfezA1OUMyOEU2LTI4QTQtNDY2Ny1CODMzLUIwQUM0NDVFODBDNH0]", "acceptance_criteria": "* Finalize that the work on Source-to-Target Map for DIM_PO is correct.\n** Review the Source-to-Target Map for DIM_PO against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.", "comments": "", "text": "Summary\nReview Source-to-Target Map for Dim_PO\n\n---\n\nDescription\nReview the mapping of DIM_PO Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=KnX8aA&nav=MTVfezA1OUMyOEU2LTI4QTQtNDY2Ny1CODMzLUIwQUM0NDVFODBDNH0]\n\n---\n\nAcceptance Criteria\n* Finalize that the work on Source-to-Target Map for DIM_PO is correct.\n** Review the Source-to-Target Map for DIM_PO against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 255}}
{"issue_key": "CSCI-386", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "02/Sep/25 11:40 AM", "updated": "12/Sep/25 3:24 PM", "labels": [], "summary": "CWRetail EDP - Details Tech Design - 2/9 - 14/9", "description": "Work on Detailed Tech Design for EDP CWRetails .", "acceptance_criteria": "", "comments": "Making Physical Diagram of Snowflake, ADF and combined", "text": "Summary\nCWRetail EDP - Details Tech Design - 2/9 - 14/9\n\n---\n\nDescription\nWork on Detailed Tech Design for EDP CWRetails .\n\n---\n\nComments\nMaking Physical Diagram of Snowflake, ADF and combined", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 256}}
{"issue_key": "CSCI-385", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "02/Sep/25 9:56 AM", "updated": "12/Sep/25 6:32 PM", "labels": [], "summary": "Silver model for facts", "description": "Develop a Silver data model for Facts, transforming and integrating data from the Bronze layer to provide a cleansed, structured, and business-ready dataset. \n\nThe Silver model should address data quality issues, apply necessary business logic, and ensure consistency for downstream analytics and reporting.", "acceptance_criteria": "* -Silver (Kimball Medallion model) for Facts to be constructed.-\n* -Diagram in tool i.e. [http://draw.io|http://draw.io] -\n* peer reviewed\n* Documentation\n* Stakeholder Sign-off", "comments": "Facts covered:\n\n* fact_sales_retail\n* fact_sales_retail_electronic_payments\n* Fact_sales_audit_log_history\n\nFct_Retail_Sales completed with Columns and definitions\nFCT_Sales_Retail_Electronic_Payments completed with Columns and definitions\n\nAll Fact tables definitions are completed\n\nFCT_Sales_Retail \n\nFCT_Sales_Retail_Electronic_Payments\nFCT_Transactions_Audit_History\n\nNeed to review with Harrison and Alan\n\nStructure is Updted in Merge Co Folder\n\n[Business Matrix_Retail - Simplified.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Business%20Matrix_Retail%20-%20Simplified.xlsx?d=w0e262bf7be84407d8dbff2bb4d4c5e00&csf=1&web=1&e=dGjeuu]\n\nIssue split into:\n|CSCI-437|Silver model for facts - review for Alan|", "text": "Summary\nSilver model for facts\n\n---\n\nDescription\nDevelop a Silver data model for Facts, transforming and integrating data from the Bronze layer to provide a cleansed, structured, and business-ready dataset. \n\nThe Silver model should address data quality issues, apply necessary business logic, and ensure consistency for downstream analytics and reporting.\n\n---\n\nAcceptance Criteria\n* -Silver (Kimball Medallion model) for Facts to be constructed.-\n* -Diagram in tool i.e. [http://draw.io|http://draw.io] -\n* peer reviewed\n* Documentation\n* Stakeholder Sign-off\n\n---\n\nComments\nFacts covered:\n\n* fact_sales_retail\n* fact_sales_retail_electronic_payments\n* Fact_sales_audit_log_history\n\nFct_Retail_Sales completed with Columns and definitions\nFCT_Sales_Retail_Electronic_Payments completed with Columns and definitions\n\nAll Fact tables definitions are completed\n\nFCT_Sales_Retail \n\nFCT_Sales_Retail_Electronic_Payments\nFCT_Transactions_Audit_History\n\nNeed to review with Harrison and Alan\n\nStructure is Updted in Merge Co Folder\n\n[Business Matrix_Retail - Simplified.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Business%20Matrix_Retail%20-%20Simplified.xlsx?d=w0e262bf7be84407d8dbff2bb4d4c5e00&csf=1&web=1&e=dGjeuu]\n\nIssue split into:\n|CSCI-437|Silver model for facts - review for Alan|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 257}}
{"issue_key": "CSCI-384", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "02/Sep/25 9:56 AM", "updated": "12/Sep/25 9:49 AM", "labels": [], "summary": "Silver model for Dims", "description": "Develop a Silver data model for Dims, transforming and integrating data from the Bronze layer to provide a cleansed, structured, and business-ready dataset. \n\nThe Silver model should address data quality issues, apply necessary business logic, and ensure consistency for downstream analytics and reporting.", "acceptance_criteria": "* -Silver (Kimball Medallion model) for Dims to be constructed.-\n* -Diagram in tool i.e. draw.io-\n* -peer reviewed-\n* Documentation\n* Stakeholder Sign-off", "comments": "Workshop for Silver Model for Product and Store is complete now. Agreed data model will be published on Confluence soon.\n\nStore dim is complete and is available on Confluence\nProduct Dim final review complete. Suggestions to be incorporated and made available in Confluence\n\n@user - to update on details on what dims are covered\n\nHey @user - do you have links to the [draw.io|http://draw.io] diagram?\n\nDim tables Silver Layer Structure is updated in Confluence\n[https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model|https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model] . Tool used is Lucid Chart\n\nReview completed with Alan. All good to go.", "text": "Summary\nSilver model for Dims\n\n---\n\nDescription\nDevelop a Silver data model for Dims, transforming and integrating data from the Bronze layer to provide a cleansed, structured, and business-ready dataset. \n\nThe Silver model should address data quality issues, apply necessary business logic, and ensure consistency for downstream analytics and reporting.\n\n---\n\nAcceptance Criteria\n* -Silver (Kimball Medallion model) for Dims to be constructed.-\n* -Diagram in tool i.e. draw.io-\n* -peer reviewed-\n* Documentation\n* Stakeholder Sign-off\n\n---\n\nComments\nWorkshop for Silver Model for Product and Store is complete now. Agreed data model will be published on Confluence soon.\n\nStore dim is complete and is available on Confluence\nProduct Dim final review complete. Suggestions to be incorporated and made available in Confluence\n\n@user - to update on details on what dims are covered\n\nHey @user - do you have links to the [draw.io|http://draw.io] diagram?\n\nDim tables Silver Layer Structure is updated in Confluence\n[https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model|https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model] . Tool used is Lucid Chart\n\nReview completed with Alan. All good to go.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 258}}
{"issue_key": "CSCI-383", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "02/Sep/25 8:42 AM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Extract parquet for AR_SHIPMENT_DETAIL table", "description": "Extract the parquets files for these tables\n\n* AR_SHIPMENT_DETAIL", "acceptance_criteria": "* -Extract the parquet files-\n* -File validation-\n* -Upload it in Azure blob storage-\n* -Post-upload verification-", "comments": "Extracted the parquet for AR_shipment Detail, \n\nValidated the files, sizes and row count\n\nUploaded the parquet extracts in the blob storage\n\nCC: @user @user", "text": "Summary\nExtract parquet for AR_SHIPMENT_DETAIL table\n\n---\n\nDescription\nExtract the parquets files for these tables\n\n* AR_SHIPMENT_DETAIL\n\n---\n\nAcceptance Criteria\n* -Extract the parquet files-\n* -File validation-\n* -Upload it in Azure blob storage-\n* -Post-upload verification-\n\n---\n\nComments\nExtracted the parquet for AR_shipment Detail, \n\nValidated the files, sizes and row count\n\nUploaded the parquet extracts in the blob storage\n\nCC: @user @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 259}}
{"issue_key": "CSCI-382", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Sep/25 9:50 AM", "updated": "10/Sep/25 4:29 PM", "labels": [], "summary": "DAta ingestion - SPSWHSPurchase", "description": "Ingest data in the below tables from the database SPSWHSPurchase.\n\n|TDB08AX2012|SpsWhsPurchase|POInvoiceLine|781|\n|TDB08AX2012|SpsWhsPurchase|POInvoiceTable|251|\n|TDB08AX2012|SpsWhsPurchase|PurchReqLine|1,692|\n|TDB08AX2012|SpsWhsPurchase|PurchReqTable|368|\n|TDB08AX2012|SpsWhsPurchase|PurchStatus|83|\n|TDB08AX2012|SpsWhsPurchase|VendorType|75|", "acceptance_criteria": "* -Insert records in extract Meta-\n* -Create linked services -\n* -Configure and run end to end pipelines-", "comments": "", "text": "Summary\nDAta ingestion - SPSWHSPurchase\n\n---\n\nDescription\nIngest data in the below tables from the database SPSWHSPurchase.\n\n|TDB08AX2012|SpsWhsPurchase|POInvoiceLine|781|\n|TDB08AX2012|SpsWhsPurchase|POInvoiceTable|251|\n|TDB08AX2012|SpsWhsPurchase|PurchReqLine|1,692|\n|TDB08AX2012|SpsWhsPurchase|PurchReqTable|368|\n|TDB08AX2012|SpsWhsPurchase|PurchStatus|83|\n|TDB08AX2012|SpsWhsPurchase|VendorType|75|\n\n---\n\nAcceptance Criteria\n* -Insert records in extract Meta-\n* -Create linked services -\n* -Configure and run end to end pipelines-", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 260}}
{"issue_key": "CSCI-381", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Sep/25 2:30 AM", "updated": "19/Nov/25 11:46 AM", "labels": [], "summary": "Implement Snowflake CD pipelines", "description": "Review output of [https://sigmahealthcare.atlassian.net/browse/CSCI-151|https://sigmahealthcare.atlassian.net/browse/CSCI-151] and [https://sigmahealthcare.atlassian.net/browse/CSCI-159|https://sigmahealthcare.atlassian.net/browse/CSCI-159]", "acceptance_criteria": "", "comments": "I’ll reach out for what’s covered in part 3 @user\n\nThis is to pick up from what was left in late July. @user\n\nChloe and Eugene were working on the Release pipeline for Snowflake which follows the sequenced logic as fleshed out in the edp-snowflake-release.\n\nNext step: Eugene to review and fix the PowerShell command scripts.", "text": "Summary\nImplement Snowflake CD pipelines\n\n---\n\nDescription\nReview output of [https://sigmahealthcare.atlassian.net/browse/CSCI-151|https://sigmahealthcare.atlassian.net/browse/CSCI-151] and [https://sigmahealthcare.atlassian.net/browse/CSCI-159|https://sigmahealthcare.atlassian.net/browse/CSCI-159]\n\n---\n\nComments\nI’ll reach out for what’s covered in part 3 @user\n\nThis is to pick up from what was left in late July. @user\n\nChloe and Eugene were working on the Release pipeline for Snowflake which follows the sequenced logic as fleshed out in the edp-snowflake-release.\n\nNext step: Eugene to review and fix the PowerShell command scripts.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 261}}
{"issue_key": "CSCI-380", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Sep/25 2:29 AM", "updated": "12/Sep/25 6:25 PM", "labels": [], "summary": "ExpressRoute Monitoring - (1 Sep - 15th Sep)", "description": "Monitor ExpressRoute throughput utilisation", "acceptance_criteria": "# *Metric Visibility:*\n#* ExpressRoute connection metrics (e.g., Ingress/Egress throughput) are visible in Azure Monitor or Network Insights.\n#* Bandwidth usage can be correlated with ADF pipeline execution timelines.\n# *Alerting Rules:*\n#* Alerts are configured to trigger when bandwidth usage exceeds a defined threshold (e.g., 70% of provisioned bandwidth).\n#* Alerts include pipeline metadata (e.g., pipeline name, run ID) when possible, to trace high-usage executions.\n# *Log Collection:*\n#* Network bandwidth logs (e.g., from Network Performance Monitor or Azure Network Watcher) are retained for a minimum of 30 days.\n#* ADF pipeline run logs include integration runtime IPs or endpoints for cross-reference.\n# *Validation Testing:*\n#* At least one high-throughput ADF pipeline is executed to validate that bandwidth spikes are captured and logged as expected.\n#* Test scenarios simulate concurrent pipeline runs to verify system responsiveness and alert thresholds.\n# *Governance & Review:*\n#* A monthly review is conducted to assess if bandwidth consumption patterns require scaling up/down of ExpressRoute or ADF optimization.\n#* A weekly review to be initially done on until Oct 2025.", "comments": "", "text": "Summary\nExpressRoute Monitoring - (1 Sep - 15th Sep)\n\n---\n\nDescription\nMonitor ExpressRoute throughput utilisation\n\n---\n\nAcceptance Criteria\n# *Metric Visibility:*\n#* ExpressRoute connection metrics (e.g., Ingress/Egress throughput) are visible in Azure Monitor or Network Insights.\n#* Bandwidth usage can be correlated with ADF pipeline execution timelines.\n# *Alerting Rules:*\n#* Alerts are configured to trigger when bandwidth usage exceeds a defined threshold (e.g., 70% of provisioned bandwidth).\n#* Alerts include pipeline metadata (e.g., pipeline name, run ID) when possible, to trace high-usage executions.\n# *Log Collection:*\n#* Network bandwidth logs (e.g., from Network Performance Monitor or Azure Network Watcher) are retained for a minimum of 30 days.\n#* ADF pipeline run logs include integration runtime IPs or endpoints for cross-reference.\n# *Validation Testing:*\n#* At least one high-throughput ADF pipeline is executed to validate that bandwidth spikes are captured and logged as expected.\n#* Test scenarios simulate concurrent pipeline runs to verify system responsiveness and alert thresholds.\n# *Governance & Review:*\n#* A monthly review is conducted to assess if bandwidth consumption patterns require scaling up/down of ExpressRoute or ADF optimization.\n#* A weekly review to be initially done on until Oct 2025.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 262}}
{"issue_key": "CSCI-379", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Sep/25 2:18 AM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Extract parquet for TransactionAuditSaleActivityLog - part 2", "description": "Extract the parquets files for these tables \n\n* TransactionStorage.dbo.TransactionAuditSaleActivityLog\n* TransactionStorage.dbo.TransactionAuditSaleActivityLogHistory\n* TransactionStorage.dbo.Transactions_ElectronicPayments", "acceptance_criteria": "* -Extract the parquet files-\n* -File validation-\n* -Upload it in Azure blob storage-\n* -Post-upload verification-", "comments": "Uploaded the parquet extracts in the blob storage\n\ncc @user @user", "text": "Summary\nExtract parquet for TransactionAuditSaleActivityLog - part 2\n\n---\n\nDescription\nExtract the parquets files for these tables \n\n* TransactionStorage.dbo.TransactionAuditSaleActivityLog\n* TransactionStorage.dbo.TransactionAuditSaleActivityLogHistory\n* TransactionStorage.dbo.Transactions_ElectronicPayments\n\n---\n\nAcceptance Criteria\n* -Extract the parquet files-\n* -File validation-\n* -Upload it in Azure blob storage-\n* -Post-upload verification-\n\n---\n\nComments\nUploaded the parquet extracts in the blob storage\n\ncc @user @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 263}}
{"issue_key": "CSCI-378", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Sep/25 2:17 AM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Create Source-to-Target Map for Dim_PO - part 2", "description": "Source to target mapping for Dim_PO - filling out sheet as per Source-to-target dims", "acceptance_criteria": "* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "Completed the DIM_PO mapping [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=U4d8Ue&nav=MTVfezA1OUMyOEU2LTI4QTQtNDY2Ny1CODMzLUIwQUM0NDVFODBDNH0]\n\nCreate review ticket\n[https://sigmahealthcare.atlassian.net/browse/CSCI-387|https://sigmahealthcare.atlassian.net/browse/CSCI-387]", "text": "Summary\nCreate Source-to-Target Map for Dim_PO - part 2\n\n---\n\nDescription\nSource to target mapping for Dim_PO - filling out sheet as per Source-to-target dims\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nCompleted the DIM_PO mapping [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=U4d8Ue&nav=MTVfezA1OUMyOEU2LTI4QTQtNDY2Ny1CODMzLUIwQUM0NDVFODBDNH0]\n\nCreate review ticket\n[https://sigmahealthcare.atlassian.net/browse/CSCI-387|https://sigmahealthcare.atlassian.net/browse/CSCI-387]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 264}}
{"issue_key": "CSCI-377", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Sep/25 2:14 AM", "updated": "12/Sep/25 6:22 PM", "labels": [], "summary": "Discussion - how we should approach getting specs/info on MAWM visibility - part 2", "description": "Discussion - how we should approach getting specs/info on MAWM visibility\n\nThis is mainly captured in sessions with Chandan about where data lives for Manhattan Scale and Manhattan active.", "acceptance_criteria": "Discussion - how we should approach getting specs/info on MAWM visibility\n\n* Gaining insight on Scale and Active structure from Chandan’s sessions\n* Documentation about Scale and Active", "comments": "Hey @user - would it be OK to note what’s being captured from Chandan’s sessions here?\n\nThank you\n\nIssue split into:\n|CSCI-423|Discussion - how we should approach getting specs/info on MAWM visibility - part 2|", "text": "Summary\nDiscussion - how we should approach getting specs/info on MAWM visibility - part 2\n\n---\n\nDescription\nDiscussion - how we should approach getting specs/info on MAWM visibility\n\nThis is mainly captured in sessions with Chandan about where data lives for Manhattan Scale and Manhattan active.\n\n---\n\nAcceptance Criteria\nDiscussion - how we should approach getting specs/info on MAWM visibility\n\n* Gaining insight on Scale and Active structure from Chandan’s sessions\n* Documentation about Scale and Active\n\n---\n\nComments\nHey @user - would it be OK to note what’s being captured from Chandan’s sessions here?\n\nThank you\n\nIssue split into:\n|CSCI-423|Discussion - how we should approach getting specs/info on MAWM visibility - part 2|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 265}}
{"issue_key": "CSCI-376", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Sep/25 2:13 AM", "updated": "12/Sep/25 8:55 AM", "labels": [], "summary": "Review Source-to-Target Map for Fact_Store_Inventory_Snapshot", "description": "Source to target mapping for Fact_Store_Inventory_Snapshot - filling out sheet as per Source-to-target dims", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "Hey @user is this something we get @user to review? (much like @user 's Dim tables)", "text": "Summary\nReview Source-to-Target Map for Fact_Store_Inventory_Snapshot\n\n---\n\nDescription\nSource to target mapping for Fact_Store_Inventory_Snapshot - filling out sheet as per Source-to-target dims\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nHey @user is this something we get @user to review? (much like @user 's Dim tables)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 266}}
{"issue_key": "CSCI-375", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Aug/25 3:13 PM", "updated": "11/Sep/25 9:57 AM", "labels": [], "summary": "ILS - Manhattan Scale Integration - Ingestion of remaining 7 tables - review", "description": "DATA IN for critical identified objects from Manhattan Scale db (ILS)\n\nImplement data ingestion for critical identity objects from the on-premises Manhattan Scale (ILS) database into the cloud data platform. This includes configuring the necessary Azure Data Factory (ADF) components to support automated and reliable data flow.\n\nLatest update;\n\n* Ticket raised. REQ0156142\n* Currently with the infrastructure cloud team - Brent - Cloud services manager.\n* !image-20250729-063227 (1ed4c70f-781d-46cb-9ced-b8db1e7f0691).png|width=961,height=959,alt=\"image-20250729-063227.png\"!\nBelow are the list of tables to be ingested. Out of these the first one in the list i.e. Carrier is ingested in phase 1.\n\n|ILS|Carrier|\n|ILS|Functional_area_status_flow|\n|ILS|Item|\n|ILS|Location|\n|ILS|Location_inventory|\n|ILS|Routing_guide|\n|ILS|Shipment_detail|\n|ILS|Shipment_header|", "acceptance_criteria": "Definition of done:\n\n* -Extract meta created ()-\n* -Linked services created-\n* -Pipeline run end to end-\n* Whitelisting of on-prem prod database server. (Done by @user )", "comments": "Hi @user ,\n\nAs we discussed on Friday, the tables Shipment_detail & Shipment_header contains only 15 days worth of data in source. So if we intend to load more history we may have to ingest their historic tables presented below highlighted by @user .\n\n# AR_SHIPMENT_DETAIL\n# AR_SHIPMENT_HEADER\n\nPlease help to get this confirmed and I can add these in current sprint.\n\n@user confirmed with @user - these should be included.\n\nWill start with delta part only", "text": "Summary\nILS - Manhattan Scale Integration - Ingestion of remaining 7 tables - review\n\n---\n\nDescription\nDATA IN for critical identified objects from Manhattan Scale db (ILS)\n\nImplement data ingestion for critical identity objects from the on-premises Manhattan Scale (ILS) database into the cloud data platform. This includes configuring the necessary Azure Data Factory (ADF) components to support automated and reliable data flow.\n\nLatest update;\n\n* Ticket raised. REQ0156142\n* Currently with the infrastructure cloud team - Brent - Cloud services manager.\n* !image-20250729-063227 (1ed4c70f-781d-46cb-9ced-b8db1e7f0691).png|width=961,height=959,alt=\"image-20250729-063227.png\"!\nBelow are the list of tables to be ingested. Out of these the first one in the list i.e. Carrier is ingested in phase 1.\n\n|ILS|Carrier|\n|ILS|Functional_area_status_flow|\n|ILS|Item|\n|ILS|Location|\n|ILS|Location_inventory|\n|ILS|Routing_guide|\n|ILS|Shipment_detail|\n|ILS|Shipment_header|\n\n---\n\nAcceptance Criteria\nDefinition of done:\n\n* -Extract meta created ()-\n* -Linked services created-\n* -Pipeline run end to end-\n* Whitelisting of on-prem prod database server. (Done by @user )\n\n---\n\nComments\nHi @user ,\n\nAs we discussed on Friday, the tables Shipment_detail & Shipment_header contains only 15 days worth of data in source. So if we intend to load more history we may have to ingest their historic tables presented below highlighted by @user .\n\n# AR_SHIPMENT_DETAIL\n# AR_SHIPMENT_HEADER\n\nPlease help to get this confirmed and I can add these in current sprint.\n\n@user confirmed with @user - these should be included.\n\nWill start with delta part only", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 267}}
{"issue_key": "CSCI-374", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Aug/25 3:12 PM", "updated": "12/Sep/25 6:26 PM", "labels": [], "summary": "Understanding current architecture - please fill out coverage", "description": "As an engineer - I would like to receive the current in future state of architecture so that I know what type of work I need to create and where my work is going towards (i.e. a goal)\n\nKey areas:\n\n* DLA\n* @user - I might need your help to fill the rest of this out\n\nCovered areas as per 12/9:\n\n# Got a fair idea of how the ingestion framework (ADF +Snowflake ) from various source systems to snowflake stage works.\n# Became aware of the branching strategy that are being discussed\n# CI/CD is in initial stage at the moment . Confident to get acquainted with this when the approach is finalized etc.", "acceptance_criteria": "* -receive documentation about the current architecture - now reviewing-\n** receive documentation about architecture we are working towards\n* -Ask relevant questions about where we currently are at to fill knowledge gaps-\n* -Able to apply professional knowledge and opinion on current work-\n* Able to leverage architecture knowledge to create own Jira tasks with acceptance criteria levera", "comments": "Can you fill the rest of this out @user\n\n# Got a fair idea of how the ingestion framework (ADF +Snowflake ) from various source systems to snowflake stage works.\n# Became aware of the branching strategy that are being discussed\n# CI/CD is in initial stage at the moment . Confident to get acquainted with this when the approach is finalized etc.\n\nIf this ticket is to cover above point only, then I am fine to complete this one. In case we need a separate ticket going forward, we can very well raise to cover other aspects not listed here.", "text": "Summary\nUnderstanding current architecture - please fill out coverage\n\n---\n\nDescription\nAs an engineer - I would like to receive the current in future state of architecture so that I know what type of work I need to create and where my work is going towards (i.e. a goal)\n\nKey areas:\n\n* DLA\n* @user - I might need your help to fill the rest of this out\n\nCovered areas as per 12/9:\n\n# Got a fair idea of how the ingestion framework (ADF +Snowflake ) from various source systems to snowflake stage works.\n# Became aware of the branching strategy that are being discussed\n# CI/CD is in initial stage at the moment . Confident to get acquainted with this when the approach is finalized etc.\n\n---\n\nAcceptance Criteria\n* -receive documentation about the current architecture - now reviewing-\n** receive documentation about architecture we are working towards\n* -Ask relevant questions about where we currently are at to fill knowledge gaps-\n* -Able to apply professional knowledge and opinion on current work-\n* Able to leverage architecture knowledge to create own Jira tasks with acceptance criteria levera\n\n---\n\nComments\nCan you fill the rest of this out @user\n\n# Got a fair idea of how the ingestion framework (ADF +Snowflake ) from various source systems to snowflake stage works.\n# Became aware of the branching strategy that are being discussed\n# CI/CD is in initial stage at the moment . Confident to get acquainted with this when the approach is finalized etc.\n\nIf this ticket is to cover above point only, then I am fine to complete this one. In case we need a separate ticket going forward, we can very well raise to cover other aspects not listed here.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 268}}
{"issue_key": "CSCI-373", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Aug/25 3:10 PM", "updated": "11/Sep/25 9:56 AM", "labels": [], "summary": "General Reference-Integration - Ingestion of 2 tables - review", "description": "DATA IN for critical identified objects from General Reference db\n\nBelow are the list of tables to be ingested. \n\n|General_Reference|AXVendorDetails_EDP|\n|General_Reference|BranchInfoGlobal_EDP|", "acceptance_criteria": "Definition of done:\n\n* -Extract meta created ()-\n* -Linked services created-\n* -Pipeline run end to end-\n* Whitelisting of on-prem prod database server. (Done by @user )", "comments": "", "text": "Summary\nGeneral Reference-Integration - Ingestion of 2 tables - review\n\n---\n\nDescription\nDATA IN for critical identified objects from General Reference db\n\nBelow are the list of tables to be ingested. \n\n|General_Reference|AXVendorDetails_EDP|\n|General_Reference|BranchInfoGlobal_EDP|\n\n---\n\nAcceptance Criteria\nDefinition of done:\n\n* -Extract meta created ()-\n* -Linked services created-\n* -Pipeline run end to end-\n* Whitelisting of on-prem prod database server. (Done by @user )", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 269}}
{"issue_key": "CSCI-372", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Aug/25 3:10 PM", "updated": "12/Sep/25 6:25 PM", "labels": [], "summary": "APPRISS Objects ingestion - remaining talbes ingestion", "description": "h2. Summary\n\nWe need to identify any additional objects required for APPRISS that have not yet been ingested into EDP Snowflake.\n\nh2. Context\n\nMost tables from the StockDb and TransactionStorage databases have already been ingested. This issue focuses on ensuring that all necessary objects for APPRISS are accounted for.\n\nh2. Other information\n\nN/A", "acceptance_criteria": "We need to set up pipelines for initial and delta loads for the following APPRISS tables\n\n* -Extract meta created-\n* -Linked services created-\n* -pipelines set up-\n* -initial load tested-\n* delta load tested\n* pipeline run from end to end", "comments": "starting on delta ingestion. cc @user\n\nneed to catch up with Adeel about access\n\n@user Diafy Gerardo was working on these. I was told they should have finished by yesterday. Needs confirmation\n\n@user cc @user\n\n@user : Could you please confirm if access to Blob is sorted?\n\ncc @user , @user\n\n@user / @user - would it be OK to get the servicenow ticket number for this?\n\nThanks\n\n@user Created a project task for it for the team. PRJTASK0189032. Diafy is aware of it and told me it shouldn’t take more than a couple of hours.\n\n@user [RITM0173947 | Requested Item | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/sc_req_item.do%3Fsys_id%3Daf06e80f3323e21047764f945d5c7b20%26sysparm_view%3D]\n\nFYI ticket complete\n\nAswini to get blob storage access as per next dependency discovered.\n\nRequest for Jumpbox for data engineers to use to access BLOB storage created. REQ0159213\n\nHi @user ,@ @user,\n\nHistoric data ingestion for TRANSACTIONS_ELECTRONICPAYMENTS , TransactionAuditSaleActivityLogHistory into snowflake is in progress. I am facing some data/ column issues which I will debug and reach out to @user to see how we can handle this issue in framework . Hopefully we will ingest these 2 tables this sprint.\n\nIssue split into:\n|CSCI-419|APPRISS Objects ingestion - remaining talbes ingestion part 2|", "text": "Summary\nAPPRISS Objects ingestion - remaining talbes ingestion\n\n---\n\nDescription\nh2. Summary\n\nWe need to identify any additional objects required for APPRISS that have not yet been ingested into EDP Snowflake.\n\nh2. Context\n\nMost tables from the StockDb and TransactionStorage databases have already been ingested. This issue focuses on ensuring that all necessary objects for APPRISS are accounted for.\n\nh2. Other information\n\nN/A\n\n---\n\nAcceptance Criteria\nWe need to set up pipelines for initial and delta loads for the following APPRISS tables\n\n* -Extract meta created-\n* -Linked services created-\n* -pipelines set up-\n* -initial load tested-\n* delta load tested\n* pipeline run from end to end\n\n---\n\nComments\nstarting on delta ingestion. cc @user\n\nneed to catch up with Adeel about access\n\n@user Diafy Gerardo was working on these. I was told they should have finished by yesterday. Needs confirmation\n\n@user cc @user\n\n@user : Could you please confirm if access to Blob is sorted?\n\ncc @user , @user\n\n@user / @user - would it be OK to get the servicenow ticket number for this?\n\nThanks\n\n@user Created a project task for it for the team. PRJTASK0189032. Diafy is aware of it and told me it shouldn’t take more than a couple of hours.\n\n@user [RITM0173947 | Requested Item | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/sc_req_item.do%3Fsys_id%3Daf06e80f3323e21047764f945d5c7b20%26sysparm_view%3D]\n\nFYI ticket complete\n\nAswini to get blob storage access as per next dependency discovered.\n\nRequest for Jumpbox for data engineers to use to access BLOB storage created. REQ0159213\n\nHi @user ,@ @user,\n\nHistoric data ingestion for TRANSACTIONS_ELECTRONICPAYMENTS , TransactionAuditSaleActivityLogHistory into snowflake is in progress. I am facing some data/ column issues which I will debug and reach out to @user to see how we can handle this issue in framework . Hopefully we will ingest these 2 tables this sprint.\n\nIssue split into:\n|CSCI-419|APPRISS Objects ingestion - remaining talbes ingestion part 2|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 270}}
{"issue_key": "CSCI-371", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Aug/25 2:41 PM", "updated": "16/Sep/25 9:56 AM", "labels": [], "summary": "APPRISS Sample datashare - review", "description": "Data share - Data Generated by CK - Manual data created, now need to be uploaded.\nThe sample data is placed under below share point location by CK. These sample data needs to be placed/loaded in data share.", "acceptance_criteria": "* -Data is uploaded.-", "comments": "", "text": "Summary\nAPPRISS Sample datashare - review\n\n---\n\nDescription\nData share - Data Generated by CK - Manual data created, now need to be uploaded.\nThe sample data is placed under below share point location by CK. These sample data needs to be placed/loaded in data share.\n\n---\n\nAcceptance Criteria\n* -Data is uploaded.-", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 271}}
{"issue_key": "CSCI-370", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Aug/25 2:34 PM", "updated": "12/Sep/25 2:48 PM", "labels": [], "summary": "Create an SSO approach with SCIM + SAML - part 2", "description": "This task involves implementing secure Single Sign-On (SSO) and automated user/group provisioning between *Microsoft Entra ID* and *Snowflake*, using *SAML 2.0* and *SCIM v2*, with all traffic routed through *Azure Private Link* to eliminate public exposure.\n\nThe configuration includes:\n\n* Creating and configuring the Snowflake Enterprise App in Entra ID.\n* Establishing SAML trust and metadata exchange between Entra ID and Snowflake.\n* Enabling SCIM API integration in Snowflake.\n* Setting up automatic provisioning in Entra ID.\n* Validating SSO and SCIM flows.\n* Applying security best practices and ongoing maintenance.", "acceptance_criteria": "* The Entra ID Enterprise Application for Snowflake is created and configured with correct SAML attributes.\n* SAML metadata is exchanged and validated between Entra ID and Snowflake.\n* SCIM API integration is successfully created in Snowflake with OAuth credentials.\n* Automatic provisioning is enabled in Entra ID and tested for users and groups.\n* SSO login via Entra ID (both IdP-initiated and SP-initiated) is functional.\n* All traffic is confirmed to route through Azure Private Link, with no public endpoints.\n* Logging and monitoring are enabled in both Snowflake and Entra ID.\n* Security controls such as MFA, RBAC, NSGs, and certificate rotation are implemented.\n* Documentation and disaster recovery procedures are updated and tested.", "comments": "Feel free to cut down the AC if you wish @user\n\nDocument available at: [SCIM+SAML - Snowflake and Entra ID SSO Configuration via Azure Private Link - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5170/SCIM-SAML-Snowflake-and-Entra-ID-SSO-Configuration-via-Azure-Private-Link]", "text": "Summary\nCreate an SSO approach with SCIM + SAML - part 2\n\n---\n\nDescription\nThis task involves implementing secure Single Sign-On (SSO) and automated user/group provisioning between *Microsoft Entra ID* and *Snowflake*, using *SAML 2.0* and *SCIM v2*, with all traffic routed through *Azure Private Link* to eliminate public exposure.\n\nThe configuration includes:\n\n* Creating and configuring the Snowflake Enterprise App in Entra ID.\n* Establishing SAML trust and metadata exchange between Entra ID and Snowflake.\n* Enabling SCIM API integration in Snowflake.\n* Setting up automatic provisioning in Entra ID.\n* Validating SSO and SCIM flows.\n* Applying security best practices and ongoing maintenance.\n\n---\n\nAcceptance Criteria\n* The Entra ID Enterprise Application for Snowflake is created and configured with correct SAML attributes.\n* SAML metadata is exchanged and validated between Entra ID and Snowflake.\n* SCIM API integration is successfully created in Snowflake with OAuth credentials.\n* Automatic provisioning is enabled in Entra ID and tested for users and groups.\n* SSO login via Entra ID (both IdP-initiated and SP-initiated) is functional.\n* All traffic is confirmed to route through Azure Private Link, with no public endpoints.\n* Logging and monitoring are enabled in both Snowflake and Entra ID.\n* Security controls such as MFA, RBAC, NSGs, and certificate rotation are implemented.\n* Documentation and disaster recovery procedures are updated and tested.\n\n---\n\nComments\nFeel free to cut down the AC if you wish @user\n\nDocument available at: [SCIM+SAML - Snowflake and Entra ID SSO Configuration via Azure Private Link - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5170/SCIM-SAML-Snowflake-and-Entra-ID-SSO-Configuration-via-Azure-Private-Link]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 272}}
{"issue_key": "CSCI-369", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Aug/25 11:06 AM", "updated": "10/Sep/25 4:38 AM", "labels": [], "summary": "VDI change in direction - notes", "description": "* Size VDI\n** CReate VM\n* Install tools inside VDI - \n\nExpecatation - for people to log into VDI for work.", "acceptance_criteria": "VDI should be accessible to access privatelink and blob storage", "comments": "", "text": "Summary\nVDI change in direction - notes\n\n---\n\nDescription\n* Size VDI\n** CReate VM\n* Install tools inside VDI - \n\nExpecatation - for people to log into VDI for work.\n\n---\n\nAcceptance Criteria\nVDI should be accessible to access privatelink and blob storage", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 273}}
{"issue_key": "CSCI-368", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Aug/25 10:37 AM", "updated": "30/Aug/25 3:10 PM", "labels": [], "summary": "General Reference-Integration - Ingestion of 2 tables - development", "description": "DATA IN for critical identified objects from General Reference db\n\nBelow are the list of tables to be ingested. \n\n|General_Reference|AXVendorDetails_EDP|\n|General_Reference|BranchInfoGlobal_EDP|", "acceptance_criteria": "Definition of done:\n\n* -Extract meta created ()-\n* -Linked services created-\n* -Pipeline run end to end-\n* Whitelisting of on-prem prod database server. (Done by @user )", "comments": "Ingestion of these 2 tables are done in snowflake dev.\n\nIssue split into:\n|CSCI-373|General Reference-Integration - Ingestion of 2 tables - review|", "text": "Summary\nGeneral Reference-Integration - Ingestion of 2 tables - development\n\n---\n\nDescription\nDATA IN for critical identified objects from General Reference db\n\nBelow are the list of tables to be ingested. \n\n|General_Reference|AXVendorDetails_EDP|\n|General_Reference|BranchInfoGlobal_EDP|\n\n---\n\nAcceptance Criteria\nDefinition of done:\n\n* -Extract meta created ()-\n* -Linked services created-\n* -Pipeline run end to end-\n* Whitelisting of on-prem prod database server. (Done by @user )\n\n---\n\nComments\nIngestion of these 2 tables are done in snowflake dev.\n\nIssue split into:\n|CSCI-373|General Reference-Integration - Ingestion of 2 tables - review|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 274}}
{"issue_key": "CSCI-367", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 2:58 PM", "updated": "27/Aug/25 3:00 PM", "labels": [], "summary": "202734 - Share Snowflake metadata with Entra Admin", "description": "Provide metadata to complete trust setup.\n\n_Steps:_\n\n* Copy {{SAML2_SNOWFLAKE_METADATA}} value\n* Share securely with Entra ID admin", "acceptance_criteria": "", "comments": "", "text": "Summary\n202734 - Share Snowflake metadata with Entra Admin\n\n---\n\nDescription\nProvide metadata to complete trust setup.\n\n_Steps:_\n\n* Copy {{SAML2_SNOWFLAKE_METADATA}} value\n* Share securely with Entra ID admin", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 275}}
{"issue_key": "CSCI-366", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 2:58 PM", "updated": "27/Aug/25 3:00 PM", "labels": [], "summary": "202732 - Validate Integration", "description": "Ensure Snowflake accepted configuration.\n\n_Steps:_\n\n* Run {{DESC SECURITY INTEGRATION ENTRA_ID_SSO}}\n* Confirm values (Issuer, SSO URL, Cert) are correct", "acceptance_criteria": "", "comments": "", "text": "Summary\n202732 - Validate Integration\n\n---\n\nDescription\nEnsure Snowflake accepted configuration.\n\n_Steps:_\n\n* Run {{DESC SECURITY INTEGRATION ENTRA_ID_SSO}}\n* Confirm values (Issuer, SSO URL, Cert) are correct", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 276}}
{"issue_key": "CSCI-365", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 2:57 PM", "updated": "27/Aug/25 3:00 PM", "labels": [], "summary": "202731 - Create SAML Security Integration", "description": "Define SAML integration with Entra ID.\n\n_Steps:_\n\n* Connect to Snowflake via Snowsight/CLI\n* Run {{CREATE OR REPLACE SECURITY INTEGRATION}} with Tenant ID, SSO URL, certificate", "acceptance_criteria": "", "comments": "", "text": "Summary\n202731 - Create SAML Security Integration\n\n---\n\nDescription\nDefine SAML integration with Entra ID.\n\n_Steps:_\n\n* Connect to Snowflake via Snowsight/CLI\n* Run {{CREATE OR REPLACE SECURITY INTEGRATION}} with Tenant ID, SSO URL, certificate", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 277}}
{"issue_key": "CSCI-354", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 2:32 PM", "updated": "10/Oct/25 5:42 PM", "labels": [], "summary": "202750 - Enable Provisioning", "description": "[devops task 202750|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202750]\n\n_Steps:_\n\n* Set provisioning *On*\n* Monitor sync progress in logs", "acceptance_criteria": "", "comments": "", "text": "Summary\n202750 - Enable Provisioning\n\n---\n\nDescription\n[devops task 202750|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202750]\n\n_Steps:_\n\n* Set provisioning *On*\n* Monitor sync progress in logs", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 278}}
{"issue_key": "CSCI-353", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 2:32 PM", "updated": "10/Oct/25 5:42 PM", "labels": [], "summary": "202749 - Configure attribute mappings", "description": "[devops task 202749|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202749]\n\n_Steps:_\n\n* Map user attributes (UPN → userName, mail → email, etc.)\n* Optionally configure groups", "acceptance_criteria": "", "comments": "", "text": "Summary\n202749 - Configure attribute mappings\n\n---\n\nDescription\n[devops task 202749|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202749]\n\n_Steps:_\n\n* Map user attributes (UPN → userName, mail → email, etc.)\n* Optionally configure groups", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 279}}
{"issue_key": "CSCI-352", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 2:31 PM", "updated": "10/Oct/25 5:42 PM", "labels": [], "summary": "202748 - Test Connection", "description": "[devops task 202748|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202748]\n\n_Steps:_\n\n* Click *Test Connection*\n* Verify success via PrivateLink", "acceptance_criteria": "", "comments": "", "text": "Summary\n202748 - Test Connection\n\n---\n\nDescription\n[devops task 202748|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202748]\n\n_Steps:_\n\n* Click *Test Connection*\n* Verify success via PrivateLink", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 280}}
{"issue_key": "CSCI-351", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 2:31 PM", "updated": "10/Oct/25 5:42 PM", "labels": [], "summary": "202747 - Input SCIM endpoint & secret", "description": "[devops task 202747|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202747]\n\n_Steps:_\n\n* Paste Snowflake SCIM endpoint\n* Paste {{OAUTH_CLIENT_SECRET}}", "acceptance_criteria": "", "comments": "", "text": "Summary\n202747 - Input SCIM endpoint & secret\n\n---\n\nDescription\n[devops task 202747|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202747]\n\n_Steps:_\n\n* Paste Snowflake SCIM endpoint\n* Paste {{OAUTH_CLIENT_SECRET}}", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 281}}
{"issue_key": "CSCI-350", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 2:31 PM", "updated": "10/Oct/25 5:42 PM", "labels": [], "summary": "202746 - Configure provisioning mode = Automatic", "description": "[devops link 202746|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202746]\n\n_Steps:_\n\n* Open Snowflake Enterprise App → *Provisioning*\n* Select *Automatic*", "acceptance_criteria": "", "comments": "", "text": "Summary\n202746 - Configure provisioning mode = Automatic\n\n---\n\nDescription\n[devops link 202746|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202746]\n\n_Steps:_\n\n* Open Snowflake Enterprise App → *Provisioning*\n* Select *Automatic*", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 282}}
{"issue_key": "CSCI-345", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:27 PM", "updated": "02/Sep/25 4:00 AM", "labels": [], "summary": "203066 - Add DNS entries to privatelink.snowflakecomputing.com", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\n203066 - Add DNS entries to privatelink.snowflakecomputing.com", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 283}}
{"issue_key": "CSCI-344", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:26 PM", "updated": "02/Sep/25 4:01 AM", "labels": [], "summary": "202774 - Add NSG outbound rule to service tag AzureActiveDirectory from snet-vm", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\n202774 - Add NSG outbound rule to service tag AzureActiveDirectory from snet-vm", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 284}}
{"issue_key": "CSCI-343", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:26 PM", "updated": "11/Sep/25 9:32 AM", "labels": [], "summary": "202739 - Troubleshoot errors if required", "description": "Resolve SAML issues.\n\n_Steps:_\n\n* Check Entra Sign-in logs\n* Validate DNS points to Private Endpoint\n* Re-check certificate format", "acceptance_criteria": "", "comments": "Hi @user - just want to know where this task is up to. Thanks.", "text": "Summary\n202739 - Troubleshoot errors if required\n\n---\n\nDescription\nResolve SAML issues.\n\n_Steps:_\n\n* Check Entra Sign-in logs\n* Validate DNS points to Private Endpoint\n* Re-check certificate format\n\n---\n\nComments\nHi @user - just want to know where this task is up to. Thanks.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 285}}
{"issue_key": "CSCI-342", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:26 PM", "updated": "27/Aug/25 1:29 PM", "labels": [], "summary": "202738 - Test SP-initiated login", "description": "Confirm login works via Snowflake portal.\n\n_Steps:_\n\n* Open PrivateLink Snowflake URL\n* Select “Sign in with SSO”\n* Verify access granted", "acceptance_criteria": "", "comments": "", "text": "Summary\n202738 - Test SP-initiated login\n\n---\n\nDescription\nConfirm login works via Snowflake portal.\n\n_Steps:_\n\n* Open PrivateLink Snowflake URL\n* Select “Sign in with SSO”\n* Verify access granted", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 286}}
{"issue_key": "CSCI-341", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:26 PM", "updated": "27/Aug/25 1:31 PM", "labels": [], "summary": "202737 - Test IdP-initiated login", "description": "Confirm login works via MyApps.\n\n_Steps:_\n\n* Sign into MyApps portal\n* Launch Snowflake app → expect MFA + successful login", "acceptance_criteria": "", "comments": "", "text": "Summary\n202737 - Test IdP-initiated login\n\n---\n\nDescription\nConfirm login works via MyApps.\n\n_Steps:_\n\n* Sign into MyApps portal\n* Launch Snowflake app → expect MFA + successful login", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 287}}
{"issue_key": "CSCI-340", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:26 PM", "updated": "19/Sep/25 3:47 PM", "labels": [], "summary": "202736 - Raveen - Update Entra SAML Config with Snowflake metadata", "description": "Finalize trust with Snowflake metadata.\n\n_Steps:_\n\n* Update Identifier/Reply URL with Snowflake metadata\n* Save changes in Entra ID", "acceptance_criteria": "", "comments": "", "text": "Summary\n202736 - Raveen - Update Entra SAML Config with Snowflake metadata\n\n---\n\nDescription\nFinalize trust with Snowflake metadata.\n\n_Steps:_\n\n* Update Identifier/Reply URL with Snowflake metadata\n* Save changes in Entra ID", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 288}}
{"issue_key": "CSCI-339", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:18 PM", "updated": "03/Sep/25 9:54 AM", "labels": [], "summary": "202733 - Document Tenant ID, App ID, SAML URLs", "description": "[202733|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202733]\n\nCapture values for use in Snowflake config.\n\n_Steps:_\n\n* Note Tenant ID (GUID)\n* Note Application (client) ID\n* Store in shared secure documentation (e.g., OneNote, Key Vault)", "acceptance_criteria": "", "comments": "", "text": "Summary\n202733 - Document Tenant ID, App ID, SAML URLs\n\n---\n\nDescription\n[202733|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202733]\n\nCapture values for use in Snowflake config.\n\n_Steps:_\n\n* Note Tenant ID (GUID)\n* Note Application (client) ID\n* Store in shared secure documentation (e.g., OneNote, Key Vault)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 289}}
{"issue_key": "CSCI-338", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:18 PM", "updated": "03/Sep/25 9:53 AM", "labels": [], "summary": "TASK 202729 202729 Extract Entra ID Certificate and Metadata", "description": "[202729|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202729]\n\nExport required certificate for Snowflake.\n\n_Steps:_\n\n* Download *Base64 certificate*\n* Copy *App Federation Metadata URL*", "acceptance_criteria": "", "comments": "", "text": "Summary\nTASK 202729 202729 Extract Entra ID Certificate and Metadata\n\n---\n\nDescription\n[202729|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202729]\n\nExport required certificate for Snowflake.\n\n_Steps:_\n\n* Download *Base64 certificate*\n* Copy *App Federation Metadata URL*", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 290}}
{"issue_key": "CSCI-337", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:18 PM", "updated": "03/Sep/25 9:52 AM", "labels": [], "summary": "202728 Add SAML Claims", "description": "[202728 |https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202728]\n\nMap Entra user attributes to Snowflake claims.\n\n_Steps:_\n\n* Add claims for email, givenName, surname, and UPN\n* Verify values match Snowflake requirements", "acceptance_criteria": "", "comments": "", "text": "Summary\n202728 Add SAML Claims\n\n---\n\nDescription\n[202728 |https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202728]\n\nMap Entra user attributes to Snowflake claims.\n\n_Steps:_\n\n* Add claims for email, givenName, surname, and UPN\n* Verify values match Snowflake requirements", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 291}}
{"issue_key": "CSCI-336", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:17 PM", "updated": "03/Sep/25 9:51 AM", "labels": [], "summary": "202727 Configure Basic SAML Settings", "description": "[202727|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202727]\n\nEnsure SAML endpoints use PrivateLink.\n\n_Steps:_\n\n* Open the Snowflake Enterprise App → *Single Sign-On*\n* Configure Identifier, Reply URL, Sign-On URL, and Logout URL using {{https://cw-au.privatelink.snowflakecomputing.com}}", "acceptance_criteria": "", "comments": "", "text": "Summary\n202727 Configure Basic SAML Settings\n\n---\n\nDescription\n[202727|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202727]\n\nEnsure SAML endpoints use PrivateLink.\n\n_Steps:_\n\n* Open the Snowflake Enterprise App → *Single Sign-On*\n* Configure Identifier, Reply URL, Sign-On URL, and Logout URL using {{https://cw-au.privatelink.snowflakecomputing.com}}", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 292}}
{"issue_key": "CSCI-335", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:17 PM", "updated": "03/Sep/25 9:50 AM", "labels": [], "summary": "202726 Create Snowflake Enterprise Application", "description": "[Devops|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202276]\n\nAdd Snowflake as an application in Entra ID.\n\n_Steps:_\n\n* Go to *Entra ID → Enterprise Applications*\n* Click *+ New Application* → Search *Snowflake*\n* Select and click *Create*", "acceptance_criteria": "", "comments": "", "text": "Summary\n202726 Create Snowflake Enterprise Application\n\n---\n\nDescription\n[Devops|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202276]\n\nAdd Snowflake as an application in Entra ID.\n\n_Steps:_\n\n* Go to *Entra ID → Enterprise Applications*\n* Click *+ New Application* → Search *Snowflake*\n* Select and click *Create*", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 293}}
{"issue_key": "CSCI-334", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:14 PM", "updated": "19/Nov/25 11:38 AM", "labels": [], "summary": "202757 - Apply Security Best Practices", "description": "[devops task 202757|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202757]\n\n_As a Security Engineer, I want to enforce security best practices so the integration remains compliant._", "acceptance_criteria": "* Public endpoints disabled\n* NSGs enforced\n* Conditional Access & MFA applied\n* Least privilege for SCIM account", "comments": "", "text": "Summary\n202757 - Apply Security Best Practices\n\n---\n\nDescription\n[devops task 202757|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202757]\n\n_As a Security Engineer, I want to enforce security best practices so the integration remains compliant._\n\n---\n\nAcceptance Criteria\n* Public endpoints disabled\n* NSGs enforced\n* Conditional Access & MFA applied\n* Least privilege for SCIM account", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 294}}
{"issue_key": "CSCI-333", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:12 PM", "updated": "19/Nov/25 11:38 AM", "labels": [], "summary": "202751 - Test & Monitor Integration", "description": "[devops task 202751|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202751]\n\n_As a Security Engineer, I want to test and monitor SSO + SCIM so I can ensure reliability and compliance._", "acceptance_criteria": "* Test user provisioned successfully\n* Group sync validated\n* Logs visible in Snowflake & Entra", "comments": "", "text": "Summary\n202751 - Test & Monitor Integration\n\n---\n\nDescription\n[devops task 202751|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202751]\n\n_As a Security Engineer, I want to test and monitor SSO + SCIM so I can ensure reliability and compliance._\n\n---\n\nAcceptance Criteria\n* Test user provisioned successfully\n* Group sync validated\n* Logs visible in Snowflake & Entra", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 295}}
{"issue_key": "CSCI-332", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:12 PM", "updated": "10/Oct/25 5:42 PM", "labels": [], "summary": "202745 - Raveen - Configure SCIM Provision in Entra ID", "description": "_As an Azure Admin, I want to configure SCIM so that Entra ID provisions users/groups automatically._", "acceptance_criteria": "[devops link 202745|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202745]\n\n* Automatic provisioning enabled\n* Test connection succeeds\n* Attribute mappings configured\n* Initial sync successful", "comments": "@user - to confirm the new naming convention on AD groups.\n\n@user cc @user for more details\n\nHi @user , @user ,\n\nKindly review, on the AD snowflake groups, we would like to propose these.\n\n|*Current*|*Proposed*|\n|DEV_PlatformEngineers|Snowflake-dev-PlatformEngineers|\n|DEV_DataEngineers|Snowflake-dev-DataEngineers|\n|DEV_Consultants|Snowflake-dev-Consultants|\n|STEST_PlatformEngineers|Snowflake-stest-PlatformEngineers|\n|STEST_DataEngineers|Snowflake-stest-DataEngineers|\n|STEST_Consultants|Snowflake-stest-Consultants|\n|QA_PlatformEngineers|Snowflake-qa-PlatformEngineers|\n|QA_DataEngineers|Snowflake-qa-DataEngineers|\n|QA_DataAnalysts|Snowflake-qa-DataAnalysts|\n|QA_BusinessUsers|Snowflake-qa-BusinessUsers|\n|PROD_PlatformEngineers|Snowflake-prod-PlatformEngineers|\n|PROD_DataEngineers|Snowflake-prod-DataEngineers|\n|PROD_Consultants|Snowflake-prod-Consultants|\n|PROD_DataAnalysts|Snowflake-prod-DataAnalysts|\n|PROD_IT_Audit|Snowflake-prod-IT-Audit|\n\nIf we need one for PowerBI, we can have similar group created and associate the member to that group. e.g\n\nPowerBI-dev-Admin\n\nPowerBI-dev-Developer\n\nPowerBI-dev-Viewer\n\n@user @user , @user\n\nHI @user \n\nAs Alan confirmed, we are progressing to provision these AD groups and associate members to it.\n\n[^snowflake_ad_groups_members_V0.1.xlsx]\n\n@user , @user\n\nA [RITM0174818 |https://cwretail.service-now.com/nav_to.do?uri=sc_req_item.do?sys_id=96d0763a3337ae5047764f945d5c7b69]has been raised to provision the groups and associate members\n\n@user , @user \n\nTask is progressing. We have the groups created in AD and it was sync to EntraId, member association is in progress\n\n@user Thanks\nI was wondering if there are any paper trails to this progress Ravee?\nIf not, no stress.\n\n@user please note, SCIM implementation steps are [here|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5170/SCIM-SAML-Snowflake-and-Entra-ID-SSO-Configuration-via-Azure-Private-Link]\n\nCreating of Groups/Members in AD/Endra are managed by SysOps/Cloud\n\nHi @user This task be closed now. We were able to sync successfully groups/members to snowflake\n\n@user @user", "text": "Summary\n202745 - Raveen - Configure SCIM Provision in Entra ID\n\n---\n\nDescription\n_As an Azure Admin, I want to configure SCIM so that Entra ID provisions users/groups automatically._\n\n---\n\nAcceptance Criteria\n[devops link 202745|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202745]\n\n* Automatic provisioning enabled\n* Test connection succeeds\n* Attribute mappings configured\n* Initial sync successful\n\n---\n\nComments\n@user - to confirm the new naming convention on AD groups.\n\n@user cc @user for more details\n\nHi @user , @user ,\n\nKindly review, on the AD snowflake groups, we would like to propose these.\n\n|*Current*|*Proposed*|\n|DEV_PlatformEngineers|Snowflake-dev-PlatformEngineers|\n|DEV_DataEngineers|Snowflake-dev-DataEngineers|\n|DEV_Consultants|Snowflake-dev-Consultants|\n|STEST_PlatformEngineers|Snowflake-stest-PlatformEngineers|\n|STEST_DataEngineers|Snowflake-stest-DataEngineers|\n|STEST_Consultants|Snowflake-stest-Consultants|\n|QA_PlatformEngineers|Snowflake-qa-PlatformEngineers|\n|QA_DataEngineers|Snowflake-qa-DataEngineers|\n|QA_DataAnalysts|Snowflake-qa-DataAnalysts|\n|QA_BusinessUsers|Snowflake-qa-BusinessUsers|\n|PROD_PlatformEngineers|Snowflake-prod-PlatformEngineers|\n|PROD_DataEngineers|Snowflake-prod-DataEngineers|\n|PROD_Consultants|Snowflake-prod-Consultants|\n|PROD_DataAnalysts|Snowflake-prod-DataAnalysts|\n|PROD_IT_Audit|Snowflake-prod-IT-Audit|\n\nIf we need one for PowerBI, we can have similar group created and associate the member to that group. e.g\n\nPowerBI-dev-Admin\n\nPowerBI-dev-Developer\n\nPowerBI-dev-Viewer\n\n@user @user , @user\n\nHI @user \n\nAs Alan confirmed, we are progressing to provision these AD groups and associate members to it.\n\n[^snowflake_ad_groups_members_V0.1.xlsx]\n\n@user , @user\n\nA [RITM0174818 |https://cwretail.service-now.com/nav_to.do?uri=sc_req_item.do?sys_id=96d0763a3337ae5047764f945d5c7b69]has been raised to provision the groups and associate members\n\n@user , @user \n\nTask is progressing. We have the groups created in AD and it was sync to EntraId, member association is in progress\n\n@user Thanks\nI was wondering if there are any paper trails to this progress Ravee?\nIf not, no stress.\n\n@user please note, SCIM implementation steps are [here|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5170/SCIM-SAML-Snowflake-and-Entra-ID-SSO-Configuration-via-Azure-Private-Link]\n\nCreating of Groups/Members in AD/Endra are managed by SysOps/Cloud\n\nHi @user This task be closed now. We were able to sync successfully groups/members to snowflake\n\n@user @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 296}}
{"issue_key": "CSCI-331", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:09 PM", "updated": "02/Oct/25 2:15 PM", "labels": [], "summary": "202740 - Enable SCIM API Integration in Snowflake", "description": "[devops task 202740|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202740]\n\n_As a Snowflake Admin, I want to enable SCIM API integration so Entra ID can provision users automatically._", "acceptance_criteria": "* SCIM integration created\n* OAuth credentials generated\n* SCIM endpoint available", "comments": "", "text": "Summary\n202740 - Enable SCIM API Integration in Snowflake\n\n---\n\nDescription\n[devops task 202740|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202740]\n\n_As a Snowflake Admin, I want to enable SCIM API integration so Entra ID can provision users automatically._\n\n---\n\nAcceptance Criteria\n* SCIM integration created\n* OAuth credentials generated\n* SCIM endpoint available", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 297}}
{"issue_key": "CSCI-330", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:09 PM", "updated": "10/Oct/25 10:12 AM", "labels": [], "summary": "Authenticate to Snowflake via Entra ID", "description": "As a user, I want to authenticate to Snowflake via Entra ID so that I can use my corporate credentials for seamless sign-on.", "acceptance_criteria": "* IdP-initiated login works via MyApps\n* SP-initiated login works via PrivateLink\n* MFA enforced\n* DNS resolution works", "comments": "SSO Authentication and SCIM sync are successfully configured.\n\nTested by Ashu (authentication) and Chloe (SCIM - AAD provisioned)", "text": "Summary\nAuthenticate to Snowflake via Entra ID\n\n---\n\nDescription\nAs a user, I want to authenticate to Snowflake via Entra ID so that I can use my corporate credentials for seamless sign-on.\n\n---\n\nAcceptance Criteria\n* IdP-initiated login works via MyApps\n* SP-initiated login works via PrivateLink\n* MFA enforced\n* DNS resolution works\n\n---\n\nComments\nSSO Authentication and SCIM sync are successfully configured.\n\nTested by Ashu (authentication) and Chloe (SCIM - AAD provisioned)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 298}}
{"issue_key": "CSCI-329", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:04 PM", "updated": "27/Aug/25 2:57 PM", "labels": [], "summary": "202730 - Snowflake SAML Security Integration", "description": "As a Snowflake Administrator, I want to configure SAML authentication so that users can log in using Entra ID credentials", "acceptance_criteria": "* SAML2 security integration created in Snowflake\n* Snowflake metadata generated and shared with Entra ID\n* Test query confirms integration is enabled", "comments": "", "text": "Summary\n202730 - Snowflake SAML Security Integration\n\n---\n\nDescription\nAs a Snowflake Administrator, I want to configure SAML authentication so that users can log in using Entra ID credentials\n\n---\n\nAcceptance Criteria\n* SAML2 security integration created in Snowflake\n* Snowflake metadata generated and shared with Entra ID\n* Test query confirms integration is enabled", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 299}}
{"issue_key": "CSCI-328", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "27/Aug/25 1:03 PM", "updated": "03/Sep/25 9:51 AM", "labels": [], "summary": "202725 - Raveen - Entra ID enterprise Application setup", "description": "[202725|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202725]\n\nAs an Identity Administrator, I want to create and configure the Snowflake enterprise application in Entra ID so that the foundation for SSO is established", "acceptance_criteria": "* Snowflake enterprise application is created from Azure AD gallery\n* Basic SAML settings are configured with Private Link URLs\n* SAML attributes are properly mapped for user claims\n* Entra ID certificate and metadata are extracted", "comments": "", "text": "Summary\n202725 - Raveen - Entra ID enterprise Application setup\n\n---\n\nDescription\n[202725|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/202725]\n\nAs an Identity Administrator, I want to create and configure the Snowflake enterprise application in Entra ID so that the foundation for SSO is established\n\n---\n\nAcceptance Criteria\n* Snowflake enterprise application is created from Azure AD gallery\n* Basic SAML settings are configured with Private Link URLs\n* SAML attributes are properly mapped for user claims\n* Entra ID certificate and metadata are extracted", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 300}}
{"issue_key": "CSCI-327", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "26/Aug/25 11:56 AM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Review Source-to-Target Map for Dim_Employee", "description": "Review the mapping of DIM_Employee Dimension: DIM_Employee", "acceptance_criteria": "* Finalize that the work on Source-to-Target Map for DIM_Employee correct.\n** Review the Source-to-Target Map for DIM_Employee against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.", "comments": "", "text": "Summary\nReview Source-to-Target Map for Dim_Employee\n\n---\n\nDescription\nReview the mapping of DIM_Employee Dimension: DIM_Employee\n\n---\n\nAcceptance Criteria\n* Finalize that the work on Source-to-Target Map for DIM_Employee correct.\n** Review the Source-to-Target Map for DIM_Employee against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 301}}
{"issue_key": "CSCI-326", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Aug/25 12:16 PM", "updated": "01/Sep/25 2:18 AM", "labels": [], "summary": "Extract parquet for TransactionAuditSaleActivityLog - part 1", "description": "Extract the parquets files for these tables \n\n* TransactionStorage.dbo.TransactionAuditSaleActivityLog\n* TransactionStorage.dbo.TransactionAuditSaleActivityLogHistory\n* TransactionStorage.dbo.Transactions_ElectronicPayments", "acceptance_criteria": "* Extract the parquet files\n* File validation\n* Upload it in Azure blob storage\n* Post-upload verification\n* Documentation", "comments": "Created this task as @user has requested parquet extracts\n\ncc:- @user\n\n@user \n\nupdated AC - LMK if that makes sense - can change back if needed\n\n@user I’m currently blocked by this firewall error when attempting to access Blob Storage to upload the Parquet file.\n\n@user do you have a servicenow ticket for this? - If so just screenshot here and I’ll create a Jira ticket to establish parity.\n\ncc @user\n\n@user Raised the incident\n[INC0474862 | Incident | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/incident.do%3Fsys_id%3D8103494e932b2a50f226b5058aba1078%26sysparm_stack%3D%26sysparm_view%3D]\n\nExtracted the parquet files for TDB14.TransactionStorage.dbo.TransactionAuditSaleActivityLogHistory\n\nAccess to the blob storage is still blocked by the firewall, I cannot upload the files into the container for ingestion.\n\n@user @user Need assistance from the network team to resolve this, can you help escalate? [INC0474862 | Incident | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/incident.do%3Fsys_id%3D8103494e932b2a50f226b5058aba1078%26sysparm_stack%3D%26sysparm_view%3D]\n\nI see you already raised ticket. @user will follow up and talk to the cloud team on this. \n\nThis is due to the changes to firewall where conditional forwarding for {{privatelink.dfs.core.windows.net}} to setup private connectivity from On-prem/VDI to our Azure Data Lake File System Gen2 Storage Accounts. so since you guys are using to directly access the storage accounts manually then we must allow connections from on-prem/vdi to the storage accounts.\n\n@user the Infra Cloud team has raised a request with the Information Security team to allow my IP address for blob storage access.\n\n[RITM0173947 | Requested Item | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/sc_req_item.do%3Fsys_id%3Daf06e80f3323e21047764f945d5c7b20%26sysparm_stack%26sysparm_view]\n\nIssue split into:\n|CSCI-379|Extract parquet for TransactionAuditSaleActivityLog - part 2|", "text": "Summary\nExtract parquet for TransactionAuditSaleActivityLog - part 1\n\n---\n\nDescription\nExtract the parquets files for these tables \n\n* TransactionStorage.dbo.TransactionAuditSaleActivityLog\n* TransactionStorage.dbo.TransactionAuditSaleActivityLogHistory\n* TransactionStorage.dbo.Transactions_ElectronicPayments\n\n---\n\nAcceptance Criteria\n* Extract the parquet files\n* File validation\n* Upload it in Azure blob storage\n* Post-upload verification\n* Documentation\n\n---\n\nComments\nCreated this task as @user has requested parquet extracts\n\ncc:- @user\n\n@user \n\nupdated AC - LMK if that makes sense - can change back if needed\n\n@user I’m currently blocked by this firewall error when attempting to access Blob Storage to upload the Parquet file.\n\n@user do you have a servicenow ticket for this? - If so just screenshot here and I’ll create a Jira ticket to establish parity.\n\ncc @user\n\n@user Raised the incident\n[INC0474862 | Incident | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/incident.do%3Fsys_id%3D8103494e932b2a50f226b5058aba1078%26sysparm_stack%3D%26sysparm_view%3D]\n\nExtracted the parquet files for TDB14.TransactionStorage.dbo.TransactionAuditSaleActivityLogHistory\n\nAccess to the blob storage is still blocked by the firewall, I cannot upload the files into the container for ingestion.\n\n@user @user Need assistance from the network team to resolve this, can you help escalate? [INC0474862 | Incident | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/incident.do%3Fsys_id%3D8103494e932b2a50f226b5058aba1078%26sysparm_stack%3D%26sysparm_view%3D]\n\nI see you already raised ticket. @user will follow up and talk to the cloud team on this. \n\nThis is due to the changes to firewall where conditional forwarding for {{privatelink.dfs.core.windows.net}} to setup private connectivity from On-prem/VDI to our Azure Data Lake File System Gen2 Storage Accounts. so since you guys are using to directly access the storage accounts manually then we must allow connections from on-prem/vdi to the storage accounts.\n\n@user the Infra Cloud team has raised a request with the Information Security team to allow my IP address for blob storage access.\n\n[RITM0173947 | Requested Item | Chemist Warehouse Retail|https://cwretail.service-now.com/now/nav/ui/classic/params/target/sc_req_item.do%3Fsys_id%3Daf06e80f3323e21047764f945d5c7b20%26sysparm_stack%26sysparm_view]\n\nIssue split into:\n|CSCI-379|Extract parquet for TransactionAuditSaleActivityLog - part 2|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 302}}
{"issue_key": "CSCI-325", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Aug/25 9:50 AM", "updated": "09/Oct/25 9:35 AM", "labels": [], "summary": "CT - Snowflake CI/CD process - Create PR", "description": "* Extract Meta\n* Schema DDL (tables + views)\n* Create PR\n* Review and merge\n* Test end to end", "acceptance_criteria": "", "comments": "", "text": "Summary\nCT - Snowflake CI/CD process - Create PR\n\n---\n\nDescription\n* Extract Meta\n* Schema DDL (tables + views)\n* Create PR\n* Review and merge\n* Test end to end", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 303}}
{"issue_key": "CSCI-324", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Aug/25 9:49 AM", "updated": "08/Oct/25 4:44 PM", "labels": [], "summary": "AA - Snowflake CI/CD process - Create PR", "description": "* Extract Meta\n* Schema DDL (tables + views)\n* Create PR\n* Review and merge\n* Test end to end", "acceptance_criteria": "", "comments": "", "text": "Summary\nAA - Snowflake CI/CD process - Create PR\n\n---\n\nDescription\n* Extract Meta\n* Schema DDL (tables + views)\n* Create PR\n* Review and merge\n* Test end to end", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 304}}
{"issue_key": "CSCI-323", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Aug/25 9:45 AM", "updated": "12/Sep/25 6:32 PM", "labels": [], "summary": "APPRISS - next steps on data modelling", "description": "* -Identify business process for APPRISS-\n\nIdentify source table(s) for APPRISS - partially done (review)\n\nCreate structure for FActs and Dims\n\n* Finalizing identification of business processes relevant to APPRISS.\n* Reviewing and confirming source tables for APPRISS data ingestion.\n* Designing and documenting the structure for Fact and Dimension tables to support robust analytics and reporting.\nThis task aims to ensure a clear, scalable, and auditable data model foundation for APPRISS integration within the CW Cloud Data Platform Interim Solution.", "acceptance_criteria": "* -Business Process Identification-\n* -Source Table Confirmation-\n* -Review Initial Data Extract Queries-\n* - Dimension Table Design-\n* -Fact Tables Design-\n* -Sample Data Validation-\n* -Documentation-\n* Stakeholder Review\n* -Readiness for Next Steps-", "comments": "Appriss Data points for Ingestion Purpose are identified. Received the queries for sample data that has been shared with APPRISS team. \n\nWill review the queries today.\n\nFact table structure is in progress.\n\nBusiness Process Identification Complete\n\nSource Table Identification Complete\n\nInitial Query review Complete. Good from ingestion point of view.\n\nFact Table Structure is updated in MergeCo Folder \n[Business Matrix_Retail - Simplified.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Business%20Matrix_Retail%20-%20Simplified.xlsx?d=w0e262bf7be84407d8dbff2bb4d4c5e00&csf=1&web=1&e=dGjeuu]\n\nDim tables Silver Layer Structure is updated in Confluence\n[https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model|https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model]\n\nDim tables Silver Layer Structure is updated in Confluence\n[https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model|https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model]\n\nSample source data is validated\n\nSource Data is identified. Target table structure is there now. So the next step would be to start with transformation logic documentation\n\nIssue split into:\n|CSCI-438|APPRISS - next steps on data modelling - alan to review|", "text": "Summary\nAPPRISS - next steps on data modelling\n\n---\n\nDescription\n* -Identify business process for APPRISS-\n\nIdentify source table(s) for APPRISS - partially done (review)\n\nCreate structure for FActs and Dims\n\n* Finalizing identification of business processes relevant to APPRISS.\n* Reviewing and confirming source tables for APPRISS data ingestion.\n* Designing and documenting the structure for Fact and Dimension tables to support robust analytics and reporting.\nThis task aims to ensure a clear, scalable, and auditable data model foundation for APPRISS integration within the CW Cloud Data Platform Interim Solution.\n\n---\n\nAcceptance Criteria\n* -Business Process Identification-\n* -Source Table Confirmation-\n* -Review Initial Data Extract Queries-\n* - Dimension Table Design-\n* -Fact Tables Design-\n* -Sample Data Validation-\n* -Documentation-\n* Stakeholder Review\n* -Readiness for Next Steps-\n\n---\n\nComments\nAppriss Data points for Ingestion Purpose are identified. Received the queries for sample data that has been shared with APPRISS team. \n\nWill review the queries today.\n\nFact table structure is in progress.\n\nBusiness Process Identification Complete\n\nSource Table Identification Complete\n\nInitial Query review Complete. Good from ingestion point of view.\n\nFact Table Structure is updated in MergeCo Folder \n[Business Matrix_Retail - Simplified.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Business%20Matrix_Retail%20-%20Simplified.xlsx?d=w0e262bf7be84407d8dbff2bb4d4c5e00&csf=1&web=1&e=dGjeuu]\n\nDim tables Silver Layer Structure is updated in Confluence\n[https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model|https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model]\n\nDim tables Silver Layer Structure is updated in Confluence\n[https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model|https://sigmahealthcare.atlassian.net/wiki/spaces/DE/pages/1610711044/Enterprise+Data+Model]\n\nSample source data is validated\n\nSource Data is identified. Target table structure is there now. So the next step would be to start with transformation logic documentation\n\nIssue split into:\n|CSCI-438|APPRISS - next steps on data modelling - alan to review|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 305}}
{"issue_key": "CSCI-321", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Aug/25 9:40 AM", "updated": "09/Oct/25 9:36 AM", "labels": [], "summary": "ILS Manhattan Scale Integration : Whitelist Prod server", "description": "To create a request to whitelist the PROD server for ADF SHIR in Dev", "acceptance_criteria": "", "comments": "I think this is blocked @user ?\n\ncc @user \n\nWill raise servicenow request - near end of sprint 8\n\nI will attend to this next week. Gotta focus on the Detailed Design doco @user\n\nChloe to catch up with Adeel on details", "text": "Summary\nILS Manhattan Scale Integration : Whitelist Prod server\n\n---\n\nDescription\nTo create a request to whitelist the PROD server for ADF SHIR in Dev\n\n---\n\nComments\nI think this is blocked @user ?\n\ncc @user \n\nWill raise servicenow request - near end of sprint 8\n\nI will attend to this next week. Gotta focus on the Detailed Design doco @user\n\nChloe to catch up with Adeel on details", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 306}}
{"issue_key": "CSCI-320", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Aug/25 9:30 AM", "updated": "08/Dec/25 1:38 PM", "labels": [], "summary": "AP - Snowflake CI/CD process - Create PR", "description": "* Extract Meta\n* Schema DDL (tables + views)\n* Create PR\n* Review and merge\n* Test end to end", "acceptance_criteria": "", "comments": "", "text": "Summary\nAP - Snowflake CI/CD process - Create PR\n\n---\n\nDescription\n* Extract Meta\n* Schema DDL (tables + views)\n* Create PR\n* Review and merge\n* Test end to end", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 307}}
{"issue_key": "CSCI-319", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Aug/25 9:28 AM", "updated": "25/Sep/25 9:49 AM", "labels": [], "summary": "CT - ADF CI/CD process - Create PR", "description": "* Deployment checklist\n* Add trigger\n* Create PR\n* Get approval and Merger\n* End to end testing\n* Publish schedule", "acceptance_criteria": "", "comments": "To walk through the ADF process to create a PR with Ashu, Aswini and Jess", "text": "Summary\nCT - ADF CI/CD process - Create PR\n\n---\n\nDescription\n* Deployment checklist\n* Add trigger\n* Create PR\n* Get approval and Merger\n* End to end testing\n* Publish schedule\n\n---\n\nComments\nTo walk through the ADF process to create a PR with Ashu, Aswini and Jess", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 308}}
{"issue_key": "CSCI-318", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Aug/25 9:27 AM", "updated": "25/Sep/25 11:51 AM", "labels": [], "summary": "AA - ADF CI/CD process - Create PR", "description": "* Deployment checklist\n* Add trigger\n* Create PR\n* Get approval and Merger\n* End to end testing\n* Publish schedule", "acceptance_criteria": "", "comments": "ran this thru Rovo, is it OK to check @user ?\n\nyou can use this for future tickets as a scaffold\n\n----\n\nh3. Context\n\n* The project requires implementing a CI/CD process for Azure Data Factory (ADF) pipelines as part of the CW Cloud Data Platform Interim Solution. This involves automating deployment steps, ensuring code quality, and enabling smooth release management through pull requests and approvals.\n\nh3. Objective\n\n* To establish and execute a robust CI/CD workflow for ADF, including deployment automation, code review, approval, and scheduling, ensuring reliable and efficient delivery of data platform changes.\n\nh3. Steps\n\n# Complete the deployment checklist for ADF CI/CD.\n# Add the necessary trigger(s) to automate the pipeline.\n# Create a Pull Request (PR) for the changes.\n# Obtain approval and merge the PR.\n# Conduct end-to-end testing of the deployment.\n# Publish and schedule the pipeline as required.\n\nh3. Deliverables\n\n* A fully operational CI/CD process for ADF, including:\n** Documented deployment checklist\n** Configured triggers\n** Merged and approved PR\n** Evidence of successful end-to-end testing\n** Published and scheduled ADF pipeline\n\nh3. Assumptions (Optional)\n\n* Required permissions are available for repository and ADF resources.\n* Stakeholders are available for timely PR review and approval.\n* Test environments are set up and accessible.\n\nh3. Acceptance criteria\n\n* Deployment checklist is completed and documented.\n* Trigger(s) are added and verified.\n* Pull Request is created, reviewed, approved, and merged.\n* End-to-end testing is completed with no critical issues.\n* Pipeline is published and scheduled as per requirements.\n\nCreated a PR, reviewing another branch that got created during the process.", "text": "Summary\nAA - ADF CI/CD process - Create PR\n\n---\n\nDescription\n* Deployment checklist\n* Add trigger\n* Create PR\n* Get approval and Merger\n* End to end testing\n* Publish schedule\n\n---\n\nComments\nran this thru Rovo, is it OK to check @user ?\n\nyou can use this for future tickets as a scaffold\n\n----\n\nh3. Context\n\n* The project requires implementing a CI/CD process for Azure Data Factory (ADF) pipelines as part of the CW Cloud Data Platform Interim Solution. This involves automating deployment steps, ensuring code quality, and enabling smooth release management through pull requests and approvals.\n\nh3. Objective\n\n* To establish and execute a robust CI/CD workflow for ADF, including deployment automation, code review, approval, and scheduling, ensuring reliable and efficient delivery of data platform changes.\n\nh3. Steps\n\n# Complete the deployment checklist for ADF CI/CD.\n# Add the necessary trigger(s) to automate the pipeline.\n# Create a Pull Request (PR) for the changes.\n# Obtain approval and merge the PR.\n# Conduct end-to-end testing of the deployment.\n# Publish and schedule the pipeline as required.\n\nh3. Deliverables\n\n* A fully operational CI/CD process for ADF, including:\n** Documented deployment checklist\n** Configured triggers\n** Merged and approved PR\n** Evidence of successful end-to-end testing\n** Published and scheduled ADF pipeline\n\nh3. Assumptions (Optional)\n\n* Required permissions are available for repository and ADF resources.\n* Stakeholders are available for timely PR review and approval.\n* Test environments are set up and accessible.\n\nh3. Acceptance criteria\n\n* Deployment checklist is completed and documented.\n* Trigger(s) are added and verified.\n* Pull Request is created, reviewed, approved, and merged.\n* End-to-end testing is completed with no critical issues.\n* Pipeline is published and scheduled as per requirements.\n\nCreated a PR, reviewing another branch that got created during the process.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 309}}
{"issue_key": "CSCI-317", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Aug/25 9:26 AM", "updated": "29/Sep/25 9:00 AM", "labels": [], "summary": "AP - ADF CI/CD process - Create PR", "description": "* Deployment checklist\n* Add trigger\n* Create PR\n* Get approval and Merger\n* End to end testing\n* Publish schedule", "acceptance_criteria": "", "comments": "", "text": "Summary\nAP - ADF CI/CD process - Create PR\n\n---\n\nDescription\n* Deployment checklist\n* Add trigger\n* Create PR\n* Get approval and Merger\n* End to end testing\n* Publish schedule", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 310}}
{"issue_key": "CSCI-316", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Aug/25 9:16 AM", "updated": "29/Aug/25 4:47 PM", "labels": [], "summary": "ADF/Altis DLA - Snowflake pipeline showcase", "description": "To organise a session on ADF/Altis DLA - Snowflake pipeline showcase with the Sigma/CW Data Engineering team.", "acceptance_criteria": "", "comments": "", "text": "Summary\nADF/Altis DLA - Snowflake pipeline showcase\n\n---\n\nDescription\nTo organise a session on ADF/Altis DLA - Snowflake pipeline showcase with the Sigma/CW Data Engineering team.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 311}}
{"issue_key": "CSCI-315", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Aug/25 8:45 PM", "updated": "29/Sep/25 9:40 AM", "labels": [], "summary": "SF-004 - Configure role-based access control framework", "description": "Implement Snowflake roles aligned to business; map to Entra ID groups.", "acceptance_criteria": "Implement Snowflake roles aligned to business; map to Entra ID groups.", "comments": "", "text": "Summary\nSF-004 - Configure role-based access control framework\n\n---\n\nDescription\nImplement Snowflake roles aligned to business; map to Entra ID groups.\n\n---\n\nAcceptance Criteria\nImplement Snowflake roles aligned to business; map to Entra ID groups.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 312}}
{"issue_key": "CSCI-314", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Aug/25 8:44 PM", "updated": "29/Sep/25 9:40 AM", "labels": [], "summary": "SF-003 - Set up user and group attribute mappings", "description": "Define mappings from Entra ID to Snowflake users/roles for automated provisioning.", "acceptance_criteria": "Define mappings from Entra ID to Snowflake users/roles for automated provisioning.", "comments": "", "text": "Summary\nSF-003 - Set up user and group attribute mappings\n\n---\n\nDescription\nDefine mappings from Entra ID to Snowflake users/roles for automated provisioning.\n\n---\n\nAcceptance Criteria\nDefine mappings from Entra ID to Snowflake users/roles for automated provisioning.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 313}}
{"issue_key": "CSCI-313", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Aug/25 8:44 PM", "updated": "29/Sep/25 9:40 AM", "labels": [], "summary": "SF-002 - Configure SCIM API integration", "description": "Automated user provisioning from Entra ID; lifecycle based on group membership.", "acceptance_criteria": "Automated user provisioning from Entra ID; lifecycle based on group membership.", "comments": "", "text": "Summary\nSF-002 - Configure SCIM API integration\n\n---\n\nDescription\nAutomated user provisioning from Entra ID; lifecycle based on group membership.\n\n---\n\nAcceptance Criteria\nAutomated user provisioning from Entra ID; lifecycle based on group membership.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 314}}
{"issue_key": "CSCI-308", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Aug/25 8:27 PM", "updated": "29/Sep/25 9:41 AM", "labels": [], "summary": "GRP-003 Access pattern role mapping strategy", "description": "Define patterns and role assignments for on-prem, remote, specialist tools scenarios.", "acceptance_criteria": "Define patterns and role assignments for on-prem, remote, specialist tools scenarios.", "comments": "", "text": "Summary\nGRP-003 Access pattern role mapping strategy\n\n---\n\nDescription\nDefine patterns and role assignments for on-prem, remote, specialist tools scenarios.\n\n---\n\nAcceptance Criteria\nDefine patterns and role assignments for on-prem, remote, specialist tools scenarios.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 315}}
{"issue_key": "CSCI-307", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Aug/25 8:26 PM", "updated": "01/Oct/25 3:29 PM", "labels": [], "summary": "GRP-002 - Authorization framework documentation", "description": "Map Entra ID groups to Snowflake roles, infra access, and tool permissions for auditability.", "acceptance_criteria": "Map Entra ID groups to Snowflake roles, infra access, and tool permissions for auditability.", "comments": "", "text": "Summary\nGRP-002 - Authorization framework documentation\n\n---\n\nDescription\nMap Entra ID groups to Snowflake roles, infra access, and tool permissions for auditability.\n\n---\n\nAcceptance Criteria\nMap Entra ID groups to Snowflake roles, infra access, and tool permissions for auditability.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 316}}
{"issue_key": "CSCI-306", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Aug/25 8:26 PM", "updated": "29/Sep/25 9:41 AM", "labels": [], "summary": "GRP-001 - Developer access group definitions and access matrix", "description": "Define Entra ID groups for Snowflake.", "acceptance_criteria": "Define Entra ID groups for Snowflake.", "comments": "", "text": "Summary\nGRP-001 - Developer access group definitions and access matrix\n\n---\n\nDescription\nDefine Entra ID groups for Snowflake.\n\n---\n\nAcceptance Criteria\nDefine Entra ID groups for Snowflake.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 317}}
{"issue_key": "CSCI-304", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Aug/25 8:24 PM", "updated": "09/Oct/25 9:42 AM", "labels": [], "summary": "AUTH-003 - Development tool authentication standardization", "description": "Standardize auth configs across Snow CLI, VS Code, Tabular Editor, Power BI (externalbrowser).", "acceptance_criteria": "Standardize auth configs across Snow CLI, VS Code, Tabular Editor, Power BI (externalbrowser).", "comments": "", "text": "Summary\nAUTH-003 - Development tool authentication standardization\n\n---\n\nDescription\nStandardize auth configs across Snow CLI, VS Code, Tabular Editor, Power BI (externalbrowser).\n\n---\n\nAcceptance Criteria\nStandardize auth configs across Snow CLI, VS Code, Tabular Editor, Power BI (externalbrowser).", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 318}}
{"issue_key": "CSCI-302", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Aug/25 8:22 PM", "updated": "30/Oct/25 9:50 AM", "labels": [], "summary": "End-to-end SSO integration testing across all access methods", "description": "Validate externalbrowser SSO flow across on-prem, VPN, jumpboxes, Azure VMs before prod rollout.", "acceptance_criteria": "Validate externalbrowser SSO flow across on-prem, VPN, jumpboxes, Azure VMs before prod rollout.", "comments": "Pending other tasks - provision of jump box + Azure DNS Private Resolver\n\nChloe to catch up with Raveen to check on status\n\nLogging to Snowflake from a VDI is unsuccessful after the Azure DNS Private Resolver was implemented. Raveen to schedule a call with Eugene, myself and Mike to troubleshoot and discuss next step.\n\nHave reached out to Raveen. Awaiting for resolution updates\n\nTask now unblocked 🙂\n\nTest done successfully with Raveen on login to Snowflake Privatelink via VDI. \n\nHowever default roles (provisioned via SCIM) have been wiped out. Raveen to check which settings have caused removal of default roles.\n\nI’ve tested the default role provision via SCIM. All is working well. Happy to close this ticket.\n\nNext steps:\n\n* @user to add a ticket for @user to verify and update subnet in the IaC. \n* @user to let us know about VDI/ Jumpbox access so we can all access Snowflake Privatelink.", "text": "Summary\nEnd-to-end SSO integration testing across all access methods\n\n---\n\nDescription\nValidate externalbrowser SSO flow across on-prem, VPN, jumpboxes, Azure VMs before prod rollout.\n\n---\n\nAcceptance Criteria\nValidate externalbrowser SSO flow across on-prem, VPN, jumpboxes, Azure VMs before prod rollout.\n\n---\n\nComments\nPending other tasks - provision of jump box + Azure DNS Private Resolver\n\nChloe to catch up with Raveen to check on status\n\nLogging to Snowflake from a VDI is unsuccessful after the Azure DNS Private Resolver was implemented. Raveen to schedule a call with Eugene, myself and Mike to troubleshoot and discuss next step.\n\nHave reached out to Raveen. Awaiting for resolution updates\n\nTask now unblocked 🙂\n\nTest done successfully with Raveen on login to Snowflake Privatelink via VDI. \n\nHowever default roles (provisioned via SCIM) have been wiped out. Raveen to check which settings have caused removal of default roles.\n\nI’ve tested the default role provision via SCIM. All is working well. Happy to close this ticket.\n\nNext steps:\n\n* @user to add a ticket for @user to verify and update subnet in the IaC. \n* @user to let us know about VDI/ Jumpbox access so we can all access Snowflake Privatelink.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 319}}
{"issue_key": "CSCI-294", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Aug/25 7:10 PM", "updated": "19/Nov/25 1:30 PM", "labels": [], "summary": "RITM0173487 - Implement Azure DNS Private Resolver", "description": "Currently worked on by Shagun A.\n\nImplement Azure DNS Private Resolver to enable on-premises DNS resolution for existing Snowflake Private Link endpoint ([privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com]).\n\nThe Azure Private Link for Snowflake is already configured and operational. This DNS implementation will enable on-premises systems to resolve the private Snowflake endpoint, which is a prerequisite for subsequent firewall access configuration for Enterprise Data Platform and critical applications requiring Snowflake access.\n\nSpoke to Network team and they suggested we create AD security group for snowflake private link access control and add the relevant users who need access to this group as the user laptop ip address is dynamic so this way we can control this via global protect and firewall. \n\n \n\nCurrent status:", "acceptance_criteria": "* -Azure DNS Private Resolver is deployed and configured-\n* -On-premises DNS resolution is verified-\n* -AD Security Group for Snowflake Access is created-\n* -Firewall and GlobalProtect integration is validated-\n* -Documentation is updated-\n* -SSO working as expected-\n* -Stakeholder sign-off-", "comments": "Hi @user - LMK if this AC is correct as per box\n\ncurrently in progress.\n\n@user - to check with @user\n\nCurrent status: in fulfilment \nLinked to task RITM0173487\n\nHi @user - just assigned this to you as I may be able to liaise with you what the status of this ticket is.\n\nChecking on this @user\n\nHi @user , I tried to reach you to schedule a call on this to explain :)\n\nCould you review attached spreadsheet please\n\n[^RITM0173487_snowflake_connectivity_requirements.xlsx]\n\n @user, @user\n\n@user what is this for?\n\nHere are two Wiki’s that have dependencies on the privatelink integration:\n\n* [SCIM+SAML - Snowflake and Entra ID SSO Configuration via Azure Private Link - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5170/SCIM-SAML-Snowflake-and-Entra-ID-SSO-Configuration-via-Azure-Private-Link]\n\n* [Developer Access - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5145/Developer-Access]\n\nHey@user , are we saying this Jira card is no longer required? or are you talking about the spreadsheet?\n\nwe need to collect and consolidate our requests so the team can focus on specifics like VDI, on-prem network requirements, and so forth. Given that not everyone can access the wiki, and it contains quite a lot of information, we should pull together the specific details for them.\n\n@user the private DNS resolver is already in place and configured in Azure ([privatelink.snowflakecomputing.com - Microsoft Azure|https://portal.azure.com/#@mychemist.com.au/resource/subscriptions/154716e9-0cb4-45c8-9fa7-dfea0954bbe1/resourceGroups/cwr-ase-platform-rg/providers/Microsoft.Network/privateDnsZones/privatelink.snowflakecomputing.com/recordsets]), what’s needed is for the DNS forwarding from on-premise DNS servers to the Azure privatelink DNS. See: [Resolve Azure and on-premises domains. | Microsoft Learn|https://learn.microsoft.com/en-us/azure/dns/private-resolver-hybrid-dns]\n\nThanks@user That’s exactly what I was trying to clarify. Since most of the configurations from RITM0173487 have already been completed. Had a chat with @user, we need to provide detailed information and specifics for the Network team. We can either repurpose the existing card or create a new one to track this outstanding work for Daify\n\nCC @user @user\n\nWe have raised CR CHG0052044 for next week to configure Conditional Forwarder\n\nHi @user Raised Firewall request *RITM0176342* (pre-requisite task for Conditional Forwarding)\n\nHi @user , @user , @user ,\n\nWe are creating Firewall rule in Azure but can you review and confirm sources please? \n\n# CWR Jumpbox M1\n# CWR Jumpbox M2\n# M1 RBT Vendor Jumpbox\n# M2 RBT Vendor Jumpbox\n\nJust FYI\n\n{noformat}      source_addresses = [\n        \"172.25.130.0/24\", # CWR Jumpbox M1\n        \"172.27.130.0/24\", # CWR Jumpbox M2\n        \"172.25.150.0/26\", # M1 RBT Vendor Jumpbox\n        \"172.27.150.0/26\", # M2 RBT Vendor Jumpbox\n        \"10.15.0.0/19\",    # Albert Street Head Office\n      ]{noformat}\n\nPlease note, once we have our own VDI range, we will revisit and update the firewall rule accordingly. \n\nWe should also enforce network policies on snowflake side to block public access.\n\n@user cc @user @user @user\n\n@user @user @user @user , please confirm. \n\nWe can add or exclude VDI Groups later if required\n\n@user / @user @user - do we know if these are the jumpboxes we need?\nJust trying to get this over the line so you guys get access.\n\nHi @user ,\n\nFYI..\n\n@user @user , when we get a separate VDI group for our team, that new group will be part (subset) of this above wider range.\n\nWhen we receive are VDI group, we will reconfigure the firewall rule to be specific to our group.\n\nhi Raveen, happy to proceed. Thanks! @user\n\nThanks@user \n\nWe have successfully implemented Azure Firewall rule. \n\nRaised new CR *CHG0052343* for Active Directory DNS - Configure Conditional Forwarding\n\nPlanned start date: 13-10-2025 11:00:00 AM\nPlanned end date: 13-10-2025 12:00:00 PM\nAssigned to: Louis Allsop\n\nIssue split into:\n|CSCI-535|RITM0173487 - Implement Azure DNS Private Resolver - Sprint 10|\n\nTo continue in sprint 10\n\nHey @user - any reason this needs to be reopen but not using the split one in sprint 10? [https://sigmahealthcare.atlassian.net/browse/CSCI-535|https://sigmahealthcare.atlassian.net/browse/CSCI-535] \n\nThanks\n\nHan\n\nLogging to Snowflake from a VDI is unsuccessful after the Azure DNS Private Resolver was implemented. Raveen to schedule a call with Eugene, myself and Mike to troubleshoot and discuss next step. @user\n\nh3. \n*Raveendran Kumaravelu*\n\nOctober 13, 2025 at 9:05 AM\n\nNew Change Request CHG0052343 has been scheduled for today\n\n[+CHG0052343+|https://cwretail.service-now.com/nav_to.do?uri=change_request.do?sys_id=61cc0f803360b25047764f945d5c7b23]\n\nPlanned start date: 13-10-2025 11:00:00 AM\nPlanned end date: 13-10-2025 12:00:00 PM\nImplementor: Louis Allsop\nActive Directory DNS - Configure Conditional Forwarding to Azure for Private Endpoint resolution ([http://snowflakecomputing.com|http://snowflakecomputing.com] )\n\nChanged was executed successfully and connectivity testing was all good. \n\nHowever, I have scheduled a meeting for Tuesday 21/10 with @user to test SSO \n\nCC: @user , @user , @user\n\nThx Raveendran Can I confirm this means all Acceptance Criteria are passed except for SSO?\n\n@user Yes\n\n@user thx can we ensure this is tracked moving forward so others know the progress at a glance.\n\n@user Whom’s review is this pending now?\n\nFYI @user \n\nthanks\n\n@user I think this is good and we can close this ticket. Thanks @user", "text": "Summary\nRITM0173487 - Implement Azure DNS Private Resolver\n\n---\n\nDescription\nCurrently worked on by Shagun A.\n\nImplement Azure DNS Private Resolver to enable on-premises DNS resolution for existing Snowflake Private Link endpoint ([privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com]).\n\nThe Azure Private Link for Snowflake is already configured and operational. This DNS implementation will enable on-premises systems to resolve the private Snowflake endpoint, which is a prerequisite for subsequent firewall access configuration for Enterprise Data Platform and critical applications requiring Snowflake access.\n\nSpoke to Network team and they suggested we create AD security group for snowflake private link access control and add the relevant users who need access to this group as the user laptop ip address is dynamic so this way we can control this via global protect and firewall. \n\n \n\nCurrent status:\n\n---\n\nAcceptance Criteria\n* -Azure DNS Private Resolver is deployed and configured-\n* -On-premises DNS resolution is verified-\n* -AD Security Group for Snowflake Access is created-\n* -Firewall and GlobalProtect integration is validated-\n* -Documentation is updated-\n* -SSO working as expected-\n* -Stakeholder sign-off-\n\n---\n\nComments\nHi @user - LMK if this AC is correct as per box\n\ncurrently in progress.\n\n@user - to check with @user\n\nCurrent status: in fulfilment \nLinked to task RITM0173487\n\nHi @user - just assigned this to you as I may be able to liaise with you what the status of this ticket is.\n\nChecking on this @user\n\nHi @user , I tried to reach you to schedule a call on this to explain :)\n\nCould you review attached spreadsheet please\n\n[^RITM0173487_snowflake_connectivity_requirements.xlsx]\n\n @user, @user\n\n@user what is this for?\n\nHere are two Wiki’s that have dependencies on the privatelink integration:\n\n* [SCIM+SAML - Snowflake and Entra ID SSO Configuration via Azure Private Link - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5170/SCIM-SAML-Snowflake-and-Entra-ID-SSO-Configuration-via-Azure-Private-Link]\n\n* [Developer Access - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5145/Developer-Access]\n\nHey@user , are we saying this Jira card is no longer required? or are you talking about the spreadsheet?\n\nwe need to collect and consolidate our requests so the team can focus on specifics like VDI, on-prem network requirements, and so forth. Given that not everyone can access the wiki, and it contains quite a lot of information, we should pull together the specific details for them.\n\n@user the private DNS resolver is already in place and configured in Azure ([privatelink.snowflakecomputing.com - Microsoft Azure|https://portal.azure.com/#@mychemist.com.au/resource/subscriptions/154716e9-0cb4-45c8-9fa7-dfea0954bbe1/resourceGroups/cwr-ase-platform-rg/providers/Microsoft.Network/privateDnsZones/privatelink.snowflakecomputing.com/recordsets]), what’s needed is for the DNS forwarding from on-premise DNS servers to the Azure privatelink DNS. See: [Resolve Azure and on-premises domains. | Microsoft Learn|https://learn.microsoft.com/en-us/azure/dns/private-resolver-hybrid-dns]\n\nThanks@user That’s exactly what I was trying to clarify. Since most of the configurations from RITM0173487 have already been completed. Had a chat with @user, we need to provide detailed information and specifics for the Network team. We can either repurpose the existing card or create a new one to track this outstanding work for Daify\n\nCC @user @user\n\nWe have raised CR CHG0052044 for next week to configure Conditional Forwarder\n\nHi @user Raised Firewall request *RITM0176342* (pre-requisite task for Conditional Forwarding)\n\nHi @user , @user , @user ,\n\nWe are creating Firewall rule in Azure but can you review and confirm sources please? \n\n# CWR Jumpbox M1\n# CWR Jumpbox M2\n# M1 RBT Vendor Jumpbox\n# M2 RBT Vendor Jumpbox\n\nJust FYI\n\n{noformat}      source_addresses = [\n        \"172.25.130.0/24\", # CWR Jumpbox M1\n        \"172.27.130.0/24\", # CWR Jumpbox M2\n        \"172.25.150.0/26\", # M1 RBT Vendor Jumpbox\n        \"172.27.150.0/26\", # M2 RBT Vendor Jumpbox\n        \"10.15.0.0/19\",    # Albert Street Head Office\n      ]{noformat}\n\nPlease note, once we have our own VDI range, we will revisit and update the firewall rule accordingly. \n\nWe should also enforce network policies on snowflake side to block public access.\n\n@user cc @user @user @user\n\n@user @user @user @user , please confirm. \n\nWe can add or exclude VDI Groups later if required\n\n@user / @user @user - do we know if these are the jumpboxes we need?\nJust trying to get this over the line so you guys get access.\n\nHi @user ,\n\nFYI..\n\n@user @user , when we get a separate VDI group for our team, that new group will be part (subset) of this above wider range.\n\nWhen we receive are VDI group, we will reconfigure the firewall rule to be specific to our group.\n\nhi Raveen, happy to proceed. Thanks! @user\n\nThanks@user \n\nWe have successfully implemented Azure Firewall rule. \n\nRaised new CR *CHG0052343* for Active Directory DNS - Configure Conditional Forwarding\n\nPlanned start date: 13-10-2025 11:00:00 AM\nPlanned end date: 13-10-2025 12:00:00 PM\nAssigned to: Louis Allsop\n\nIssue split into:\n|CSCI-535|RITM0173487 - Implement Azure DNS Private Resolver - Sprint 10|\n\nTo continue in sprint 10\n\nHey @user - any reason this needs to be reopen but not using the split one in sprint 10? [https://sigmahealthcare.atlassian.net/browse/CSCI-535|https://sigmahealthcare.atlassian.net/browse/CSCI-535] \n\nThanks\n\nHan\n\nLogging to Snowflake from a VDI is unsuccessful after the Azure DNS Private Resolver was implemented. Raveen to schedule a call with Eugene, myself and Mike to troubleshoot and discuss next step. @user\n\nh3. \n*Raveendran Kumaravelu*\n\nOctober 13, 2025 at 9:05 AM\n\nNew Change Request CHG0052343 has been scheduled for today\n\n[+CHG0052343+|https://cwretail.service-now.com/nav_to.do?uri=change_request.do?sys_id=61cc0f803360b25047764f945d5c7b23]\n\nPlanned start date: 13-10-2025 11:00:00 AM\nPlanned end date: 13-10-2025 12:00:00 PM\nImplementor: Louis Allsop\nActive Directory DNS - Configure Conditional Forwarding to Azure for Private Endpoint resolution ([http://snowflakecomputing.com|http://snowflakecomputing.com] )\n\nChanged was executed successfully and connectivity testing was all good. \n\nHowever, I have scheduled a meeting for Tuesday 21/10 with @user to test SSO \n\nCC: @user , @user , @user\n\nThx Raveendran Can I confirm this means all Acceptance Criteria are passed except for SSO?\n\n@user Yes\n\n@user thx can we ensure this is tracked moving forward so others know the progress at a glance.\n\n@user Whom’s review is this pending now?\n\nFYI @user \n\nthanks\n\n@user I think this is good and we can close this ticket. Thanks @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 320}}
{"issue_key": "CSCI-293", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Aug/25 3:41 PM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Review Source-to-Target Map for Dim_location", "description": "Review the mapping of DIM_LOCATION Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=XQdnyO&nav=MTVfe0Q4MDZEQUY3LTJCQzUtNDRBMy1CNDEwLUNBMEE4RjVFQkYwNX0]", "acceptance_criteria": "* Finalize that the work on Source-to-Target Map for DIM_LOCATIONis correct.\n** Review the Source-to-Target Map for DIM_LOCATION against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.", "comments": "", "text": "Summary\nReview Source-to-Target Map for Dim_location\n\n---\n\nDescription\nReview the mapping of DIM_LOCATION Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=XQdnyO&nav=MTVfe0Q4MDZEQUY3LTJCQzUtNDRBMy1CNDEwLUNBMEE4RjVFQkYwNX0]\n\n---\n\nAcceptance Criteria\n* Finalize that the work on Source-to-Target Map for DIM_LOCATIONis correct.\n** Review the Source-to-Target Map for DIM_LOCATION against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 321}}
{"issue_key": "CSCI-292", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Aug/25 1:41 PM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Review Source-to-Target Map for Dim_DC", "description": "Review the mapping of DIM_DC Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=XQdnyO&nav=MTVfe0Q4MDZEQUY3LTJCQzUtNDRBMy1CNDEwLUNBMEE4RjVFQkYwNX0]", "acceptance_criteria": "* Finalize that the work on Source-to-Target Map for DIM_DC is correct.\n** Review the Source-to-Target Map for DIM_DC against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.", "comments": "", "text": "Summary\nReview Source-to-Target Map for Dim_DC\n\n---\n\nDescription\nReview the mapping of DIM_DC Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=XQdnyO&nav=MTVfe0Q4MDZEQUY3LTJCQzUtNDRBMy1CNDEwLUNBMEE4RjVFQkYwNX0]\n\n---\n\nAcceptance Criteria\n* Finalize that the work on Source-to-Target Map for DIM_DC is correct.\n** Review the Source-to-Target Map for DIM_DC against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** Communicate changes of mapping file to the Jira/standup for traceability.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 322}}
{"issue_key": "CSCI-291", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Aug/25 9:24 AM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Review Source-to-Target Map for Dim_Product_UOM", "description": "Review the mapping of DIM_PRODICT_UOM Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=67yqFK&nav=MTVfe0RDNzkwNDhDLTg0NDAtNEREOS05NkE3LUM2ODRFOTNBNjkzQX0]", "acceptance_criteria": "* Finalize that the work on Source-to-Target Map for DIM_PRODICT_UOM is correct.\n** Review the Source-to-Target Map for DIM_PRODICT_UOM against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** -Communicate any changes to relevant business stakeholders.-\n** Communicate changes of mapping file to the Jira/standup for traceability.", "comments": "Reviewed.\nThanks @user\n\n@userupdated.\n\nPlease have a look and see if there are points to change\n\nIssue split into:\n|CSCI-433|Review Source-to-Target Map for Dim_Product_UOM part 2|", "text": "Summary\nReview Source-to-Target Map for Dim_Product_UOM\n\n---\n\nDescription\nReview the mapping of DIM_PRODICT_UOM Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=67yqFK&nav=MTVfe0RDNzkwNDhDLTg0NDAtNEREOS05NkE3LUM2ODRFOTNBNjkzQX0]\n\n---\n\nAcceptance Criteria\n* Finalize that the work on Source-to-Target Map for DIM_PRODICT_UOM is correct.\n** Review the Source-to-Target Map for DIM_PRODICT_UOM against the current production environment.\n** Update the mapping file to reflect any changes or corrections found.\n** Peer review completed and signed off by Chloe/Harrison\n** -Communicate any changes to relevant business stakeholders.-\n** Communicate changes of mapping file to the Jira/standup for traceability.\n\n---\n\nComments\nReviewed.\nThanks @user\n\n@userupdated.\n\nPlease have a look and see if there are points to change\n\nIssue split into:\n|CSCI-433|Review Source-to-Target Map for Dim_Product_UOM part 2|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 323}}
{"issue_key": "CSCI-290", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "20/Aug/25 1:54 PM", "updated": "22/Aug/25 8:50 AM", "labels": ["Integeration"], "summary": "Add NOLOCK for all the tables getting ingested in snowflake", "description": "Add NOLOCK for all the tables getting ingested in snowflake and test if the query is running as expected and data is getting loaded", "acceptance_criteria": "* -Data ingestion processes execute successfully without errors after adding NOLOCK.-\n* -Query performance is not negatively impacted beyond acceptable thresholds after implementing NOLOCK.-\n* -All changes are tested and validated in a non-production environment before deployment.-\n* -Documentation is updated to reflect the use of NOLOCK in the ingestion process.-", "comments": "@useryou can edit the AC where needed.\n\nHey @user \n\nJust want to confirm if docs are not required.\n\nI can strike thru the text if that’s the case", "text": "Summary\nAdd NOLOCK for all the tables getting ingested in snowflake\n\n---\n\nDescription\nAdd NOLOCK for all the tables getting ingested in snowflake and test if the query is running as expected and data is getting loaded\n\n---\n\nAcceptance Criteria\n* -Data ingestion processes execute successfully without errors after adding NOLOCK.-\n* -Query performance is not negatively impacted beyond acceptable thresholds after implementing NOLOCK.-\n* -All changes are tested and validated in a non-production environment before deployment.-\n* -Documentation is updated to reflect the use of NOLOCK in the ingestion process.-\n\n---\n\nComments\n@useryou can edit the AC where needed.\n\nHey @user \n\nJust want to confirm if docs are not required.\n\nI can strike thru the text if that’s the case", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 324}}
{"issue_key": "CSCI-289", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "20/Aug/25 9:48 AM", "updated": "12/Sep/25 6:32 PM", "labels": [], "summary": "MergeCo Conformed Reporting - Pull Data from CW to SF", "description": "Copy data from CW on-prem SQL Server (PBI05) to SF via ADF.\n\nGrain:\n\n* High-level: Per DC, Per Day", "acceptance_criteria": "", "comments": "Spoke with CK about enabling PBI05 to be whitelisted with ADF.\n\nCK has raised a request: *REQ0157732*\n\n@user I’ll find screenshot for this.\n\nFollowed up on ticket *REQ0157732* - ticket closed due to wrong ticket type.\n\nCK has raised a new ticket as per instructions: *REQ0158466*\n\nI wasn’t really too sure how much work was actually spent here @user before it got blocked - so I moved this task rather than splitting it.\n\n@user 1-2 hours\n\n@user\n\n@user @user is it OK to reply back to this ticket? Thank you.\n\nHad a meeting on 10/9/25 with Cloud Services team + members of the SC Snowflake project.\n\nCloud team are concerned that PBI05 is a “production server” and our Azure Subscription is listed as “dev”. Explained the reasoning behind this in great detail.\n\nCloud team have advised if the owner of PBI05 can provide their approval then they can proceed with this request. \n\nSpoke with Rachel Wan and she has approved this within the SNOW ticket - awaiting on the cloud team to action the request.\n\nIssue split into:\n|CSCI-429|MergeCo Conformed Reporting - Pull Data from CW to SF|\n\nIssue split into:\n|CSCI-439|MergeCo Conformed Reporting - Pull Data from CW to SF - part 2|", "text": "Summary\nMergeCo Conformed Reporting - Pull Data from CW to SF\n\n---\n\nDescription\nCopy data from CW on-prem SQL Server (PBI05) to SF via ADF.\n\nGrain:\n\n* High-level: Per DC, Per Day\n\n---\n\nComments\nSpoke with CK about enabling PBI05 to be whitelisted with ADF.\n\nCK has raised a request: *REQ0157732*\n\n@user I’ll find screenshot for this.\n\nFollowed up on ticket *REQ0157732* - ticket closed due to wrong ticket type.\n\nCK has raised a new ticket as per instructions: *REQ0158466*\n\nI wasn’t really too sure how much work was actually spent here @user before it got blocked - so I moved this task rather than splitting it.\n\n@user 1-2 hours\n\n@user\n\n@user @user is it OK to reply back to this ticket? Thank you.\n\nHad a meeting on 10/9/25 with Cloud Services team + members of the SC Snowflake project.\n\nCloud team are concerned that PBI05 is a “production server” and our Azure Subscription is listed as “dev”. Explained the reasoning behind this in great detail.\n\nCloud team have advised if the owner of PBI05 can provide their approval then they can proceed with this request. \n\nSpoke with Rachel Wan and she has approved this within the SNOW ticket - awaiting on the cloud team to action the request.\n\nIssue split into:\n|CSCI-429|MergeCo Conformed Reporting - Pull Data from CW to SF|\n\nIssue split into:\n|CSCI-439|MergeCo Conformed Reporting - Pull Data from CW to SF - part 2|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 325}}
{"issue_key": "CSCI-288", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "20/Aug/25 9:40 AM", "updated": "08/Oct/25 11:15 AM", "labels": [], "summary": "MergeCo Conformed Reporting - Develop Outbound Performance Agg Fact", "description": "* Outbound Performance\n** ATP%\n*** Available to Promise\n**** Confirmed / Ordered\n** DIF%\n*** Invoiced / Confirmed\n** Cust(Customer) Impact (% + $)\n*** Invoiced / Ordered\n** DOT%\n*** Qty Units Dispatched in 24 hrs / Qty Units Dispatched", "acceptance_criteria": "Develop facts from CW to create these KPIs\n\n* ATP%\n* DIF%\n* Cust Impact%\n* Cust Impact $\n* DOT%", "comments": "Have access to required tables. However data source lives in separate servers (PDB08 and PDB19B) - so cannot create a single view over different servers. Need to request DBA team to bring relevant data into a single server in order to merge and create a single object.\n\nMessage sent to Heshan (DBA team) to request we load data into PBI05 - so that I can consume in PBI + we can then pull into Snowflake\n\nhave raised a SNOW request to pull relevant tables into PBI05:\n\n*REQ0158141*\n\nI wasn’t really too sure how much work was actually spent here @user before it got blocked - so I moved this task rather than splitting it.\n\n@user 3-4 days\n\n@user Hi Phil,\n\nI have been working on the following with this specific task:\n\n* Performed Data Discovery of SCAX2012 and PDB08 to locate and find where this data exists in CW (2 story points)\n* Developed SQL views of data (1 story point)\n* Performed validation/sense checking of data (0.5 story points)\n* Gathered all requirements of what data we need to ingest in PBI05 (0.5 story points)\n\nValidated data loaded into TBI05 - have identified an aggregation issue in the Orders table with the DC Location ID (is causing duplicate values to appear).\n\nHave identified the issue and raised a request with Bhavya to action.\n\nHave updated current fact table to include current ordered/confirmed/invoiced logic.\n\n@user *REQ0158141* paused (pending)", "text": "Summary\nMergeCo Conformed Reporting - Develop Outbound Performance Agg Fact\n\n---\n\nDescription\n* Outbound Performance\n** ATP%\n*** Available to Promise\n**** Confirmed / Ordered\n** DIF%\n*** Invoiced / Confirmed\n** Cust(Customer) Impact (% + $)\n*** Invoiced / Ordered\n** DOT%\n*** Qty Units Dispatched in 24 hrs / Qty Units Dispatched\n\n---\n\nAcceptance Criteria\nDevelop facts from CW to create these KPIs\n\n* ATP%\n* DIF%\n* Cust Impact%\n* Cust Impact $\n* DOT%\n\n---\n\nComments\nHave access to required tables. However data source lives in separate servers (PDB08 and PDB19B) - so cannot create a single view over different servers. Need to request DBA team to bring relevant data into a single server in order to merge and create a single object.\n\nMessage sent to Heshan (DBA team) to request we load data into PBI05 - so that I can consume in PBI + we can then pull into Snowflake\n\nhave raised a SNOW request to pull relevant tables into PBI05:\n\n*REQ0158141*\n\nI wasn’t really too sure how much work was actually spent here @user before it got blocked - so I moved this task rather than splitting it.\n\n@user 3-4 days\n\n@user Hi Phil,\n\nI have been working on the following with this specific task:\n\n* Performed Data Discovery of SCAX2012 and PDB08 to locate and find where this data exists in CW (2 story points)\n* Developed SQL views of data (1 story point)\n* Performed validation/sense checking of data (0.5 story points)\n* Gathered all requirements of what data we need to ingest in PBI05 (0.5 story points)\n\nValidated data loaded into TBI05 - have identified an aggregation issue in the Orders table with the DC Location ID (is causing duplicate values to appear).\n\nHave identified the issue and raised a request with Bhavya to action.\n\nHave updated current fact table to include current ordered/confirmed/invoiced logic.\n\n@user *REQ0158141* paused (pending)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 326}}
{"issue_key": "CSCI-287", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "19/Aug/25 12:57 PM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Review Source-to-Target Map for Dim_Date", "description": "to have a discussion with you @user about this task.", "acceptance_criteria": "* Finalize that the work on Source-to-Target Map for DIM_DATE is correct.", "comments": "linked.\n\n @user@user\n\nIssue split into:\n|CSCI-399|Review Source-to-Target Map for Dim_Date part 2|", "text": "Summary\nReview Source-to-Target Map for Dim_Date\n\n---\n\nDescription\nto have a discussion with you @user about this task.\n\n---\n\nAcceptance Criteria\n* Finalize that the work on Source-to-Target Map for DIM_DATE is correct.\n\n---\n\nComments\nlinked.\n\n @user@user\n\nIssue split into:\n|CSCI-399|Review Source-to-Target Map for Dim_Date part 2|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 327}}
{"issue_key": "CSCI-286", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "19/Aug/25 11:00 AM", "updated": "30/Aug/25 3:08 PM", "labels": [], "summary": "APPRISS Sample datashare - development", "description": "Data share - Data Generated by CK - Manual data created, now need to be uploaded.\nThe sample data is placed under below share point location by CK. These sample data needs to be placed/loaded in data share.", "acceptance_criteria": "* -Data is uploaded.-", "comments": "Will validate records. - perhaps test case?\n\nardm csv files are uploaded to datashare in tables. Also validated the some sample records between files and corresponding tables.\n\nIssue split into:\n|CSCI-371|APPRISS Sample datashare - part 2|", "text": "Summary\nAPPRISS Sample datashare - development\n\n---\n\nDescription\nData share - Data Generated by CK - Manual data created, now need to be uploaded.\nThe sample data is placed under below share point location by CK. These sample data needs to be placed/loaded in data share.\n\n---\n\nAcceptance Criteria\n* -Data is uploaded.-\n\n---\n\nComments\nWill validate records. - perhaps test case?\n\nardm csv files are uploaded to datashare in tables. Also validated the some sample records between files and corresponding tables.\n\nIssue split into:\n|CSCI-371|APPRISS Sample datashare - part 2|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 328}}
{"issue_key": "CSCI-285", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Aug/25 2:37 PM", "updated": "21/Aug/25 2:48 PM", "labels": [], "summary": "Update Source Table Glossary with APPRISS tables", "description": "I need to find and update the Source Table Glossary with APPROSS related tables. The tasks include:\n\n* Identifying the source systems\n* Finding and recording the table sizes.\n* Identifying key columns (primary keys).\n\nTransactionStorage\n\n* TRANSACTIONS_INVOICE (Transaction Headers)\n* TRANSACTIONS (Line Items)\n* TRANSACTIONS_ELECTRONICPAYMENTS (Payment Details)\n* TransactionAuditSalesActivityLog and TransactionsAuditSalesActivityLogHistory (Audit Trail)\n* TransactionsReturns\n* TransactionTypes\n* TransactionType_Invoice\n\n \n\nStockDB\n\n* Products\n* Employee (Staff Details)\n* Location (BranchInfoGlobal)", "acceptance_criteria": "* Find the sources of these tables\n* Find the table sizes\n* Find key columns", "comments": "@userI have created this ticket as there is a request from @user. Please make necessary changes if required\n\n@user Added the Transaction and StockDb tables in data source spreadsheet\n\ncc @user", "text": "Summary\nUpdate Source Table Glossary with APPRISS tables\n\n---\n\nDescription\nI need to find and update the Source Table Glossary with APPROSS related tables. The tasks include:\n\n* Identifying the source systems\n* Finding and recording the table sizes.\n* Identifying key columns (primary keys).\n\nTransactionStorage\n\n* TRANSACTIONS_INVOICE (Transaction Headers)\n* TRANSACTIONS (Line Items)\n* TRANSACTIONS_ELECTRONICPAYMENTS (Payment Details)\n* TransactionAuditSalesActivityLog and TransactionsAuditSalesActivityLogHistory (Audit Trail)\n* TransactionsReturns\n* TransactionTypes\n* TransactionType_Invoice\n\n \n\nStockDB\n\n* Products\n* Employee (Staff Details)\n* Location (BranchInfoGlobal)\n\n---\n\nAcceptance Criteria\n* Find the sources of these tables\n* Find the table sizes\n* Find key columns\n\n---\n\nComments\n@userI have created this ticket as there is a request from @user. Please make necessary changes if required\n\n@user Added the Transaction and StockDb tables in data source spreadsheet\n\ncc @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 329}}
{"issue_key": "CSCI-284", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Aug/25 10:15 AM", "updated": "20/Aug/25 9:43 AM", "labels": [], "summary": "follow up with data extraction to prod", "description": "", "acceptance_criteria": "* access for chloe dev prod DB", "comments": "", "text": "Summary\nfollow up with data extraction to prod\n\n---\n\nAcceptance Criteria\n* access for chloe dev prod DB", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 330}}
{"issue_key": "CSCI-283", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Aug/25 9:58 AM", "updated": "01/Sep/25 2:23 AM", "labels": [], "summary": "Discussion - how we should approach getting specs/info on MAWM visibility - part 1", "description": "Discussion - how we should approach getting specs/info on MAWM visibility", "acceptance_criteria": "Discussion - how we should approach getting specs/info on MAWM visibility", "comments": "hi @user im guessing MAWM means Manhattan warehouse management? so this would the sessions that we had with Chandan?\n\nHey@user sorry I didn’t get back to you on this.\n\nYep, MAWM is manhattan active - so Chandan’s sessions would be one of them - we’re trying to discover ways of finding out how to get this info.\n\nIssue split into:\n|CSCI-377|Discussion - how we should approach getting specs/info on MAWM visibility - part 2|", "text": "Summary\nDiscussion - how we should approach getting specs/info on MAWM visibility - part 1\n\n---\n\nDescription\nDiscussion - how we should approach getting specs/info on MAWM visibility\n\n---\n\nAcceptance Criteria\nDiscussion - how we should approach getting specs/info on MAWM visibility\n\n---\n\nComments\nhi @user im guessing MAWM means Manhattan warehouse management? so this would the sessions that we had with Chandan?\n\nHey@user sorry I didn’t get back to you on this.\n\nYep, MAWM is manhattan active - so Chandan’s sessions would be one of them - we’re trying to discover ways of finding out how to get this info.\n\nIssue split into:\n|CSCI-377|Discussion - how we should approach getting specs/info on MAWM visibility - part 2|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 331}}
{"issue_key": "CSCI-282", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Story", "status": "Done", "priority": "Medium", "created": "18/Aug/25 8:56 AM", "updated": "18/Aug/25 3:39 PM", "labels": ["Integeration"], "summary": "Data time Review in snowflake", "description": "Chloe has changed the datetime in snowflake to UTC time. We need to confirm everything is looking as expected.", "acceptance_criteria": "", "comments": "@user - will add task in lieu of this story.\n\nNote:\n- Store transactions - local date time. -\n\n e.g. CWH perth - Local date and time (WST) CWH melb - AEST\n\nThe source time zone is intact in the snowflake and there is no impact of the changing the time zone of snowflake account.\n\nAswini is confirming my findings\n\nSnowflake is not tempering the data in source system. So, the time zone changes of the snowflake accounts are snowflake specific. No impact on import process and unification of time zone should be handled at presentation layer.", "text": "Summary\nData time Review in snowflake\n\n---\n\nDescription\nChloe has changed the datetime in snowflake to UTC time. We need to confirm everything is looking as expected.\n\n---\n\nComments\n@user - will add task in lieu of this story.\n\nNote:\n- Store transactions - local date time. -\n\n e.g. CWH perth - Local date and time (WST) CWH melb - AEST\n\nThe source time zone is intact in the snowflake and there is no impact of the changing the time zone of snowflake account.\n\nAswini is confirming my findings\n\nSnowflake is not tempering the data in source system. So, the time zone changes of the snowflake accounts are snowflake specific. No impact on import process and unification of time zone should be handled at presentation layer.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 332}}
{"issue_key": "CSCI-281", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Aug/25 4:39 PM", "updated": "01/Sep/25 2:31 AM", "labels": [], "summary": "Review of CI/CD Pipeline - part 2", "description": "Review output of [https://sigmahealthcare.atlassian.net/browse/CSCI-151|https://sigmahealthcare.atlassian.net/browse/CSCI-151] and [https://sigmahealthcare.atlassian.net/browse/CSCI-159|https://sigmahealthcare.atlassian.net/browse/CSCI-159]", "acceptance_criteria": "", "comments": "Stretch Sprint 5 goal\n\nI have reviewed the CD pipeline and am satisfied with the following:\n\n* *Branching strategy:* Use a feature branch, merge to {{master}}, then deploy to each environment upon approval.\n* *Deployment monitoring:* Check DevOps Releases for deployment status and logs.\n* *Script execution:* {{Config_meta.sql}} will be sequenced and run within {{deployment_master_scripts}}.\n* *Versioning:* The current mechanism functions but remains limited. We still require a defined process for version naming, review and auto-validation.\n\n*Next step:* Chloe to define the desired version control process with detailed requirements.\n\nhi Eugene, please review this PR and the Release's for Snowflake :\n\n[https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-snowflake/pullrequest/22486|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-snowflake/pullrequest/22486] \n\n[https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_releaseDefinition?definitionId=7&_a=definition-tasks&environmentId=16|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_releaseDefinition?definitionId=7&_a=definition-tasks&environmentId=16]\n\nHigh level approach for Snowflake CI/CD:\n\n@userto review.\n\nIssue split into:\n|CSCI-381|Review of CI/CD Pipeline - part 3|", "text": "Summary\nReview of CI/CD Pipeline - part 2\n\n---\n\nDescription\nReview output of [https://sigmahealthcare.atlassian.net/browse/CSCI-151|https://sigmahealthcare.atlassian.net/browse/CSCI-151] and [https://sigmahealthcare.atlassian.net/browse/CSCI-159|https://sigmahealthcare.atlassian.net/browse/CSCI-159]\n\n---\n\nComments\nStretch Sprint 5 goal\n\nI have reviewed the CD pipeline and am satisfied with the following:\n\n* *Branching strategy:* Use a feature branch, merge to {{master}}, then deploy to each environment upon approval.\n* *Deployment monitoring:* Check DevOps Releases for deployment status and logs.\n* *Script execution:* {{Config_meta.sql}} will be sequenced and run within {{deployment_master_scripts}}.\n* *Versioning:* The current mechanism functions but remains limited. We still require a defined process for version naming, review and auto-validation.\n\n*Next step:* Chloe to define the desired version control process with detailed requirements.\n\nhi Eugene, please review this PR and the Release's for Snowflake :\n\n[https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-snowflake/pullrequest/22486|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-snowflake/pullrequest/22486] \n\n[https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_releaseDefinition?definitionId=7&_a=definition-tasks&environmentId=16|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_releaseDefinition?definitionId=7&_a=definition-tasks&environmentId=16]\n\nHigh level approach for Snowflake CI/CD:\n\n@userto review.\n\nIssue split into:\n|CSCI-381|Review of CI/CD Pipeline - part 3|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 333}}
{"issue_key": "CSCI-280", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Aug/25 4:37 PM", "updated": "01/Sep/25 2:13 AM", "labels": [], "summary": "Source to target Facts - Fact_Store_Inventory - part 2", "description": "This task involves mapping out target fact tables using the source-to-target sheet, with a focus on the key facts outlined in the acceptance criteria. The main fact tables to be addressed include:\n\n* {{Fact_Store_Inventory_Intra}}\n* {{Fact_Store_Inventory_Adjustment}}\n* {{Fact_Store_Inventory_Snapshot}}\n\nThese tables are listed as Priority 1 and Priority 2 in the control table and are critical for meeting the acceptance criteria. The goal is to ensure accurate and complete mapping from source to target for these facts.", "acceptance_criteria": "Priority 1 and Priority 2 Fact tables on control table will be mapped\n\n* -Fact_Store_Inventory_Intra-\n* -Fact_Store_Inventory_Adjustment-\n* -Fact_Store_Inventory_Snapshot-", "comments": "Note - not a daily snapshot - just movement\n\nGoing thru logic i.e. filtration.\nWill take longer time\n\nFact_Store_Inventory_snapshot and Fact_Store_Inventory_Intra\n\nETL Logic onPrem. \n\nStep 1: \n\nInsert into Staging : \n\n{noformat}select rn=row_number() over (partition by DateStampDayOnly, MyCHemID, OriginBranchID order by DateStamp desc, RecordID desc)\n ,*\n ,1 as isPlaceholder\n from Fact_Store_IncrementalSOHChanges\n where DateStamp >= \"PreviousDate\" and DateStamp < \"CurrentDate\"\n ) dt2\n where rn = 1 {noformat}\n\nInsert into the new data from source which is only housing data changes \n\n{noformat}insert Into Staging \nselect \n RecordID, MychemID, AvgRealCost, SOH, DateStamp\n , coalesce(ChangeType, 26) as ChangeType -- set null ChangeType to SOH Import\n , coalesce(SOHChange, 0) as SOHChange\n , StaffID, RealCost, ExpectedSales, OriginBranchID, DateStampDayOnly\n , 0 as SOH_Last\n , 0 as ExpectedSales_Last\n , 0 as LineNumber\n , DateStamp as DateStampOriginal\n , 0 as isPlaceholder\n , hasExcessiveValue\nfrom \n Store_IncrementalSOHChanges\nwhere \n DateStamp >= \"currentDate\" and \n DateStamp < \"nextDate\" and\n RecordID > \"maxRecordID\" {noformat}\n\nThen It Self joins and deletes all records where isPlaceholder = 1 and has a record in the second join where the isPlaceholder = 0\n\n{noformat}delete Staging \nfrom\n(\n select nr.OriginBranchID, nr.MyCHemID\n from Staging nr\n where \n nr.isPlaceholder = 0\n and nr.DateStamp >= @currentDate\n and nr.DateStamp < @nextDate \n\n) dt\nwhere \n Staging.isPlaceholder = 1\n and Staging.DateStamp >= \"currentDate\"\n and Staging.DateStamp < \"nextDate\"\n and Staging.OriginBranchID = dt.OriginBranchID\n and Staging.MychemID = dt.MychemID{noformat}\n\nthis ensures that we will have one record for every ChemID , for every Store fore everyday. \n\nThe ETL for this table must be really thought through as it can get quite resource hungry.\n\nsplit this task to part 2 for next sprint - please adjust story point as per req", "text": "Summary\nSource to target Facts - Fact_Store_Inventory - part 2\n\n---\n\nDescription\nThis task involves mapping out target fact tables using the source-to-target sheet, with a focus on the key facts outlined in the acceptance criteria. The main fact tables to be addressed include:\n\n* {{Fact_Store_Inventory_Intra}}\n* {{Fact_Store_Inventory_Adjustment}}\n* {{Fact_Store_Inventory_Snapshot}}\n\nThese tables are listed as Priority 1 and Priority 2 in the control table and are critical for meeting the acceptance criteria. The goal is to ensure accurate and complete mapping from source to target for these facts.\n\n---\n\nAcceptance Criteria\nPriority 1 and Priority 2 Fact tables on control table will be mapped\n\n* -Fact_Store_Inventory_Intra-\n* -Fact_Store_Inventory_Adjustment-\n* -Fact_Store_Inventory_Snapshot-\n\n---\n\nComments\nNote - not a daily snapshot - just movement\n\nGoing thru logic i.e. filtration.\nWill take longer time\n\nFact_Store_Inventory_snapshot and Fact_Store_Inventory_Intra\n\nETL Logic onPrem. \n\nStep 1: \n\nInsert into Staging : \n\n{noformat}select rn=row_number() over (partition by DateStampDayOnly, MyCHemID, OriginBranchID order by DateStamp desc, RecordID desc)\n ,*\n ,1 as isPlaceholder\n from Fact_Store_IncrementalSOHChanges\n where DateStamp >= \"PreviousDate\" and DateStamp < \"CurrentDate\"\n ) dt2\n where rn = 1 {noformat}\n\nInsert into the new data from source which is only housing data changes \n\n{noformat}insert Into Staging \nselect \n RecordID, MychemID, AvgRealCost, SOH, DateStamp\n , coalesce(ChangeType, 26) as ChangeType -- set null ChangeType to SOH Import\n , coalesce(SOHChange, 0) as SOHChange\n , StaffID, RealCost, ExpectedSales, OriginBranchID, DateStampDayOnly\n , 0 as SOH_Last\n , 0 as ExpectedSales_Last\n , 0 as LineNumber\n , DateStamp as DateStampOriginal\n , 0 as isPlaceholder\n , hasExcessiveValue\nfrom \n Store_IncrementalSOHChanges\nwhere \n DateStamp >= \"currentDate\" and \n DateStamp < \"nextDate\" and\n RecordID > \"maxRecordID\" {noformat}\n\nThen It Self joins and deletes all records where isPlaceholder = 1 and has a record in the second join where the isPlaceholder = 0\n\n{noformat}delete Staging \nfrom\n(\n select nr.OriginBranchID, nr.MyCHemID\n from Staging nr\n where \n nr.isPlaceholder = 0\n and nr.DateStamp >= @currentDate\n and nr.DateStamp < @nextDate \n\n) dt\nwhere \n Staging.isPlaceholder = 1\n and Staging.DateStamp >= \"currentDate\"\n and Staging.DateStamp < \"nextDate\"\n and Staging.OriginBranchID = dt.OriginBranchID\n and Staging.MychemID = dt.MychemID{noformat}\n\nthis ensures that we will have one record for every ChemID , for every Store fore everyday. \n\nThe ETL for this table must be really thought through as it can get quite resource hungry.\n\nsplit this task to part 2 for next sprint - please adjust story point as per req", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 334}}
{"issue_key": "CSCI-279", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Aug/25 4:37 PM", "updated": "27/Aug/25 9:41 AM", "labels": [], "summary": "Source to Target for Facts - Fact_Product_Network_Average_Cost_History - review", "description": "Mapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]\n\nmain facts to focus on are as per acceptance criteria\n\n@user -will split this task out so that fits in to sprint cadence- done\n\nnotes from @user \n\n{quote}StockDB.dbo.productnetworkcosts and StockDB.dbo.productnetworkcostshistory{quote}\n\nProduct Network Cost Standardised Network cost that needs to be applied across all the inventory tables.\n\nCWR standardised Names, definitions and prices across all the teams. This needs to be carried into the Data platform as well. \n\nattached is the PDF with the names as definitions. \n\n[^Cost Prices in CWR Network_Finalised.pdf]", "acceptance_criteria": "* Priority 1 and Priority 2 Fact tables on control table will be mapped\n** Reviewing Fact_Product_Network_Average_Cost_History", "comments": "Product Network Cost Standardised Network cost that needs to be applied across all the inventory tables.\n\non Prem there are 2 tables Product Network Cost and Product Network Cost history. \n\none is today’s value and the other has a daily snapshot. we need to join on all the snapshot tables to this on the snapshot date to get the prices for the correct day. \n\nthe grain is 1 record per product per day.\n\n@user and @user \nPlease review this ticket.", "text": "Summary\nSource to Target for Facts - Fact_Product_Network_Average_Cost_History - review\n\n---\n\nDescription\nMapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]\n\nmain facts to focus on are as per acceptance criteria\n\n@user -will split this task out so that fits in to sprint cadence- done\n\nnotes from @user \n\n{quote}StockDB.dbo.productnetworkcosts and StockDB.dbo.productnetworkcostshistory{quote}\n\nProduct Network Cost Standardised Network cost that needs to be applied across all the inventory tables.\n\nCWR standardised Names, definitions and prices across all the teams. This needs to be carried into the Data platform as well. \n\nattached is the PDF with the names as definitions. \n\n[^Cost Prices in CWR Network_Finalised.pdf]\n\n---\n\nAcceptance Criteria\n* Priority 1 and Priority 2 Fact tables on control table will be mapped\n** Reviewing Fact_Product_Network_Average_Cost_History\n\n---\n\nComments\nProduct Network Cost Standardised Network cost that needs to be applied across all the inventory tables.\n\non Prem there are 2 tables Product Network Cost and Product Network Cost history. \n\none is today’s value and the other has a daily snapshot. we need to join on all the snapshot tables to this on the snapshot date to get the prices for the correct day. \n\nthe grain is 1 record per product per day.\n\n@user and @user \nPlease review this ticket.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 335}}
{"issue_key": "CSCI-278", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Aug/25 4:36 PM", "updated": "27/Aug/25 9:41 AM", "labels": [], "summary": "Source to Target for Facts - Fact_Warehouse_Location_inventory - Review", "description": "Mapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]\n\nmain facts to focus on are as per acceptance criteria\n\n@user -will split this task out so that fits in to sprint cadence- done\n\nnotes from @user \n\n{quote}The Stock Fact_Warehouse_Inventory_Snapshot- the mapping is done based off the PBI05.supplychain.scax.dcsinventoryhistory\n\nbase tables: INVENTDIM and INVENTDIM. \nThings to note: \n\n* DataareaID- this defines the business entities. \n* read from the most current partition: This function gives the most current partition; SCAXLink.[dbo].[udf_GetPartition] (null)\n* ensure Nolock is used for these reads.{quote}\n\n{quote}\n\nFact_Warehouse_Inventory_Adjustment: Based of [BI_Presentation].[dbo].[WarehouseInventoryAdjustments]\n\nE.g\n\n{quote}", "acceptance_criteria": "* Priority 1 and Priority 2 Fact tables on control table will be mapped\n** -Reviewing Fact_Warehouse_Location_Inventory_Intra-\n** -Reviewing Fact_Warehouse_Location_Inventory_Snapshot-\n** Fact_Warehouse_Location_Inventory_Adjustment\n** Cycle count\n*", "comments": "Hey @user ,\n\nWho created the details here? Description and ticket Acceptance Criteria dont seem to marry up?\n\nThanks,\n\nHarrison\n\nReviewing Fact_Warehouse_Location_Inventory_Snapshot: \n\nTable is based of ILS.dbo.LocationInventory\n\nThis is a live table out of the WH Management system.\n\nFact_Warehouse_Inventory_Adjustment:\n\nis based on the SC schematic model. As this was not part of the pervious EDP project.", "text": "Summary\nSource to Target for Facts - Fact_Warehouse_Location_inventory - Review\n\n---\n\nDescription\nMapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]\n\nmain facts to focus on are as per acceptance criteria\n\n@user -will split this task out so that fits in to sprint cadence- done\n\nnotes from @user \n\n{quote}The Stock Fact_Warehouse_Inventory_Snapshot- the mapping is done based off the PBI05.supplychain.scax.dcsinventoryhistory\n\nbase tables: INVENTDIM and INVENTDIM. \nThings to note: \n\n* DataareaID- this defines the business entities. \n* read from the most current partition: This function gives the most current partition; SCAXLink.[dbo].[udf_GetPartition] (null)\n* ensure Nolock is used for these reads.{quote}\n\n{quote}\n\nFact_Warehouse_Inventory_Adjustment: Based of [BI_Presentation].[dbo].[WarehouseInventoryAdjustments]\n\nE.g\n\n{quote}\n\n---\n\nAcceptance Criteria\n* Priority 1 and Priority 2 Fact tables on control table will be mapped\n** -Reviewing Fact_Warehouse_Location_Inventory_Intra-\n** -Reviewing Fact_Warehouse_Location_Inventory_Snapshot-\n** Fact_Warehouse_Location_Inventory_Adjustment\n** Cycle count\n*\n\n---\n\nComments\nHey @user ,\n\nWho created the details here? Description and ticket Acceptance Criteria dont seem to marry up?\n\nThanks,\n\nHarrison\n\nReviewing Fact_Warehouse_Location_Inventory_Snapshot: \n\nTable is based of ILS.dbo.LocationInventory\n\nThis is a live table out of the WH Management system.\n\nFact_Warehouse_Inventory_Adjustment:\n\nis based on the SC schematic model. As this was not part of the pervious EDP project.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 336}}
{"issue_key": "CSCI-277", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Aug/25 4:35 PM", "updated": "27/Aug/25 9:42 AM", "labels": [], "summary": "Source to Target for Facts - DC Inventory - review", "description": "Source to target mapping for Inventory history related Facts - filling out sheet that covers:\n\n* Fact_DC_inventory_history\n* Fact_DC_inventory_intra\n* Fact_DC_Inventory_Location_History\n* Fact_DC_Inventory_Adjustments", "acceptance_criteria": "* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Inventory%20Source-to-target%20document%20-%20Facts.xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=uvHOeg&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "This task will/should be split out - I can do this.\n\n@user\n\nneed ot map the BI tables for DC\n\nis approaching secondary mapping for DC\n\nTo verify ‘Data type’ whether it’s needed to be specified.\n\\ @user@user\n\n* Fact_DC_inventory_intra\n\nHarrison had entered a few more fields. SAFETYSTOCKQTY, DamagedQty,  UnsellableQty and ExpiredQty.\n\nI have done some research to to be able to get these information out of the ERP system. \nexpired qty can possibly be found from INVENTBATCH table but when i checked the table it doesn't contain any useful information. \n\nHave started a conversation with CWR AX Consultants and jess to see if can get this data out. \n\n@user , we might want to think about this table. As i think we cant get a lot these fields out of the scale system.", "text": "Summary\nSource to Target for Facts - DC Inventory - review\n\n---\n\nDescription\nSource to target mapping for Inventory history related Facts - filling out sheet that covers:\n\n* Fact_DC_inventory_history\n* Fact_DC_inventory_intra\n* Fact_DC_Inventory_Location_History\n* Fact_DC_Inventory_Adjustments\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Inventory%20Source-to-target%20document%20-%20Facts.xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=uvHOeg&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nThis task will/should be split out - I can do this.\n\n@user\n\nneed ot map the BI tables for DC\n\nis approaching secondary mapping for DC\n\nTo verify ‘Data type’ whether it’s needed to be specified.\n\\ @user@user\n\n* Fact_DC_inventory_intra\n\nHarrison had entered a few more fields. SAFETYSTOCKQTY, DamagedQty,  UnsellableQty and ExpiredQty.\n\nI have done some research to to be able to get these information out of the ERP system. \nexpired qty can possibly be found from INVENTBATCH table but when i checked the table it doesn't contain any useful information. \n\nHave started a conversation with CWR AX Consultants and jess to see if can get this data out. \n\n@user , we might want to think about this table. As i think we cant get a lot these fields out of the scale system.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 337}}
{"issue_key": "CSCI-276", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Aug/25 11:08 AM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Review Source-to-Target Map for Dim_Product - Alan", "description": "to have a discussion with you @user about this task.", "acceptance_criteria": "* Finalise that the work on Source-to-Target Map for Dim_product is correct.", "comments": "Sounds good @user , feel free to book a catch up some time next week\n\nThanks @user. I have booked a meeting for Monday\n\nIssue split into:\n|CSCI-434|Review Source-to-Target Map for Dim_Product - Alan - part 2|", "text": "Summary\nReview Source-to-Target Map for Dim_Product - Alan\n\n---\n\nDescription\nto have a discussion with you @user about this task.\n\n---\n\nAcceptance Criteria\n* Finalise that the work on Source-to-Target Map for Dim_product is correct.\n\n---\n\nComments\nSounds good @user , feel free to book a catch up some time next week\n\nThanks @user. I have booked a meeting for Monday\n\nIssue split into:\n|CSCI-434|Review Source-to-Target Map for Dim_Product - Alan - part 2|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 338}}
{"issue_key": "CSCI-275", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Aug/25 10:44 AM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Review Source-to-Target Map for Dim_Store - Alan", "description": "Reviewing Source-to-Target Map for Dim_Store in Supply chain", "acceptance_criteria": "* Finalise that the work on Source-to-Target Map for Dim_Store is correct.", "comments": "", "text": "Summary\nReview Source-to-Target Map for Dim_Store - Alan\n\n---\n\nDescription\nReviewing Source-to-Target Map for Dim_Store in Supply chain\n\n---\n\nAcceptance Criteria\n* Finalise that the work on Source-to-Target Map for Dim_Store is correct.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 339}}
{"issue_key": "CSCI-273", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Aug/25 9:01 AM", "updated": "18/Aug/25 10:13 AM", "labels": [], "summary": "Create APPRISS Data Share", "description": "Create APPRISS Data Share based on the following info:\n\nOrganization Name: OKB13541\n\nAccount Name: NSWAZ\n\nCloud Platform: AZURE\n\nEdition: Business Critical", "acceptance_criteria": "", "comments": "Datashare created and confirmed by Appriss\n\n@user - will follow up with Harrison/Frank fir next steps\n\n@user FYI\n\nFrom @user \nNext steps:\n\n* sample data present - will place in data share (produced by CK - need to place)\n** CK also got info of where he got that info\n* Creating production data push from our end\n\nWill create separate ticket for @user to start.\ncc @user / @user", "text": "Summary\nCreate APPRISS Data Share\n\n---\n\nDescription\nCreate APPRISS Data Share based on the following info:\n\nOrganization Name: OKB13541\n\nAccount Name: NSWAZ\n\nCloud Platform: AZURE\n\nEdition: Business Critical\n\n---\n\nComments\nDatashare created and confirmed by Appriss\n\n@user - will follow up with Harrison/Frank fir next steps\n\n@user FYI\n\nFrom @user \nNext steps:\n\n* sample data present - will place in data share (produced by CK - need to place)\n** CK also got info of where he got that info\n* Creating production data push from our end\n\nWill create separate ticket for @user to start.\ncc @user / @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 340}}
{"issue_key": "CSCI-272", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Aug/25 3:27 PM", "updated": "02/Oct/25 2:13 PM", "labels": [], "summary": "VPN Network for developers to Snowflake VNet", "description": "(servicenow ticket no - Sorry still not available) Visibility Ticket: On-Prem Network Access to Snowflake VNet Setup (Mirroring ServiceNow ticket)\n\n@user @user @user \n\nNotes to consider\n\n* QoS? - traffic prioritisation\n* What tools are involved.\n* CW - Palo Alto GlobalProtect compatibility\n* Ports to be allowed: 443, 80, 1433", "acceptance_criteria": "Test cases\n\n* Access with no VPN\n* Access via GlobalProtect (CW)\n* Access via Sigma VPN\n* Access via CW VPN", "comments": "Caught up with CK - \n\nFinish off SSO first\n\nThen get developer access\n\n@user due to this, No servicenow ticket available yet. @user\n\n@user \n\nAgreed with Security and Azure team to not use VPN, so we can remove this.\n\ncc: @user", "text": "Summary\nVPN Network for developers to Snowflake VNet\n\n---\n\nDescription\n(servicenow ticket no - Sorry still not available) Visibility Ticket: On-Prem Network Access to Snowflake VNet Setup (Mirroring ServiceNow ticket)\n\n@user @user @user \n\nNotes to consider\n\n* QoS? - traffic prioritisation\n* What tools are involved.\n* CW - Palo Alto GlobalProtect compatibility\n* Ports to be allowed: 443, 80, 1433\n\n---\n\nAcceptance Criteria\nTest cases\n\n* Access with no VPN\n* Access via GlobalProtect (CW)\n* Access via Sigma VPN\n* Access via CW VPN\n\n---\n\nComments\nCaught up with CK - \n\nFinish off SSO first\n\nThen get developer access\n\n@user due to this, No servicenow ticket available yet. @user\n\n@user \n\nAgreed with Security and Azure team to not use VPN, so we can remove this.\n\ncc: @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 341}}
{"issue_key": "CSCI-271", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Aug/25 3:23 PM", "updated": "10/Oct/25 2:30 PM", "labels": [], "summary": "VDI / Jumpbox network to Snowflake VNet", "description": "(servicenow ticket no - Sorry still not available) Visibility Ticket: VDI / Jumpbox network to Snowflake VNet Setup (Mirroring ServiceNow ticket)\n\n@user @user @user \n\n*Points to consider*\n\n* How many jump boxes we need?\n** What are the components we need? (From CW IT)", "acceptance_criteria": "Test cases\n\n* Access with no VPN\n* Access via GlobalProtect (CW)\n* Access via Sigma VPN\n* Access via CW VPN", "comments": "Caught up with CK - \n\nFinish off SSO first\n\nThen get developer access\n\n@user due to this, No servicenow ticket available yet. @user\n\nI’ve been in discussion with Raveen to proceed with the firewall/ DNS resolution forwarding. \n\nHappy to continue with the firewall rules to enable access from VDI.", "text": "Summary\nVDI / Jumpbox network to Snowflake VNet\n\n---\n\nDescription\n(servicenow ticket no - Sorry still not available) Visibility Ticket: VDI / Jumpbox network to Snowflake VNet Setup (Mirroring ServiceNow ticket)\n\n@user @user @user \n\n*Points to consider*\n\n* How many jump boxes we need?\n** What are the components we need? (From CW IT)\n\n---\n\nAcceptance Criteria\nTest cases\n\n* Access with no VPN\n* Access via GlobalProtect (CW)\n* Access via Sigma VPN\n* Access via CW VPN\n\n---\n\nComments\nCaught up with CK - \n\nFinish off SSO first\n\nThen get developer access\n\n@user due to this, No servicenow ticket available yet. @user\n\nI’ve been in discussion with Raveen to proceed with the firewall/ DNS resolution forwarding. \n\nHappy to continue with the firewall rules to enable access from VDI.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 342}}
{"issue_key": "CSCI-270", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Aug/25 3:19 PM", "updated": "02/Oct/25 2:13 PM", "labels": [], "summary": "On-Prem Network access to Snowflake Vnet setup", "description": "(servicenow ticket no - Sorry still not available) Visibility Ticket: On-Prem Network Access to Snowflake VNet Setup (Mirroring ServiceNow ticket)\n\n@user @user @user", "acceptance_criteria": "* On premises network access to snowflake Vnet setup\n* Connection tested", "comments": "Caught up with CK - \n\nFinish off SSO first\n\nThen get developer access\n\n@user due to this, No servicenow ticket available yet. @user\n\n@user \n\nAgreed with CK, Security and Azure that developer access will be VDI, so no need to do this. \n\ncc:@user", "text": "Summary\nOn-Prem Network access to Snowflake Vnet setup\n\n---\n\nDescription\n(servicenow ticket no - Sorry still not available) Visibility Ticket: On-Prem Network Access to Snowflake VNet Setup (Mirroring ServiceNow ticket)\n\n@user @user @user\n\n---\n\nAcceptance Criteria\n* On premises network access to snowflake Vnet setup\n* Connection tested\n\n---\n\nComments\nCaught up with CK - \n\nFinish off SSO first\n\nThen get developer access\n\n@user due to this, No servicenow ticket available yet. @user\n\n@user \n\nAgreed with CK, Security and Azure that developer access will be VDI, so no need to do this. \n\ncc:@user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 343}}
{"issue_key": "CSCI-269", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Aug/25 3:06 PM", "updated": "15/Aug/25 4:38 PM", "labels": [], "summary": "Source to Target for Facts - Fact_Product_Network_Average_Cost_History", "description": "Mapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]\n\nmain facts to focus on are as per acceptance criteria\n\n@user -will split this task out so that fits in to sprint cadence- done\n\nnotes from @user \n\n{quote}StockDB.dbo.productnetworkcosts and StockDB.dbo.productnetworkcostshistory{quote}\n\nProduct Network Cost Standardised Network cost that needs to be applied across all the inventory tables.\n\nCWR standardised Names, definitions and prices across all the teams. This needs to be carried into the Data platform as well. \n\nattached is the PDF with the names as definitions. \n\n[^Cost Prices in CWR Network_Finalised.pdf]", "acceptance_criteria": "* Priority 1 and Priority 2 Fact tables on control table will be mapped\n** Reviewing Fact_Product_Network_Average_Cost_History", "comments": "Product Network Cost Standardised Network cost that needs to be applied across all the inventory tables.\n\non Prem there are 2 tables Product Network Cost and Product Network Cost history. \n\none is today’s value and the other has a daily snapshot. we need to join on all the snapshot tables to this on the snapshot date to get the prices for the correct day. \n\nthe grain is 1 record per product per day.", "text": "Summary\nSource to Target for Facts - Fact_Product_Network_Average_Cost_History\n\n---\n\nDescription\nMapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]\n\nmain facts to focus on are as per acceptance criteria\n\n@user -will split this task out so that fits in to sprint cadence- done\n\nnotes from @user \n\n{quote}StockDB.dbo.productnetworkcosts and StockDB.dbo.productnetworkcostshistory{quote}\n\nProduct Network Cost Standardised Network cost that needs to be applied across all the inventory tables.\n\nCWR standardised Names, definitions and prices across all the teams. This needs to be carried into the Data platform as well. \n\nattached is the PDF with the names as definitions. \n\n[^Cost Prices in CWR Network_Finalised.pdf]\n\n---\n\nAcceptance Criteria\n* Priority 1 and Priority 2 Fact tables on control table will be mapped\n** Reviewing Fact_Product_Network_Average_Cost_History\n\n---\n\nComments\nProduct Network Cost Standardised Network cost that needs to be applied across all the inventory tables.\n\non Prem there are 2 tables Product Network Cost and Product Network Cost history. \n\none is today’s value and the other has a daily snapshot. we need to join on all the snapshot tables to this on the snapshot date to get the prices for the correct day. \n\nthe grain is 1 record per product per day.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 344}}
{"issue_key": "CSCI-266", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Aug/25 7:46 AM", "updated": "10/Oct/25 10:31 AM", "labels": [], "summary": "ILS - Manhattan Scale Integration - Ingestion of remaining 7 tables - development", "description": "DATA IN for critical identified objects from Manhattan Scale db (ILS)\n\nImplement data ingestion for critical identity objects from the on-premises Manhattan Scale (ILS) database into the cloud data platform. This includes configuring the necessary Azure Data Factory (ADF) components to support automated and reliable data flow.\n\nLatest update;\n\n* Ticket raised. REQ0156142\n* Currently with the infrastructure cloud team - Brent - Cloud services manager.\n* !image-20250729-063227 (1ed4c70f-781d-46cb-9ced-b8db1e7f0691).png|width=961,height=959,alt=\"image-20250729-063227.png\"!\nBelow are the list of tables to be ingested. Out of these the first one in the list i.e. Carrier is ingested in phase 1.\n\n|ILS|Carrier|\n|ILS|Functional_area_status_flow|\n|ILS|Item|\n|ILS|Location|\n|ILS|Location_inventory|\n|ILS|Routing_guide|\n|ILS|Shipment_detail|\n|ILS|Shipment_header|", "acceptance_criteria": "Definition of done:\n\n* -Extract meta created ()-\n* -Linked services created-\n* -Pipeline run end to end-\n* Whitelisting of on-prem prod database server. (Done by @user )", "comments": "ils db being added to firewall per new ticket from CK per Chloe request\n\nFollowing up with @user on this\n\nHave followed up with @user on this - have updated description - and will get further updates on this shortly.\n\nUpdate:\n- need to whiltelist this in Azure firewall.\n\n* Ticket raised. REQ0156142\n* Currently with the infrastructure cloud team - Brent.\n*\n\nHave caught up with @user CK|\nThis is tested and resolved. \nCK will update this.\n\nHave updated acceptance criteria \n\n* whitelisting on prem server done by @user - TYSM!\n\n@user can you follow this up\n\n@user will do\n\n@user @user I have split this into part 1 and part 2 so that if this doesn’t finish there will be work flowing to the next sprint.\n\nWill work with @user on date/time\n\nwill be monitoring loads\n\nHi @user , As discussed please find below overview of data ingestion for ILS:\n\n{adf:display=block}\n{\"type\":\"table\",\"attrs\":{\"isNumberColumnEnabled\":false,\"layout\":\"center\",\"localId\":\"09c6f457-ef8a-4abc-b537-2088001bf9b1\"},\"content\":[{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212,106,159,159],\"colspan\":4},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"ILS Ingestion into Snowflake\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Table Name\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Row Count\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Copy to BLOB\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Total Pipeline run end to end\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Carrier\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"1,551\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"1m 26s\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159],\"rowspan\":8},\"content\":[{\"type\":\"paragraph\"},{\"type\":\"paragraph\"},{\"type\":\"paragraph\"},{\"type\":\"paragraph\"},{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\" 9 mins\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Functional_area_status_flow\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"25\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"1m 25s\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Item\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"217,135\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"3m 19s\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Location\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"208,815\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"1m 32s\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Location_inventory\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"118,029\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"3m 13s\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Routing_guide\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"38\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"1m 43s\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Shipment_detail\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"103,025\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"3m 31s\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Shipment_header\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"15,157\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"1m 6s\"}]}]}]}]}\n{adf}\n\nNext step: @userto contact CK to raise a ticket to whitelist the Prod Server.\n\nHi @user @user ,\n\nBelow 2 tables have got 15 days of history and not beyond that as data gets archived to other tables. \n\n|TDB15|ILS|Shipment_detail|\n|TDB15|ILS|Shipment_header|\n\nDo we need to consider the other 2 tables for ingestion where data gets archived from above tables? If yes, I am happy to consider in our next sprint. Currently these are not listed in our spreadsheet for ingestion.\n\n[Data Source As-IS Details.xlsx (sharepoint.com)|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B3077E8A5-A280-43D1-BF3F-4A38BB4C4258%7D&file=Data%20Source%20As-IS%20Details.xlsx&action=default&mobileredirect=true]\n\n|TDB15|ILS|AR_SHIPMENT_DETAIL|\n|TDB15|ILS|AR_SHIPMENT_HEADER|\n\nIssue split into:\n|CSCI-375|ILS - Manhattan Scale Integration - Ingestion of remaining 7 tables - review|", "text": "Summary\nILS - Manhattan Scale Integration - Ingestion of remaining 7 tables - development\n\n---\n\nDescription\nDATA IN for critical identified objects from Manhattan Scale db (ILS)\n\nImplement data ingestion for critical identity objects from the on-premises Manhattan Scale (ILS) database into the cloud data platform. This includes configuring the necessary Azure Data Factory (ADF) components to support automated and reliable data flow.\n\nLatest update;\n\n* Ticket raised. REQ0156142\n* Currently with the infrastructure cloud team - Brent - Cloud services manager.\n* !image-20250729-063227 (1ed4c70f-781d-46cb-9ced-b8db1e7f0691).png|width=961,height=959,alt=\"image-20250729-063227.png\"!\nBelow are the list of tables to be ingested. Out of these the first one in the list i.e. Carrier is ingested in phase 1.\n\n|ILS|Carrier|\n|ILS|Functional_area_status_flow|\n|ILS|Item|\n|ILS|Location|\n|ILS|Location_inventory|\n|ILS|Routing_guide|\n|ILS|Shipment_detail|\n|ILS|Shipment_header|\n\n---\n\nAcceptance Criteria\nDefinition of done:\n\n* -Extract meta created ()-\n* -Linked services created-\n* -Pipeline run end to end-\n* Whitelisting of on-prem prod database server. (Done by @user )\n\n---\n\nComments\nils db being added to firewall per new ticket from CK per Chloe request\n\nFollowing up with @user on this\n\nHave followed up with @user on this - have updated description - and will get further updates on this shortly.\n\nUpdate:\n- need to whiltelist this in Azure firewall.\n\n* Ticket raised. REQ0156142\n* Currently with the infrastructure cloud team - Brent.\n*\n\nHave caught up with @user CK|\nThis is tested and resolved. \nCK will update this.\n\nHave updated acceptance criteria \n\n* whitelisting on prem server done by @user - TYSM!\n\n@user can you follow this up\n\n@user will do\n\n@user @user I have split this into part 1 and part 2 so that if this doesn’t finish there will be work flowing to the next sprint.\n\nWill work with @user on date/time\n\nwill be monitoring loads\n\nHi @user , As discussed please find below overview of data ingestion for ILS:\n\n{adf:display=block}\n{\"type\":\"table\",\"attrs\":{\"isNumberColumnEnabled\":false,\"layout\":\"center\",\"localId\":\"09c6f457-ef8a-4abc-b537-2088001bf9b1\"},\"content\":[{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212,106,159,159],\"colspan\":4},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"ILS Ingestion into Snowflake\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Table Name\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Row Count\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Copy to BLOB\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Total Pipeline run end to end\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Carrier\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"1,551\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"1m 26s\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159],\"rowspan\":8},\"content\":[{\"type\":\"paragraph\"},{\"type\":\"paragraph\"},{\"type\":\"paragraph\"},{\"type\":\"paragraph\"},{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\" 9 mins\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Functional_area_status_flow\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"25\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"1m 25s\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Item\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"217,135\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"3m 19s\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Location\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"208,815\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"1m 32s\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Location_inventory\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"118,029\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"3m 13s\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Routing_guide\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"38\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"1m 43s\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Shipment_detail\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"103,025\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"3m 31s\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[212]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Shipment_header\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[106]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"15,157\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colwidth\":[159]},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"1m 6s\"}]}]}]}]}\n{adf}\n\nNext step: @userto contact CK to raise a ticket to whitelist the Prod Server.\n\nHi @user @user ,\n\nBelow 2 tables have got 15 days of history and not beyond that as data gets archived to other tables. \n\n|TDB15|ILS|Shipment_detail|\n|TDB15|ILS|Shipment_header|\n\nDo we need to consider the other 2 tables for ingestion where data gets archived from above tables? If yes, I am happy to consider in our next sprint. Currently these are not listed in our spreadsheet for ingestion.\n\n[Data Source As-IS Details.xlsx (sharepoint.com)|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B3077E8A5-A280-43D1-BF3F-4A38BB4C4258%7D&file=Data%20Source%20As-IS%20Details.xlsx&action=default&mobileredirect=true]\n\n|TDB15|ILS|AR_SHIPMENT_DETAIL|\n|TDB15|ILS|AR_SHIPMENT_HEADER|\n\nIssue split into:\n|CSCI-375|ILS - Manhattan Scale Integration - Ingestion of remaining 7 tables - review|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 345}}
{"issue_key": "CSCI-264", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 3:41 PM", "updated": "21/Aug/25 9:58 AM", "labels": [], "summary": "SKU objects ingestion - Initial and delta load", "description": "", "acceptance_criteria": "We need to set up pipelines for initial Delta loads for the following SKU Objects\n\n|SKU|m_DestinationCode|\n|SKU|m_DestinationType|\n|SKU|m_LCP|\n|SKU|SkuLcpActivityLog|\n|SKU|SKUHeader|\n|SKU|SKUdetails|\n|SKU|SkuStoreExceptions|\n\n* -Extract meta created-\n* -Linked services created-\n* -pipelines set up-\n* -initial load tested-\n* -delta load tested-\n* -pipeline run from end to end-", "comments": "seeking assistance from @user", "text": "Summary\nSKU objects ingestion - Initial and delta load\n\n---\n\nAcceptance Criteria\nWe need to set up pipelines for initial Delta loads for the following SKU Objects\n\n|SKU|m_DestinationCode|\n|SKU|m_DestinationType|\n|SKU|m_LCP|\n|SKU|SkuLcpActivityLog|\n|SKU|SKUHeader|\n|SKU|SKUdetails|\n|SKU|SkuStoreExceptions|\n\n* -Extract meta created-\n* -Linked services created-\n* -pipelines set up-\n* -initial load tested-\n* -delta load tested-\n* -pipeline run from end to end-\n\n---\n\nComments\nseeking assistance from @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 346}}
{"issue_key": "CSCI-263", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 3:41 PM", "updated": "26/Aug/25 12:53 PM", "labels": [], "summary": "CWMgtStoreInvoices objects ingestion", "description": "", "acceptance_criteria": "We need to set up pipelines for initial Delta loads for the following CWMgtStoreInvoices Objects\n\n|CWMgtStoreInvoices|NonPharmXInvoiceHeader|\n|CWMgtStoreInvoices|NonPharmxInvoiceItems|\n|CWMgtStoreInvoices|PharmXInvoiceHeader3V0|\n|CWMgtStoreInvoices|PharmxInvoiceItems3V0|\n|CWMgtStoreInvoices|PharmXSupplier|\n\n* -Extract meta created-\n* -Linked services created-\n* -pipelines set up-\n* -initial load tested-\n* -delta load tested-\n* pipeline run from end to end", "comments": "PharmXSupplier Skipped due to no records in production\n\nAdded one table. Starting historical load for big tables.", "text": "Summary\nCWMgtStoreInvoices objects ingestion\n\n---\n\nAcceptance Criteria\nWe need to set up pipelines for initial Delta loads for the following CWMgtStoreInvoices Objects\n\n|CWMgtStoreInvoices|NonPharmXInvoiceHeader|\n|CWMgtStoreInvoices|NonPharmxInvoiceItems|\n|CWMgtStoreInvoices|PharmXInvoiceHeader3V0|\n|CWMgtStoreInvoices|PharmxInvoiceItems3V0|\n|CWMgtStoreInvoices|PharmXSupplier|\n\n* -Extract meta created-\n* -Linked services created-\n* -pipelines set up-\n* -initial load tested-\n* -delta load tested-\n* pipeline run from end to end\n\n---\n\nComments\nPharmXSupplier Skipped due to no records in production\n\nAdded one table. Starting historical load for big tables.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 347}}
{"issue_key": "CSCI-262", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 3:37 PM", "updated": "07/Oct/25 2:14 PM", "labels": [], "summary": "APPRISS Objects ingestion", "description": "h2. Summary\n\nWe need to identify any additional objects required for APPRISS that have not yet been ingested into EDP Snowflake.\n\nh2. Context\n\nMost tables from the StockDb and TransactionStorage databases have already been ingested. This issue focuses on ensuring that all necessary objects for APPRISS are accounted for.\n\nh2. Other information\n\nN/A", "acceptance_criteria": "We need to set up pipelines for initial and delta loads for the following APPRISS tables\n\nTBA from [doc|https://mychemist-my.sharepoint.com/:w:/g/personal/frank_perez_chemistwarehouse_com_au/EehJD-B4GqdAv9Xehsr2r9ABiWtU--dVk-O07qbnzCP9vA?email=sig_phillip.yuen%40chemistwarehouse.com.au&e=bp6jUs] monday @user (apologies)\n\n* -Extract meta created-\n* -Linked services created-\n* -pipelines set up-\n* -initial load tested-\n* -delta load tested-\n* pipeline run from end to end", "comments": "To follow up CK today\n\nThanks for bringing this into sprint @user\n\nWill check with Harrison/Chloe today if we can get the data up. @user\n\nTo start with we have received below tables needed for APPRISS.\n\nTransactionStorage.dbo.TRANSACTIONS_INVOICE\nTransactionStorage.dbo.TRANSACTIONS\nTransactionStorage.dbo.TRANSACTIONS_ELECTRONICPAYMENTS \nTransactionStorage.dbo.TransactionAuditSaleActivityLog\nTransactionStorage.dbo.TransactionAuditSaleActivityLogHistory\nTransactionStorage.dbo.TransactionReturns\nTransactionStorage.dbo.TransactionType\nTransactionStorage.dbo.TransactionType_Invoice\n\nTDB08AX2012.StockDB.dbo.Products\nTDB08AX2012.StockDB.dbo.StoreStaffDetails \nTDB08AX2012.GeneralReference.dbo.BranchInfoGlobal_EDP\n\nHi @user ,\n\nBecause below two tables needs historic Ingestion through Parquet and @user is still not able to access blob storage due to network policy changes , the ingestion of these two tables move into next sprint for which we need to create a separate ticket.\n\n|TDB14|TransactionStorage|TRANSACTIONS_ELECTRONICPAYMENTS|\n\n|TDB14|TransactionStorage|TransactionAuditSaleActivityLogHistory|\n\n@user just confirmed wiht @user that we need 8 weeks.\nbest to create another ticket\n\n@user Confirming, 8 weeks of transactional and audit history in initial data dump, followed by daily delta at T-2.\n\nIssue split into:\n|CSCI-372|APPRISS Objects ingestion - remaining talbes ingestion|", "text": "Summary\nAPPRISS Objects ingestion\n\n---\n\nDescription\nh2. Summary\n\nWe need to identify any additional objects required for APPRISS that have not yet been ingested into EDP Snowflake.\n\nh2. Context\n\nMost tables from the StockDb and TransactionStorage databases have already been ingested. This issue focuses on ensuring that all necessary objects for APPRISS are accounted for.\n\nh2. Other information\n\nN/A\n\n---\n\nAcceptance Criteria\nWe need to set up pipelines for initial and delta loads for the following APPRISS tables\n\nTBA from [doc|https://mychemist-my.sharepoint.com/:w:/g/personal/frank_perez_chemistwarehouse_com_au/EehJD-B4GqdAv9Xehsr2r9ABiWtU--dVk-O07qbnzCP9vA?email=sig_phillip.yuen%40chemistwarehouse.com.au&e=bp6jUs] monday @user (apologies)\n\n* -Extract meta created-\n* -Linked services created-\n* -pipelines set up-\n* -initial load tested-\n* -delta load tested-\n* pipeline run from end to end\n\n---\n\nComments\nTo follow up CK today\n\nThanks for bringing this into sprint @user\n\nWill check with Harrison/Chloe today if we can get the data up. @user\n\nTo start with we have received below tables needed for APPRISS.\n\nTransactionStorage.dbo.TRANSACTIONS_INVOICE\nTransactionStorage.dbo.TRANSACTIONS\nTransactionStorage.dbo.TRANSACTIONS_ELECTRONICPAYMENTS \nTransactionStorage.dbo.TransactionAuditSaleActivityLog\nTransactionStorage.dbo.TransactionAuditSaleActivityLogHistory\nTransactionStorage.dbo.TransactionReturns\nTransactionStorage.dbo.TransactionType\nTransactionStorage.dbo.TransactionType_Invoice\n\nTDB08AX2012.StockDB.dbo.Products\nTDB08AX2012.StockDB.dbo.StoreStaffDetails \nTDB08AX2012.GeneralReference.dbo.BranchInfoGlobal_EDP\n\nHi @user ,\n\nBecause below two tables needs historic Ingestion through Parquet and @user is still not able to access blob storage due to network policy changes , the ingestion of these two tables move into next sprint for which we need to create a separate ticket.\n\n|TDB14|TransactionStorage|TRANSACTIONS_ELECTRONICPAYMENTS|\n\n|TDB14|TransactionStorage|TransactionAuditSaleActivityLogHistory|\n\n@user just confirmed wiht @user that we need 8 weeks.\nbest to create another ticket\n\n@user Confirming, 8 weeks of transactional and audit history in initial data dump, followed by daily delta at T-2.\n\nIssue split into:\n|CSCI-372|APPRISS Objects ingestion - remaining talbes ingestion|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 348}}
{"issue_key": "CSCI-261", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 3:23 PM", "updated": "29/Sep/25 8:10 AM", "labels": [], "summary": "Create Source-to-Target Map for Fact_Warehouse_Location_Outbound_Deliveries", "description": "Source to target mapping for Fact_Warehouse_Location_Outbound_Deliveries - filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "", "text": "Summary\nCreate Source-to-Target Map for Fact_Warehouse_Location_Outbound_Deliveries\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Outbound_Deliveries - filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 349}}
{"issue_key": "CSCI-260", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 3:21 PM", "updated": "12/Sep/25 6:21 PM", "labels": [], "summary": "Create Source-to-Target Map for Fact_Warehouse_Location_Space_Utilisation", "description": "Source to target mapping for Fact_Warehouse_Location_Space_Utilisation - filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "", "text": "Summary\nCreate Source-to-Target Map for Fact_Warehouse_Location_Space_Utilisation\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Space_Utilisation - filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 350}}
{"issue_key": "CSCI-259", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 3:20 PM", "updated": "16/Sep/25 10:29 AM", "labels": [], "summary": "Create Source-to-Target Map for Fact_Warehouse_Location_Cycle_Count", "description": "Source to target mapping for Fact_Warehouse_Location_Cycle_Count- filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "Aasking for business process doc from Harrison - will follow up @user\n\nIssue split into:\n|CSCI-435|Create Source-to-Target Map for Fact_Warehouse_Location_Cycle_Count - part 2|", "text": "Summary\nCreate Source-to-Target Map for Fact_Warehouse_Location_Cycle_Count\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Cycle_Count- filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nAasking for business process doc from Harrison - will follow up @user\n\nIssue split into:\n|CSCI-435|Create Source-to-Target Map for Fact_Warehouse_Location_Cycle_Count - part 2|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 351}}
{"issue_key": "CSCI-258", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 3:19 PM", "updated": "12/Sep/25 6:22 PM", "labels": [], "summary": "Create Source-to-Target Map for Fact_Warehouse_Location_Inbound_Transactions", "description": "Source to target mapping for Fact_Warehouse_Location_Inbound_Transactions - filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "", "text": "Summary\nCreate Source-to-Target Map for Fact_Warehouse_Location_Inbound_Transactions\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Inbound_Transactions - filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 352}}
{"issue_key": "CSCI-257", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 3:18 PM", "updated": "07/Oct/25 9:08 AM", "labels": [], "summary": "Create Source-to-Target Map for Fact_Warehouse_Location_Packing", "description": "Source to target mapping for Fact_Warehouse_Location_Packing - filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "Completed the mapping for Packing [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&action=default&mobileredirect=true&DefaultItemOpen=1&ct=1756953007481&wdOrigin=OFFICECOM-WEB.START.EDGEWORTH&cid=a737358a-69d2-4fb4-a238-a2d225802a7c&wdPreviousSessionSrc=HarmonyWeb&wdPreviousSession=363d545e-c6e7-4e45-a585-9d9cd0302ef7]\n\nCC:- @user@user\n\nIssue split into:\n|CSCI-514|Review Source-to-Target Map for Fact_Warehouse_Location_Packing|", "text": "Summary\nCreate Source-to-Target Map for Fact_Warehouse_Location_Packing\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Packing - filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nCompleted the mapping for Packing [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&action=default&mobileredirect=true&DefaultItemOpen=1&ct=1756953007481&wdOrigin=OFFICECOM-WEB.START.EDGEWORTH&cid=a737358a-69d2-4fb4-a238-a2d225802a7c&wdPreviousSessionSrc=HarmonyWeb&wdPreviousSession=363d545e-c6e7-4e45-a585-9d9cd0302ef7]\n\nCC:- @user@user\n\nIssue split into:\n|CSCI-514|Review Source-to-Target Map for Fact_Warehouse_Location_Packing|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 353}}
{"issue_key": "CSCI-256", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 3:17 PM", "updated": "07/Oct/25 9:08 AM", "labels": [], "summary": "Create Source-to-Target Map for Fact_Warehouse_Location_Replenishment", "description": "Source to target mapping for Fact_Warehouse_Location_Replenishment - filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "", "text": "Summary\nCreate Source-to-Target Map for Fact_Warehouse_Location_Replenishment\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Replenishment - filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 354}}
{"issue_key": "CSCI-255", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 3:17 PM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Create Source-to-Target Map for Fact_Warehouse_Location_Picking", "description": "Source to target mapping for Fact_Warehouse_Location_Picking - filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "", "text": "Summary\nCreate Source-to-Target Map for Fact_Warehouse_Location_Picking\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Picking - filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 355}}
{"issue_key": "CSCI-254", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 3:16 PM", "updated": "12/Sep/25 6:20 PM", "labels": [], "summary": "Create Source-to-Target Map for Fact_Warehouse_Location_Putaway", "description": "Source to target mapping for Fact_Warehouse_Location_Putaway - filling out sheet as per Source-to-target facts", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "", "text": "Summary\nCreate Source-to-Target Map for Fact_Warehouse_Location_Putaway\n\n---\n\nDescription\nSource to target mapping for Fact_Warehouse_Location_Putaway - filling out sheet as per Source-to-target facts\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 356}}
{"issue_key": "CSCI-253", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 3:11 PM", "updated": "01/Sep/25 2:13 AM", "labels": [], "summary": "Create Source-to-Target Map for Fact_Store_Inventory_Snapshot - Mapping", "description": "Source to target mapping for Fact_Store_Inventory_Snapshot - filling out sheet as per Source-to-target dims", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "mapping has been completed\n\nIssue split into:\n|CSCI-376|Create Source-to-Target Map for Fact_Store_Inventory_Snapshot - Review|", "text": "Summary\nCreate Source-to-Target Map for Fact_Store_Inventory_Snapshot - Mapping\n\n---\n\nDescription\nSource to target mapping for Fact_Store_Inventory_Snapshot - filling out sheet as per Source-to-target dims\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nmapping has been completed\n\nIssue split into:\n|CSCI-376|Create Source-to-Target Map for Fact_Store_Inventory_Snapshot - Review|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 357}}
{"issue_key": "CSCI-252", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 3:09 PM", "updated": "27/Aug/25 9:41 AM", "labels": [], "summary": "Create Source-to-Target Map for Fact_Store_Inventory_Intra", "description": "Source to target mapping for Fact_Store_Inventory_Intra - filling out sheet as per Source-to-target dims", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "The mapping has been completed", "text": "Summary\nCreate Source-to-Target Map for Fact_Store_Inventory_Intra\n\n---\n\nDescription\nSource to target mapping for Fact_Store_Inventory_Intra - filling out sheet as per Source-to-target dims\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nThe mapping has been completed", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 358}}
{"issue_key": "CSCI-250", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 2:53 PM", "updated": "09/Sep/25 9:15 AM", "labels": [], "summary": "Create Source-to-Target Map for Fact_Store_inventory_adjustment", "description": "Source to target mapping for Fact_Store_Inventory_Adjustment - filling out sheet as per Source-to-target dims", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "the Store incremental is an movement table. which included all the type of movements. from sales to adjustments.", "text": "Summary\nCreate Source-to-Target Map for Fact_Store_inventory_adjustment\n\n---\n\nDescription\nSource to target mapping for Fact_Store_Inventory_Adjustment - filling out sheet as per Source-to-target dims\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nthe Store incremental is an movement table. which included all the type of movements. from sales to adjustments.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 359}}
{"issue_key": "CSCI-248", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "13/Aug/25 9:38 AM", "updated": "01/Sep/25 2:31 AM", "labels": [], "summary": "Create an SSO approach with SCIM + SAML", "description": "This task involves implementing secure Single Sign-On (SSO) and automated user/group provisioning between *Microsoft Entra ID* and *Snowflake*, using *SAML 2.0* and *SCIM v2*, with all traffic routed through *Azure Private Link* to eliminate public exposure.\n\nThe configuration includes:\n\n* Creating and configuring the Snowflake Enterprise App in Entra ID.\n* Establishing SAML trust and metadata exchange between Entra ID and Snowflake.\n* Enabling SCIM API integration in Snowflake.\n* Setting up automatic provisioning in Entra ID.\n* Validating SSO and SCIM flows.\n* Applying security best practices and ongoing maintenance.", "acceptance_criteria": "* The Entra ID Enterprise Application for Snowflake is created and configured with correct SAML attributes.\n* SAML metadata is exchanged and validated between Entra ID and Snowflake.\n* SCIM API integration is successfully created in Snowflake with OAuth credentials.\n* Automatic provisioning is enabled in Entra ID and tested for users and groups.\n* SSO login via Entra ID (both IdP-initiated and SP-initiated) is functional.\n* All traffic is confirmed to route through Azure Private Link, with no public endpoints.\n* Logging and monitoring are enabled in both Snowflake and Entra ID.\n* Security controls such as MFA, RBAC, NSGs, and certificate rotation are implemented.\n* Documentation and disaster recovery procedures are updated and tested.", "comments": "Design is present - session to be held tomorrow\n\n@user to reach out to @user @user later about filling this\n\nDocument available at: [SCIM+SAML - Snowflake and Entra ID SSO Configuration via Azure Private Link - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5170/SCIM-SAML-Snowflake-and-Entra-ID-SSO-Configuration-via-Azure-Private-Link]\n\nIssue split into:\n|CSCI-370|Create an SSO approach with SCIM + SAML part 2|", "text": "Summary\nCreate an SSO approach with SCIM + SAML\n\n---\n\nDescription\nThis task involves implementing secure Single Sign-On (SSO) and automated user/group provisioning between *Microsoft Entra ID* and *Snowflake*, using *SAML 2.0* and *SCIM v2*, with all traffic routed through *Azure Private Link* to eliminate public exposure.\n\nThe configuration includes:\n\n* Creating and configuring the Snowflake Enterprise App in Entra ID.\n* Establishing SAML trust and metadata exchange between Entra ID and Snowflake.\n* Enabling SCIM API integration in Snowflake.\n* Setting up automatic provisioning in Entra ID.\n* Validating SSO and SCIM flows.\n* Applying security best practices and ongoing maintenance.\n\n---\n\nAcceptance Criteria\n* The Entra ID Enterprise Application for Snowflake is created and configured with correct SAML attributes.\n* SAML metadata is exchanged and validated between Entra ID and Snowflake.\n* SCIM API integration is successfully created in Snowflake with OAuth credentials.\n* Automatic provisioning is enabled in Entra ID and tested for users and groups.\n* SSO login via Entra ID (both IdP-initiated and SP-initiated) is functional.\n* All traffic is confirmed to route through Azure Private Link, with no public endpoints.\n* Logging and monitoring are enabled in both Snowflake and Entra ID.\n* Security controls such as MFA, RBAC, NSGs, and certificate rotation are implemented.\n* Documentation and disaster recovery procedures are updated and tested.\n\n---\n\nComments\nDesign is present - session to be held tomorrow\n\n@user to reach out to @user @user later about filling this\n\nDocument available at: [SCIM+SAML - Snowflake and Entra ID SSO Configuration via Azure Private Link - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5170/SCIM-SAML-Snowflake-and-Entra-ID-SSO-Configuration-via-Azure-Private-Link]\n\nIssue split into:\n|CSCI-370|Create an SSO approach with SCIM + SAML part 2|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 360}}
{"issue_key": "CSCI-247", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/Aug/25 12:09 PM", "updated": "13/Aug/25 9:49 AM", "labels": [], "summary": "Calculate daily data source sizes", "description": "As a DBA, I want to collect the daily sizes of source data tables for TransactionStorage.", "acceptance_criteria": "Get the data sizes of transactionStorage tables.\n\n* *Data Collection*\n** Retrieve the *daily size (in MB/GB)* of each table in the {{TransactionStorage}} schema.\n** Include the following tables at minimum:\n{{BranchOrderItems}}\n{{BranchOrders}}\n{{Store_IncrementalSOHChanges}}\n{{Transactions}}\n{{Transactions_Invoice}}\n* *Row Count Metrics*\n** Capture the *average number of rows added daily* for each table listed above.\n** Use production environment data for accuracy.\n* *Output Format*\n** Present the data in a *tabular format* with columns:\n{{TableName}}\n{{AvgDailyRows}}\n{{DailySizeMB}}\n** Optionally include a timestamp or date for each data point.\n* *Automation*\n** The data collection process should be *automated to run daily*.\n** Results should be stored or logged in a location accessible to the DBA team (e.g., shared folder, dashboard, or monitoring tool).\n* *Validation*\n** Ensure the script or process runs successfully for at least *3 consecutive days* without errors.\n** Validate that the reported sizes and row counts match actual database metrics.", "comments": "@user Average no of daily rows added in TransactionStorage tables in Prod and DEV\n\n|TableName|PROD|DEV|\n|BranchOrderItems|       569,871|   651,906|\n|BranchOrders|          22,503|     24,467|\n|Store_IncrementalSOHChanges|   2,617,223|     14,557|\n|Transactions|       195,153|   189,938|\n|Transactions_Invoice|       504,750|   475,245|", "text": "Summary\nCalculate daily data source sizes\n\n---\n\nDescription\nAs a DBA, I want to collect the daily sizes of source data tables for TransactionStorage.\n\n---\n\nAcceptance Criteria\nGet the data sizes of transactionStorage tables.\n\n* *Data Collection*\n** Retrieve the *daily size (in MB/GB)* of each table in the {{TransactionStorage}} schema.\n** Include the following tables at minimum:\n{{BranchOrderItems}}\n{{BranchOrders}}\n{{Store_IncrementalSOHChanges}}\n{{Transactions}}\n{{Transactions_Invoice}}\n* *Row Count Metrics*\n** Capture the *average number of rows added daily* for each table listed above.\n** Use production environment data for accuracy.\n* *Output Format*\n** Present the data in a *tabular format* with columns:\n{{TableName}}\n{{AvgDailyRows}}\n{{DailySizeMB}}\n** Optionally include a timestamp or date for each data point.\n* *Automation*\n** The data collection process should be *automated to run daily*.\n** Results should be stored or logged in a location accessible to the DBA team (e.g., shared folder, dashboard, or monitoring tool).\n* *Validation*\n** Ensure the script or process runs successfully for at least *3 consecutive days* without errors.\n** Validate that the reported sizes and row counts match actual database metrics.\n\n---\n\nComments\n@user Average no of daily rows added in TransactionStorage tables in Prod and DEV\n\n|TableName|PROD|DEV|\n|BranchOrderItems|       569,871|   651,906|\n|BranchOrders|          22,503|     24,467|\n|Store_IncrementalSOHChanges|   2,617,223|     14,557|\n|Transactions|       195,153|   189,938|\n|Transactions_Invoice|       504,750|   475,245|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 361}}
{"issue_key": "CSCI-246", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "08/Aug/25 3:08 PM", "updated": "11/Aug/25 9:16 AM", "labels": [], "summary": "UPdating control table to have DIMs and status colour codes", "description": "Enhance the control table by:\n\n* Adding Dimension (DIM) references.\n* Implementing status colour codes to visually represent the state of each DIM.\n\nThis update aims to improve traceability and provide quick visual cues for data status across the pipeline.", "acceptance_criteria": "* -control table has DIMS-\n* -Colour codes implemented-", "comments": "cc @user", "text": "Summary\nUPdating control table to have DIMs and status colour codes\n\n---\n\nDescription\nEnhance the control table by:\n\n* Adding Dimension (DIM) references.\n* Implementing status colour codes to visually represent the state of each DIM.\n\nThis update aims to improve traceability and provide quick visual cues for data status across the pipeline.\n\n---\n\nAcceptance Criteria\n* -control table has DIMS-\n* -Colour codes implemented-\n\n---\n\nComments\ncc @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 362}}
{"issue_key": "CSCI-245", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "06/Aug/25 4:31 PM", "updated": "12/Sep/25 6:14 PM", "labels": [], "summary": "Enable developer access to snowflake via Privatelink", "description": "Enable developer access to snowflake via Privatelink - via browser \n\nUser story:\n\n* As a snowflake person (not service) user, I want to access snowflake via Privatelink so that it is accessible and secure (confirming to security requirements).\n\n@user - Below are the key action items as identified by Eugene for us as next steps.\n\nDetails are [here|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5145/Developer-Access?anchor=**developer-access-to-snowflake-via-azure-privatelink**]\n\nCan you please help us engage with Cloud team and raise a servicenow ticket.\n\ncc @user \n\n*Key action items* \n\n* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com]\n* Configure firewalls (On-Prem, Azure) as follows\n\n* On-Prem Network for developers → Snowflake VNet *(servnicenow ticket)*\n* VDI / Jumpbox network → Snowflake VNet *(servicenow tickets)*\n** *how many jumpboxes* we need?\n** *What are the things we need?*\n* VPN Network for developers → Snowflake VNet (servicenow ticket)\n** QoS? - traffic prioritisation\n** What tools are involved.\n** CW - Palo Alto GlobalProtect.\n* Note:\n** Ports to be allowed: 443, 80, 1433\n* Configure VDI / Jumbox images to pre-install tools defined above\n* Create Development VMs in Snowflake Subscription", "acceptance_criteria": "* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/]\n* Access snowflake via [https://app.snowflake.com/cw/au/#/homepage|https://app.snowflake.com/cw/au/#/homepage] after infrastructure changes.\n* Test cases\n** Access on premises (office network)\n** Access off premises\n*** (fail path) access with no VPN\n*** Access via GlobalProtect (CW)\n*** Access via Sigma VPN\n** Access via PBI\n*", "comments": "@usercan you please check with CK where we are? As CK doesn't attend the stand-up, maybe this item should be under Eugene?\n\nUpdate from CK - this is still in progress.\n\nHey @user - would you like me to create separate tickets for this to reflect the work required for this?\nOr would you like to keep everything in this ticket?\n\nI’ll assign this ticket to you - let me know how you would like to structure/manage this ticket.\n\nThank you.\n\nCaught up with @user\n\nUpdated ticket and indicated which parts need servicenow tickets\n\nUpdated AC with light test case.\n\ncc @user @user\n\nIssue split into:\n|CSCI-431|Enable developer access to snowflake via Privatelink |", "text": "Summary\nEnable developer access to snowflake via Privatelink\n\n---\n\nDescription\nEnable developer access to snowflake via Privatelink - via browser \n\nUser story:\n\n* As a snowflake person (not service) user, I want to access snowflake via Privatelink so that it is accessible and secure (confirming to security requirements).\n\n@user - Below are the key action items as identified by Eugene for us as next steps.\n\nDetails are [here|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5145/Developer-Access?anchor=**developer-access-to-snowflake-via-azure-privatelink**]\n\nCan you please help us engage with Cloud team and raise a servicenow ticket.\n\ncc @user \n\n*Key action items* \n\n* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com]\n* Configure firewalls (On-Prem, Azure) as follows\n\n* On-Prem Network for developers → Snowflake VNet *(servnicenow ticket)*\n* VDI / Jumpbox network → Snowflake VNet *(servicenow tickets)*\n** *how many jumpboxes* we need?\n** *What are the things we need?*\n* VPN Network for developers → Snowflake VNet (servicenow ticket)\n** QoS? - traffic prioritisation\n** What tools are involved.\n** CW - Palo Alto GlobalProtect.\n* Note:\n** Ports to be allowed: 443, 80, 1433\n* Configure VDI / Jumbox images to pre-install tools defined above\n* Create Development VMs in Snowflake Subscription\n\n---\n\nAcceptance Criteria\n* On-Premise DNS Resolution for [privatelink.snowflakecomputing.com|http://privatelink.snowflakecomputing.com/]\n* Access snowflake via [https://app.snowflake.com/cw/au/#/homepage|https://app.snowflake.com/cw/au/#/homepage] after infrastructure changes.\n* Test cases\n** Access on premises (office network)\n** Access off premises\n*** (fail path) access with no VPN\n*** Access via GlobalProtect (CW)\n*** Access via Sigma VPN\n** Access via PBI\n*\n\n---\n\nComments\n@usercan you please check with CK where we are? As CK doesn't attend the stand-up, maybe this item should be under Eugene?\n\nUpdate from CK - this is still in progress.\n\nHey @user - would you like me to create separate tickets for this to reflect the work required for this?\nOr would you like to keep everything in this ticket?\n\nI’ll assign this ticket to you - let me know how you would like to structure/manage this ticket.\n\nThank you.\n\nCaught up with @user\n\nUpdated ticket and indicated which parts need servicenow tickets\n\nUpdated AC with light test case.\n\ncc @user @user\n\nIssue split into:\n|CSCI-431|Enable developer access to snowflake via Privatelink |", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 363}}
{"issue_key": "CSCI-244", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "06/Aug/25 9:43 AM", "updated": "11/Aug/25 8:56 AM", "labels": [], "summary": "Understanding current architecture", "description": "As an engineer - I would like to receive the current in future state of architecture so that I know what type of work I need to create and where my work is going towards (i.e. a goal)\n\nKey areas:\n\n* DLA\n* @user - I might need your help to fill the rest of this out", "acceptance_criteria": "* -receive documentation about the current architecture-\n** -receive documentation about architecture we are working towards-\n* Ask relevant questions about where we currently are at to fill knowledge gaps\n* Able to apply professional knowledge and opinion on current work\n* Able to leverage architecture knowledge to create own Jira tasks with acceptance criteria levera", "comments": "@useryou may be able to help on this\nI think some parts of this we can already tick off\n\n@user I’ve ticked some of the above off. In saying that of course you can ask for further info where needed.\n\nLMK if you require further assistance", "text": "Summary\nUnderstanding current architecture\n\n---\n\nDescription\nAs an engineer - I would like to receive the current in future state of architecture so that I know what type of work I need to create and where my work is going towards (i.e. a goal)\n\nKey areas:\n\n* DLA\n* @user - I might need your help to fill the rest of this out\n\n---\n\nAcceptance Criteria\n* -receive documentation about the current architecture-\n** -receive documentation about architecture we are working towards-\n* Ask relevant questions about where we currently are at to fill knowledge gaps\n* Able to apply professional knowledge and opinion on current work\n* Able to leverage architecture knowledge to create own Jira tasks with acceptance criteria levera\n\n---\n\nComments\n@useryou may be able to help on this\nI think some parts of this we can already tick off\n\n@user I’ve ticked some of the above off. In saying that of course you can ask for further info where needed.\n\nLMK if you require further assistance", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 364}}
{"issue_key": "CSCI-243", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "06/Aug/25 9:34 AM", "updated": "30/Aug/25 3:12 PM", "labels": [], "summary": "Understanding current architecture", "description": "As an engineer - I would like to receive the current in future state of architecture so that I know what type of work I need to create and where my work is going towards (i.e. a goal)\n\nKey areas:\n\n* DLA\n* @user - I might need your help to fill the rest of this out", "acceptance_criteria": "* -receive documentation about the current architecture - now reviewing-\n** receive documentation about architecture we are working towards\n* Ask relevant questions about where we currently are at to fill knowledge gaps\n* Able to apply professional knowledge and opinion on current work\n* Able to leverage architecture knowledge to create own Jira tasks with acceptance criteria levera", "comments": "@useryou may be able to help on this\nI think some parts of this we can already tick off\n\nMuch of these we have covered yesterday in the office. @user pls reach out if you have any question. \n\nFYI @user\n\n@user I think we can tick some of the above off. In saying that of course you can ask for further info where needed.\n\nLMK if you require further assistance\n\nPhil\n\nHi @user, Currently going through the Documentation from altis for DLA and walkthroughs provided by Chloe. Yesterday, got few questions clarified. Let’s keep this till today as we have adf walkthrough scheduled.\n\nIssue split into:\n|CSCI-374|Understanding current architecture - please fill out coverage |", "text": "Summary\nUnderstanding current architecture\n\n---\n\nDescription\nAs an engineer - I would like to receive the current in future state of architecture so that I know what type of work I need to create and where my work is going towards (i.e. a goal)\n\nKey areas:\n\n* DLA\n* @user - I might need your help to fill the rest of this out\n\n---\n\nAcceptance Criteria\n* -receive documentation about the current architecture - now reviewing-\n** receive documentation about architecture we are working towards\n* Ask relevant questions about where we currently are at to fill knowledge gaps\n* Able to apply professional knowledge and opinion on current work\n* Able to leverage architecture knowledge to create own Jira tasks with acceptance criteria levera\n\n---\n\nComments\n@useryou may be able to help on this\nI think some parts of this we can already tick off\n\nMuch of these we have covered yesterday in the office. @user pls reach out if you have any question. \n\nFYI @user\n\n@user I think we can tick some of the above off. In saying that of course you can ask for further info where needed.\n\nLMK if you require further assistance\n\nPhil\n\nHi @user, Currently going through the Documentation from altis for DLA and walkthroughs provided by Chloe. Yesterday, got few questions clarified. Let’s keep this till today as we have adf walkthrough scheduled.\n\nIssue split into:\n|CSCI-374|Understanding current architecture - please fill out coverage |", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 365}}
{"issue_key": "CSCI-242", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "06/Aug/25 9:28 AM", "updated": "07/Aug/25 11:33 AM", "labels": [], "summary": "removing duplicates from CUSTINVOICETRANS", "description": "I have created this ticket in lieu of data quality checks @user \n\nI may need more details - will ask during standup 6/8", "acceptance_criteria": "* Duplicates are removed from CUSTINVOICETRANS table", "comments": "Getting the below SQL access error\n\nSQL access control error:\n\n{noformat}Insufficient privileges to operate on table 'CUSTINVOICETRANS'{noformat}\n\n@user I am assign to OPS_DEV_RO read only role. Please grant delete permission for this table.\n\nDuplicates removed from the CUSTINVOICETRANS Table.\n\nThere is no straight forward delete as this table does not have any unique column to identify the delete and snowflake does not support rownum, cte and qualify for deleting duplicates without unique column. Used the below workaround to achieve this.\n\nActions Taken: -\n\n* Created CUSTINVOICETRANS_DuplicatesRemoved with distinct records.\n* Renamed CUSTINVOICETRANS to CUSTINVOICETRANS_Backup.\n* Renamed the new table to CUSTINVOICETRANS.\n\nScripts:-\n\nCreate table CUSTINVOICETRANS_DuplicatesRemoved as\nSELECT ASSETBOOKID,ASSETID,BILLINGCODE,COMMISSAMOUNTCUR,COMMISSAMOUNTMST,COMMISSCALC,COUNTRYREGIONOFSHIPMENT,CREATEDBY,CREATEDDATETIME,CURRENCYCODE,CUSTINVOICELINEIDREF,CUSTOMERLINENUM,DATAAREAID,DEFAULTDIMENSION,DELIVERYPOSTALADDRESS,DELIVERYTYPE,DEL_CREATEDTIME,DISCAMOUNT,DISCPERCENT,DLVDATE,EINVOICEACCOUNTCODE,EXTERNALITEMID,INTERCOMPANYINVENTTRANSID,INTRASTATDISPATCHID,INTRASTATFULFILLMENTDATE_HU,INVENTDIMID,INVENTQTY,INVENTREFID,INVENTREFTRANSID,INVENTREFTYPE,INVENTTRANSID,INVOICEDATE,INVOICEID,ITEMCODEID,ITEMID,LEDGERDIMENSION,LINEAMOUNT,LINEAMOUNTMST,LINEAMOUNTTAX,LINEAMOUNTTAXMST,LINEDISC,LINEHEADER,LINENUM,LINEPERCENT,MCRDELIVERYNAME,MCRDLVMODE,MODIFIEDDATETIME,MULTILNDISC,MULTILNPERCENT,NAME,NGPCODESTABLE_FR,NUMBERSEQUENCEGROUP,OLAPCOSTVALUE,ORDERLINEREFERENCE_NO,ORIGCOUNTRYREGIONID,ORIGSALESID,ORIGSTATE,PARENTRECID,PARTDELIVERY,PARTITION,PDSCWQTY,PDSCWQTYPHYSICAL,PDSCWREMAIN,PORT,PRICEUNIT,QTY,QTYPHYSICAL,REASONREFRECID,RECID,RECVERSION,REMAIN,REMAINBEFORE,RETAILCATEGORY,RETURNARRIVALDATE,RETURNCLOSEDDATE,RETURNDISPOSITIONCODEID,REVERSECHARGEAPPLIES_UK,REVERSEDRECID,SALESCATEGORY,SALESGROUP,SALESID,SALESMARKUP,SALESPRICE,SALESUNIT,SOURCEDOCUMENTLINE,STATLINEAMOUNTMST,STATPROCID,STOCKEDPRODUCT,SUMLINEDISC,SUMLINEDISCMST,TAXAMOUNT,TAXAMOUNTMST,TAXAUTOGENERATED,TAXGROUP,TAXITEMGROUP,TAXWITHHOLDGROUP_TH,TAXWITHHOLDITEMGROUPHEADING_TH,TAXWRITECODE,TRANSACTIONCODE,TRANSPORT,WEIGHT,ETL_FILE_NAME,ETL_ROW_DELETED_FLAG,ETL_INSERT_AUDIT_KEY,ETL_UPDATE_AUDIT_KEY,ETL_ROW_EFFECTIVE_DATE,ETL_ROW_EXPIRY_DATE,ETL_ROW_CURRENT_FLAG,ETL_INSERT_DATETIME,ETL_UPDATE_DATETIME,ETL_INSERT_JOB_NAME,ETL_UPDATE_JOB_NAME\nFROM (\n SELECT *,\n ROW_NUMBER() OVER (PARTITION BY RECID ORDER BY RECID) AS rn\n FROM CUSTINVOICETRANS\n) AS check_dups\nWHERE rn = 1;\n\nALTER TABLE EDP_DEV.STG_SCAX2012.CUSTINVOICETRANS RENAME TO EDP_DEV.STG_SCAX2012.CUSTINVOICETRANS_BACKUP070825\n\nALTER TABLE EDP_DEV.STG_SCAX2012.CUSTINVOICETRANS_DuplicatesRemoved RENAME TO EDP_DEV.STG_SCAX2012.CUSTINVOICETRANS", "text": "Summary\nremoving duplicates from CUSTINVOICETRANS\n\n---\n\nDescription\nI have created this ticket in lieu of data quality checks @user \n\nI may need more details - will ask during standup 6/8\n\n---\n\nAcceptance Criteria\n* Duplicates are removed from CUSTINVOICETRANS table\n\n---\n\nComments\nGetting the below SQL access error\n\nSQL access control error:\n\n{noformat}Insufficient privileges to operate on table 'CUSTINVOICETRANS'{noformat}\n\n@user I am assign to OPS_DEV_RO read only role. Please grant delete permission for this table.\n\nDuplicates removed from the CUSTINVOICETRANS Table.\n\nThere is no straight forward delete as this table does not have any unique column to identify the delete and snowflake does not support rownum, cte and qualify for deleting duplicates without unique column. Used the below workaround to achieve this.\n\nActions Taken: -\n\n* Created CUSTINVOICETRANS_DuplicatesRemoved with distinct records.\n* Renamed CUSTINVOICETRANS to CUSTINVOICETRANS_Backup.\n* Renamed the new table to CUSTINVOICETRANS.\n\nScripts:-\n\nCreate table CUSTINVOICETRANS_DuplicatesRemoved as\nSELECT ASSETBOOKID,ASSETID,BILLINGCODE,COMMISSAMOUNTCUR,COMMISSAMOUNTMST,COMMISSCALC,COUNTRYREGIONOFSHIPMENT,CREATEDBY,CREATEDDATETIME,CURRENCYCODE,CUSTINVOICELINEIDREF,CUSTOMERLINENUM,DATAAREAID,DEFAULTDIMENSION,DELIVERYPOSTALADDRESS,DELIVERYTYPE,DEL_CREATEDTIME,DISCAMOUNT,DISCPERCENT,DLVDATE,EINVOICEACCOUNTCODE,EXTERNALITEMID,INTERCOMPANYINVENTTRANSID,INTRASTATDISPATCHID,INTRASTATFULFILLMENTDATE_HU,INVENTDIMID,INVENTQTY,INVENTREFID,INVENTREFTRANSID,INVENTREFTYPE,INVENTTRANSID,INVOICEDATE,INVOICEID,ITEMCODEID,ITEMID,LEDGERDIMENSION,LINEAMOUNT,LINEAMOUNTMST,LINEAMOUNTTAX,LINEAMOUNTTAXMST,LINEDISC,LINEHEADER,LINENUM,LINEPERCENT,MCRDELIVERYNAME,MCRDLVMODE,MODIFIEDDATETIME,MULTILNDISC,MULTILNPERCENT,NAME,NGPCODESTABLE_FR,NUMBERSEQUENCEGROUP,OLAPCOSTVALUE,ORDERLINEREFERENCE_NO,ORIGCOUNTRYREGIONID,ORIGSALESID,ORIGSTATE,PARENTRECID,PARTDELIVERY,PARTITION,PDSCWQTY,PDSCWQTYPHYSICAL,PDSCWREMAIN,PORT,PRICEUNIT,QTY,QTYPHYSICAL,REASONREFRECID,RECID,RECVERSION,REMAIN,REMAINBEFORE,RETAILCATEGORY,RETURNARRIVALDATE,RETURNCLOSEDDATE,RETURNDISPOSITIONCODEID,REVERSECHARGEAPPLIES_UK,REVERSEDRECID,SALESCATEGORY,SALESGROUP,SALESID,SALESMARKUP,SALESPRICE,SALESUNIT,SOURCEDOCUMENTLINE,STATLINEAMOUNTMST,STATPROCID,STOCKEDPRODUCT,SUMLINEDISC,SUMLINEDISCMST,TAXAMOUNT,TAXAMOUNTMST,TAXAUTOGENERATED,TAXGROUP,TAXITEMGROUP,TAXWITHHOLDGROUP_TH,TAXWITHHOLDITEMGROUPHEADING_TH,TAXWRITECODE,TRANSACTIONCODE,TRANSPORT,WEIGHT,ETL_FILE_NAME,ETL_ROW_DELETED_FLAG,ETL_INSERT_AUDIT_KEY,ETL_UPDATE_AUDIT_KEY,ETL_ROW_EFFECTIVE_DATE,ETL_ROW_EXPIRY_DATE,ETL_ROW_CURRENT_FLAG,ETL_INSERT_DATETIME,ETL_UPDATE_DATETIME,ETL_INSERT_JOB_NAME,ETL_UPDATE_JOB_NAME\nFROM (\n SELECT *,\n ROW_NUMBER() OVER (PARTITION BY RECID ORDER BY RECID) AS rn\n FROM CUSTINVOICETRANS\n) AS check_dups\nWHERE rn = 1;\n\nALTER TABLE EDP_DEV.STG_SCAX2012.CUSTINVOICETRANS RENAME TO EDP_DEV.STG_SCAX2012.CUSTINVOICETRANS_BACKUP070825\n\nALTER TABLE EDP_DEV.STG_SCAX2012.CUSTINVOICETRANS_DuplicatesRemoved RENAME TO EDP_DEV.STG_SCAX2012.CUSTINVOICETRANS", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 366}}
{"issue_key": "CSCI-241", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Aug/25 5:17 PM", "updated": "15/Aug/25 4:38 PM", "labels": [], "summary": "Source to target Facts - Fact_Store_Inventory", "description": "This task involves mapping out target fact tables using the source-to-target sheet, with a focus on the key facts outlined in the acceptance criteria. The main fact tables to be addressed include:\n\n* {{Fact_Store_Inventory_Intra}}\n* {{Fact_Store_Inventory_Adjustment}}\n* {{Fact_Store_Inventory_Snapshot}}\n\nThese tables are listed as Priority 1 and Priority 2 in the control table and are critical for meeting the acceptance criteria. The goal is to ensure accurate and complete mapping from source to target for these facts.", "acceptance_criteria": "Priority 1 and Priority 2 Fact tables on control table will be mapped\n\n* Fact_Store_Inventory_Intra\n* Fact_Store_Inventory_Adjustment\n* Fact_Store_Inventory_Snapshot", "comments": "Note - not a daily snapshot - just movement\n\nGoing thru logic i.e. filtration.\nWill take longer time\n\nFact_Store_Inventory_snapshot and Fact_Store_Inventory_Intra\n\nETL Logic onPrem. \n\nStep 1: \n\nInsert into Staging : \n\n{noformat}select rn=row_number() over (partition by DateStampDayOnly, MyCHemID, OriginBranchID order by DateStamp desc, RecordID desc)\n ,*\n ,1 as isPlaceholder\n from Fact_Store_IncrementalSOHChanges\n where DateStamp >= \"PreviousDate\" and DateStamp < \"CurrentDate\"\n ) dt2\n where rn = 1 {noformat}\n\nInsert into the new data from source which is only housing data changes \n\n{noformat}insert Into Staging \nselect \n RecordID, MychemID, AvgRealCost, SOH, DateStamp\n , coalesce(ChangeType, 26) as ChangeType -- set null ChangeType to SOH Import\n , coalesce(SOHChange, 0) as SOHChange\n , StaffID, RealCost, ExpectedSales, OriginBranchID, DateStampDayOnly\n , 0 as SOH_Last\n , 0 as ExpectedSales_Last\n , 0 as LineNumber\n , DateStamp as DateStampOriginal\n , 0 as isPlaceholder\n , hasExcessiveValue\nfrom \n Store_IncrementalSOHChanges\nwhere \n DateStamp >= \"currentDate\" and \n DateStamp < \"nextDate\" and\n RecordID > \"maxRecordID\" {noformat}\n\nThen It Self joins and deletes all records where isPlaceholder = 1 and has a record in the second join where the isPlaceholder = 0\n\n{noformat}delete Staging \nfrom\n(\n select nr.OriginBranchID, nr.MyCHemID\n from Staging nr\n where \n nr.isPlaceholder = 0\n and nr.DateStamp >= @currentDate\n and nr.DateStamp < @nextDate \n\n) dt\nwhere \n Staging.isPlaceholder = 1\n and Staging.DateStamp >= \"currentDate\"\n and Staging.DateStamp < \"nextDate\"\n and Staging.OriginBranchID = dt.OriginBranchID\n and Staging.MychemID = dt.MychemID{noformat}\n\nthis ensures that we will have one record for every ChemID , for every Store fore everyday. \n\nThe ETL for this table must be really thought through as it can get quite resource hungry.", "text": "Summary\nSource to target Facts - Fact_Store_Inventory\n\n---\n\nDescription\nThis task involves mapping out target fact tables using the source-to-target sheet, with a focus on the key facts outlined in the acceptance criteria. The main fact tables to be addressed include:\n\n* {{Fact_Store_Inventory_Intra}}\n* {{Fact_Store_Inventory_Adjustment}}\n* {{Fact_Store_Inventory_Snapshot}}\n\nThese tables are listed as Priority 1 and Priority 2 in the control table and are critical for meeting the acceptance criteria. The goal is to ensure accurate and complete mapping from source to target for these facts.\n\n---\n\nAcceptance Criteria\nPriority 1 and Priority 2 Fact tables on control table will be mapped\n\n* Fact_Store_Inventory_Intra\n* Fact_Store_Inventory_Adjustment\n* Fact_Store_Inventory_Snapshot\n\n---\n\nComments\nNote - not a daily snapshot - just movement\n\nGoing thru logic i.e. filtration.\nWill take longer time\n\nFact_Store_Inventory_snapshot and Fact_Store_Inventory_Intra\n\nETL Logic onPrem. \n\nStep 1: \n\nInsert into Staging : \n\n{noformat}select rn=row_number() over (partition by DateStampDayOnly, MyCHemID, OriginBranchID order by DateStamp desc, RecordID desc)\n ,*\n ,1 as isPlaceholder\n from Fact_Store_IncrementalSOHChanges\n where DateStamp >= \"PreviousDate\" and DateStamp < \"CurrentDate\"\n ) dt2\n where rn = 1 {noformat}\n\nInsert into the new data from source which is only housing data changes \n\n{noformat}insert Into Staging \nselect \n RecordID, MychemID, AvgRealCost, SOH, DateStamp\n , coalesce(ChangeType, 26) as ChangeType -- set null ChangeType to SOH Import\n , coalesce(SOHChange, 0) as SOHChange\n , StaffID, RealCost, ExpectedSales, OriginBranchID, DateStampDayOnly\n , 0 as SOH_Last\n , 0 as ExpectedSales_Last\n , 0 as LineNumber\n , DateStamp as DateStampOriginal\n , 0 as isPlaceholder\n , hasExcessiveValue\nfrom \n Store_IncrementalSOHChanges\nwhere \n DateStamp >= \"currentDate\" and \n DateStamp < \"nextDate\" and\n RecordID > \"maxRecordID\" {noformat}\n\nThen It Self joins and deletes all records where isPlaceholder = 1 and has a record in the second join where the isPlaceholder = 0\n\n{noformat}delete Staging \nfrom\n(\n select nr.OriginBranchID, nr.MyCHemID\n from Staging nr\n where \n nr.isPlaceholder = 0\n and nr.DateStamp >= @currentDate\n and nr.DateStamp < @nextDate \n\n) dt\nwhere \n Staging.isPlaceholder = 1\n and Staging.DateStamp >= \"currentDate\"\n and Staging.DateStamp < \"nextDate\"\n and Staging.OriginBranchID = dt.OriginBranchID\n and Staging.MychemID = dt.MychemID{noformat}\n\nthis ensures that we will have one record for every ChemID , for every Store fore everyday. \n\nThe ETL for this table must be really thought through as it can get quite resource hungry.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 367}}
{"issue_key": "CSCI-239", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Aug/25 2:24 PM", "updated": "08/Oct/25 11:15 AM", "labels": [], "summary": "MergeCo Conformed Reporting - Test PBI Dashboard", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nMergeCo Conformed Reporting - Test PBI Dashboard", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 368}}
{"issue_key": "CSCI-238", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Aug/25 2:24 PM", "updated": "08/Oct/25 11:15 AM", "labels": [], "summary": "MergeCo Conformed Reporting - Develop PBI Dashboard", "description": "", "acceptance_criteria": "", "comments": "@user\n\nCurrently blocked as unable to proceed further in development.\n\nCurrently awaiting on access from [https://sigmahealthcare.atlassian.net/browse/CSCI-288|https://sigmahealthcare.atlassian.net/browse/CSCI-288] to be unblocked before continuing further.\n\nI wasn’t really too sure how much work was actually spent here @user before it got blocked - so I moved this task rather than splitting it.\n\n@user 1 day", "text": "Summary\nMergeCo Conformed Reporting - Develop PBI Dashboard\n\n---\n\nComments\n@user\n\nCurrently blocked as unable to proceed further in development.\n\nCurrently awaiting on access from [https://sigmahealthcare.atlassian.net/browse/CSCI-288|https://sigmahealthcare.atlassian.net/browse/CSCI-288] to be unblocked before continuing further.\n\nI wasn’t really too sure how much work was actually spent here @user before it got blocked - so I moved this task rather than splitting it.\n\n@user 1 day", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 369}}
{"issue_key": "CSCI-237", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Aug/25 2:23 PM", "updated": "08/Oct/25 11:15 AM", "labels": [], "summary": "MergeCo Conformed Reporting - Combine Facts and Dims from CW + Sigma", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nMergeCo Conformed Reporting - Combine Facts and Dims from CW + Sigma", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 370}}
{"issue_key": "CSCI-236", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Aug/25 2:23 PM", "updated": "12/Sep/25 2:33 PM", "labels": [], "summary": "MergeCo Conformed Reporting - Pull Data from Sigma to CW SF Tenancy", "description": "Copy over from Sigma the below KPIs:\n\n* Inventory\n** *SOH* (Units + $)\n*** Stock on Hand\n** *DOI*\n*** Days of Inventory\n**** Avg SOH $ / avg Daily Sales $ at Cost (last 12 weeks)\n* Outbound Performance\n** *ATP*%\n*** Available to Promise\n**** Confirmed / Ordered\n** *DIF*%\n*** Invoiced / Confirmed\n** *Cust(Customer) Impact* (% + $)\n*** Invoiced / Ordered\n** *DOT*%\n*** Qty Units Dispatched in 24 hrs / Qty Units Dispatched\n\nGrain:\n\n* High-level: Per DC, Per Day", "acceptance_criteria": "need to build queries on sigma side for mergeco KPI reporting view.\n\n# *Data Accuracy*:\n#* Ensure all KPIs copied from Sigma are accurate and match the source data.\n#* Verify that the values for SOH, DOI, ATP%, DIF%, Cust Impact, and DOT% are correctly reflected.\n# *KPI Completeness*:\n#* All attriubutes are exist to compute the KPIs:\n#** Inventory: SOH (Units + $), DOI\n#** Outbound Performance: ATP%, DIF%, Cust Impact (% + $), DOT%\n# *Calculation Verification*:\n#* Confirm that the calculations for Avg SOH $ and avg Daily Sales $ at Cost for DOI are performed correctly based on the last 12 weeks of data.\n# *Documentation*:\n#* Provide documentation outlining the data sources, calculations, and any assumptions made during the data pull process.\n# *User Acceptance Testing (UAT)*:\n#* Conduct UAT with end-users to ensure the report meets their needs and expectations.\n#* Can also be done by cross check with existing KPI report", "comments": "Per DC per day all attribute used to calculate the KPIs\n\n@user - Try this\n\n[^3PL Phase 2 Internal Report.pbix]\n\n[^colorTheme.json]\n\nRaw script\n\n{noformat}SELECT kpi.\"Date_Key\",\n dd.\"Full Date\",\n dd.\"Day Month Year (DD/MM/YYYY)\",\n \"Day Indicator\",\n \"Month Indicator\",\n \"Year Indicator\",\n kpi_attr.\"Plant_Code\",\n kpi_attr.\"Plant_Name\",\n -- SOH qty and $\n SUM(IFF(kpi_def.\"System_Name\" = 'SOHDollar', kpi.\"Value_Current_Numerator\", 0)) AS \"SOH_amt\",\n SUM(IFF(kpi_def.\"System_Name\" = 'SOHQty', kpi.\"Value_Current_Numerator\", 0)) AS \"SOH_Qty\",\n -- DOI TO REVIEW\n SUM(IFF(kpi_def.\"System_Name\" = 'VPRSDollar', kpi.\"Value_Current_Numerator\", 0)) as \"VPRS$\",\n --ATP%\n DIV0(SUM(IFF(kpi_def.\"System_Name\" = 'ATP', kpi.\"Value_Current_Numerator\", 0)),\n SUM(IFF(kpi_def.\"System_Name\" = 'ATP', kpi.\"Value_Current_Denominator\", 0))) AS \"ATP\",\n --DIF%\n DIV0(SUM(IFF(kpi_def.\"System_Name\" = 'DIF', kpi.\"Value_Current_Numerator\", 0)),\n SUM(IFF(kpi_def.\"System_Name\" = 'DIF', kpi.\"Value_Current_Denominator\", 0))) AS \"DIF\",\n --Cust Impact$,%\n DIV0(SUM(IFF(kpi_def.\"System_Name\" = 'CustImpactPercentage', kpi.\"Value_Current_Numerator\", 0)),\n SUM(IFF(kpi_def.\"System_Name\" = 'CustImpactPercentage', kpi.\"Value_Current_Denominator\",\n 0))) AS \"CustImpact%\",\n SUM(IFF(kpi_def.\"System_Name\" = 'CustImpactDollar', kpi.\"Value_Current_Numerator\",\n 0)) AS \"CustImpact$\",\n --DOT %\n DIV0(SUM(IFF(kpi_def.\"System_Name\" = 'DOT', kpi.\"Value_Current_Numerator\", 0)),\n SUM(IFF(kpi_def.\"System_Name\" = 'DOT', kpi.\"Value_Current_Denominator\", 0))) AS \"DOT%\",\n--Others\n SUM(IFF(kpi_def.\"System_Name\" = 'OrderQty', kpi.\"Value_Current_Numerator\", 0)) AS \"OrderQty\",\n SUM(IFF(kpi_def.\"System_Name\" = 'ConfirmedOrderQty', kpi.\"Value_Current_Numerator\",\n 0)) AS \"ConfirmedOrderQty\",\n SUM(IFF(kpi_def.\"System_Name\" = 'InvoiceQty', kpi.\"Value_Current_Numerator\",\n 0)) AS \"InvoiceQty\"\nFROM EDP_PROD.PRES.\"Fact_KPI_Summary\" kpi\n INNER JOIN\n pres.\"Dim_KPI_Attribute\" kpi_attr ON kpi.\"KPI_Attribute_Key\" = kpi_attr.\"KPI_Attribute_Key\"\n INNER JOIN\n pres.\"Dim_KPI_Definition\" kpi_def ON kpi.\"KPI_Definition_Key\" = kpi_def.\"KPI_Definition_Key\"\n INNER JOIN pres.\"Dim_Date_VW\" dd on kpi.\"Date_Key\" = dd.\"Date Key\"\nWHERE 0 = 0\n AND kpi_attr.\"Material_Group_1_Code\" NOT IN ('MED', 'UKN', 'N/A')\nGROUP BY kpi.\"Date_Key\", dd.\"Full Date\",\n kpi_attr.\"Plant_Code\",\n kpi_attr.\"Plant_Code\", kpi_attr.\"Plant_Name\", dd.\"Day Month Year (DD/MM/YYYY)\", \"Day Indicator\",\n \"Month Indicator\", \"Year Indicator\"\n\n{noformat}\n\nNext:\n\nReview DOI logic & build test case\n\nTest Cases - WIP (Will need to add more cases?)\n\n# Figure to match current Sigma KPI report\n\n{code:python}(\n'1201',\n'20250903',\n[\n (\"SOH_amt\",97867067.1,DecimalType()),\n (\"OrderQty\",364891,IntegerType()),\n ('ConfirmedOrderQty',325137,IntegerType()),\n ('InvoiceQty',319391,IntegerType()),\n (\"DIF%\",0.9823,DecimalType()),\n ( \"CustImpact$\",705547.99,DecimalType()),\n (\"CustImpact%\",0.8753,DecimalType()),\n (\"DOT%\",1.00,DecimalType())\n]\n)\n]{code}\n\n[^test_sigdatashare.py]\n\nHey @user ,\n\nI ran the testing script and is align with sigma’s KPI report\n\nThe only attribute that is not directly pulled is DOI as it would be easier to calculate the 12 week avg in PBI → i.e. SOH$/VPRS$(12 week avg)\n\nDo you wanna give it a good and see that align what you need? Then i will adjust base on that\n\n@user Hi Han!\n\nAs discussed this looks great 🙂 I will speak to Harrison about looking to see if we want to include the product level in this view. As well as getting him to enable datasharing of this to our CW SF tenancy.\n\nWill mark this ticket as done 🙂 \n\nCheers!\n\nJust note for self\n\nif we wanna bring in product grain need to\n\nupdate {{DIM_KPI_ATTRIBUTE}} worker view → Currently we have Dim_product info hence just need to add the column", "text": "Summary\nMergeCo Conformed Reporting - Pull Data from Sigma to CW SF Tenancy\n\n---\n\nDescription\nCopy over from Sigma the below KPIs:\n\n* Inventory\n** *SOH* (Units + $)\n*** Stock on Hand\n** *DOI*\n*** Days of Inventory\n**** Avg SOH $ / avg Daily Sales $ at Cost (last 12 weeks)\n* Outbound Performance\n** *ATP*%\n*** Available to Promise\n**** Confirmed / Ordered\n** *DIF*%\n*** Invoiced / Confirmed\n** *Cust(Customer) Impact* (% + $)\n*** Invoiced / Ordered\n** *DOT*%\n*** Qty Units Dispatched in 24 hrs / Qty Units Dispatched\n\nGrain:\n\n* High-level: Per DC, Per Day\n\n---\n\nAcceptance Criteria\nneed to build queries on sigma side for mergeco KPI reporting view.\n\n# *Data Accuracy*:\n#* Ensure all KPIs copied from Sigma are accurate and match the source data.\n#* Verify that the values for SOH, DOI, ATP%, DIF%, Cust Impact, and DOT% are correctly reflected.\n# *KPI Completeness*:\n#* All attriubutes are exist to compute the KPIs:\n#** Inventory: SOH (Units + $), DOI\n#** Outbound Performance: ATP%, DIF%, Cust Impact (% + $), DOT%\n# *Calculation Verification*:\n#* Confirm that the calculations for Avg SOH $ and avg Daily Sales $ at Cost for DOI are performed correctly based on the last 12 weeks of data.\n# *Documentation*:\n#* Provide documentation outlining the data sources, calculations, and any assumptions made during the data pull process.\n# *User Acceptance Testing (UAT)*:\n#* Conduct UAT with end-users to ensure the report meets their needs and expectations.\n#* Can also be done by cross check with existing KPI report\n\n---\n\nComments\nPer DC per day all attribute used to calculate the KPIs\n\n@user - Try this\n\n[^3PL Phase 2 Internal Report.pbix]\n\n[^colorTheme.json]\n\nRaw script\n\n{noformat}SELECT kpi.\"Date_Key\",\n dd.\"Full Date\",\n dd.\"Day Month Year (DD/MM/YYYY)\",\n \"Day Indicator\",\n \"Month Indicator\",\n \"Year Indicator\",\n kpi_attr.\"Plant_Code\",\n kpi_attr.\"Plant_Name\",\n -- SOH qty and $\n SUM(IFF(kpi_def.\"System_Name\" = 'SOHDollar', kpi.\"Value_Current_Numerator\", 0)) AS \"SOH_amt\",\n SUM(IFF(kpi_def.\"System_Name\" = 'SOHQty', kpi.\"Value_Current_Numerator\", 0)) AS \"SOH_Qty\",\n -- DOI TO REVIEW\n SUM(IFF(kpi_def.\"System_Name\" = 'VPRSDollar', kpi.\"Value_Current_Numerator\", 0)) as \"VPRS$\",\n --ATP%\n DIV0(SUM(IFF(kpi_def.\"System_Name\" = 'ATP', kpi.\"Value_Current_Numerator\", 0)),\n SUM(IFF(kpi_def.\"System_Name\" = 'ATP', kpi.\"Value_Current_Denominator\", 0))) AS \"ATP\",\n --DIF%\n DIV0(SUM(IFF(kpi_def.\"System_Name\" = 'DIF', kpi.\"Value_Current_Numerator\", 0)),\n SUM(IFF(kpi_def.\"System_Name\" = 'DIF', kpi.\"Value_Current_Denominator\", 0))) AS \"DIF\",\n --Cust Impact$,%\n DIV0(SUM(IFF(kpi_def.\"System_Name\" = 'CustImpactPercentage', kpi.\"Value_Current_Numerator\", 0)),\n SUM(IFF(kpi_def.\"System_Name\" = 'CustImpactPercentage', kpi.\"Value_Current_Denominator\",\n 0))) AS \"CustImpact%\",\n SUM(IFF(kpi_def.\"System_Name\" = 'CustImpactDollar', kpi.\"Value_Current_Numerator\",\n 0)) AS \"CustImpact$\",\n --DOT %\n DIV0(SUM(IFF(kpi_def.\"System_Name\" = 'DOT', kpi.\"Value_Current_Numerator\", 0)),\n SUM(IFF(kpi_def.\"System_Name\" = 'DOT', kpi.\"Value_Current_Denominator\", 0))) AS \"DOT%\",\n--Others\n SUM(IFF(kpi_def.\"System_Name\" = 'OrderQty', kpi.\"Value_Current_Numerator\", 0)) AS \"OrderQty\",\n SUM(IFF(kpi_def.\"System_Name\" = 'ConfirmedOrderQty', kpi.\"Value_Current_Numerator\",\n 0)) AS \"ConfirmedOrderQty\",\n SUM(IFF(kpi_def.\"System_Name\" = 'InvoiceQty', kpi.\"Value_Current_Numerator\",\n 0)) AS \"InvoiceQty\"\nFROM EDP_PROD.PRES.\"Fact_KPI_Summary\" kpi\n INNER JOIN\n pres.\"Dim_KPI_Attribute\" kpi_attr ON kpi.\"KPI_Attribute_Key\" = kpi_attr.\"KPI_Attribute_Key\"\n INNER JOIN\n pres.\"Dim_KPI_Definition\" kpi_def ON kpi.\"KPI_Definition_Key\" = kpi_def.\"KPI_Definition_Key\"\n INNER JOIN pres.\"Dim_Date_VW\" dd on kpi.\"Date_Key\" = dd.\"Date Key\"\nWHERE 0 = 0\n AND kpi_attr.\"Material_Group_1_Code\" NOT IN ('MED', 'UKN', 'N/A')\nGROUP BY kpi.\"Date_Key\", dd.\"Full Date\",\n kpi_attr.\"Plant_Code\",\n kpi_attr.\"Plant_Code\", kpi_attr.\"Plant_Name\", dd.\"Day Month Year (DD/MM/YYYY)\", \"Day Indicator\",\n \"Month Indicator\", \"Year Indicator\"\n\n{noformat}\n\nNext:\n\nReview DOI logic & build test case\n\nTest Cases - WIP (Will need to add more cases?)\n\n# Figure to match current Sigma KPI report\n\n{code:python}(\n'1201',\n'20250903',\n[\n (\"SOH_amt\",97867067.1,DecimalType()),\n (\"OrderQty\",364891,IntegerType()),\n ('ConfirmedOrderQty',325137,IntegerType()),\n ('InvoiceQty',319391,IntegerType()),\n (\"DIF%\",0.9823,DecimalType()),\n ( \"CustImpact$\",705547.99,DecimalType()),\n (\"CustImpact%\",0.8753,DecimalType()),\n (\"DOT%\",1.00,DecimalType())\n]\n)\n]{code}\n\n[^test_sigdatashare.py]\n\nHey @user ,\n\nI ran the testing script and is align with sigma’s KPI report\n\nThe only attribute that is not directly pulled is DOI as it would be easier to calculate the 12 week avg in PBI → i.e. SOH$/VPRS$(12 week avg)\n\nDo you wanna give it a good and see that align what you need? Then i will adjust base on that\n\n@user Hi Han!\n\nAs discussed this looks great 🙂 I will speak to Harrison about looking to see if we want to include the product level in this view. As well as getting him to enable datasharing of this to our CW SF tenancy.\n\nWill mark this ticket as done 🙂 \n\nCheers!\n\nJust note for self\n\nif we wanna bring in product grain need to\n\nupdate {{DIM_KPI_ATTRIBUTE}} worker view → Currently we have Dim_product info hence just need to add the column", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 371}}
{"issue_key": "CSCI-235", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Story", "status": "Done", "priority": "Medium", "created": "05/Aug/25 8:44 AM", "updated": "20/Nov/25 8:44 AM", "labels": [], "summary": "Source-To-Target-Mapping for DIMS - Core Master Data", "description": "h3. *Objective*\n\nTo establish a comprehensive Source-To-Target Mapping (STTM) for key Core Master Data dimensions that will be used in the Control Table and downstream data processes. This mapping will serve as a foundational reference for data ingestion, transformation, and reporting within the Supply Chain domain.\n\nh3. *Description*\n\nThis task involves identifying and documenting the source and target structures for the following core Master Data dimensions:\n\n*Core Master Data*\n\n* Product\n* DC/Warehouse/Plant\n* Store\n* Manhattan Warehouse Location (What Grain? The Bin?)\n* Supplier\n* Manufacturer\n* Product-Plant (SKU)\n* Date\n* Batch (Product Batch)\n\nThe mapping should include:\n\n* Source system and table details\n* Target schema and table structure\n* Transformation logic (if applicable)\n* Grain and SCD (Slowly Changing Dimension) policies for each dimension ( Confirmed? @user )\n\n* confirmed that SCD policy is needed for each dimension", "acceptance_criteria": "* Mapping is completed and reviewed for all listed dimensions\n* -[STTM document|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true] updated with Dims-\n* Mapping Completed for these DIMs\n** -Product (to be reviewed)-\n** DC/Warehouse/Plant\n** -Store (to be reviewed)-\n** Manhattan Warehouse Location (What Grain? The Bin?)\n** Supplier\n** Manufacturer\n** Product-Plant (SKU)\n** Date\n** Batch (Product Batch)", "comments": "Hey @user , @user , @user ,\n\nCan you review and suggest any other core Master Data elements we should include in this ticket for initial core master Data?\n\n@user can you create sample DIM in STTM document and add all DIMs to Control Table?\n\nThanks,\n\nHarrison\n\n@user thanks Harrison, these look great 🙂 \n\nOnly thing I think is missing is a Date dim 🗓️\n\n@user \n\nhaha Im an idiot, good point\n\nDim_Store and Dim_Product mapping is completed\n\n@user @user Can you review it please\n\nWill split this card to separate tickets and retire this card. \n\nWill link cards\n\nConverted to story, linked all cards, and removed from backlog to reduce noise\n\n@user @user @user", "text": "Summary\nSource-To-Target-Mapping for DIMS - Core Master Data\n\n---\n\nDescription\nh3. *Objective*\n\nTo establish a comprehensive Source-To-Target Mapping (STTM) for key Core Master Data dimensions that will be used in the Control Table and downstream data processes. This mapping will serve as a foundational reference for data ingestion, transformation, and reporting within the Supply Chain domain.\n\nh3. *Description*\n\nThis task involves identifying and documenting the source and target structures for the following core Master Data dimensions:\n\n*Core Master Data*\n\n* Product\n* DC/Warehouse/Plant\n* Store\n* Manhattan Warehouse Location (What Grain? The Bin?)\n* Supplier\n* Manufacturer\n* Product-Plant (SKU)\n* Date\n* Batch (Product Batch)\n\nThe mapping should include:\n\n* Source system and table details\n* Target schema and table structure\n* Transformation logic (if applicable)\n* Grain and SCD (Slowly Changing Dimension) policies for each dimension ( Confirmed? @user )\n\n* confirmed that SCD policy is needed for each dimension\n\n---\n\nAcceptance Criteria\n* Mapping is completed and reviewed for all listed dimensions\n* -[STTM document|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true] updated with Dims-\n* Mapping Completed for these DIMs\n** -Product (to be reviewed)-\n** DC/Warehouse/Plant\n** -Store (to be reviewed)-\n** Manhattan Warehouse Location (What Grain? The Bin?)\n** Supplier\n** Manufacturer\n** Product-Plant (SKU)\n** Date\n** Batch (Product Batch)\n\n---\n\nComments\nHey @user , @user , @user ,\n\nCan you review and suggest any other core Master Data elements we should include in this ticket for initial core master Data?\n\n@user can you create sample DIM in STTM document and add all DIMs to Control Table?\n\nThanks,\n\nHarrison\n\n@user thanks Harrison, these look great 🙂 \n\nOnly thing I think is missing is a Date dim 🗓️\n\n@user \n\nhaha Im an idiot, good point\n\nDim_Store and Dim_Product mapping is completed\n\n@user @user Can you review it please\n\nWill split this card to separate tickets and retire this card. \n\nWill link cards\n\nConverted to story, linked all cards, and removed from backlog to reduce noise\n\n@user @user @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 372}}
{"issue_key": "CSCI-230", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "04/Aug/25 6:46 AM", "updated": "14/Aug/25 11:37 AM", "labels": [], "summary": "PowerBI - Infrastructure /gateway setup - review", "description": "", "acceptance_criteria": "Checking if the below setup aligns to connection needs.\n\n# *Connection Availability*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Two Power BI connections are available in Fabric:\"}],\"attrs\":{\"localId\":\"455a3c43-7f3a-4fa2-b526-d6e824179873\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"2c1368f5-d548-4845-bb95-7855192fb70c\"}}\n{adf}\n#* {{SF_S_POWERBI_DEV}} (On-Premise Data Gateway)\n#* {{SF_S_POWERBI_DEV_VGW}} (VNet Data Gateway)\n# *Access Permissions*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"The Azure group \"},{\"type\":\"text\",\"text\":\"Azure-Permissions-dev-edp-aus-Contributor\",\"marks\":[{\"type\":\"code\"}]},{\"type\":\"text\",\"text\":\" has access to both connections.\"}],\"attrs\":{\"localId\":\"eb67f980-c57b-4fb1-95ad-8d98f3bc1b73\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"3f813f8b-704e-46e0-b564-8e9b0cab0748\"}}\n{adf}\n# *Compute Management for On-Premise Gateway*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"When using \"},{\"type\":\"text\",\"text\":\"SF_S_POWERBI_DEV\",\"marks\":[{\"type\":\"code\"}]},{\"type\":\"text\",\"text\":\", the VM \"},{\"type\":\"text\",\"text\":\"cwr-ase-edpdatagateway-dev-vm-01\",\"marks\":[{\"type\":\"code\"}]},{\"type\":\"text\",\"text\":\" must be started in Azure.\"}],\"attrs\":{\"localId\":\"1508f1e3-d3c5-4200-984c-eac7ddf2f787\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"After usage, the VM must be stopped to reduce cost.\"}],\"attrs\":{\"localId\":\"55934423-95d3-4ed3-984c-2ce9d40ec550\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"b1828406-1e16-4fd0-9d61-b7fe195d2117\"}}\n{adf}\n# *Compute Management for VNet Gateway*\n\n* When using {{SF_S_POWERBI_DEV_VGW}}, the Fabric capacity {{cwraseedpdevfc}} must be resumed.\n* After usage, the capacity must be paused to save cost.\n\n# *Trial Capacity Option*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"The VNet Data Gateway \"},{\"type\":\"text\",\"text\":\"cwr-ase-edp-dev-vdgw\",\"marks\":[{\"type\":\"code\"}]},{\"type\":\"text\",\"text\":\" can optionally be assigned to a Trial Capacity.\"}],\"attrs\":{\"localId\":\"8f87142b-75dd-4bae-b49f-f310272f43bf\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"10f6f76d-e77f-4a61-be43-c42152915315\"}}\n{adf}", "comments": "", "text": "Summary\nPowerBI - Infrastructure /gateway setup - review\n\n---\n\nAcceptance Criteria\nChecking if the below setup aligns to connection needs.\n\n# *Connection Availability*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Two Power BI connections are available in Fabric:\"}],\"attrs\":{\"localId\":\"455a3c43-7f3a-4fa2-b526-d6e824179873\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"2c1368f5-d548-4845-bb95-7855192fb70c\"}}\n{adf}\n#* {{SF_S_POWERBI_DEV}} (On-Premise Data Gateway)\n#* {{SF_S_POWERBI_DEV_VGW}} (VNet Data Gateway)\n# *Access Permissions*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"The Azure group \"},{\"type\":\"text\",\"text\":\"Azure-Permissions-dev-edp-aus-Contributor\",\"marks\":[{\"type\":\"code\"}]},{\"type\":\"text\",\"text\":\" has access to both connections.\"}],\"attrs\":{\"localId\":\"eb67f980-c57b-4fb1-95ad-8d98f3bc1b73\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"3f813f8b-704e-46e0-b564-8e9b0cab0748\"}}\n{adf}\n# *Compute Management for On-Premise Gateway*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"When using \"},{\"type\":\"text\",\"text\":\"SF_S_POWERBI_DEV\",\"marks\":[{\"type\":\"code\"}]},{\"type\":\"text\",\"text\":\", the VM \"},{\"type\":\"text\",\"text\":\"cwr-ase-edpdatagateway-dev-vm-01\",\"marks\":[{\"type\":\"code\"}]},{\"type\":\"text\",\"text\":\" must be started in Azure.\"}],\"attrs\":{\"localId\":\"1508f1e3-d3c5-4200-984c-eac7ddf2f787\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"After usage, the VM must be stopped to reduce cost.\"}],\"attrs\":{\"localId\":\"55934423-95d3-4ed3-984c-2ce9d40ec550\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"b1828406-1e16-4fd0-9d61-b7fe195d2117\"}}\n{adf}\n# *Compute Management for VNet Gateway*\n\n* When using {{SF_S_POWERBI_DEV_VGW}}, the Fabric capacity {{cwraseedpdevfc}} must be resumed.\n* After usage, the capacity must be paused to save cost.\n\n# *Trial Capacity Option*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"The VNet Data Gateway \"},{\"type\":\"text\",\"text\":\"cwr-ase-edp-dev-vdgw\",\"marks\":[{\"type\":\"code\"}]},{\"type\":\"text\",\"text\":\" can optionally be assigned to a Trial Capacity.\"}],\"attrs\":{\"localId\":\"8f87142b-75dd-4bae-b49f-f310272f43bf\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"10f6f76d-e77f-4a61-be43-c42152915315\"}}\n{adf}", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 373}}
{"issue_key": "CSCI-225", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "04/Aug/25 6:22 AM", "updated": "14/Aug/25 11:37 AM", "labels": [], "summary": "Team Access to SF via PrivateLink Setup - Review", "description": "This card is a review card in relation to [https://sigmahealthcare.atlassian.net/browse/CSCI-199|https://sigmahealthcare.atlassian.net/browse/CSCI-199] \n\nWork has already been done for this card.", "acceptance_criteria": "* -Work for Team access to privatelink has been reviewed and passed-\n** -A clear decision is made on how to organize team access to Snowflake via PrivateLink.-\n** -Documentation of the current and proposed access methods for both SIGMA and CW.-\n** -Identification of any blockers or dependencies.-\n** -Agreement on next steps and responsible parties.-", "comments": "To check if there is a Jira ticket for Developer Access/ VDI for Snowflake log in @user. If there isnt, create a new one. Story points: 3.", "text": "Summary\nTeam Access to SF via PrivateLink Setup - Review\n\n---\n\nDescription\nThis card is a review card in relation to [https://sigmahealthcare.atlassian.net/browse/CSCI-199|https://sigmahealthcare.atlassian.net/browse/CSCI-199] \n\nWork has already been done for this card.\n\n---\n\nAcceptance Criteria\n* -Work for Team access to privatelink has been reviewed and passed-\n** -A clear decision is made on how to organize team access to Snowflake via PrivateLink.-\n** -Documentation of the current and proposed access methods for both SIGMA and CW.-\n** -Identification of any blockers or dependencies.-\n** -Agreement on next steps and responsible parties.-\n\n---\n\nComments\nTo check if there is a Jira ticket for Developer Access/ VDI for Snowflake log in @user. If there isnt, create a new one. Story points: 3.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 374}}
{"issue_key": "CSCI-223", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "04/Aug/25 6:02 AM", "updated": "11/Aug/25 3:24 PM", "labels": [], "summary": "Adeel - On-prem DEV/PROD Database Environment Sync - part 2", "description": "Background\n\n{quote}Following a review of the on-prem DEV and PROD environments for key databases (StockDB, SCAX2012, ILS, TransactionStorage), discrepancies were identified in schema alignment and data volume. Adeel Syed provided a detailed comparison, highlighting schema mismatches in StockDB and data size differences in TransactionStorage and ILS.\n\nNotably, SCAX2012 was recently refreshed and is up-to-date.\n\n@user has proposed a prioritised refresh plan to align DEV with PROD, particularly to support accurate modelling and pipeline validation in Snowflake environments.\n\nRachel Wan confirmed that DEV environments are refreshed only upon request and are not automatically synced with PROD.\n\n*Summary of Findings:*\n\n* *SCAX2012*: Recently refreshed (11 July 2025) – no further action required.\n* *StockDB*: Schema drift identified across 7 tables (refer to _SchemaCompare.docx_) – refresh required to align DEV with PROD.\n* *TransactionStorage*: DEV has more records than PROD due to regular archiving in PROD (not duplication). Tables archived include:\n** {{BranchOrderItems}}, {{BranchOrders}}, {{Store_IncrementalSOHChanges}}, {{Transactions}}, {{Transactions_Invoice}}\n** {{Shipment_detail}}, {{Shipment_header}}\n* *ILS*: Data size gaps observed – refresh recommended but not urgent.\n\n*Proposed Refresh Priority:*\n\n# *StockDB* – to resolve schema mismatches.\n# *TransactionStorage* – to align data volume and ensure DEV reflects current PROD state.\n# *ILS* – to be scheduled after the above.{quote}", "acceptance_criteria": "*StockDB refresh*\n\n* -NA - due to dev work from other project this DB (@user you may need to help me elaborate on this)-\n\n*TransactionStorage refresh:*\n\n* -TransactionStorage Restore complete-\n* @user I’ll need help creating further points from here.\n\n*ILS refresh:*\n\n* -Clarify risks that are associated with ILS refresh-", "comments": "how soon do we need ILS refresh?\n - TBD @user\n\nWill not proceeed with ILS refresh @user @user", "text": "Summary\nAdeel - On-prem DEV/PROD Database Environment Sync - part 2\n\n---\n\nDescription\nBackground\n\n{quote}Following a review of the on-prem DEV and PROD environments for key databases (StockDB, SCAX2012, ILS, TransactionStorage), discrepancies were identified in schema alignment and data volume. Adeel Syed provided a detailed comparison, highlighting schema mismatches in StockDB and data size differences in TransactionStorage and ILS.\n\nNotably, SCAX2012 was recently refreshed and is up-to-date.\n\n@user has proposed a prioritised refresh plan to align DEV with PROD, particularly to support accurate modelling and pipeline validation in Snowflake environments.\n\nRachel Wan confirmed that DEV environments are refreshed only upon request and are not automatically synced with PROD.\n\n*Summary of Findings:*\n\n* *SCAX2012*: Recently refreshed (11 July 2025) – no further action required.\n* *StockDB*: Schema drift identified across 7 tables (refer to _SchemaCompare.docx_) – refresh required to align DEV with PROD.\n* *TransactionStorage*: DEV has more records than PROD due to regular archiving in PROD (not duplication). Tables archived include:\n** {{BranchOrderItems}}, {{BranchOrders}}, {{Store_IncrementalSOHChanges}}, {{Transactions}}, {{Transactions_Invoice}}\n** {{Shipment_detail}}, {{Shipment_header}}\n* *ILS*: Data size gaps observed – refresh recommended but not urgent.\n\n*Proposed Refresh Priority:*\n\n# *StockDB* – to resolve schema mismatches.\n# *TransactionStorage* – to align data volume and ensure DEV reflects current PROD state.\n# *ILS* – to be scheduled after the above.{quote}\n\n---\n\nAcceptance Criteria\n*StockDB refresh*\n\n* -NA - due to dev work from other project this DB (@user you may need to help me elaborate on this)-\n\n*TransactionStorage refresh:*\n\n* -TransactionStorage Restore complete-\n* @user I’ll need help creating further points from here.\n\n*ILS refresh:*\n\n* -Clarify risks that are associated with ILS refresh-\n\n---\n\nComments\nhow soon do we need ILS refresh?\n - TBD @user\n\nWill not proceeed with ILS refresh @user @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 375}}
{"issue_key": "CSCI-222", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Aug/25 5:48 PM", "updated": "23/Sep/25 3:40 PM", "labels": [], "summary": "Enable SSO login to snowflake - SSO", "description": "Consdering the options between \n\n* MS Entra ID\n* Okta\n\nCK to have a follow-up call with Brent from Cloud to firm up the plan for SSO implementation\n\n* -book next tuesday to follow up-\n\nMeeting completed on Tues:\n\nImplementation of Microsoft Entra ID SSO for Snowflake access across the entire data platform, enabling unified authentication for all user personas and tools including PowerBI, Tabular Editor 3, Azure Data Factory, DBT, and direct Snowflake access.", "acceptance_criteria": "* --booking made for tuesday meeting--\n* --To follow-up from the results of the call on Tuesday--\n* --Decision made between MS Entra ID vs Okta- ✅ *MS Entra ID Selected*-\n\n* -Meeting with Brent from Cloud team scheduled- ✅ *Completed*\n* *Entra ID SSO configured and functional for Snowflake*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Work with Raveendran from our cloud team \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84\",\"accessLevel\":\"\",\"localId\":\"5a778d84-3fcf-46a6-8eed-a75d321d3ac5\"}},{\"type\":\"text\",\"text\":\" and \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"712020:a3bb398f-73df-472d-a226-f67ed86b49ee\",\"accessLevel\":\"\",\"localId\":\"542bb25d-3701-49bc-be49-7b5f9c0a5bea\"}},{\"type\":\"text\",\"text\":\" \"}],\"attrs\":{\"localId\":\"0a6a01ee-0890-4b42-99a3-17e68a53d8bb\",\"state\":\"DONE\"}}],\"attrs\":{\"localId\":\"340fd36c-fd9c-494d-93b5-65b83210118a\"}}\n{adf}\n* *Service Principal OAuth setup for PowerBI Service scheduled refresh*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Work with Raveendran from our cloud team \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84\",\"accessLevel\":\"\",\"localId\":\"29da3589-921e-4bd8-9020-7c2027ba319a\"}},{\"type\":\"text\",\"text\":\" and \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"712020:a3bb398f-73df-472d-a226-f67ed86b49ee\",\"accessLevel\":\"\",\"localId\":\"7cfbbe27-380e-4949-a2c5-ab2e425ff05a\"}},{\"type\":\"text\",\"text\":\" \"}],\"attrs\":{\"localId\":\"7d2f681f-168c-4a83-8bc1-c56634d2d967\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"340fd36c-fd9c-494d-93b5-65b83210118a\"}}\n{adf}\n* *Network security implemented (VNet Gateway)*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\" \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84\",\"accessLevel\":\"\",\"localId\":\"4d11860e-95ff-49b1-bcd3-89eb13c3af94\"}},{\"type\":\"text\",\"text\":\" - as part of the SSO can we also collaborate on this\"}],\"attrs\":{\"localId\":\"736cca4b-dae9-4882-9b4e-de06b4790373\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"819123db-3a96-41ca-8543-00cb537f3b64\"}}\n{adf}\n* *All user personas can authenticate successfully*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Testing post entra ID sso dependencies completed\"}],\"attrs\":{\"localId\":\"c9b6f8b8-7224-4cd6-9ba4-1b8c34b4d47d\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"db194595-b904-41c9-8026-19c645d2aaac\"}}\n{adf}\n* *Conditional access policies and MFA enforced*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Testing post entra ID sso dependencies completed\"}],\"attrs\":{\"localId\":\"b541fd8a-1751-4451-945b-4cd78a6a3069\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"db194595-b904-41c9-8026-19c645d2aaac\"}}\n{adf}\n* *Production rollout completed*", "comments": "pretty sure I can update this @user \nI think we got a foloow up session but the result is to use VDI for DE’s for WFH\n\n* I’ll enter notes of phase 1 and 2 as per presentation and timeline/resource requirements\n\nLMK if there’s anything extra I need ot add\n\nFollow up session - as per @useremail - will elaborate on this.\n\n@user Thanks for the update CK!\n\nNeed servicenow tickets for:\n\n* *Entra ID SSO configured and functional for Snowflake*\n\nSplitting out Jira tickets here for monitoring.\n\nFor reference @user @user\n\nsnowflake user group created\n\nIssue split into:\n|CSCI-462|Enable SSO login to snowflake - SCIM|", "text": "Summary\nEnable SSO login to snowflake - SSO\n\n---\n\nDescription\nConsdering the options between \n\n* MS Entra ID\n* Okta\n\nCK to have a follow-up call with Brent from Cloud to firm up the plan for SSO implementation\n\n* -book next tuesday to follow up-\n\nMeeting completed on Tues:\n\nImplementation of Microsoft Entra ID SSO for Snowflake access across the entire data platform, enabling unified authentication for all user personas and tools including PowerBI, Tabular Editor 3, Azure Data Factory, DBT, and direct Snowflake access.\n\n---\n\nAcceptance Criteria\n* --booking made for tuesday meeting--\n* --To follow-up from the results of the call on Tuesday--\n* --Decision made between MS Entra ID vs Okta- ✅ *MS Entra ID Selected*-\n\n* -Meeting with Brent from Cloud team scheduled- ✅ *Completed*\n* *Entra ID SSO configured and functional for Snowflake*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Work with Raveendran from our cloud team \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84\",\"accessLevel\":\"\",\"localId\":\"5a778d84-3fcf-46a6-8eed-a75d321d3ac5\"}},{\"type\":\"text\",\"text\":\" and \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"712020:a3bb398f-73df-472d-a226-f67ed86b49ee\",\"accessLevel\":\"\",\"localId\":\"542bb25d-3701-49bc-be49-7b5f9c0a5bea\"}},{\"type\":\"text\",\"text\":\" \"}],\"attrs\":{\"localId\":\"0a6a01ee-0890-4b42-99a3-17e68a53d8bb\",\"state\":\"DONE\"}}],\"attrs\":{\"localId\":\"340fd36c-fd9c-494d-93b5-65b83210118a\"}}\n{adf}\n* *Service Principal OAuth setup for PowerBI Service scheduled refresh*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Work with Raveendran from our cloud team \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84\",\"accessLevel\":\"\",\"localId\":\"29da3589-921e-4bd8-9020-7c2027ba319a\"}},{\"type\":\"text\",\"text\":\" and \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"712020:a3bb398f-73df-472d-a226-f67ed86b49ee\",\"accessLevel\":\"\",\"localId\":\"7cfbbe27-380e-4949-a2c5-ab2e425ff05a\"}},{\"type\":\"text\",\"text\":\" \"}],\"attrs\":{\"localId\":\"7d2f681f-168c-4a83-8bc1-c56634d2d967\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"340fd36c-fd9c-494d-93b5-65b83210118a\"}}\n{adf}\n* *Network security implemented (VNet Gateway)*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\" \"},{\"type\":\"mention\",\"attrs\":{\"id\":\"557058:ee707013-ba36-4df4-9ee3-43a3a2b3fb84\",\"accessLevel\":\"\",\"localId\":\"4d11860e-95ff-49b1-bcd3-89eb13c3af94\"}},{\"type\":\"text\",\"text\":\" - as part of the SSO can we also collaborate on this\"}],\"attrs\":{\"localId\":\"736cca4b-dae9-4882-9b4e-de06b4790373\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"819123db-3a96-41ca-8543-00cb537f3b64\"}}\n{adf}\n* *All user personas can authenticate successfully*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Testing post entra ID sso dependencies completed\"}],\"attrs\":{\"localId\":\"c9b6f8b8-7224-4cd6-9ba4-1b8c34b4d47d\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"db194595-b904-41c9-8026-19c645d2aaac\"}}\n{adf}\n* *Conditional access policies and MFA enforced*\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Testing post entra ID sso dependencies completed\"}],\"attrs\":{\"localId\":\"b541fd8a-1751-4451-945b-4cd78a6a3069\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"db194595-b904-41c9-8026-19c645d2aaac\"}}\n{adf}\n* *Production rollout completed*\n\n---\n\nComments\npretty sure I can update this @user \nI think we got a foloow up session but the result is to use VDI for DE’s for WFH\n\n* I’ll enter notes of phase 1 and 2 as per presentation and timeline/resource requirements\n\nLMK if there’s anything extra I need ot add\n\nFollow up session - as per @useremail - will elaborate on this.\n\n@user Thanks for the update CK!\n\nNeed servicenow tickets for:\n\n* *Entra ID SSO configured and functional for Snowflake*\n\nSplitting out Jira tickets here for monitoring.\n\nFor reference @user @user\n\nsnowflake user group created\n\nIssue split into:\n|CSCI-462|Enable SSO login to snowflake - SCIM|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 376}}
{"issue_key": "CSCI-221", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Aug/25 2:28 PM", "updated": "15/Aug/25 9:46 AM", "labels": [], "summary": "Implement Partial Full Apply Mode with Sliding Window Delete Tracking", "description": "We need to follow up on the implementation of a partial full apply mode for handling very large datasets. Instead of a full apply, the approach will track deletes within a sliding window (e.g. last 6 months). This should also be integrated with archive and retention policy analysis on the source system.", "acceptance_criteria": "* Define the parameters for the sliding window.\n* Align with data retention and archival policies.\n* Document the implementation plan and any dependencies.", "comments": "need\n- how data is archived\n\ncsci-186", "text": "Summary\nImplement Partial Full Apply Mode with Sliding Window Delete Tracking\n\n---\n\nDescription\nWe need to follow up on the implementation of a partial full apply mode for handling very large datasets. Instead of a full apply, the approach will track deletes within a sliding window (e.g. last 6 months). This should also be integrated with archive and retention policy analysis on the source system.\n\n---\n\nAcceptance Criteria\n* Define the parameters for the sliding window.\n* Align with data retention and archival policies.\n* Document the implementation plan and any dependencies.\n\n---\n\nComments\nneed\n- how data is archived\n\ncsci-186", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 377}}
{"issue_key": "CSCI-220", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "31/Jul/25 5:29 PM", "updated": "15/Aug/25 4:38 PM", "labels": [], "summary": "Source to Target for Facts - Fact_Warehouse_Location_inventory", "description": "Mapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]\n\nmain facts to focus on are as per acceptance criteria\n\n@user -will split this task out so that fits in to sprint cadence- done\n\nnotes from @user \n\n{quote}The Stock Fact_Warehouse_Inventory_Snapshot- the mapping is done based off the PBI05.supplychain.scax.dcsinventoryhistory\n\nbase tables: INVENTDIM and INVENTDIM. \nThings to note: \n\n* DataareaID- this defines the business entities. \n* read from the most current partition: This function gives the most current partition; SCAXLink.[dbo].[udf_GetPartition] (null)\n* ensure Nolock is used for these reads.{quote}\n\n{quote}\n\nFact_Warehouse_Inventory_Adjustment: Based of [BI_Presentation].[dbo].[WarehouseInventoryAdjustments]\n\nE.g\n\n{quote}", "acceptance_criteria": "* Priority 1 and Priority 2 Fact tables on control table will be mapped\n** -Reviewing Fact_Warehouse_Location_Inventory_Intra-\n** -Reviewing Fact_Warehouse_Location_Inventory_Snapshot-\n** Fact_Warehouse_Location_Inventory_Adjustment", "comments": "Hey @user ,\n\nWho created the details here? Description and ticket Acceptance Criteria dont seem to marry up?\n\nThanks,\n\nHarrison\n\nReviewing Fact_Warehouse_Location_Inventory_Snapshot: \n\nTable is based of ILS.dbo.LocationInventory\n\nThis is a live table out of the WH Management system.\n\nFact_Warehouse_Inventory_Adjustment:\n\nis based on the SC schematic model. As this was not part of the pervious EDP project.", "text": "Summary\nSource to Target for Facts - Fact_Warehouse_Location_inventory\n\n---\n\nDescription\nMapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]\n\nmain facts to focus on are as per acceptance criteria\n\n@user -will split this task out so that fits in to sprint cadence- done\n\nnotes from @user \n\n{quote}The Stock Fact_Warehouse_Inventory_Snapshot- the mapping is done based off the PBI05.supplychain.scax.dcsinventoryhistory\n\nbase tables: INVENTDIM and INVENTDIM. \nThings to note: \n\n* DataareaID- this defines the business entities. \n* read from the most current partition: This function gives the most current partition; SCAXLink.[dbo].[udf_GetPartition] (null)\n* ensure Nolock is used for these reads.{quote}\n\n{quote}\n\nFact_Warehouse_Inventory_Adjustment: Based of [BI_Presentation].[dbo].[WarehouseInventoryAdjustments]\n\nE.g\n\n{quote}\n\n---\n\nAcceptance Criteria\n* Priority 1 and Priority 2 Fact tables on control table will be mapped\n** -Reviewing Fact_Warehouse_Location_Inventory_Intra-\n** -Reviewing Fact_Warehouse_Location_Inventory_Snapshot-\n** Fact_Warehouse_Location_Inventory_Adjustment\n\n---\n\nComments\nHey @user ,\n\nWho created the details here? Description and ticket Acceptance Criteria dont seem to marry up?\n\nThanks,\n\nHarrison\n\nReviewing Fact_Warehouse_Location_Inventory_Snapshot: \n\nTable is based of ILS.dbo.LocationInventory\n\nThis is a live table out of the WH Management system.\n\nFact_Warehouse_Inventory_Adjustment:\n\nis based on the SC schematic model. As this was not part of the pervious EDP project.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 378}}
{"issue_key": "CSCI-219", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "31/Jul/25 9:30 AM", "updated": "04/Aug/25 5:18 PM", "labels": [], "summary": "Source to Target for Facts - Fact_Warehouse", "description": "Mapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]\n\nmain facts to focus on are as per acceptance criteria\n\n@user -will split this task out so that fits in to sprint cadence- done", "acceptance_criteria": "* Priority 1 fact tables on control table will be mapped\n** -Fact_Warehouse_Inventory_Intra-\n** -Fact_Warehouse_Inventory_Snapshot-\n** -Fact_Warehouse_Inventory_Adjustment-", "comments": "@user will update ticket - \n\nahve started mapping fields.\n\nneed overview of tables i.e. description and what should be housed and granularity\n\nHarrison mentioned that one is snapshot\n- e.g. AX inventory_intra_day - per hour\n\nnoted from @user that structure will be the same.\n\nDiscussed about stock incremental\n\n@user @user - will discuss\n\nThe Stock Fact_Warehouse_Inventory_Snapshot- the mapping is done based off the PBI05.supplychain.scax.dcsinventoryhistory\n\nbase tables: INVENTDIM and INVENTDIM. \nThings to note: \n\n* DataareaID- this defines the business entities. \n* read from the most current partition: This function gives the most current partition; SCAXLink.[dbo].[udf_GetPartition] (null)\n* ensure Nolock is used for these reads.\n\nFact_Warehouse_Inventory_Adjustment: Based of [BI_Presentation].[dbo].[WarehouseInventoryAdjustments]\n\nE.g\n\nmarking task as done for sprint burndown\n\n@user \nI’ll carry over the notes to next card for completeness.", "text": "Summary\nSource to Target for Facts - Fact_Warehouse\n\n---\n\nDescription\nMapping out Targets as source as per [Source-to-target sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=SCIscR&nav=MTVfezNGRjZGNUUzLTVCRjEtNDU5QS1CQkMwLURERUUyMzJBMzJFRH0]\n\nmain facts to focus on are as per acceptance criteria\n\n@user -will split this task out so that fits in to sprint cadence- done\n\n---\n\nAcceptance Criteria\n* Priority 1 fact tables on control table will be mapped\n** -Fact_Warehouse_Inventory_Intra-\n** -Fact_Warehouse_Inventory_Snapshot-\n** -Fact_Warehouse_Inventory_Adjustment-\n\n---\n\nComments\n@user will update ticket - \n\nahve started mapping fields.\n\nneed overview of tables i.e. description and what should be housed and granularity\n\nHarrison mentioned that one is snapshot\n- e.g. AX inventory_intra_day - per hour\n\nnoted from @user that structure will be the same.\n\nDiscussed about stock incremental\n\n@user @user - will discuss\n\nThe Stock Fact_Warehouse_Inventory_Snapshot- the mapping is done based off the PBI05.supplychain.scax.dcsinventoryhistory\n\nbase tables: INVENTDIM and INVENTDIM. \nThings to note: \n\n* DataareaID- this defines the business entities. \n* read from the most current partition: This function gives the most current partition; SCAXLink.[dbo].[udf_GetPartition] (null)\n* ensure Nolock is used for these reads.\n\nFact_Warehouse_Inventory_Adjustment: Based of [BI_Presentation].[dbo].[WarehouseInventoryAdjustments]\n\nE.g\n\nmarking task as done for sprint burndown\n\n@user \nI’ll carry over the notes to next card for completeness.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 379}}
{"issue_key": "CSCI-218", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Jul/25 4:12 PM", "updated": "30/Jul/25 4:27 PM", "labels": [], "summary": "SC BAU Sprint 4 Work", "description": "# Assisted Matthew Jones with his SNOW ticket request (*INC0444602/RITM0171602*). Needed help with creating a workspace to share his KPI Scorecard with his team", "acceptance_criteria": "", "comments": "", "text": "Summary\nSC BAU Sprint 4 Work\n\n---\n\nDescription\n# Assisted Matthew Jones with his SNOW ticket request (*INC0444602/RITM0171602*). Needed help with creating a workspace to share his KPI Scorecard with his team", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 380}}
{"issue_key": "CSCI-217", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Jul/25 3:02 PM", "updated": "01/Aug/25 9:16 AM", "labels": [], "summary": "Getting Alan Yuen CSCI access pack", "description": "Getting Alan Yuen access to CW SC resources and board.", "acceptance_criteria": "Getting Alan Yuen access to \n\n* -Board-\n* Mergeco folder\n\n* -Daily standup meetings-", "comments": "", "text": "Summary\nGetting Alan Yuen CSCI access pack\n\n---\n\nDescription\nGetting Alan Yuen access to CW SC resources and board.\n\n---\n\nAcceptance Criteria\nGetting Alan Yuen access to \n\n* -Board-\n* Mergeco folder\n\n* -Daily standup meetings-", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 381}}
{"issue_key": "CSCI-216", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Jul/25 10:55 AM", "updated": "01/Aug/25 5:48 PM", "labels": [], "summary": "meeting to book - Enable SSO login to snowflake", "description": "Consdering the options between \n\n* MS Entra ID\n* Okta\n\nCK to have a follow-up call with Brent/ Cloud to firm up the plan for SSO implementation\n\n* -book next tuesday to follow up-", "acceptance_criteria": "* -booking made for tuesday meeting-\n* To follow-up from the results of the call on Tuesday", "comments": "", "text": "Summary\nmeeting to book - Enable SSO login to snowflake\n\n---\n\nDescription\nConsdering the options between \n\n* MS Entra ID\n* Okta\n\nCK to have a follow-up call with Brent/ Cloud to firm up the plan for SSO implementation\n\n* -book next tuesday to follow up-\n\n---\n\nAcceptance Criteria\n* -booking made for tuesday meeting-\n* To follow-up from the results of the call on Tuesday", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 382}}
{"issue_key": "CSCI-215", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Jul/25 10:49 AM", "updated": "15/Aug/25 3:53 PM", "labels": [], "summary": "Phil to get access ServiceNow via AD", "description": "", "acceptance_criteria": "* SErviceNow access granted for @user @user and @user", "comments": "@user FYI - i’ll get access from you soon.\n\nHave spoken to mark Barton - would be best to get me Azure AD access that gives me the whole package - ticket changed in servicenow @user @user\n\nwill now follow up with Mikhail\n\nin lieu of @user tickets I will have parity with Service now tickets in JIRA", "text": "Summary\nPhil to get access ServiceNow via AD\n\n---\n\nAcceptance Criteria\n* SErviceNow access granted for @user @user and @user\n\n---\n\nComments\n@user FYI - i’ll get access from you soon.\n\nHave spoken to mark Barton - would be best to get me Azure AD access that gives me the whole package - ticket changed in servicenow @user @user\n\nwill now follow up with Mikhail\n\nin lieu of @user tickets I will have parity with Service now tickets in JIRA", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 383}}
{"issue_key": "CSCI-214", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Jul/25 9:59 AM", "updated": "09/Oct/25 8:39 AM", "labels": [], "summary": "Review of existing dbt transformation from Silver to Gold", "description": "", "acceptance_criteria": "", "comments": "@user your account in Azure Devops is already changed to Basic:\n\nPlease confirm if you can access the repo at: [edp_supplychain_dbt - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp_supplychain_dbt?version=GBmaster]\n\nplaced this under [https://sigmahealthcare.atlassian.net/browse/CSCI-160|https://sigmahealthcare.atlassian.net/browse/CSCI-160]", "text": "Summary\nReview of existing dbt transformation from Silver to Gold\n\n---\n\nComments\n@user your account in Azure Devops is already changed to Basic:\n\nPlease confirm if you can access the repo at: [edp_supplychain_dbt - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp_supplychain_dbt?version=GBmaster]\n\nplaced this under [https://sigmahealthcare.atlassian.net/browse/CSCI-160|https://sigmahealthcare.atlassian.net/browse/CSCI-160]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 384}}
{"issue_key": "CSCI-212", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Jul/25 7:40 AM", "updated": "18/Nov/25 5:18 PM", "labels": [], "summary": "Microsoft Engagement Planning Session Agenda for Nagaraj", "description": "Nagaraj is the MS Solutions architect assigned to the CW/Sigma account.\n\nNagaraj has requested we provide a list of topics to cover in the PBI best practice sessions being held in August.\n\nNeed to create a session plan.", "acceptance_criteria": "", "comments": "", "text": "Summary\nMicrosoft Engagement Planning Session Agenda for Nagaraj\n\n---\n\nDescription\nNagaraj is the MS Solutions architect assigned to the CW/Sigma account.\n\nNagaraj has requested we provide a list of topics to cover in the PBI best practice sessions being held in August.\n\nNeed to create a session plan.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 385}}
{"issue_key": "CSCI-211", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Jul/25 7:32 AM", "updated": "12/Sep/25 6:17 PM", "labels": [], "summary": "Analysis of Existing EDP Project Transformations into STTM", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nAnalysis of Existing EDP Project Transformations into STTM", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 386}}
{"issue_key": "CSCI-210", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Jul/25 7:22 AM", "updated": "09/Oct/25 8:39 AM", "labels": [], "summary": "Source-To-Target-Mapping (STTM) Document Template", "description": "Template for Fact and DIM Source-To-Target-Mapping documentation.\n\n*Document Link:* [https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&action=default&mobileredirect=true] \n\n*Control Table Template*\n\n*Fact Table Template*", "acceptance_criteria": "", "comments": "", "text": "Summary\nSource-To-Target-Mapping (STTM) Document Template\n\n---\n\nDescription\nTemplate for Fact and DIM Source-To-Target-Mapping documentation.\n\n*Document Link:* [https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&action=default&mobileredirect=true] \n\n*Control Table Template*\n\n*Fact Table Template*", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 387}}
{"issue_key": "CSCI-208", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Jul/25 7:16 AM", "updated": "01/Sep/25 2:30 AM", "labels": [], "summary": "ExpressRoute Monitoring (18 Aug - 1 Sept)", "description": "Monitor ExpressRoute throughput utilisation", "acceptance_criteria": "# *Metric Visibility:*\n#* ExpressRoute connection metrics (e.g., Ingress/Egress throughput) are visible in Azure Monitor or Network Insights.\n#* Bandwidth usage can be correlated with ADF pipeline execution timelines.\n# *Alerting Rules:*\n#* Alerts are configured to trigger when bandwidth usage exceeds a defined threshold (e.g., 70% of provisioned bandwidth).\n#* Alerts include pipeline metadata (e.g., pipeline name, run ID) when possible, to trace high-usage executions.\n# *Log Collection:*\n#* Network bandwidth logs (e.g., from Network Performance Monitor or Azure Network Watcher) are retained for a minimum of 30 days.\n#* ADF pipeline run logs include integration runtime IPs or endpoints for cross-reference.\n# *Validation Testing:*\n#* At least one high-throughput ADF pipeline is executed to validate that bandwidth spikes are captured and logged as expected.\n#* Test scenarios simulate concurrent pipeline runs to verify system responsiveness and alert thresholds.\n# *Governance & Review:*\n#* A monthly review is conducted to assess if bandwidth consumption patterns require scaling up/down of ExpressRoute or ADF optimization.\n#* A weekly review to be initially done on until Oct 2025.", "comments": "What is required in Sprint 5 - Aug 04 to Aug 15 for this ticket\n\nWe just need to baseline current usage, which is already available in the Azure Dashboard. Currently, we are passing thru AXON. \n\nWe cannot establish yet our bandwidth requirements, but DBAs can estimate max daily data volume from the past 90 days.\n\n@user I have truncated/ removed delta load the following tables so we can see how the ExpressRoute cope with a larger load (~ 550K - 2.1 M) in the next run\n\n* SCAX2012.INVENTJOURNALTRANS ~ 1 M\n* SCAX2012.INVENTJOURNALTABLE ~ 1.1 M\n* SCAX2012.INVENTSUM ~ 600 K\n* SCAX2012.PURCHLINE ~ 2.1 M\n\n @user\n\nIssue split into:\n|CSCI-380|ExpressRoute Monitoring - 1 Sep - 15th Sep|", "text": "Summary\nExpressRoute Monitoring (18 Aug - 1 Sept)\n\n---\n\nDescription\nMonitor ExpressRoute throughput utilisation\n\n---\n\nAcceptance Criteria\n# *Metric Visibility:*\n#* ExpressRoute connection metrics (e.g., Ingress/Egress throughput) are visible in Azure Monitor or Network Insights.\n#* Bandwidth usage can be correlated with ADF pipeline execution timelines.\n# *Alerting Rules:*\n#* Alerts are configured to trigger when bandwidth usage exceeds a defined threshold (e.g., 70% of provisioned bandwidth).\n#* Alerts include pipeline metadata (e.g., pipeline name, run ID) when possible, to trace high-usage executions.\n# *Log Collection:*\n#* Network bandwidth logs (e.g., from Network Performance Monitor or Azure Network Watcher) are retained for a minimum of 30 days.\n#* ADF pipeline run logs include integration runtime IPs or endpoints for cross-reference.\n# *Validation Testing:*\n#* At least one high-throughput ADF pipeline is executed to validate that bandwidth spikes are captured and logged as expected.\n#* Test scenarios simulate concurrent pipeline runs to verify system responsiveness and alert thresholds.\n# *Governance & Review:*\n#* A monthly review is conducted to assess if bandwidth consumption patterns require scaling up/down of ExpressRoute or ADF optimization.\n#* A weekly review to be initially done on until Oct 2025.\n\n---\n\nComments\nWhat is required in Sprint 5 - Aug 04 to Aug 15 for this ticket\n\nWe just need to baseline current usage, which is already available in the Azure Dashboard. Currently, we are passing thru AXON. \n\nWe cannot establish yet our bandwidth requirements, but DBAs can estimate max daily data volume from the past 90 days.\n\n@user I have truncated/ removed delta load the following tables so we can see how the ExpressRoute cope with a larger load (~ 550K - 2.1 M) in the next run\n\n* SCAX2012.INVENTJOURNALTRANS ~ 1 M\n* SCAX2012.INVENTJOURNALTABLE ~ 1.1 M\n* SCAX2012.INVENTSUM ~ 600 K\n* SCAX2012.PURCHLINE ~ 2.1 M\n\n @user\n\nIssue split into:\n|CSCI-380|ExpressRoute Monitoring - 1 Sep - 15th Sep|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 388}}
{"issue_key": "CSCI-207", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Jul/25 7:08 AM", "updated": "15/Aug/25 4:39 PM", "labels": [], "summary": "Review of CI/CD Pipeline", "description": "Review output of [https://sigmahealthcare.atlassian.net/browse/CSCI-151|https://sigmahealthcare.atlassian.net/browse/CSCI-151] and [https://sigmahealthcare.atlassian.net/browse/CSCI-159|https://sigmahealthcare.atlassian.net/browse/CSCI-159]", "acceptance_criteria": "", "comments": "Stretch Sprint 5 goal\n\nI have reviewed the CD pipeline and am satisfied with the following:\n\n* *Branching strategy:* Use a feature branch, merge to {{master}}, then deploy to each environment upon approval.\n* *Deployment monitoring:* Check DevOps Releases for deployment status and logs.\n* *Script execution:* {{Config_meta.sql}} will be sequenced and run within {{deployment_master_scripts}}.\n* *Versioning:* The current mechanism functions but remains limited. We still require a defined process for version naming, review and auto-validation.\n\n*Next step:* Chloe to define the desired version control process with detailed requirements.", "text": "Summary\nReview of CI/CD Pipeline\n\n---\n\nDescription\nReview output of [https://sigmahealthcare.atlassian.net/browse/CSCI-151|https://sigmahealthcare.atlassian.net/browse/CSCI-151] and [https://sigmahealthcare.atlassian.net/browse/CSCI-159|https://sigmahealthcare.atlassian.net/browse/CSCI-159]\n\n---\n\nComments\nStretch Sprint 5 goal\n\nI have reviewed the CD pipeline and am satisfied with the following:\n\n* *Branching strategy:* Use a feature branch, merge to {{master}}, then deploy to each environment upon approval.\n* *Deployment monitoring:* Check DevOps Releases for deployment status and logs.\n* *Script execution:* {{Config_meta.sql}} will be sequenced and run within {{deployment_master_scripts}}.\n* *Versioning:* The current mechanism functions but remains limited. We still require a defined process for version naming, review and auto-validation.\n\n*Next step:* Chloe to define the desired version control process with detailed requirements.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 389}}
{"issue_key": "CSCI-206", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Jul/25 2:21 PM", "updated": "21/Aug/25 3:32 PM", "labels": [], "summary": "Snowpark PBI API Environment Setup", "description": "Configure Snowpark to allow PBI APIs to be queried. Environment can then be used to productionize existing PBI Metadata reports for the SC team.", "acceptance_criteria": "* -Successfully pull PBI metadata via API call in Snowpark-\n* -Be able to save this output as a table in Snowflake-", "comments": "@userto approach this some time next week\n\n@user - are you able to accept the use of Anaconda 3rd party packages for Snowpark?\n\ncan do @user. I will move to this as soon as I clear out my Snowflake ci/cd and SSO reviews. ETA ~Wed/Thurs.\n\nHi @user the *Anaconda Python packages* feature has been enabled in Snowflake, please check on your side. Cheers\n\nTested Snowpark query - we are now able to run queries within Snowpark (no longer receiving previous error).\n\nHowever, am still unable to run a PBI API query (see below):\n\n{code:python}import msal ## MS authentication package\n\n## configure tenant and service principal details (note IDs replaced with false variables as they're sensitive info)\ntenant_id = 'abc' ## CW Tenancy ID\nclient_id = 'abc' ## Service Princiapl ID\nclient_secret = 'abc' ## Service Princial 'Secret'\n\n## set MS authentication URLs\nauthority_url = f\"https://login.microsoftonline.com/{tenant_id}/\"\nscope_url = list()\nscope_url.append('https://analysis.windows.net/powerbi/api/.default')\n\n## configure credentials to MS and retrive access token\napp = msal.ConfidentialClientApplication(\n client_id,client_credential=client_secret,\n authority=authority_url\n)\n\nresult = app.acquire_token_for_client(scopes=scope_url)\n\ntoken = result.get('access_token')\n\nprint(token){code}\n\nError received appears to be a networking issue:\n\nDiscussing with Chloe to identify who is best contact to resolve (Eugene or CK)\n\nhi @user either Eugene or CK will be able to provide you with those details.\n\nspoke wiht CK and reconfigured network rules.\n\nCK/Chloe helped resolve the network issues.\nNow encountering a syntax error when trying to create a python UDF. Need to break code down step-by-step to understand what the syntax error relates to.\n\nResolve syntax issues - was trying to orignally test connection to the APIs via a Snowsight Python Worksheet - have instead move code to a Python Notebook and now syntax errors have resolved.\n\nStill unable to connect to the PBI API as the network rules created by Chloe has not been shared with my RBAC role - Chloe to action.\n\nAble to successfully query 4 separate APIs:\n\n* MSAL client authentication\n* PBI Semantic Model Refreshes (GET)\n* PBI Semantic Model Data Sources\n* PBI Semantic Model Refresh (POST)\n\nAble to successfully save test data to a SF Table in the Sandbox Schema.", "text": "Summary\nSnowpark PBI API Environment Setup\n\n---\n\nDescription\nConfigure Snowpark to allow PBI APIs to be queried. Environment can then be used to productionize existing PBI Metadata reports for the SC team.\n\n---\n\nAcceptance Criteria\n* -Successfully pull PBI metadata via API call in Snowpark-\n* -Be able to save this output as a table in Snowflake-\n\n---\n\nComments\n@userto approach this some time next week\n\n@user - are you able to accept the use of Anaconda 3rd party packages for Snowpark?\n\ncan do @user. I will move to this as soon as I clear out my Snowflake ci/cd and SSO reviews. ETA ~Wed/Thurs.\n\nHi @user the *Anaconda Python packages* feature has been enabled in Snowflake, please check on your side. Cheers\n\nTested Snowpark query - we are now able to run queries within Snowpark (no longer receiving previous error).\n\nHowever, am still unable to run a PBI API query (see below):\n\n{code:python}import msal ## MS authentication package\n\n## configure tenant and service principal details (note IDs replaced with false variables as they're sensitive info)\ntenant_id = 'abc' ## CW Tenancy ID\nclient_id = 'abc' ## Service Princiapl ID\nclient_secret = 'abc' ## Service Princial 'Secret'\n\n## set MS authentication URLs\nauthority_url = f\"https://login.microsoftonline.com/{tenant_id}/\"\nscope_url = list()\nscope_url.append('https://analysis.windows.net/powerbi/api/.default')\n\n## configure credentials to MS and retrive access token\napp = msal.ConfidentialClientApplication(\n client_id,client_credential=client_secret,\n authority=authority_url\n)\n\nresult = app.acquire_token_for_client(scopes=scope_url)\n\ntoken = result.get('access_token')\n\nprint(token){code}\n\nError received appears to be a networking issue:\n\nDiscussing with Chloe to identify who is best contact to resolve (Eugene or CK)\n\nhi @user either Eugene or CK will be able to provide you with those details.\n\nspoke wiht CK and reconfigured network rules.\n\nCK/Chloe helped resolve the network issues.\nNow encountering a syntax error when trying to create a python UDF. Need to break code down step-by-step to understand what the syntax error relates to.\n\nResolve syntax issues - was trying to orignally test connection to the APIs via a Snowsight Python Worksheet - have instead move code to a Python Notebook and now syntax errors have resolved.\n\nStill unable to connect to the PBI API as the network rules created by Chloe has not been shared with my RBAC role - Chloe to action.\n\nAble to successfully query 4 separate APIs:\n\n* MSAL client authentication\n* PBI Semantic Model Refreshes (GET)\n* PBI Semantic Model Data Sources\n* PBI Semantic Model Refresh (POST)\n\nAble to successfully save test data to a SF Table in the Sandbox Schema.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 390}}
{"issue_key": "CSCI-205", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Jul/25 2:21 PM", "updated": "07/Aug/25 6:45 AM", "labels": [], "summary": "PBI to Snowflake VNet Gateway Connection POC", "description": "POC to document and test how users will be able to connect to our VNet gateway to connect to SF from PBI.\n\nNeed to test:\n\n* How to authenticate\n* How users will need to be granted access\n* At what level (in SF) do we grant permissions for access (do we do this at a DB level or tenancy level? etc)", "acceptance_criteria": "* -To test and cover all requirements.-\n* -Have all requirements documented (in Confluence/Sharepoint).-", "comments": "Currently unable to connect via the Privatelink URL (cw-au.privatelink.snowflakecomputing.com) via PBI desktop - have asked Eugene to look into this\n\nh2. How to Authenticate\n\nDevelopers will start in PBI Desktop/Tabular Editor. To begin development, the user will select the Snowflake connector type and enter in their credentials:\n\n* Server: cw-au.privatelink.snowflakecomputing.com\n* Warehouse: <spoke’s dedicated PBI warehouse>\n* Login details: SSO with AAD\n\nFrom here a developer can select the table/s they wish to pull from Snowflake. As they’ve signed in as themselves they will view all tables that they have access to in Snowflake.\n\nOnce selected, the developer will choose ‘import mode’ for their data (we will discourage the use of DirectQuery).\n\nFrom here they will develop and build their dashboard. Once completed they will publish to their Workspace in PBI Service.\n\nOnce published, the user will go to their semantic model settings. They will need to navigate to ‘Gateway and cloud connections’, select +On+ for ‘Use an On-premises or VNet data gateway’. Locate to the VNet gateway and map the Snowflake data source to their relevant connection name (their connection will be to a Service Account, based on the spoke’s dedicated warehouse and roles granted). After selected, hit ‘apply’ and the semantic model is ready to be deployed and refreshed.\n\nh2. What developers need access to\n\nh3. Snowflake\n\n* Snowflake login (setup with AAD SSO)\n* Their spoke’s environment in SF\n** Configured at the role level\n*** Each role will have a dedicated warehouse to be used for PBI reports and refreshes\n*** The roles grant access to the relevant DB’s/schema’s/table’s in SF\n*** Each role will have a warehouse where users can query SF for analytics and sourcing data\n\nh3. Power BI\n\n* Power BI Desktop\n* Tabular Editor Pro (if applicable for super users)\n* PBI VNet Gateway connection created (for each spoke) at a service account level\n** Access distinguished by the role and warehouse for that spoke\n* PBI VNet Gateway connection shared (user permissions)\n** This should be configured at an AD Group level\n\nEugene clarified that there are 2 things that are pending that need to happen to resolve this:\n\n* DNS resolution\n* Network team to build firewall settings\n\nOnce completed, users will be able to query SF’s privatelink even in PBI Desktop\n\nDocumentation complete:\n[PBI_POC_VNet_Gateway.docx|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/PBI_POC_VNet_Gateway.docx?d=wcb0e96cdfbd94f6facd2d9a2b81261d6&csf=1&web=1&e=0FaBkg]", "text": "Summary\nPBI to Snowflake VNet Gateway Connection POC\n\n---\n\nDescription\nPOC to document and test how users will be able to connect to our VNet gateway to connect to SF from PBI.\n\nNeed to test:\n\n* How to authenticate\n* How users will need to be granted access\n* At what level (in SF) do we grant permissions for access (do we do this at a DB level or tenancy level? etc)\n\n---\n\nAcceptance Criteria\n* -To test and cover all requirements.-\n* -Have all requirements documented (in Confluence/Sharepoint).-\n\n---\n\nComments\nCurrently unable to connect via the Privatelink URL (cw-au.privatelink.snowflakecomputing.com) via PBI desktop - have asked Eugene to look into this\n\nh2. How to Authenticate\n\nDevelopers will start in PBI Desktop/Tabular Editor. To begin development, the user will select the Snowflake connector type and enter in their credentials:\n\n* Server: cw-au.privatelink.snowflakecomputing.com\n* Warehouse: <spoke’s dedicated PBI warehouse>\n* Login details: SSO with AAD\n\nFrom here a developer can select the table/s they wish to pull from Snowflake. As they’ve signed in as themselves they will view all tables that they have access to in Snowflake.\n\nOnce selected, the developer will choose ‘import mode’ for their data (we will discourage the use of DirectQuery).\n\nFrom here they will develop and build their dashboard. Once completed they will publish to their Workspace in PBI Service.\n\nOnce published, the user will go to their semantic model settings. They will need to navigate to ‘Gateway and cloud connections’, select +On+ for ‘Use an On-premises or VNet data gateway’. Locate to the VNet gateway and map the Snowflake data source to their relevant connection name (their connection will be to a Service Account, based on the spoke’s dedicated warehouse and roles granted). After selected, hit ‘apply’ and the semantic model is ready to be deployed and refreshed.\n\nh2. What developers need access to\n\nh3. Snowflake\n\n* Snowflake login (setup with AAD SSO)\n* Their spoke’s environment in SF\n** Configured at the role level\n*** Each role will have a dedicated warehouse to be used for PBI reports and refreshes\n*** The roles grant access to the relevant DB’s/schema’s/table’s in SF\n*** Each role will have a warehouse where users can query SF for analytics and sourcing data\n\nh3. Power BI\n\n* Power BI Desktop\n* Tabular Editor Pro (if applicable for super users)\n* PBI VNet Gateway connection created (for each spoke) at a service account level\n** Access distinguished by the role and warehouse for that spoke\n* PBI VNet Gateway connection shared (user permissions)\n** This should be configured at an AD Group level\n\nEugene clarified that there are 2 things that are pending that need to happen to resolve this:\n\n* DNS resolution\n* Network team to build firewall settings\n\nOnce completed, users will be able to query SF’s privatelink even in PBI Desktop\n\nDocumentation complete:\n[PBI_POC_VNet_Gateway.docx|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/PBI_POC_VNet_Gateway.docx?d=wcb0e96cdfbd94f6facd2d9a2b81261d6&csf=1&web=1&e=0FaBkg]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 391}}
{"issue_key": "CSCI-204", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Jul/25 11:23 AM", "updated": "13/Aug/25 9:16 PM", "labels": [], "summary": "VNET Gateway testing support", "description": "Harrison to validate any requirements for Eugene to enable Jess C", "acceptance_criteria": "", "comments": "Next Eugene Activites\n\n* VNET Gateway testing\n* Snowflake Privatelink/network policy config\n* ExpressRoute - validate in standup what is required - likely monitoring\n* CI/CD pipeline - done but waiting for review and test from Chloe\n\nJust waiting for @user if support is required, I believe access is already provided.\n\nJess pls advise if you want to setup a call to go through the config.\n\n@user \n\nHi Eugene 🙂 \n\nFYI been testing connecting PBI to Snowflake today, so far so good with everything regarding the VNet gateway you configured 🙂 Thanks for all your help.\n\nOne thing I noticed is within PBI Desktop, I can’t seem to connect to our Snowflake server with the privatelink URL (cw-au.*privatelink*.snowflakecomputing.com). I have no issues in PBI Service.\n\nAre you able to enable the use of the privatelink in the PBI Desktop app? Would we need to be able to do the same in tabular editor also?\n\nLet me know, thanks! 🙂\n\n@user are you connecting directly from PowerBI? Or are you using a Semantic Model from PowerBI?\n\n@user connecting directly from PBI desktop.\n\nAs discussed on our call just now, this question is now resolved 🙂 we are awaiting for the DNS resolution to be completed, plus for firewalls to be configured with the network team before this is able to be done.", "text": "Summary\nVNET Gateway testing support\n\n---\n\nDescription\nHarrison to validate any requirements for Eugene to enable Jess C\n\n---\n\nComments\nNext Eugene Activites\n\n* VNET Gateway testing\n* Snowflake Privatelink/network policy config\n* ExpressRoute - validate in standup what is required - likely monitoring\n* CI/CD pipeline - done but waiting for review and test from Chloe\n\nJust waiting for @user if support is required, I believe access is already provided.\n\nJess pls advise if you want to setup a call to go through the config.\n\n@user \n\nHi Eugene 🙂 \n\nFYI been testing connecting PBI to Snowflake today, so far so good with everything regarding the VNet gateway you configured 🙂 Thanks for all your help.\n\nOne thing I noticed is within PBI Desktop, I can’t seem to connect to our Snowflake server with the privatelink URL (cw-au.*privatelink*.snowflakecomputing.com). I have no issues in PBI Service.\n\nAre you able to enable the use of the privatelink in the PBI Desktop app? Would we need to be able to do the same in tabular editor also?\n\nLet me know, thanks! 🙂\n\n@user are you connecting directly from PowerBI? Or are you using a Semantic Model from PowerBI?\n\n@user connecting directly from PBI desktop.\n\nAs discussed on our call just now, this question is now resolved 🙂 we are awaiting for the DNS resolution to be completed, plus for firewalls to be configured with the network team before this is able to be done.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 392}}
{"issue_key": "CSCI-203", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Jul/25 11:17 AM", "updated": "14/Aug/25 11:37 AM", "labels": [], "summary": "High Level Solution Design Walkthrough for Internal Data Engineering", "description": "Walkthrough with Alan Y", "acceptance_criteria": "Create high level architecture document and walk through \n\n* Snowflake and ADF design/ setup\n* Framework setup\n* Environment walkthrough\n* Eugene Intro", "comments": "* -Created High Level Logical Architecture and sent to the wider team-\n* -Walk-through and discussion with Alan Y. on the solution design-\n* -Updated the Snowflake design based on Alan’s suggestion-\n\n \n\n @user @user\n\nI am working on documenting ADF logical architecture and pipeline design.", "text": "Summary\nHigh Level Solution Design Walkthrough for Internal Data Engineering\n\n---\n\nDescription\nWalkthrough with Alan Y\n\n---\n\nAcceptance Criteria\nCreate high level architecture document and walk through \n\n* Snowflake and ADF design/ setup\n* Framework setup\n* Environment walkthrough\n* Eugene Intro\n\n---\n\nComments\n* -Created High Level Logical Architecture and sent to the wider team-\n* -Walk-through and discussion with Alan Y. on the solution design-\n* -Updated the Snowflake design based on Alan’s suggestion-\n\n \n\n @user @user\n\nI am working on documenting ADF logical architecture and pipeline design.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 393}}
{"issue_key": "CSCI-202", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Jul/25 11:05 AM", "updated": "15/Aug/25 3:45 PM", "labels": [], "summary": "SQL3 (PDB14) Delta Load", "description": "", "acceptance_criteria": "Sorry I’ll need help with filling out the AC here. (from Phil)\n\nas per [https://sigmahealthcare.atlassian.net/browse/CSCI-134|https://sigmahealthcare.atlassian.net/browse/CSCI-134]\n\nWe need to set up pipelines for the delta loads for the 5 tables.\n\n|BranchOrderItems|\n|BranchOrders|\n|Store_IncrementalSOHChanges|\n|Transactions|\n|Transactions_Invoice|", "comments": "4 tables that need to be moved to snowflake\n- will move smallest first\n\nWorking on the patterns identification and performance with one table. Once successful we will build the pipeline for all tables.\n\nLoading delta for transactions table. Identifying the patterns and testing the delta load. \n\nThere are no valid date columns that can be used. We are using transactions_invoce to get data.", "text": "Summary\nSQL3 (PDB14) Delta Load\n\n---\n\nAcceptance Criteria\nSorry I’ll need help with filling out the AC here. (from Phil)\n\nas per [https://sigmahealthcare.atlassian.net/browse/CSCI-134|https://sigmahealthcare.atlassian.net/browse/CSCI-134]\n\nWe need to set up pipelines for the delta loads for the 5 tables.\n\n|BranchOrderItems|\n|BranchOrders|\n|Store_IncrementalSOHChanges|\n|Transactions|\n|Transactions_Invoice|\n\n---\n\nComments\n4 tables that need to be moved to snowflake\n- will move smallest first\n\nWorking on the patterns identification and performance with one table. Once successful we will build the pipeline for all tables.\n\nLoading delta for transactions table. Identifying the patterns and testing the delta load. \n\nThere are no valid date columns that can be used. We are using transactions_invoce to get data.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 394}}
{"issue_key": "CSCI-201", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Jul/25 11:02 AM", "updated": "14/Aug/25 9:49 AM", "labels": [], "summary": "ADF and Snowflake Implementation Walkthrough From Chloe (Ashutosh)", "description": "* Walkthrough of Integration and Transformation Framework\n* Walkthrough of Development standards", "acceptance_criteria": "2 new resources able to implement integration and transformation", "comments": "will reach out to @user", "text": "Summary\nADF and Snowflake Implementation Walkthrough From Chloe (Ashutosh)\n\n---\n\nDescription\n* Walkthrough of Integration and Transformation Framework\n* Walkthrough of Development standards\n\n---\n\nAcceptance Criteria\n2 new resources able to implement integration and transformation\n\n---\n\nComments\nwill reach out to @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 395}}
{"issue_key": "CSCI-200", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Jul/25 11:01 AM", "updated": "08/Aug/25 9:56 AM", "labels": [], "summary": "ADF and Snowflake Implementation Walkthrough From Chloe (Aswini)", "description": "* Walkthrough of Integration and Transformation Framework\n* Walkthrough of Development standards", "acceptance_criteria": "2 new resources able to implement integration and transformation", "comments": "assgined to @user", "text": "Summary\nADF and Snowflake Implementation Walkthrough From Chloe (Aswini)\n\n---\n\nDescription\n* Walkthrough of Integration and Transformation Framework\n* Walkthrough of Development standards\n\n---\n\nAcceptance Criteria\n2 new resources able to implement integration and transformation\n\n---\n\nComments\nassgined to @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 396}}
{"issue_key": "CSCI-199", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "29/Jul/25 10:53 AM", "updated": "05/Aug/25 10:05 AM", "labels": [], "summary": "Team Access to SF via PrivateLink Setup", "description": "Discussions to include details as per below \n\n{quote}Scaffold of details as per below @user \n\n* *VPN Usage*\n** Clarify whether CW currently uses a VPN for Snowflake access.\n** Perhaps Determine if SIGMA uses a different VPN or the same infrastructure.\n* *Communication Pathways*\n** Document the communication flow and access methods for both SIGMA and CW when connecting to Snowflake.\n** Identify any differences in network architecture or security protocols.\n* *Integration Strategy*\n** Explore options for combining or aligning access methods between SIGMA and CW.\n** Evaluate the feasibility of a unified PrivateLink setup or maintaining separate configurations.{quote}\n\n*Meeting Coordination*\n\n* -A meeting has been scheduled for *10 am tomorrow* to discuss this topic in detail (as noted by Phillip Yuen).-", "acceptance_criteria": "* A clear decision is made on how to organize team access to Snowflake via PrivateLink.\n* Documentation of the current and proposed access methods for both SIGMA and CW.\n* Identification of any blockers or dependencies.\n* Agreement on next steps and responsible parties.", "comments": "@user please get across this to drive\n\nCaught up with @user\nMeeting booked specifically regarding this conversation at 10 am tomorrow.\n\n* @user to provide plan/details on how to approach this \n* @user will send details to CK @user to relay to cloud team. (Brent)\n* @user to get access to servicenow with @user and FRank\n\n@user to reach otu to @user\n\nSee doc at: [Developer Access - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5145/Developer-Access]\n\nPls review.\n\ncc: @user @user @user @user\n\n@user this card has been split to [https://sigmahealthcare.atlassian.net/browse/CSCI-225|https://sigmahealthcare.atlassian.net/browse/CSCI-225] as a review card.\n\n@user thanks Eugene! this looks great 🙂 \n\nFor the Azure VM’s as a backup - can you clarify in what kind of scenarios this would be required? How would this differ to using jumpbox?\n\nAlso for the non-CW domain users (eg: Harrison and Phillip) - in this architecture can they access CW data? Do they have access to using jumpbox etc? Maybe something else to consider 🙂 \n\nCheers\n\n@user the Azure VM is going to be used as a fallback when all the other options are not accessible, e.g. ExpressRoute is down, VPN Access not available, Jumpboxes not available.\n\nFor non-CW domain users, they may need CW accounts if there is not account federation yet.\n\ncc: @user\n\n@user Great thanks for clarifying 🙂", "text": "Summary\nTeam Access to SF via PrivateLink Setup\n\n---\n\nDescription\nDiscussions to include details as per below \n\n{quote}Scaffold of details as per below @user \n\n* *VPN Usage*\n** Clarify whether CW currently uses a VPN for Snowflake access.\n** Perhaps Determine if SIGMA uses a different VPN or the same infrastructure.\n* *Communication Pathways*\n** Document the communication flow and access methods for both SIGMA and CW when connecting to Snowflake.\n** Identify any differences in network architecture or security protocols.\n* *Integration Strategy*\n** Explore options for combining or aligning access methods between SIGMA and CW.\n** Evaluate the feasibility of a unified PrivateLink setup or maintaining separate configurations.{quote}\n\n*Meeting Coordination*\n\n* -A meeting has been scheduled for *10 am tomorrow* to discuss this topic in detail (as noted by Phillip Yuen).-\n\n---\n\nAcceptance Criteria\n* A clear decision is made on how to organize team access to Snowflake via PrivateLink.\n* Documentation of the current and proposed access methods for both SIGMA and CW.\n* Identification of any blockers or dependencies.\n* Agreement on next steps and responsible parties.\n\n---\n\nComments\n@user please get across this to drive\n\nCaught up with @user\nMeeting booked specifically regarding this conversation at 10 am tomorrow.\n\n* @user to provide plan/details on how to approach this \n* @user will send details to CK @user to relay to cloud team. (Brent)\n* @user to get access to servicenow with @user and FRank\n\n@user to reach otu to @user\n\nSee doc at: [Developer Access - Overview|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_wiki/wikis/Enterprise-Data-Platform-Implementation.wiki/5145/Developer-Access]\n\nPls review.\n\ncc: @user @user @user @user\n\n@user this card has been split to [https://sigmahealthcare.atlassian.net/browse/CSCI-225|https://sigmahealthcare.atlassian.net/browse/CSCI-225] as a review card.\n\n@user thanks Eugene! this looks great 🙂 \n\nFor the Azure VM’s as a backup - can you clarify in what kind of scenarios this would be required? How would this differ to using jumpbox?\n\nAlso for the non-CW domain users (eg: Harrison and Phillip) - in this architecture can they access CW data? Do they have access to using jumpbox etc? Maybe something else to consider 🙂 \n\nCheers\n\n@user the Azure VM is going to be used as a fallback when all the other options are not accessible, e.g. ExpressRoute is down, VPN Access not available, Jumpboxes not available.\n\nFor non-CW domain users, they may need CW accounts if there is not account federation yet.\n\ncc: @user\n\n@user Great thanks for clarifying 🙂", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 397}}
{"issue_key": "CSCI-198", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Jul/25 3:54 PM", "updated": "28/Jul/25 4:57 PM", "labels": [], "summary": "Onboard Adeel, Samer, Bhavya to CSCI JIRA", "description": "User story:\n- as a _____ I want to _____ so that ______\n\nAs PO I want to onboard @user @user @user on to Jira so that they will be able to contribute to project.\n\nDeliverable:\n\n* show basic Jira functions as a video on teams.\n* Create new confluence doc that updated from previous PRoduct devliery life cycle (PDLC)", "acceptance_criteria": "Given:____\nWhen:_____\n\nThen: ______\n\n@user @user @user to be able to:\n\n* understand what’s progressing in Jira board\n* create tickets\n* create story estimates\n* know what sections they are to look out for", "comments": "Testing scope to be negotiated with @user \n\n@user as part of the recording.\n@user \n@user", "text": "Summary\nOnboard Adeel, Samer, Bhavya to CSCI JIRA\n\n---\n\nDescription\nUser story:\n- as a _____ I want to _____ so that ______\n\nAs PO I want to onboard @user @user @user on to Jira so that they will be able to contribute to project.\n\nDeliverable:\n\n* show basic Jira functions as a video on teams.\n* Create new confluence doc that updated from previous PRoduct devliery life cycle (PDLC)\n\n---\n\nAcceptance Criteria\nGiven:____\nWhen:_____\n\nThen: ______\n\n@user @user @user to be able to:\n\n* understand what’s progressing in Jira board\n* create tickets\n* create story estimates\n* know what sections they are to look out for\n\n---\n\nComments\nTesting scope to be negotiated with @user \n\n@user as part of the recording.\n@user \n@user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 398}}
{"issue_key": "CSCI-196", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Jul/25 11:11 AM", "updated": "09/Oct/25 8:39 AM", "labels": [], "summary": "Appriss Test Data Load and DataShare", "description": "* test connectivity in data share env ( week of 11/8)\n* ingest test data from location (when data is ready)", "acceptance_criteria": "* Data ingested from location", "comments": "", "text": "Summary\nAppriss Test Data Load and DataShare\n\n---\n\nDescription\n* test connectivity in data share env ( week of 11/8)\n* ingest test data from location (when data is ready)\n\n---\n\nAcceptance Criteria\n* Data ingested from location", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 399}}
{"issue_key": "CSCI-195", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Jul/25 9:55 AM", "updated": "28/Jul/25 3:50 PM", "labels": [], "summary": "waseem access to JIRA", "description": "waseem access to Jira CSCI", "acceptance_criteria": "waseem access to JIRA", "comments": "", "text": "Summary\nwaseem access to JIRA\n\n---\n\nDescription\nwaseem access to Jira CSCI\n\n---\n\nAcceptance Criteria\nwaseem access to JIRA", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 400}}
{"issue_key": "CSCI-194", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Jul/25 9:54 AM", "updated": "11/Sep/25 9:34 AM", "labels": [], "summary": "Onboarding and statement of work", "description": "Loading of Infocentric SOW into Plexus\n\nOnboarding of Ashwini and Ashutosh into CW enviornment", "acceptance_criteria": "Infocentric SOW completed\n\nOnboarding of Ashwini and Ashutosh completed", "comments": "Infocentric SOW loaded into Plexus and at CIO approval and signature stage, thanks to Frank Perez and Matthew Jones.\n\nOnboarding initiated through Matthew Jones.\n\nThanks,\n\nHarrison\n\nPretty sure this is done @user", "text": "Summary\nOnboarding and statement of work\n\n---\n\nDescription\nLoading of Infocentric SOW into Plexus\n\nOnboarding of Ashwini and Ashutosh into CW enviornment\n\n---\n\nAcceptance Criteria\nInfocentric SOW completed\n\nOnboarding of Ashwini and Ashutosh completed\n\n---\n\nComments\nInfocentric SOW loaded into Plexus and at CIO approval and signature stage, thanks to Frank Perez and Matthew Jones.\n\nOnboarding initiated through Matthew Jones.\n\nThanks,\n\nHarrison\n\nPretty sure this is done @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 401}}
{"issue_key": "CSCI-193", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Jul/25 9:38 AM", "updated": "25/Aug/25 2:31 PM", "labels": [], "summary": "Appriss Requirements Design and Walkthrough", "description": "Requirements and Design Documentation completed in mergeCo Reporting and session setup to take Chloe, Harrison and Ashwini through.\n\nAswini will be responsible for delivering however Chloe should be included also as part if initial setup of Data Platform", "acceptance_criteria": "Alignment between Waseem and team on deliverables for Appriss initiative", "comments": "CK and Waseem took team through the source data and outputs.\n\nMarked as done", "text": "Summary\nAppriss Requirements Design and Walkthrough\n\n---\n\nDescription\nRequirements and Design Documentation completed in mergeCo Reporting and session setup to take Chloe, Harrison and Ashwini through.\n\nAswini will be responsible for delivering however Chloe should be included also as part if initial setup of Data Platform\n\n---\n\nAcceptance Criteria\nAlignment between Waseem and team on deliverables for Appriss initiative\n\n---\n\nComments\nCK and Waseem took team through the source data and outputs.\n\nMarked as done", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 402}}
{"issue_key": "CSCI-192", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Jul/25 8:53 AM", "updated": "12/Sep/25 6:24 PM", "labels": [], "summary": "Data Platform Architecture and Design Documentation and Walkthrough with Internal Team", "description": "Documentation in confluence or other of CW Data Platform Architecture and Design decisions.\n\nWalkthrough for Alan and Shagun to provide feedback and approval", "acceptance_criteria": "Any feedback from internal Data Engineering team considered and incorporated where required", "comments": "Working on the Solution Architecture and Design Document v.01. I have divided the works between Ashu and myself:\n\n* Design Principles\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Add guidelines and principles that shape the solution design\"}],\"attrs\":{\"localId\":\"06223205-7d08-4dde-8da4-75e66813dc81\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"0460c10a-039d-443d-b840-4a951faedfd2\"}}\n{adf}\n* Logical architecture - Interim Solution\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\" Key decision driver, choice of technologies, design flows etc.\"}],\"attrs\":{\"localId\":\"790df446-a05d-42fd-ae2c-a13f696b52b3\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"00a154d5-047b-498b-ae88-a595990aade8\"}}\n{adf}\n* Logical architecture - Future State:\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\" Key decision driver, technologies, design flows etc.\"}],\"attrs\":{\"localId\":\"fa2e2ef1-98a8-42e3-b52b-63f22429fc0c\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"7436ecdc-a967-4dc7-935f-ce59a3bd3133\"}}\n{adf}\n* Altis DLA: \n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Detailed explanation and how it works\"}],\"attrs\":{\"localId\":\"3522c235-82db-43ec-9c99-45c41cd3ef5e\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"0d2531ef-cb73-4349-9c23-6e8fc783b5f1\"}}\n{adf}\n* Source systems: \n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Database name/ use-cases\"}],\"attrs\":{\"localId\":\"956df07f-abee-414a-92ee-cb0201ee3d60\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"SQL Servers/ IP addresses \"}],\"attrs\":{\"localId\":\"de5e8100-8dbc-4d4b-adb1-925d1bf6e8d1\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"e2a36481-0b41-4c86-a19c-b42086993884\"}}\n{adf}\n\n @user @user\n\nwill split task to sprint 8", "text": "Summary\nData Platform Architecture and Design Documentation and Walkthrough with Internal Team\n\n---\n\nDescription\nDocumentation in confluence or other of CW Data Platform Architecture and Design decisions.\n\nWalkthrough for Alan and Shagun to provide feedback and approval\n\n---\n\nAcceptance Criteria\nAny feedback from internal Data Engineering team considered and incorporated where required\n\n---\n\nComments\nWorking on the Solution Architecture and Design Document v.01. I have divided the works between Ashu and myself:\n\n* Design Principles\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Add guidelines and principles that shape the solution design\"}],\"attrs\":{\"localId\":\"06223205-7d08-4dde-8da4-75e66813dc81\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"0460c10a-039d-443d-b840-4a951faedfd2\"}}\n{adf}\n* Logical architecture - Interim Solution\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\" Key decision driver, choice of technologies, design flows etc.\"}],\"attrs\":{\"localId\":\"790df446-a05d-42fd-ae2c-a13f696b52b3\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"00a154d5-047b-498b-ae88-a595990aade8\"}}\n{adf}\n* Logical architecture - Future State:\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\" Key decision driver, technologies, design flows etc.\"}],\"attrs\":{\"localId\":\"fa2e2ef1-98a8-42e3-b52b-63f22429fc0c\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"7436ecdc-a967-4dc7-935f-ce59a3bd3133\"}}\n{adf}\n* Altis DLA: \n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Detailed explanation and how it works\"}],\"attrs\":{\"localId\":\"3522c235-82db-43ec-9c99-45c41cd3ef5e\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"0d2531ef-cb73-4349-9c23-6e8fc783b5f1\"}}\n{adf}\n* Source systems: \n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Database name/ use-cases\"}],\"attrs\":{\"localId\":\"956df07f-abee-414a-92ee-cb0201ee3d60\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"SQL Servers/ IP addresses \"}],\"attrs\":{\"localId\":\"de5e8100-8dbc-4d4b-adb1-925d1bf6e8d1\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"e2a36481-0b41-4c86-a19c-b42086993884\"}}\n{adf}\n\n @user @user\n\nwill split task to sprint 8", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 403}}
{"issue_key": "CSCI-191", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "28/Jul/25 8:23 AM", "updated": "18/Aug/25 9:43 AM", "labels": [], "summary": "ADF and Snowflake Implementation Walkthrough for New Resources", "description": "* Walkthrough of Integration and Transformation Framework\n* Walkthrough of Development standards", "acceptance_criteria": "* 2 new resources able to implement integration and transformation", "comments": "Sessions booked for Snowflake and ADF walkthrough\n\nOnboarding tasks for Ashu and Aswini \n\n* -Access in Azure and Snowflake-\n* -Walk through ADF templates and existing integrations-\n* -Walk through Snowflake schemas and high level setup-\n* -Walk through development processes and conventions-\n* -Allocate Jira tasks -\n\n @user @user\n\nNext: \n\n* Deep dive sessions in Altis DLA framework\n* Walk-through sessions for the wider project team\n\nnext steps;|\n\n- @user @user ADF pipeline \n\n* @user to walk thru PR process\n\ntrigger job in dev\n\ncc @user for review (PR)", "text": "Summary\nADF and Snowflake Implementation Walkthrough for New Resources\n\n---\n\nDescription\n* Walkthrough of Integration and Transformation Framework\n* Walkthrough of Development standards\n\n---\n\nAcceptance Criteria\n* 2 new resources able to implement integration and transformation\n\n---\n\nComments\nSessions booked for Snowflake and ADF walkthrough\n\nOnboarding tasks for Ashu and Aswini \n\n* -Access in Azure and Snowflake-\n* -Walk through ADF templates and existing integrations-\n* -Walk through Snowflake schemas and high level setup-\n* -Walk through development processes and conventions-\n* -Allocate Jira tasks -\n\n @user @user\n\nNext: \n\n* Deep dive sessions in Altis DLA framework\n* Walk-through sessions for the wider project team\n\nnext steps;|\n\n- @user @user ADF pipeline \n\n* @user to walk thru PR process\n\ntrigger job in dev\n\ncc @user for review (PR)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 404}}
{"issue_key": "CSCI-190", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Jul/25 2:37 PM", "updated": "01/Aug/25 11:01 AM", "labels": [], "summary": "Create Data Out Transformation Pipeline Template", "description": "Devops: [TASK 198087|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/198087]\n\nBuild a reusable ADF pipeline template for performing data transformations on staged data before loading it into curated Snowflake tables. The pipeline should be parameterized, support incremental and full loads, and include basic logging and error handling.", "acceptance_criteria": "* Pipeline template accepts dynamic inputs (e.g., table name, load type).\n\n* Supports transformation logic using SQL or stored procedures.", "comments": "Data Out template tested successful on Parallel load for StockDB.BranchHierarchyInfo_VW.\n\nNext step: \n\nTo test this scenario:\n\n* Sequential load/ Full Apply/ Deleted records from source\n\nData Out template tested successful on both Sequential and Parallel load. @user @userTest table created in STG_TRNS schema, using FULL APPLY load type.", "text": "Summary\nCreate Data Out Transformation Pipeline Template\n\n---\n\nDescription\nDevops: [TASK 198087|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/198087]\n\nBuild a reusable ADF pipeline template for performing data transformations on staged data before loading it into curated Snowflake tables. The pipeline should be parameterized, support incremental and full loads, and include basic logging and error handling.\n\n---\n\nAcceptance Criteria\n* Pipeline template accepts dynamic inputs (e.g., table name, load type).\n\n* Supports transformation logic using SQL or stored procedures.\n\n---\n\nComments\nData Out template tested successful on Parallel load for StockDB.BranchHierarchyInfo_VW.\n\nNext step: \n\nTo test this scenario:\n\n* Sequential load/ Full Apply/ Deleted records from source\n\nData Out template tested successful on both Sequential and Parallel load. @user @userTest table created in STG_TRNS schema, using FULL APPLY load type.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 405}}
{"issue_key": "CSCI-186", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Jul/25 11:19 AM", "updated": "01/Aug/25 4:13 PM", "labels": [], "summary": "Adeel - Document Archive Policy", "description": "Archiving policy is updated [here|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/03.%20Test%20and%20Deploy/Archiving%20Criteria.xlsx?d=w582498ed19ab4a92b3702a0c2d5cceb7&csf=1&web=1&e=RPM4tt ]\n_________________________________________\n\nFollowing recent discussions on the DEV/PROD database environment alignment, Chloe and Adeel will collaborate on the below\n\n# *Documenting the Archive Policy*\n#* Define retention periods\n#* Specify archiving criteria (e.g., based on date or status columns)\n#* Include any additional relevant details", "acceptance_criteria": "* -Archive policy document is created and shared with relevant stakeholders-\n* -Policy includes clear retention periods and archiving criteria-", "comments": "Phil to drive activity, assistance required for Adeel.\n\n@user\n\nI have caught up with @user \n\nArchiving policy is as per attached [link|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/03.%20Test%20and%20Deploy/Archiving%20Criteria.xlsx?d=w582498ed19ab4a92b3702a0c2d5cceb7&csf=1&web=1&e=RPM4tt]\n\nWill update description\n\n@user archiving policy now present - ready for you to review.\n\nDeep link to sheet here - [Archiving Criteria.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/03.%20Test%20and%20Deploy/Archiving%20Criteria.xlsx?d=w582498ed19ab4a92b3702a0c2d5cceb7&csf=1&web=1&e=TTktBA&nav=MTVfezk3OThEODNGLUQ5NzctNEMzMC05RUIxLTE3MTBCQUI2NkE4Nn0]\n\nAdeel carry over from this ticket to be transitioned to Sprint 5 @user plus assign to Adeel when done\n\n@user Review of Archival policy by Chloe and go back to Adeel to incorporate.\n\nPlan to be put in place as part of review\n\nI’ve reviewed Adeel analysis on the current retention policy. To revisit this with ticket CSCI 221", "text": "Summary\nAdeel - Document Archive Policy\n\n---\n\nDescription\nArchiving policy is updated [here|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/03.%20Test%20and%20Deploy/Archiving%20Criteria.xlsx?d=w582498ed19ab4a92b3702a0c2d5cceb7&csf=1&web=1&e=RPM4tt ]\n_________________________________________\n\nFollowing recent discussions on the DEV/PROD database environment alignment, Chloe and Adeel will collaborate on the below\n\n# *Documenting the Archive Policy*\n#* Define retention periods\n#* Specify archiving criteria (e.g., based on date or status columns)\n#* Include any additional relevant details\n\n---\n\nAcceptance Criteria\n* -Archive policy document is created and shared with relevant stakeholders-\n* -Policy includes clear retention periods and archiving criteria-\n\n---\n\nComments\nPhil to drive activity, assistance required for Adeel.\n\n@user\n\nI have caught up with @user \n\nArchiving policy is as per attached [link|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/03.%20Test%20and%20Deploy/Archiving%20Criteria.xlsx?d=w582498ed19ab4a92b3702a0c2d5cceb7&csf=1&web=1&e=RPM4tt]\n\nWill update description\n\n@user archiving policy now present - ready for you to review.\n\nDeep link to sheet here - [Archiving Criteria.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/03.%20Test%20and%20Deploy/Archiving%20Criteria.xlsx?d=w582498ed19ab4a92b3702a0c2d5cceb7&csf=1&web=1&e=TTktBA&nav=MTVfezk3OThEODNGLUQ5NzctNEMzMC05RUIxLTE3MTBCQUI2NkE4Nn0]\n\nAdeel carry over from this ticket to be transitioned to Sprint 5 @user plus assign to Adeel when done\n\n@user Review of Archival policy by Chloe and go back to Adeel to incorporate.\n\nPlan to be put in place as part of review\n\nI’ve reviewed Adeel analysis on the current retention policy. To revisit this with ticket CSCI 221", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 406}}
{"issue_key": "CSCI-185", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Jul/25 10:10 AM", "updated": "04/Aug/25 6:03 AM", "labels": [], "summary": "Adeel - On-prem DEV/PROD Database Environment Sync", "description": "Following a review of the on-prem DEV and PROD environments for key databases (StockDB, SCAX2012, ILS, TransactionStorage), discrepancies were identified in schema alignment and data volume. Adeel Syed provided a detailed comparison, highlighting schema mismatches in StockDB and data size differences in TransactionStorage and ILS. \n\nNotably, SCAX2012 was recently refreshed and is up-to-date.\n\n@user has proposed a prioritised refresh plan to align DEV with PROD, particularly to support accurate modelling and pipeline validation in Snowflake environments. \n\nRachel Wan confirmed that DEV environments are refreshed only upon request and are not automatically synced with PROD.\n\n*Summary of Findings:*\n\n* *SCAX2012*: Recently refreshed (11 July 2025) – no further action required.\n* *StockDB*: Schema drift identified across 7 tables (refer to _SchemaCompare.docx_) – refresh required to align DEV with PROD.\n* *TransactionStorage*: DEV has more records than PROD due to regular archiving in PROD (not duplication). Tables archived include:\n** {{BranchOrderItems}}, {{BranchOrders}}, {{Store_IncrementalSOHChanges}}, {{Transactions}}, {{Transactions_Invoice}}\n** {{Shipment_detail}}, {{Shipment_header}}\n* *ILS*: Data size gaps observed – refresh recommended but not urgent.\n\n*Proposed Refresh Priority:*\n\n# *StockDB* – to resolve schema mismatches.\n# *TransactionStorage* – to align data volume and ensure DEV reflects current PROD state.\n# *ILS* – to be scheduled after the above.\n\n@Rachel Wan – to advise on the formal process for requesting refreshes for these databases. \nOnce confirmed, requests to be raised.", "acceptance_criteria": "*StockDB Refresh*\n\n* Schema drift issues resolved.\n* -DEV schema matches PROD schema as per {{SchemaCompare.docx}}.-\n\n*TransactionStorage Refresh*\n\n* -Data volume aligned between DEV and PROD.-\n* -DEV duplicates addressed.-\n* -Tables listed by Adeel (e.g., {{BranchOrderItems}}, {{Transactions}}, {{Shipment_detail}}) archived appropriately in PROD.-\n\n*ILS Refresh*\n\n* -Data volume aligned between DEV and PROD.-\n* -Refresh scheduled post StockDB and TransactionStorage updates.-\n\n*Confirmation*\n\n* -Rachel Wan to advise on the formal process for initiating refresh requests.-\n* -Refresh timelines and responsible parties confirmed.-", "comments": "@user descriptions udpate FYI\n\nAdeel drive DEV refresh requests and discussions\n\nPhil to drive activity, assistance required for Adeel.\n\n@user\n\nI have caught up with @user \n\n{quote}* got to go ahead for ILS DB refresh\n* currently needing to work out on the refresh plan as it will be performed for the first time\n* schema drift in Stockdb exists due to the dev work going on with other project.\n** if this stockdb is refreshed, then we need to replicate those schema drifts in dev env again\n*** discussion with Chloe @userhappened this afternoon - stockdb refresh is not needed as for now.\n\n{quote}\n\nAdeel carry over from this ticket to be transitioned to Sprint 5 @user plus assign to Adeel when done\n\nStockDB to not be refreshed, as DEV Schema is per new build. Validate Go Live of new changes to ensure we dont go in earlier.\n\nSCAX2012 =- refreshed recently\n\nTransactioStorage - refreshing being worked through\n\nils - refreshing being worked through\n\n@user\n\napprovals gained for ILS refresh. @user\n\nExpected to finish today\nNOted:\n- refresh risk for scale - \n\n@user discuss about the above\n\nto split out task:\n\n* ILS refresh risk\n* Monitoring transactionstorage restore - in progress.\n\nWill split out card.\n\n@user done.", "text": "Summary\nAdeel - On-prem DEV/PROD Database Environment Sync\n\n---\n\nDescription\nFollowing a review of the on-prem DEV and PROD environments for key databases (StockDB, SCAX2012, ILS, TransactionStorage), discrepancies were identified in schema alignment and data volume. Adeel Syed provided a detailed comparison, highlighting schema mismatches in StockDB and data size differences in TransactionStorage and ILS. \n\nNotably, SCAX2012 was recently refreshed and is up-to-date.\n\n@user has proposed a prioritised refresh plan to align DEV with PROD, particularly to support accurate modelling and pipeline validation in Snowflake environments. \n\nRachel Wan confirmed that DEV environments are refreshed only upon request and are not automatically synced with PROD.\n\n*Summary of Findings:*\n\n* *SCAX2012*: Recently refreshed (11 July 2025) – no further action required.\n* *StockDB*: Schema drift identified across 7 tables (refer to _SchemaCompare.docx_) – refresh required to align DEV with PROD.\n* *TransactionStorage*: DEV has more records than PROD due to regular archiving in PROD (not duplication). Tables archived include:\n** {{BranchOrderItems}}, {{BranchOrders}}, {{Store_IncrementalSOHChanges}}, {{Transactions}}, {{Transactions_Invoice}}\n** {{Shipment_detail}}, {{Shipment_header}}\n* *ILS*: Data size gaps observed – refresh recommended but not urgent.\n\n*Proposed Refresh Priority:*\n\n# *StockDB* – to resolve schema mismatches.\n# *TransactionStorage* – to align data volume and ensure DEV reflects current PROD state.\n# *ILS* – to be scheduled after the above.\n\n@Rachel Wan – to advise on the formal process for requesting refreshes for these databases. \nOnce confirmed, requests to be raised.\n\n---\n\nAcceptance Criteria\n*StockDB Refresh*\n\n* Schema drift issues resolved.\n* -DEV schema matches PROD schema as per {{SchemaCompare.docx}}.-\n\n*TransactionStorage Refresh*\n\n* -Data volume aligned between DEV and PROD.-\n* -DEV duplicates addressed.-\n* -Tables listed by Adeel (e.g., {{BranchOrderItems}}, {{Transactions}}, {{Shipment_detail}}) archived appropriately in PROD.-\n\n*ILS Refresh*\n\n* -Data volume aligned between DEV and PROD.-\n* -Refresh scheduled post StockDB and TransactionStorage updates.-\n\n*Confirmation*\n\n* -Rachel Wan to advise on the formal process for initiating refresh requests.-\n* -Refresh timelines and responsible parties confirmed.-\n\n---\n\nComments\n@user descriptions udpate FYI\n\nAdeel drive DEV refresh requests and discussions\n\nPhil to drive activity, assistance required for Adeel.\n\n@user\n\nI have caught up with @user \n\n{quote}* got to go ahead for ILS DB refresh\n* currently needing to work out on the refresh plan as it will be performed for the first time\n* schema drift in Stockdb exists due to the dev work going on with other project.\n** if this stockdb is refreshed, then we need to replicate those schema drifts in dev env again\n*** discussion with Chloe @userhappened this afternoon - stockdb refresh is not needed as for now.\n\n{quote}\n\nAdeel carry over from this ticket to be transitioned to Sprint 5 @user plus assign to Adeel when done\n\nStockDB to not be refreshed, as DEV Schema is per new build. Validate Go Live of new changes to ensure we dont go in earlier.\n\nSCAX2012 =- refreshed recently\n\nTransactioStorage - refreshing being worked through\n\nils - refreshing being worked through\n\n@user\n\napprovals gained for ILS refresh. @user\n\nExpected to finish today\nNOted:\n- refresh risk for scale - \n\n@user discuss about the above\n\nto split out task:\n\n* ILS refresh risk\n* Monitoring transactionstorage restore - in progress.\n\nWill split out card.\n\n@user done.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 407}}
{"issue_key": "CSCI-184", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "24/Jul/25 9:48 AM", "updated": "19/Nov/25 11:50 AM", "labels": [], "summary": "Data Transformation Discussion Per SIGMA Issues", "description": "Harrison to discuss with Chloe a couple issues in SIGMA Ingestion and Transformation that I want to clarify for CW Snowflake Ingestion and Transformation\n\n* Issues with ETL_Update stamping\n** ETL_UPDATE in the Fact is altered even if the Fact metrics, attributes or Dim Key relationships are unchanged. An attribute of a Dim that the object connects to has changed but the Key values havent, yet the ETL_Update stamp is altered. means there are significanlty more changes in historical data in Fact then should be, this impacts ability to perform incremental refreshes etc\n* MERGE issue\n** In SIGMA environment the MERGE never handled deletes from the Fact very well, want to ensure this wont be repliacted\n** i.e. we have an agg object populated from a Base Fact where when records are deleted from Base Fact the merge doesnt work", "acceptance_criteria": "", "comments": "Hey @user ,\n\nLets include @user in Arch session next week with @user to discuss these.\n\nThanks,\n\nHarrison", "text": "Summary\nData Transformation Discussion Per SIGMA Issues\n\n---\n\nDescription\nHarrison to discuss with Chloe a couple issues in SIGMA Ingestion and Transformation that I want to clarify for CW Snowflake Ingestion and Transformation\n\n* Issues with ETL_Update stamping\n** ETL_UPDATE in the Fact is altered even if the Fact metrics, attributes or Dim Key relationships are unchanged. An attribute of a Dim that the object connects to has changed but the Key values havent, yet the ETL_Update stamp is altered. means there are significanlty more changes in historical data in Fact then should be, this impacts ability to perform incremental refreshes etc\n* MERGE issue\n** In SIGMA environment the MERGE never handled deletes from the Fact very well, want to ensure this wont be repliacted\n** i.e. we have an agg object populated from a Base Fact where when records are deleted from Base Fact the merge doesnt work\n\n---\n\nComments\nHey @user ,\n\nLets include @user in Arch session next week with @user to discuss these.\n\nThanks,\n\nHarrison", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 408}}
{"issue_key": "CSCI-181", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Jul/25 10:14 AM", "updated": "25/Jul/25 11:12 AM", "labels": [], "summary": "Vnet Data Gateway Setup", "description": "PBI ticket: ______\n\ntaken from notes on Tuesday 22/07\n\n* Task to be collaborated between @user and @user \n* Vnet gateway requires Fabric Licence (i.e. F2 capacity)\n** Note: cost concerns were raised by Eugene\n* CK: Will create a Vnet Data gateway (as Eugene doesn’t have the rights at time of writing)\n* Eugene: Will presumably be in charge of setup - Will confirm.\n\nAcceptance criteira:\n\n* Vnet Data gateway to be created", "acceptance_criteria": "* Vnet Data gateway to be created \n* Eugene granted access to gateway\n* Documentation of setup", "comments": "VNet Data Gateway for Dev is created at: [Fabric|https://app.fabric.microsoft.com/groups/me/gateways?experience=fabric-developer]\n\nNaming convention is: *cwr-ase-edp-<environment>-vdgw*\n\nUser assignment as follows: \n\nSetup Instructions: \n\n* in Fabric Console, navigate to Settings | Manage connections and gateways:\n* \nSelect the Virtual network gateways tab\n* Click on + New and fill up the form\n* \n\ncc: @user @user @user", "text": "Summary\nVnet Data Gateway Setup\n\n---\n\nDescription\nPBI ticket: ______\n\ntaken from notes on Tuesday 22/07\n\n* Task to be collaborated between @user and @user \n* Vnet gateway requires Fabric Licence (i.e. F2 capacity)\n** Note: cost concerns were raised by Eugene\n* CK: Will create a Vnet Data gateway (as Eugene doesn’t have the rights at time of writing)\n* Eugene: Will presumably be in charge of setup - Will confirm.\n\nAcceptance criteira:\n\n* Vnet Data gateway to be created\n\n---\n\nAcceptance Criteria\n* Vnet Data gateway to be created \n* Eugene granted access to gateway\n* Documentation of setup\n\n---\n\nComments\nVNet Data Gateway for Dev is created at: [Fabric|https://app.fabric.microsoft.com/groups/me/gateways?experience=fabric-developer]\n\nNaming convention is: *cwr-ase-edp-<environment>-vdgw*\n\nUser assignment as follows: \n\nSetup Instructions: \n\n* in Fabric Console, navigate to Settings | Manage connections and gateways:\n* \nSelect the Virtual network gateways tab\n* Click on + New and fill up the form\n* \n\ncc: @user @user @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 409}}
{"issue_key": "CSCI-180", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "22/Jul/25 10:09 AM", "updated": "03/Sep/25 9:40 AM", "labels": [], "summary": "Create baseline and incremental ExpressRoute usage", "description": "", "acceptance_criteria": "", "comments": "Hey @user \nWould it be OK to catch up with you about descriptions here?\n\nI just wanted to know whether I can close this as configuring expressroute is already done.\n\n@user baseline is covered by the ExpressRoute logs, we just have to establish specific durations.", "text": "Summary\nCreate baseline and incremental ExpressRoute usage\n\n---\n\nComments\nHey @user \nWould it be OK to catch up with you about descriptions here?\n\nI just wanted to know whether I can close this as configuring expressroute is already done.\n\n@user baseline is covered by the ExpressRoute logs, we just have to establish specific durations.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 410}}
{"issue_key": "CSCI-179", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "22/Jul/25 10:07 AM", "updated": "13/Aug/25 8:33 AM", "labels": [], "summary": "Raw Source Objects for Dimensions", "description": "Focusing on objects not connected to SC existing PBI Semantic Models, but other objects in BUS Matrix. Provide not just PBI05 object but the relevant objects from actual source for this object.", "acceptance_criteria": "Column G “Actual Source” in BUS Matrix filled in with relevant SQL Server/MySQL object details.\n\n*Next Step:* Source-To-Target Mapping", "comments": "Started working on Dims Source Mapping\n\nsession booked\n\nsession. part 3 booked tomorrow", "text": "Summary\nRaw Source Objects for Dimensions\n\n---\n\nDescription\nFocusing on objects not connected to SC existing PBI Semantic Models, but other objects in BUS Matrix. Provide not just PBI05 object but the relevant objects from actual source for this object.\n\n---\n\nAcceptance Criteria\nColumn G “Actual Source” in BUS Matrix filled in with relevant SQL Server/MySQL object details.\n\n*Next Step:* Source-To-Target Mapping\n\n---\n\nComments\nStarted working on Dims Source Mapping\n\nsession booked\n\nsession. part 3 booked tomorrow", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 411}}
{"issue_key": "CSCI-178", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Subtask", "status": "Done", "priority": "Medium", "created": "22/Jul/25 10:06 AM", "updated": "02/Aug/25 1:23 AM", "labels": [], "summary": "Configure SHIR QoS", "description": "Tracking development at: [Task 196417 Implement QoS to SHIR|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/196417/]", "acceptance_criteria": "", "comments": "Created a ticket with MS for QoS, implementation created issues\n\nReceived some guidance from MS, continuing work on this.\n\nUnfortunately, this is not viable as with the QoS in place, it won’t allow connection to the on-premise databbase.\n\n# Without QoS, we can connect\n# We setup a QoS\n# With QoS, it fails", "text": "Summary\nConfigure SHIR QoS\n\n---\n\nDescription\nTracking development at: [Task 196417 Implement QoS to SHIR|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/196417/]\n\n---\n\nComments\nCreated a ticket with MS for QoS, implementation created issues\n\nReceived some guidance from MS, continuing work on this.\n\nUnfortunately, this is not viable as with the QoS in place, it won’t allow connection to the on-premise databbase.\n\n# Without QoS, we can connect\n# We setup a QoS\n# With QoS, it fails", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 412}}
{"issue_key": "CSCI-176", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Jul/25 4:47 PM", "updated": "15/Aug/25 4:38 PM", "labels": [], "summary": "Source to Target for Facts - DC Inventory", "description": "Source to target mapping for Inventory history related Facts - filling out sheet that covers:\n\n* Fact_DC_inventory_history\n* Fact_DC_inventory_intra\n* Fact_DC_Inventory_Location_History\n* Fact_DC_Inventory_Adjustments", "acceptance_criteria": "* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Inventory%20Source-to-target%20document%20-%20Facts.xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=uvHOeg&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "This task will/should be split out - I can do this.\n\n@user\n\nneed ot map the BI tables for DC\n\nis approaching secondary mapping for DC\n\nTo verify ‘Data type’ whether it’s needed to be specified.\n\\ @user@user\n\n* Fact_DC_inventory_intra\n\nHarrison had entered a few more fields. SAFETYSTOCKQTY, DamagedQty,  UnsellableQty and ExpiredQty.\n\nI have done some research to to be able to get these information out of the ERP system. \nexpired qty can possibly be found from INVENTBATCH table but when i checked the table it doesn't contain any useful information. \n\nHave started a conversation with CWR AX Consultants and jess to see if can get this data out. \n\n@user , we might want to think about this table. As i think we cant get a lot these fields out of the scale system.", "text": "Summary\nSource to Target for Facts - DC Inventory\n\n---\n\nDescription\nSource to target mapping for Inventory history related Facts - filling out sheet that covers:\n\n* Fact_DC_inventory_history\n* Fact_DC_inventory_intra\n* Fact_DC_Inventory_Location_History\n* Fact_DC_Inventory_Adjustments\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Inventory%20Source-to-target%20document%20-%20Facts.xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=uvHOeg&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nThis task will/should be split out - I can do this.\n\n@user\n\nneed ot map the BI tables for DC\n\nis approaching secondary mapping for DC\n\nTo verify ‘Data type’ whether it’s needed to be specified.\n\\ @user@user\n\n* Fact_DC_inventory_intra\n\nHarrison had entered a few more fields. SAFETYSTOCKQTY, DamagedQty,  UnsellableQty and ExpiredQty.\n\nI have done some research to to be able to get these information out of the ERP system. \nexpired qty can possibly be found from INVENTBATCH table but when i checked the table it doesn't contain any useful information. \n\nHave started a conversation with CWR AX Consultants and jess to see if can get this data out. \n\n@user , we might want to think about this table. As i think we cant get a lot these fields out of the scale system.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 413}}
{"issue_key": "CSCI-171", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Jul/25 3:55 PM", "updated": "01/Sep/25 2:20 AM", "labels": [], "summary": "Create Source-to-Target Map for Dim_Product_UOM - Mapping", "description": "Source to target mapping for Dim_Product_UOM - filling out sheet as per Source-to-target dims", "acceptance_criteria": "* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Descriptions\",\"marks\":[{\"type\":\"strong\"}]},{\"type\":\"text\",\"text\":\" are clear, unambiguous, and provide both business and technical context.\"}],\"attrs\":{\"localId\":\"2b889b40-6284-4ea5-b742-52fa6b13827f\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Source details\",\"marks\":[{\"type\":\"strong\"}]},{\"type\":\"text\",\"text\":\" are accurate and validated against the current production environment.\"}],\"attrs\":{\"localId\":\"33d27bf9-950d-4e05-b79d-9434ee46ae5a\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Derived columns\",\"marks\":[{\"type\":\"strong\"}]},{\"type\":\"text\",\"text\":\" include a concise explanation of the derivation logic.\"}],\"attrs\":{\"localId\":\"a35204ac-7404-4ae1-8237-226609754794\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"The completed mapping is reviewed and validated by a peer or data steward.\",\"marks\":[{\"type\":\"strike\"}]}],\"attrs\":{\"localId\":\"0e777f9c-cf50-4a62-b19a-a7bff82987a6\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Any mismatches or gaps between the mapping and the production environment are resolved and documented.\"}],\"attrs\":{\"localId\":\"66d56fe0-d06b-47ca-bd8e-7cd2ee438f34\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"The final mapping file is uploaded to the designated SharePoint location and stakeholders are notified.\"}],\"attrs\":{\"localId\":\"5a1a8415-d6cd-45cb-aecd-7d2f5d399b60\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"(Optional) If required, update downstream documentation or metadata repositories to reflect the new mappings.\"}],\"attrs\":{\"localId\":\"3cb522be-3cd3-4959-a49f-762bb795418e\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"0fa1744a-4f74-410c-8921-a11b661237b7\"}}\n{adf}", "comments": "Completed the mapping for Dim_Product_UOM. Creating Review ticket for @user to review.\n\nCc: @user\n\nbuffed the AC @user - LMK if this is appropriate", "text": "Summary\nCreate Source-to-Target Map for Dim_Product_UOM - Mapping\n\n---\n\nDescription\nSource to target mapping for Dim_Product_UOM - filling out sheet as per Source-to-target dims\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n{adf:display=block}\n{\"type\":\"taskList\",\"content\":[{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Descriptions\",\"marks\":[{\"type\":\"strong\"}]},{\"type\":\"text\",\"text\":\" are clear, unambiguous, and provide both business and technical context.\"}],\"attrs\":{\"localId\":\"2b889b40-6284-4ea5-b742-52fa6b13827f\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Source details\",\"marks\":[{\"type\":\"strong\"}]},{\"type\":\"text\",\"text\":\" are accurate and validated against the current production environment.\"}],\"attrs\":{\"localId\":\"33d27bf9-950d-4e05-b79d-9434ee46ae5a\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Derived columns\",\"marks\":[{\"type\":\"strong\"}]},{\"type\":\"text\",\"text\":\" include a concise explanation of the derivation logic.\"}],\"attrs\":{\"localId\":\"a35204ac-7404-4ae1-8237-226609754794\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"The completed mapping is reviewed and validated by a peer or data steward.\",\"marks\":[{\"type\":\"strike\"}]}],\"attrs\":{\"localId\":\"0e777f9c-cf50-4a62-b19a-a7bff82987a6\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"Any mismatches or gaps between the mapping and the production environment are resolved and documented.\"}],\"attrs\":{\"localId\":\"66d56fe0-d06b-47ca-bd8e-7cd2ee438f34\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"The final mapping file is uploaded to the designated SharePoint location and stakeholders are notified.\"}],\"attrs\":{\"localId\":\"5a1a8415-d6cd-45cb-aecd-7d2f5d399b60\",\"state\":\"TODO\"}},{\"type\":\"taskItem\",\"content\":[{\"type\":\"text\",\"text\":\"(Optional) If required, update downstream documentation or metadata repositories to reflect the new mappings.\"}],\"attrs\":{\"localId\":\"3cb522be-3cd3-4959-a49f-762bb795418e\",\"state\":\"TODO\"}}],\"attrs\":{\"localId\":\"0fa1744a-4f74-410c-8921-a11b661237b7\"}}\n{adf}\n\n---\n\nComments\nCompleted the mapping for Dim_Product_UOM. Creating Review ticket for @user to review.\n\nCc: @user\n\nbuffed the AC @user - LMK if this is appropriate", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 414}}
{"issue_key": "CSCI-170", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Jul/25 3:43 PM", "updated": "01/Sep/25 2:20 AM", "labels": [], "summary": "Create Source-to-Target Map for Dim_Employee - Mapping", "description": "Source to target mapping for Dim_Employee - filling out sheet as per Source-to-target dims", "acceptance_criteria": "* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "", "text": "Summary\nCreate Source-to-Target Map for Dim_Employee - Mapping\n\n---\n\nDescription\nSource to target mapping for Dim_Employee - filling out sheet as per Source-to-target dims\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 415}}
{"issue_key": "CSCI-169", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Jul/25 3:42 PM", "updated": "01/Sep/25 2:18 AM", "labels": [], "summary": "Create Source-to-Target Map for Dim_PO - part 1", "description": "Source to target mapping for Dim_PO - filling out sheet as per Source-to-target dims", "acceptance_criteria": "* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "@user Can you provide the scope of this dimension? Is it related to Store POs or DC POs\n\n@user - will catchup with @user after standup today\n\nIssue split into:\n|CSCI-378|Create Source-to-Target Map for Dim_PO - part 1|", "text": "Summary\nCreate Source-to-Target Map for Dim_PO - part 1\n\n---\n\nDescription\nSource to target mapping for Dim_PO - filling out sheet as per Source-to-target dims\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\n@user Can you provide the scope of this dimension? Is it related to Store POs or DC POs\n\n@user - will catchup with @user after standup today\n\nIssue split into:\n|CSCI-378|Create Source-to-Target Map for Dim_PO - part 1|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 416}}
{"issue_key": "CSCI-168", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Jul/25 3:41 PM", "updated": "01/Sep/25 2:20 AM", "labels": [], "summary": "Create Source-to-Target Map for Dim_Location - Mapping", "description": "Source to target mapping for Dim_LOCATION - filling out sheet as per Source-to-target dims", "acceptance_criteria": "* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "", "text": "Summary\nCreate Source-to-Target Map for Dim_Location - Mapping\n\n---\n\nDescription\nSource to target mapping for Dim_LOCATION - filling out sheet as per Source-to-target dims\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 417}}
{"issue_key": "CSCI-167", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Jul/25 3:40 PM", "updated": "01/Sep/25 2:20 AM", "labels": [], "summary": "Create Source-to-Target Map for Dim_DC - Mapping", "description": "Source to target mapping for Dim_DC - filling out sheet as per Source-to-target dims", "acceptance_criteria": "* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n* *Descriptions* are clear, unambiguous, and provide both business and technical context.\n* *Source details* are accurate and validated against the current production environment.\n* *Derived columns* include a concise explanation of the derivation logic.\n* -The completed mapping is reviewed and validated by a peer or data steward.-\n* Any mismatches or gaps between the mapping and the production environment are resolved and documented.\n* The final mapping file is uploaded to the designated SharePoint location and stakeholders are notified.\n* (Optional) If required, update downstream documentation or metadata repositories to reflect the new mappings.", "comments": "Mapping of DIM_DC is complete, created a ticket and assigned to @user for review.\n\nCC: @user\n\n@user I’ve included the mapping from current source system and the gold layer mapping from previous EDP project.\n\nbuffed the AC @user - LMK if this is appropriate", "text": "Summary\nCreate Source-to-Target Map for Dim_DC - Mapping\n\n---\n\nDescription\nSource to target mapping for Dim_DC - filling out sheet as per Source-to-target dims\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n* *Descriptions* are clear, unambiguous, and provide both business and technical context.\n* *Source details* are accurate and validated against the current production environment.\n* *Derived columns* include a concise explanation of the derivation logic.\n* -The completed mapping is reviewed and validated by a peer or data steward.-\n* Any mismatches or gaps between the mapping and the production environment are resolved and documented.\n* The final mapping file is uploaded to the designated SharePoint location and stakeholders are notified.\n* (Optional) If required, update downstream documentation or metadata repositories to reflect the new mappings.\n\n---\n\nComments\nMapping of DIM_DC is complete, created a ticket and assigned to @user for review.\n\nCC: @user\n\n@user I’ve included the mapping from current source system and the gold layer mapping from previous EDP project.\n\nbuffed the AC @user - LMK if this is appropriate", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 418}}
{"issue_key": "CSCI-166", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Jul/25 3:28 PM", "updated": "15/Aug/25 3:59 PM", "labels": [], "summary": "Create Source-to-Target Map for Dim_Store", "description": "Source to target mapping for Dim_Store - filling out sheet as per Source-to-target dims", "acceptance_criteria": "* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "Added the source mapping for DIM store. Separate ticket is created for review \n\n@user", "text": "Summary\nCreate Source-to-Target Map for Dim_Store\n\n---\n\nDescription\nSource to target mapping for Dim_Store - filling out sheet as per Source-to-target dims\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\nAdded the source mapping for DIM store. Separate ticket is created for review \n\n@user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 419}}
{"issue_key": "CSCI-165", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Jul/25 3:25 PM", "updated": "01/Sep/25 2:20 AM", "labels": [], "summary": "Create Source-to-Target Map for Dim_Date - Mapping", "description": "Source to target mapping for Dim_Date - filling out sheet as per Source-to-target dims", "acceptance_criteria": "* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n* *Descriptions* are clear, unambiguous, and provide both business and technical context.\n* *Source details* are accurate and validated against the current production environment.\n* *Derived columns* include a concise explanation of the derivation logic.\n* -The completed mapping is reviewed and validated by a peer or data steward.-\n* Any mismatches or gaps between the mapping and the production environment are resolved and documented.\n* The final mapping file is uploaded to the designated SharePoint location and stakeholders are notified.\n* (Optional) If required, update downstream documentation or metadata repositories to reflect the new mappings.", "comments": "Added mapping for Date Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]\n\n@user I have created a review ticket ad assigned to @user\n\n@user thank you.\nCan you link that ticket here? Thanks.\ncc @user\n\nHey I think your tickets (along with other DIM tickets) are all done except Dim_PO @user\n\ncc @user - I’ll make it neat for you to review all at once where possible", "text": "Summary\nCreate Source-to-Target Map for Dim_Date - Mapping\n\n---\n\nDescription\nSource to target mapping for Dim_Date - filling out sheet as per Source-to-target dims\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=you9On] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n* *Descriptions* are clear, unambiguous, and provide both business and technical context.\n* *Source details* are accurate and validated against the current production environment.\n* *Derived columns* include a concise explanation of the derivation logic.\n* -The completed mapping is reviewed and validated by a peer or data steward.-\n* Any mismatches or gaps between the mapping and the production environment are resolved and documented.\n* The final mapping file is uploaded to the designated SharePoint location and stakeholders are notified.\n* (Optional) If required, update downstream documentation or metadata repositories to reflect the new mappings.\n\n---\n\nComments\nAdded mapping for Date Dimension: [Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B91FDEF3E-FF68-4F5F-B7D9-135BB7C90FC8%7D&file=Source-To-Target-Mapping%20(STTM).xlsx&nav=MTVfe0JERjIzNjcyLTdEQzgtNENDQS1CMDYxLUE0NzkzQUMxNTNGM30&action=default&mobileredirect=true]\n\n@user I have created a review ticket ad assigned to @user\n\n@user thank you.\nCan you link that ticket here? Thanks.\ncc @user\n\nHey I think your tickets (along with other DIM tickets) are all done except Dim_PO @user\n\ncc @user - I’ll make it neat for you to review all at once where possible", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 420}}
{"issue_key": "CSCI-164", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Jul/25 3:24 PM", "updated": "17/Sep/25 2:26 PM", "labels": [], "summary": "Create Source-to-Target Map for Dim_Product", "description": "Source to target mapping for Dim_Product - filling out sheet as per Source-to-target dims", "acceptance_criteria": "* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no", "comments": "@user Added the source mapping from DIM_PRODICT source table from PBI05, Also included the Gold layer mapping DimProduct table from pervious EDP project.\n[Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=ES7jdo&nav=MTVfezg3MTk2QTA1LURBNDgtNERBOS1CREVELTI0MTBDNzEwNTFFRH0]\n\n@user Conceptual ERD from EDP Gold layer\n\nHarrison and Alan to review Dims.\n @user to create new ticket if needed.", "text": "Summary\nCreate Source-to-Target Map for Dim_Product\n\n---\n\nDescription\nSource to target mapping for Dim_Product - filling out sheet as per Source-to-target dims\n\n---\n\nAcceptance Criteria\n* Columns as per[ supplied sheet|https://sigmacompanylimited-my.sharepoint.com/:x:/r/personal/phillip_yuen_sigmahealthcare_com_au/Documents/Inventory%20Source-to-target%20document%20-%20Dimensions.xlsx?d=w8e06411f59a74a9faefa002ad5a5b931&csf=1&web=1&e=cpn3bw&nav=MTVfezg1QUM4QzQwLTY1RUMtNDkxQy05N0RCLTY0QjQ2OTI4RDJDQn0] to be filled\n** Pres Column name\n** Data type\n** Description\n** Source\n*** Server\n*** DB\n*** Schema\n*** TAble \n*** Column\n*** Derived? Yes or no\n\n---\n\nComments\n@user Added the source mapping from DIM_PRODICT source table from PBI05, Also included the Gold layer mapping DimProduct table from pervious EDP project.\n[Source-To-Target-Mapping (STTM).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Projects/01.%20SC%20Interim%20Data%20Platform/01.%20Discovery/Source%20to%20Target%20Facts%20and%20Dims%20in%20Prioritised%20domains/Source-To-Target-Mapping%20(STTM).xlsx?d=w91fdef3eff684f5fb7d9135bb7c90fc8&csf=1&web=1&e=ES7jdo&nav=MTVfezg3MTk2QTA1LURBNDgtNERBOS1CREVELTI0MTBDNzEwNTFFRH0]\n\n@user Conceptual ERD from EDP Gold layer\n\nHarrison and Alan to review Dims.\n @user to create new ticket if needed.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 421}}
{"issue_key": "CSCI-163", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "21/Jul/25 10:04 AM", "updated": "01/Aug/25 4:07 PM", "labels": [], "summary": "QA Snowflake ingested data", "description": "Comparing data between:\n\n* source SQL DB (StockDB and SCAX2012) \n* Snowflake tables and views", "acceptance_criteria": "* Complete match between source and Snowflake tables", "comments": "Tested DB:\n\n* TDB08AX2012.StockDB\n\nWhat has been tested:\n\n* mismatch\n* missing tables\n* missing data types\n* duplicates\n* Key column null value check\n* Data range check\n\n*Results as per Adeel Email:*\n\n# RowCount mismatch\n\nAll tables are matched except the BranchInfoHierarchy\n\n__\n\n|*Server*|*Database*|*Table*|*DBRowCount*|*SFRowCount*|*Results*|\n|TDB08AX2012|StockDb|BranchInfoHierarchy|901|921|Not Matching|\n\n \n\n# Below tables are missing\n\n* MultiBuyTriggers\n* ProductGroup\n* ProductNetworkCosts\n* ProductNetworkCostsHistory\n* ProductsDrugMatch\n* SOHAdjustmentTypes\n* SupplierDetails\n\n \n\n \n\n# Data Type mismatch\n## All tables were ingested into Snowflake with columns as text. Views were created for type conversion to match the source, but the following column mappings are incorrect.\n\n__\n\n{adf:display=block}\n{\"type\":\"table\",\"attrs\":{\"isNumberColumnEnabled\":false,\"layout\":\"center\",\"localId\":\"efef3cf3-771c-4d29-93ab-ba8ba18cb5ab\"},\"content\":[{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colspan\":3,\"background\":\"yellow\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Source Table\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":3,\"background\":\"yellow\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"SF View\"}]}]},{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\"}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"TABLE_NAME\",\"marks\":[{\"type\":\"strong\"}]}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"COLUMN_NAME\",\"marks\":[{\"type\":\"strong\"}]}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"DATA_TYPE\",\"marks\":[{\"type\":\"strong\"}]}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"TABLE_NAME\",\"marks\":[{\"type\":\"strong\"}]}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"COLUMN_NAME\",\"marks\":[{\"type\":\"strong\"}]}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"DATA_TYPE\",\"marks\":[{\"type\":\"strong\"}]}]}]},{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Results\",\"marks\":[{\"type\":\"strong\"}]}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Catalogues\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"EndDate\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"smalldatetime\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"CATALOGUES_VW\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"ENDDATE\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"TEXT\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Not Matched\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Catalogues\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"StartDate\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"smalldatetime\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"CATALOGUES_VW\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"STARTDATE\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"TEXT\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"not matched\"}]}]}]}]}\n{adf}\n\n \n\n# Duplicate Checks\n\nResult: Passed - No duplicate records were found\n\n \n\n# Key Column null value check\n\nResult: Passed - No null values found in key columns\n\n \n\n# Date Range Validation\nInvalid date values were identified in the following fields across all tables:\n\n* ETL_ROW_EFFECTIVE_DATE\n* ETL_ROW_EXPIRY_DATE\n\n@user FYI\n\nNote:\n\n* need to know archiving policy\n* know whether we should be pulling from archives? or direct ax2012\n** Point - need to have in sync with snowflake.\n\nI have addressed all the gaps and issues raised by Adeel for StockDB. He will retest later and let us know.\n\nAdeel will also commence testing the SCAX2012 tables.\n\nTo address data gaps for SCAX2012\n\nwill catchup about duplicates today @user @user\n\nThe FULL APPLY load type for tracking source deletions has been successfully tested.", "text": "Summary\nQA Snowflake ingested data\n\n---\n\nDescription\nComparing data between:\n\n* source SQL DB (StockDB and SCAX2012) \n* Snowflake tables and views\n\n---\n\nAcceptance Criteria\n* Complete match between source and Snowflake tables\n\n---\n\nComments\nTested DB:\n\n* TDB08AX2012.StockDB\n\nWhat has been tested:\n\n* mismatch\n* missing tables\n* missing data types\n* duplicates\n* Key column null value check\n* Data range check\n\n*Results as per Adeel Email:*\n\n# RowCount mismatch\n\nAll tables are matched except the BranchInfoHierarchy\n\n__\n\n|*Server*|*Database*|*Table*|*DBRowCount*|*SFRowCount*|*Results*|\n|TDB08AX2012|StockDb|BranchInfoHierarchy|901|921|Not Matching|\n\n \n\n# Below tables are missing\n\n* MultiBuyTriggers\n* ProductGroup\n* ProductNetworkCosts\n* ProductNetworkCostsHistory\n* ProductsDrugMatch\n* SOHAdjustmentTypes\n* SupplierDetails\n\n \n\n \n\n# Data Type mismatch\n## All tables were ingested into Snowflake with columns as text. Views were created for type conversion to match the source, but the following column mappings are incorrect.\n\n__\n\n{adf:display=block}\n{\"type\":\"table\",\"attrs\":{\"isNumberColumnEnabled\":false,\"layout\":\"center\",\"localId\":\"efef3cf3-771c-4d29-93ab-ba8ba18cb5ab\"},\"content\":[{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"colspan\":3,\"background\":\"yellow\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Source Table\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":3,\"background\":\"yellow\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"SF View\"}]}]},{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\"}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"TABLE_NAME\",\"marks\":[{\"type\":\"strong\"}]}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"COLUMN_NAME\",\"marks\":[{\"type\":\"strong\"}]}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"DATA_TYPE\",\"marks\":[{\"type\":\"strong\"}]}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"TABLE_NAME\",\"marks\":[{\"type\":\"strong\"}]}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"COLUMN_NAME\",\"marks\":[{\"type\":\"strong\"}]}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"DATA_TYPE\",\"marks\":[{\"type\":\"strong\"}]}]}]},{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Results\",\"marks\":[{\"type\":\"strong\"}]}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Catalogues\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"EndDate\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"smalldatetime\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"CATALOGUES_VW\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"ENDDATE\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"TEXT\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Not Matched\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Catalogues\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"StartDate\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#daf2d0\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"smalldatetime\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"CATALOGUES_VW\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"STARTDATE\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"TEXT\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"background\":\"#c0e6f5\"},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"not matched\"}]}]}]}]}\n{adf}\n\n \n\n# Duplicate Checks\n\nResult: Passed - No duplicate records were found\n\n \n\n# Key Column null value check\n\nResult: Passed - No null values found in key columns\n\n \n\n# Date Range Validation\nInvalid date values were identified in the following fields across all tables:\n\n* ETL_ROW_EFFECTIVE_DATE\n* ETL_ROW_EXPIRY_DATE\n\n@user FYI\n\nNote:\n\n* need to know archiving policy\n* know whether we should be pulling from archives? or direct ax2012\n** Point - need to have in sync with snowflake.\n\nI have addressed all the gaps and issues raised by Adeel for StockDB. He will retest later and let us know.\n\nAdeel will also commence testing the SCAX2012 tables.\n\nTo address data gaps for SCAX2012\n\nwill catchup about duplicates today @user @user\n\nThe FULL APPLY load type for tracking source deletions has been successfully tested.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 422}}
{"issue_key": "CSCI-159", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jul/25 8:26 PM", "updated": "15/Aug/25 4:39 PM", "labels": [], "summary": "Snowflake - Configure CD pipeline in DevOps", "description": "Build and configure a CD pipeline to automate Snowflake deployments via DevOps. \n\nDevelopment task: [Task 192950 Configure Snowflake Release Pipeline|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/192950]", "acceptance_criteria": "* CD pipeline deploys Snowflake scripts automatically upon version control updates.\n* Environment-specific configurations are supported and validated.\n* Deployment logs and rollback options are available and functional.", "comments": "for review by @user\n\napproved and merged\n\ncc @user\n\nhi Eugene, I've got this error when I ran the Snowflake cd pipeline. Do you know what is missing?", "text": "Summary\nSnowflake - Configure CD pipeline in DevOps\n\n---\n\nDescription\nBuild and configure a CD pipeline to automate Snowflake deployments via DevOps. \n\nDevelopment task: [Task 192950 Configure Snowflake Release Pipeline|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/192950]\n\n---\n\nAcceptance Criteria\n* CD pipeline deploys Snowflake scripts automatically upon version control updates.\n* Environment-specific configurations are supported and validated.\n* Deployment logs and rollback options are available and functional.\n\n---\n\nComments\nfor review by @user\n\napproved and merged\n\ncc @user\n\nhi Eugene, I've got this error when I ran the Snowflake cd pipeline. Do you know what is missing?", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 423}}
{"issue_key": "CSCI-155", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jul/25 4:31 PM", "updated": "04/Aug/25 6:48 AM", "labels": [], "summary": "PowerBI - Infras/ gateway setup", "description": "This is a placeholder for PowerBI infras requirements. We don’t need to take any action now. Jess and Harrison will communicate/ confirm the details in early August once we know more. \n\n@user", "acceptance_criteria": "", "comments": "CK to create Vnet gateway\nEugene to create connections\n\nI created 2 PowerBI Connections in Fabric, one using the On-Premise Data Gateway (SF_S_POWERBI_DEV) and the other using VNet Data Gateway (SF_S_POWERBI_DEV_VGW).\n\nI also allowed the group [Azure-Permissions-dev-edp-aus-Contributor - Microsoft Azure|https://portal.azure.com/#view/Microsoft_AAD_IAM/GroupDetailsMenuBlade/~/Members/groupId/1d2b4ba7-8619-4000-a219-f488b2256e3c/menuId/] use access to it.\n\nWhen using the connection, please follow the following steps to enable the compute:\n\n# *SF_S_POWERBI_DEV* - start the VM [cwr-ase-edpdatagateway-dev-vm-01 - Microsoft Azure|https://portal.azure.com/#@mychemist.com.au/resource/subscriptions/dc516e92-8716-44f9-b09c-fc5ca9cdd01a/resourceGroups/cwr-ase-dpdev-rg/providers/Microsoft.Compute/virtualMachines/cwr-ase-edpdatagateway-dev-vm-01/overview]. Once done, please stop the VM to reduce cost.\n# *SF_S_POWERBI_DEV_VGW* - resume the Fabric capacity [cwraseedpdevfc - Microsoft Azure|https://portal.azure.com/#@mychemist.com.au/resource/subscriptions/dc516e92-8716-44f9-b09c-fc5ca9cdd01a/resourceGroups/cwr-ase-dpdev-rg/providers/Microsoft.Fabric/capacities/cwraseedpdevfc/overview]. Make sure to Pause it after use to save on cost. Currently, only CK and I can resume the capacity. \n\nNote:\n\n* For the VNet Data Gateway, *cwr-ase-edp-dev-vdgw*, it can also be assigned to a Trial Capacity, see below: \n\ncc: @user @user @user @user @user\n\n@user this is in review but task is basically done.\n\n@user marked as done and crated a review task", "text": "Summary\nPowerBI - Infras/ gateway setup\n\n---\n\nDescription\nThis is a placeholder for PowerBI infras requirements. We don’t need to take any action now. Jess and Harrison will communicate/ confirm the details in early August once we know more. \n\n@user\n\n---\n\nComments\nCK to create Vnet gateway\nEugene to create connections\n\nI created 2 PowerBI Connections in Fabric, one using the On-Premise Data Gateway (SF_S_POWERBI_DEV) and the other using VNet Data Gateway (SF_S_POWERBI_DEV_VGW).\n\nI also allowed the group [Azure-Permissions-dev-edp-aus-Contributor - Microsoft Azure|https://portal.azure.com/#view/Microsoft_AAD_IAM/GroupDetailsMenuBlade/~/Members/groupId/1d2b4ba7-8619-4000-a219-f488b2256e3c/menuId/] use access to it.\n\nWhen using the connection, please follow the following steps to enable the compute:\n\n# *SF_S_POWERBI_DEV* - start the VM [cwr-ase-edpdatagateway-dev-vm-01 - Microsoft Azure|https://portal.azure.com/#@mychemist.com.au/resource/subscriptions/dc516e92-8716-44f9-b09c-fc5ca9cdd01a/resourceGroups/cwr-ase-dpdev-rg/providers/Microsoft.Compute/virtualMachines/cwr-ase-edpdatagateway-dev-vm-01/overview]. Once done, please stop the VM to reduce cost.\n# *SF_S_POWERBI_DEV_VGW* - resume the Fabric capacity [cwraseedpdevfc - Microsoft Azure|https://portal.azure.com/#@mychemist.com.au/resource/subscriptions/dc516e92-8716-44f9-b09c-fc5ca9cdd01a/resourceGroups/cwr-ase-dpdev-rg/providers/Microsoft.Fabric/capacities/cwraseedpdevfc/overview]. Make sure to Pause it after use to save on cost. Currently, only CK and I can resume the capacity. \n\nNote:\n\n* For the VNet Data Gateway, *cwr-ase-edp-dev-vdgw*, it can also be assigned to a Trial Capacity, see below: \n\ncc: @user @user @user @user @user\n\n@user this is in review but task is basically done.\n\n@user marked as done and crated a review task", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 424}}
{"issue_key": "CSCI-154", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jul/25 3:45 PM", "updated": "01/Aug/25 4:51 PM", "labels": [], "summary": "Analyse timing of on-prem SQL server table refreshes - part 2", "description": "Currently, PBI05 SQL Server is at 95-100% CPU capacity. Issue stems from daily batch jobs running whilst SC team are trying to refresh their semantic models.\n\nTo help ease this issue, need to develop a report/dashboard that details the daily timings of when each table in PBI05 is being refreshed.", "acceptance_criteria": "", "comments": "Made below updates to PBI Dashboard:\n\n* Updated SQL script to include the ‘Object type’ (ie Table or View)\n* Factored in new logic for Linked Server objects (included a manual override column to flag)\n* Fixed the exception for PBI table FactStoreInventoryERPEOMSummary\n* Advised Rachel F that they are referencing 2 TBI03 (test server) objects (these will not be refreshed periodically)\n\nNeed to follow up Bhavya to ask about:\n\n* PDB08 and PDB15\n** Do these servers contain data that is refresh as part of an ETL batch job\n* Gain ‘view server state permission’ access to PBI03 and PDB servers as needed\n\nExpected to be done today", "text": "Summary\nAnalyse timing of on-prem SQL server table refreshes - part 2\n\n---\n\nDescription\nCurrently, PBI05 SQL Server is at 95-100% CPU capacity. Issue stems from daily batch jobs running whilst SC team are trying to refresh their semantic models.\n\nTo help ease this issue, need to develop a report/dashboard that details the daily timings of when each table in PBI05 is being refreshed.\n\n---\n\nComments\nMade below updates to PBI Dashboard:\n\n* Updated SQL script to include the ‘Object type’ (ie Table or View)\n* Factored in new logic for Linked Server objects (included a manual override column to flag)\n* Fixed the exception for PBI table FactStoreInventoryERPEOMSummary\n* Advised Rachel F that they are referencing 2 TBI03 (test server) objects (these will not be refreshed periodically)\n\nNeed to follow up Bhavya to ask about:\n\n* PDB08 and PDB15\n** Do these servers contain data that is refresh as part of an ETL batch job\n* Gain ‘view server state permission’ access to PBI03 and PDB servers as needed\n\nExpected to be done today", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 425}}
{"issue_key": "CSCI-153", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jul/25 3:28 PM", "updated": "01/Aug/25 4:06 PM", "labels": [], "summary": "SCAX2012 Integration - Very large tables", "description": "Implementing a bulk load parquet pattern and dual ingestion strategies (historical + delta) for very large tables:\n\n|Database|Schema|Table|\n|SCAX2012|dbo|CUSTINVOICEJOUR|\n|SCAX2012|dbo|CustInvoiceTrans|\n|SCAX2012|dbo|PurchLine|\n|SCAX2012|dbo|VENDINVOICETRANS|", "acceptance_criteria": "# Bulk load pattern successfully ingests ~3.8k Parquet files without failure or performance issues.\n# Historical and delta ingestion logic is implemented and validated for target tables.\n# End-to-end ingestion pipeline passes functional testing", "comments": "|Database|Schema|Table|\n|SCAX2012|dbo|CUSTINVOICEJOUR ~ 32.6 Mil|\n|SCAX2012|dbo|CustInvoiceTrans ~ 385.7 Mil|\n|SCAX2012|dbo|PurchLine ~ 181.5 mil|\n|SCAX2012|dbo|VENDINVOICETRANS ~ 181 Mil|\n\nCreated external stages to get row count from the total parquet files\n\nSuccessfully loaded below tables:\n\n* SCAX2012.VENDINVOICETRANS at 181,086,639 row count\n* SCAX2012.PURCHLINE at 181,467,367 row count\n\nHistorical and delta load implemented and unit tested for all 4 above mentioned tables. Next step: - Adeel to QA data - Chloe to monitor the job load @user @user @user@user \n\nI’ve scheduled the delta load job to run at 4am my time @user if you want to jump in to observe ExpressRoute stats.\n\n@user - to check followup ticket in backlog\n\nAwaiting Adeel’s QA results\n\nChloe to address issues raised by Adeel.\n\n@user DQ issues raised by Adeel, Chloe investigating\n\nexpected to wrap up today\n- testing/QA with @user", "text": "Summary\nSCAX2012 Integration - Very large tables\n\n---\n\nDescription\nImplementing a bulk load parquet pattern and dual ingestion strategies (historical + delta) for very large tables:\n\n|Database|Schema|Table|\n|SCAX2012|dbo|CUSTINVOICEJOUR|\n|SCAX2012|dbo|CustInvoiceTrans|\n|SCAX2012|dbo|PurchLine|\n|SCAX2012|dbo|VENDINVOICETRANS|\n\n---\n\nAcceptance Criteria\n# Bulk load pattern successfully ingests ~3.8k Parquet files without failure or performance issues.\n# Historical and delta ingestion logic is implemented and validated for target tables.\n# End-to-end ingestion pipeline passes functional testing\n\n---\n\nComments\n|Database|Schema|Table|\n|SCAX2012|dbo|CUSTINVOICEJOUR ~ 32.6 Mil|\n|SCAX2012|dbo|CustInvoiceTrans ~ 385.7 Mil|\n|SCAX2012|dbo|PurchLine ~ 181.5 mil|\n|SCAX2012|dbo|VENDINVOICETRANS ~ 181 Mil|\n\nCreated external stages to get row count from the total parquet files\n\nSuccessfully loaded below tables:\n\n* SCAX2012.VENDINVOICETRANS at 181,086,639 row count\n* SCAX2012.PURCHLINE at 181,467,367 row count\n\nHistorical and delta load implemented and unit tested for all 4 above mentioned tables. Next step: - Adeel to QA data - Chloe to monitor the job load @user @user @user@user \n\nI’ve scheduled the delta load job to run at 4am my time @user if you want to jump in to observe ExpressRoute stats.\n\n@user - to check followup ticket in backlog\n\nAwaiting Adeel’s QA results\n\nChloe to address issues raised by Adeel.\n\n@user DQ issues raised by Adeel, Chloe investigating\n\nexpected to wrap up today\n- testing/QA with @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 426}}
{"issue_key": "CSCI-152", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jul/25 3:25 PM", "updated": "15/Aug/25 3:48 PM", "labels": [], "summary": "StockDb Integration - part 2", "description": "Build data ingestion pipelines for the StockDb database.\n\nPending a few more tables from StockDb to be ingested to Snowflake next sprint:\n\n|MultiBuyTriggers|\n|ProductGroup|\n|ProductNetworkCosts|\n|ProductsDrugMatch|\n|SOHAdjustmentTypes|\n|SupplierDetails|\n\nAnd the below which will require a historical load + delta pattern: \n\n|Table|RawData|Refresh Freq| Total Row Count |\n|ProductNetworkCostsHistory|Yes|Real Time|########|", "acceptance_criteria": "Tables available in Snowflake and pass validation test\n\nPhil will catch up wiht @user to write this soon.", "comments": "@user part 2 of the task split for sprint 4\n\nfor the big table, will need to spin off as a different task. @user @user\n\n@user Created the delta job and asked @user to create some insert, update & delete at source database to track in delta load into snowflake.\n\n@user Below records are inserted deleted and updated in ProductNetworkCostsHistory table.\n\nHISTORYID Inserted\n136742278\n136742277\n\nHISTORYID Deleted\n136742275\n136742274\n\nHISTORYID Updated\n136742273\n136742272\n--PublishedDate updated from '2025-01-31' to '2025-02-01'\n\nto spin off testing ticket for this @user\ncan you tag me afterwards if need help with acceptance criteria\n\n@user,Dev work is completed for all tables in scope. Need to create a separate ticket for some testing on the big table ingestion (PRODUCTNETWORKCOSTSHISTORY).", "text": "Summary\nStockDb Integration - part 2\n\n---\n\nDescription\nBuild data ingestion pipelines for the StockDb database.\n\nPending a few more tables from StockDb to be ingested to Snowflake next sprint:\n\n|MultiBuyTriggers|\n|ProductGroup|\n|ProductNetworkCosts|\n|ProductsDrugMatch|\n|SOHAdjustmentTypes|\n|SupplierDetails|\n\nAnd the below which will require a historical load + delta pattern: \n\n|Table|RawData|Refresh Freq| Total Row Count |\n|ProductNetworkCostsHistory|Yes|Real Time|########|\n\n---\n\nAcceptance Criteria\nTables available in Snowflake and pass validation test\n\nPhil will catch up wiht @user to write this soon.\n\n---\n\nComments\n@user part 2 of the task split for sprint 4\n\nfor the big table, will need to spin off as a different task. @user @user\n\n@user Created the delta job and asked @user to create some insert, update & delete at source database to track in delta load into snowflake.\n\n@user Below records are inserted deleted and updated in ProductNetworkCostsHistory table.\n\nHISTORYID Inserted\n136742278\n136742277\n\nHISTORYID Deleted\n136742275\n136742274\n\nHISTORYID Updated\n136742273\n136742272\n--PublishedDate updated from '2025-01-31' to '2025-02-01'\n\nto spin off testing ticket for this @user\ncan you tag me afterwards if need help with acceptance criteria\n\n@user,Dev work is completed for all tables in scope. Need to create a separate ticket for some testing on the big table ingestion (PRODUCTNETWORKCOSTSHISTORY).", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 427}}
{"issue_key": "CSCI-151", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jul/25 3:13 PM", "updated": "15/Aug/25 4:39 PM", "labels": [], "summary": "Snowflake Deployment Pipeline - Versioning & Parameterisation", "description": "* *Finalise* {{config_meta.sql}} *Versioning Logic*\n** Define and implement logic to track version history and changes within the {{config_meta.sql}} script.\n* *Parameterise Deployment Execution Scripts*\n** Introduce variables and parameters to support flexible, environment-specific deployments in DevOps.\n* *Test Initial Deployment Pipeline*\n** Execute and validate the first round of deployment to ensure pipeline functionality and identify any issues.", "acceptance_criteria": "# {{config_meta.sql}} includes clear versioning logic and documentation.\n# DevOps scripts support configurable parameters for different environments.\n# Initial deployment completes successfully with logs and validation checks.", "comments": ", I've updated the below:\n\n* Updated deployment scripts with added parameters for the snowflake environment and version \n* Updated the config_meta.sql scripts with proper test values/ column names. They are still test values though. \n* Created the edp-snowflake-cd pipeline to test my configuration setup \n* Added the Library's group variables and allow pipeline permission for edp-snowflake-cd.\n* Updated the cd.yml file to reference the group variables from the Library\n\nEugene to implement the CD pipeline. Will need to test after it is up and running\n\n{{@Chloe Tran }}fill out the other variable groups for deployment\n\nEugene has working mechanism, requires review by Chloe to approval.\n\nClosing this ticket and creating new ticket for review post other prioritised items being completed - likely Sprint 7.\n\nThanks,\n\nHarrison", "text": "Summary\nSnowflake Deployment Pipeline - Versioning & Parameterisation\n\n---\n\nDescription\n* *Finalise* {{config_meta.sql}} *Versioning Logic*\n** Define and implement logic to track version history and changes within the {{config_meta.sql}} script.\n* *Parameterise Deployment Execution Scripts*\n** Introduce variables and parameters to support flexible, environment-specific deployments in DevOps.\n* *Test Initial Deployment Pipeline*\n** Execute and validate the first round of deployment to ensure pipeline functionality and identify any issues.\n\n---\n\nAcceptance Criteria\n# {{config_meta.sql}} includes clear versioning logic and documentation.\n# DevOps scripts support configurable parameters for different environments.\n# Initial deployment completes successfully with logs and validation checks.\n\n---\n\nComments\n, I've updated the below:\n\n* Updated deployment scripts with added parameters for the snowflake environment and version \n* Updated the config_meta.sql scripts with proper test values/ column names. They are still test values though. \n* Created the edp-snowflake-cd pipeline to test my configuration setup \n* Added the Library's group variables and allow pipeline permission for edp-snowflake-cd.\n* Updated the cd.yml file to reference the group variables from the Library\n\nEugene to implement the CD pipeline. Will need to test after it is up and running\n\n{{@Chloe Tran }}fill out the other variable groups for deployment\n\nEugene has working mechanism, requires review by Chloe to approval.\n\nClosing this ticket and creating new ticket for review post other prioritised items being completed - likely Sprint 7.\n\nThanks,\n\nHarrison", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 428}}
{"issue_key": "CSCI-150", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jul/25 12:55 PM", "updated": "18/Jul/25 8:51 PM", "labels": [], "summary": "Update SQL Source Documentation with DBA Lineage Information", "description": "Adeel from DBA team has provided us information on the underlying data source lineage of PBI05 and PBI03 objects.\n\nTask to incorporate this information with our existing documentation. As well as identify any gaps in the lineage information provided, to ensure all PBI objects are mapped to a root source.", "acceptance_criteria": "", "comments": "", "text": "Summary\nUpdate SQL Source Documentation with DBA Lineage Information\n\n---\n\nDescription\nAdeel from DBA team has provided us information on the underlying data source lineage of PBI05 and PBI03 objects.\n\nTask to incorporate this information with our existing documentation. As well as identify any gaps in the lineage information provided, to ensure all PBI objects are mapped to a root source.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 429}}
{"issue_key": "CSCI-149", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jul/25 12:43 PM", "updated": "30/Jul/25 4:12 PM", "labels": [], "summary": "SC BAU Sprint 3 Work", "description": "# Helping to troubleshoot PBI Report/Workspace access issue for e-Comm team, for a user from Sigma\n## Madumitha messaged via Teams on 15/7\n# Assisted Liang with access to PBI Gateway connections for SC team", "acceptance_criteria": "", "comments": "", "text": "Summary\nSC BAU Sprint 3 Work\n\n---\n\nDescription\n# Helping to troubleshoot PBI Report/Workspace access issue for e-Comm team, for a user from Sigma\n## Madumitha messaged via Teams on 15/7\n# Assisted Liang with access to PBI Gateway connections for SC team", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 430}}
{"issue_key": "CSCI-148", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Jul/25 11:57 AM", "updated": "01/Oct/25 3:29 PM", "labels": [], "summary": "Data Engineering code standards", "description": "# *Define Coding and Naming Standards*\n#* Establish conventions for SQL, Python, and pipeline components.\n# *Document Deployment and CI/CD Patterns*\n#* Outline standardised approaches for Snowflake and DevOps workflows.\n# *Create Data Modelling Guidelines*\n#* Provide guidance on dimensional modelling, schema design, and partitioning strategies.\n# *Develop Onboarding and Knowledge Base Materials*\n#* Compile key resources, templates, and walkthroughs for new team members.", "acceptance_criteria": "* Core documentation is published and accessible to the team.\n* Standards and patterns are reviewed and approved by key stakeholders.\n* Onboarding materials are complete and tested with at least one new team member.", "comments": "@user can you please elaborate for which are would this be? Off hand, these are the following where we need coding standards:\n\n* Azure Data Factory\n* Snowflake Development (DDLs and Transformations)\n* Semantic Models\n* PowerBI Reports\n\nPls advise.\n\ncc: @user\n\nThis is more of a place-holder, Chloe to specify further which code standard is required. @user @user I will add the details as per Eugene’s comment\n\nHey @user ,\n\nAdding @user to this card as I think he and you could discuss to align what standards and templates are required for future MergeCo Data Platform. Then could divvy up documenting etc.\n\nThanks,\n\nHarrison\n\nWill combine this with [https://sigmahealthcare.atlassian.net/browse/CSCI-416|https://sigmahealthcare.atlassian.net/browse/CSCI-416]", "text": "Summary\nData Engineering code standards\n\n---\n\nDescription\n# *Define Coding and Naming Standards*\n#* Establish conventions for SQL, Python, and pipeline components.\n# *Document Deployment and CI/CD Patterns*\n#* Outline standardised approaches for Snowflake and DevOps workflows.\n# *Create Data Modelling Guidelines*\n#* Provide guidance on dimensional modelling, schema design, and partitioning strategies.\n# *Develop Onboarding and Knowledge Base Materials*\n#* Compile key resources, templates, and walkthroughs for new team members.\n\n---\n\nAcceptance Criteria\n* Core documentation is published and accessible to the team.\n* Standards and patterns are reviewed and approved by key stakeholders.\n* Onboarding materials are complete and tested with at least one new team member.\n\n---\n\nComments\n@user can you please elaborate for which are would this be? Off hand, these are the following where we need coding standards:\n\n* Azure Data Factory\n* Snowflake Development (DDLs and Transformations)\n* Semantic Models\n* PowerBI Reports\n\nPls advise.\n\ncc: @user\n\nThis is more of a place-holder, Chloe to specify further which code standard is required. @user @user I will add the details as per Eugene’s comment\n\nHey @user ,\n\nAdding @user to this card as I think he and you could discuss to align what standards and templates are required for future MergeCo Data Platform. Then could divvy up documenting etc.\n\nThanks,\n\nHarrison\n\nWill combine this with [https://sigmahealthcare.atlassian.net/browse/CSCI-416|https://sigmahealthcare.atlassian.net/browse/CSCI-416]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 431}}
{"issue_key": "CSCI-147", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Jul/25 11:56 AM", "updated": "01/Sep/25 2:46 AM", "labels": [], "summary": "Configure ExpressRoute", "description": "Azure DevOps Link: [User Story 191094 Configure ExpressRoute Peering|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/191094]\n\n* Set up the QoS policy on the SHIR VM and monitor performance and bandwidth usage during the 2 AM–6 AM window.\n* Adjust ADF settings (concurrent connections, DIUs, copy parallelism) as needed based on test results.\n* Capture baseline and incremental bandwidth data to inform future scaling decisions.\n* Review results together before considering any further controls at the ExpressRoute or firewall level.\n* If the VM-level QoS proves effective, we can avoid more complex network changes. Once the Megaport rebuild is complete and the new connectivity is validated, we’ll work with Brent and the cloud team to coordinate the next phase, including a phased ramp-up of Snowflake loads and the Private Endpoint migration.\n\n@user the task description above is as per CK’s email on on ExpressRoute Limitation on 11 Jul 2025. Please feel free to make any edits or updates as you see fit.", "acceptance_criteria": "", "comments": "Implementing the QoS on the SHIR machines but encountered an issue, currently working with MS support for resolving the issue.\n\nMigration and upgrade required by <who? Cloud Team?>.\n\nNot D&A team to perform.\n\nOnly Monitoring required in sprint 5 per [https://sigmahealthcare.atlassian.net/browse/CSCI-208|https://sigmahealthcare.atlassian.net/browse/CSCI-208]. Monitor ExpressRoute throughput utilisation.\n\n@user\n\n@user we already have a dashboard for monitoring the bandwidth usage of ExpressRoute but our traffic is not yet representative of prod levels. We do have a baseline we can work with.\n\nCan you please create a task for the following: \n\n* Adjust ADF settings (concurrent connections, DIUs, copy parallelism) as needed based on test results.", "text": "Summary\nConfigure ExpressRoute\n\n---\n\nDescription\nAzure DevOps Link: [User Story 191094 Configure ExpressRoute Peering|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/191094]\n\n* Set up the QoS policy on the SHIR VM and monitor performance and bandwidth usage during the 2 AM–6 AM window.\n* Adjust ADF settings (concurrent connections, DIUs, copy parallelism) as needed based on test results.\n* Capture baseline and incremental bandwidth data to inform future scaling decisions.\n* Review results together before considering any further controls at the ExpressRoute or firewall level.\n* If the VM-level QoS proves effective, we can avoid more complex network changes. Once the Megaport rebuild is complete and the new connectivity is validated, we’ll work with Brent and the cloud team to coordinate the next phase, including a phased ramp-up of Snowflake loads and the Private Endpoint migration.\n\n@user the task description above is as per CK’s email on on ExpressRoute Limitation on 11 Jul 2025. Please feel free to make any edits or updates as you see fit.\n\n---\n\nComments\nImplementing the QoS on the SHIR machines but encountered an issue, currently working with MS support for resolving the issue.\n\nMigration and upgrade required by <who? Cloud Team?>.\n\nNot D&A team to perform.\n\nOnly Monitoring required in sprint 5 per [https://sigmahealthcare.atlassian.net/browse/CSCI-208|https://sigmahealthcare.atlassian.net/browse/CSCI-208]. Monitor ExpressRoute throughput utilisation.\n\n@user\n\n@user we already have a dashboard for monitoring the bandwidth usage of ExpressRoute but our traffic is not yet representative of prod levels. We do have a baseline we can work with.\n\nCan you please create a task for the following: \n\n* Adjust ADF settings (concurrent connections, DIUs, copy parallelism) as needed based on test results.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 432}}
{"issue_key": "CSCI-146", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Jul/25 11:50 AM", "updated": "05/Aug/25 8:35 AM", "labels": [], "summary": "Meet with Rachel Wan on SSIS Transformation Packages", "description": "Sit down with team to ask questions and go through transformation logic as well as the access to go through oursleves", "acceptance_criteria": "", "comments": "confirmed their availabilities.\n\n@user to take Bhavya/samer`/adeel\n\nRachel Wan taken through what we are requesting for access to utilise Bhavya and Adeel for. We are hoping for 80% capacity from them starting 04/08 but we are waiting for in writing response from Rachel to confirm. Second session with Bhavya and Adeel completed to take them through.\n\nThanks,\n\nHarrison", "text": "Summary\nMeet with Rachel Wan on SSIS Transformation Packages\n\n---\n\nDescription\nSit down with team to ask questions and go through transformation logic as well as the access to go through oursleves\n\n---\n\nComments\nconfirmed their availabilities.\n\n@user to take Bhavya/samer`/adeel\n\nRachel Wan taken through what we are requesting for access to utilise Bhavya and Adeel for. We are hoping for 80% capacity from them starting 04/08 but we are waiting for in writing response from Rachel to confirm. Second session with Bhavya and Adeel completed to take them through.\n\nThanks,\n\nHarrison", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 433}}
{"issue_key": "CSCI-145", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Jul/25 11:48 AM", "updated": "05/Aug/25 8:35 AM", "labels": [], "summary": "Access to SSIS packages", "description": "Raise request in SNOW for access and send email to Rachel Wan to clarify", "acceptance_criteria": "", "comments": "Request Raised for SSIS package access : Request Number : *REQ0155399*\n\n \n\nRaised for Amit , Jess and Chloe\n\n@user can i get a screenshot of req and followups?\njust want to move this along.\n\nthanks\n\nHad a chat with Heshan. Access is granted now. However, we might need Visual Studio license to open the packages. He is trying out different options if possible.\n\nNo further update. Will check on Friday with Heshan\n\nCan only see a few SSIS packages, but can’t see all - investigating. @user\n\nThere is a lot of dependency on Stored procsedures in SSIS package. So, not much info can be pulled out directly from SSIS, except the names of the responsible Store Procs. Have got Dimension update store proc from Bhavya.", "text": "Summary\nAccess to SSIS packages\n\n---\n\nDescription\nRaise request in SNOW for access and send email to Rachel Wan to clarify\n\n---\n\nComments\nRequest Raised for SSIS package access : Request Number : *REQ0155399*\n\n \n\nRaised for Amit , Jess and Chloe\n\n@user can i get a screenshot of req and followups?\njust want to move this along.\n\nthanks\n\nHad a chat with Heshan. Access is granted now. However, we might need Visual Studio license to open the packages. He is trying out different options if possible.\n\nNo further update. Will check on Friday with Heshan\n\nCan only see a few SSIS packages, but can’t see all - investigating. @user\n\nThere is a lot of dependency on Stored procsedures in SSIS package. So, not much info can be pulled out directly from SSIS, except the names of the responsible Store Procs. Have got Dimension update store proc from Bhavya.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 434}}
{"issue_key": "CSCI-144", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Duplicate", "priority": "Medium", "created": "16/Jul/25 11:27 AM", "updated": "19/Nov/25 12:23 PM", "labels": [], "summary": "Source-To-Target for Facts and Dims in Prioritised Domains", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nSource-To-Target for Facts and Dims in Prioritised Domains", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 435}}
{"issue_key": "CSCI-143", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Duplicate", "priority": "Medium", "created": "16/Jul/25 11:25 AM", "updated": "19/Nov/25 12:27 PM", "labels": [], "summary": "Source-To-Target for Facts and Dims in Prioritised Domains", "description": "Source To Target mapping for all Facts and Dims identified for delivery in prioritised domains\n\n# Conformed Master Data\n# Inventory\n\nNeed to split Business Processes and Dims prioritised across this ticket and [https://sigmahealthcare.atlassian.net/browse/CSCI-118|https://sigmahealthcare.atlassian.net/browse/CSCI-118]", "acceptance_criteria": "", "comments": "", "text": "Summary\nSource-To-Target for Facts and Dims in Prioritised Domains\n\n---\n\nDescription\nSource To Target mapping for all Facts and Dims identified for delivery in prioritised domains\n\n# Conformed Master Data\n# Inventory\n\nNeed to split Business Processes and Dims prioritised across this ticket and [https://sigmahealthcare.atlassian.net/browse/CSCI-118|https://sigmahealthcare.atlassian.net/browse/CSCI-118]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 436}}
{"issue_key": "CSCI-140", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Jul/25 11:57 AM", "updated": "25/Jul/25 9:47 AM", "labels": [], "summary": "Raw Source Object for core Business Processes", "description": "Focusing on objects not connected to SC existing PBI Semantic Models, but other objects in BUS Matrix. Provide not just PBI05 object but the relevant objects from actual source for this object.", "acceptance_criteria": "Column G “Actual Source” in BUS Matrix filled in with relevant SQL Server/MySQL object details.\n\n*Next Step:* Source-To-Target Mapping", "comments": "Split into 2 tasks, one for Business Processes and Dims\n\nStarted working on Business Processes. Reviewing existing documents. Hvaing SSIS package access will help a lot\n\nOnly a few tables left now. Will be finishing off today\n\nSource mapping completed\n\n@usercan you book session for early next week - any fill outs to call out before DBA approaches this", "text": "Summary\nRaw Source Object for core Business Processes\n\n---\n\nDescription\nFocusing on objects not connected to SC existing PBI Semantic Models, but other objects in BUS Matrix. Provide not just PBI05 object but the relevant objects from actual source for this object.\n\n---\n\nAcceptance Criteria\nColumn G “Actual Source” in BUS Matrix filled in with relevant SQL Server/MySQL object details.\n\n*Next Step:* Source-To-Target Mapping\n\n---\n\nComments\nSplit into 2 tasks, one for Business Processes and Dims\n\nStarted working on Business Processes. Reviewing existing documents. Hvaing SSIS package access will help a lot\n\nOnly a few tables left now. Will be finishing off today\n\nSource mapping completed\n\n@usercan you book session for early next week - any fill outs to call out before DBA approaches this", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 437}}
{"issue_key": "CSCI-139", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Jul/25 11:57 AM", "updated": "30/Jul/25 2:57 PM", "labels": [], "summary": "Raw Source Object for core Business Processes and Dims", "description": "Focusing on objects connected to SC existing PBI Semantic Models. Provide not just PBI05 object but the relevant objects from actual source for this object.", "acceptance_criteria": "Column G “Actual Source” in BUS Matrix filled in with relevant SQL Server/MySQL object details\\ for objects included in SC PBI Semantic Models.\n\n*Next Step:* Source-To-Target Mapping", "comments": "Clairfy/review objects with Adeel.\n\nEmail sent to Adeep (28/7/25).\n\nIdentified we are missing 12 PBI objects with their lineage (these were not picked up in the initial discovery as these are indirect sources of other PBI objects).", "text": "Summary\nRaw Source Object for core Business Processes and Dims\n\n---\n\nDescription\nFocusing on objects connected to SC existing PBI Semantic Models. Provide not just PBI05 object but the relevant objects from actual source for this object.\n\n---\n\nAcceptance Criteria\nColumn G “Actual Source” in BUS Matrix filled in with relevant SQL Server/MySQL object details\\ for objects included in SC PBI Semantic Models.\n\n*Next Step:* Source-To-Target Mapping\n\n---\n\nComments\nClairfy/review objects with Adeel.\n\nEmail sent to Adeep (28/7/25).\n\nIdentified we are missing 12 PBI objects with their lineage (these were not picked up in the initial discovery as these are indirect sources of other PBI objects).", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 438}}
{"issue_key": "CSCI-138", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Jul/25 10:50 AM", "updated": "18/Jul/25 12:31 PM", "labels": [], "summary": "Collation of PBI Admin questions for Microsoft", "description": "Time to produce list of questions for Microsoft regarding PBI Admin and Setup\n\n*Questions*\n\n# How do we connect PBI to Snowflake with SSO and Privatelink most seamlessly", "acceptance_criteria": "", "comments": "# Ask MS how best to connect Snowflake to PBI via Gateway settings (how do they advise we set this up)\n## Ensure this solution enables us to maintain a ‘privatelink’ between SF and PBI\n# Show MS the bug between switching from a CW tenant to Sigma tenant and vice versa\n## Why does the left-hand pane disappear?\n## Why can’t we switch from CW back to Sigma? (Why does it default back to CW?)\n## Why does the webpage ‘glitch out’ when trying to open settings in a semantic model, from a different tenant?\n\nEmail sent to CW (+ contractors) + Sigma team (16/7/25) to see if any other team members wish to raise any questions for MS.\nHave asked for requests to be sent by COB 17/7/25.\n\n@user Ask if for paginated reports we require an on-prem gateway? (Q came from Zhaneta/engineering team - they have been told this in the past)\n\nFrom Dili:\n\n+_From a DG perspective, if MS can provide information on whether PBI has the ability to bring in any access controls i.e. RBAC rules from Snowflake, on both desktop and Online Service (in case that differs)_+\n\nEmail sent - 18/7/25", "text": "Summary\nCollation of PBI Admin questions for Microsoft\n\n---\n\nDescription\nTime to produce list of questions for Microsoft regarding PBI Admin and Setup\n\n*Questions*\n\n# How do we connect PBI to Snowflake with SSO and Privatelink most seamlessly\n\n---\n\nComments\n# Ask MS how best to connect Snowflake to PBI via Gateway settings (how do they advise we set this up)\n## Ensure this solution enables us to maintain a ‘privatelink’ between SF and PBI\n# Show MS the bug between switching from a CW tenant to Sigma tenant and vice versa\n## Why does the left-hand pane disappear?\n## Why can’t we switch from CW back to Sigma? (Why does it default back to CW?)\n## Why does the webpage ‘glitch out’ when trying to open settings in a semantic model, from a different tenant?\n\nEmail sent to CW (+ contractors) + Sigma team (16/7/25) to see if any other team members wish to raise any questions for MS.\nHave asked for requests to be sent by COB 17/7/25.\n\n@user Ask if for paginated reports we require an on-prem gateway? (Q came from Zhaneta/engineering team - they have been told this in the past)\n\nFrom Dili:\n\n+_From a DG perspective, if MS can provide information on whether PBI has the ability to bring in any access controls i.e. RBAC rules from Snowflake, on both desktop and Online Service (in case that differs)_+\n\nEmail sent - 18/7/25", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 439}}
{"issue_key": "CSCI-136", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Jul/25 9:37 AM", "updated": "24/Nov/25 9:11 AM", "labels": [], "summary": "Manhattan Active (MySQL) Integration part 1", "description": "DATA IN for critical identified objects from Manhattan Active (MySQL) db\n\n* confirm with RFoong decision for data lake extraction rather than API\n* Whitelinsting\n* Credential needed for GCP - login and testing connection\n* Create pipeline\n\n@user", "acceptance_criteria": "Definition of done:\n\n* Extract meta created\n* Linked services created\n* Pipeline run end to end", "comments": "Decision required to be made on whether to connect to MySQL MAWM copy or connect directly through REST API to MAWM. Providing some notes from discussions below. Meeting setup for Thursday 31/07/2025 for this.\n\n@userlet’s confirm our decision on this source early next week with @user and @user. Rachel F. mentioned Gustavo would be on leave for the whole September so we might need to schedule a call with them by end of August.\n\nStatus as of Monday (15.09.2025):\n\nWill split tasks with you @user - will share load with @user\n\nIssue split into:\n|CSCI-461|Manhattan Active (MySQL) Integration part 2|\n\nnow connecting to manhattan active - new credential requred as current user is being decomissioned.\n\n@user working on a custom proc for MYSQL to get the datatypes from the source system.\n\nIssue split into:\n|CSCI-524|Manhattan Active (MySQL) Integration - (Work on MySQL specific Procedure )|\n\nreadonlyuser deprecated and need new credentials to connect to Manhattan active.\n\nGot access to a MYSQL database and work in progress to create a procedure in snowflake to get source data types. Once finished the ticket will go back to blocked until the service account is created for Manhattan Active database.\n\nCredential Received.\n\nDiscovery exercise completed. \n\nQuestions posted to database team and continue the second part.", "text": "Summary\nManhattan Active (MySQL) Integration part 1\n\n---\n\nDescription\nDATA IN for critical identified objects from Manhattan Active (MySQL) db\n\n* confirm with RFoong decision for data lake extraction rather than API\n* Whitelinsting\n* Credential needed for GCP - login and testing connection\n* Create pipeline\n\n@user\n\n---\n\nAcceptance Criteria\nDefinition of done:\n\n* Extract meta created\n* Linked services created\n* Pipeline run end to end\n\n---\n\nComments\nDecision required to be made on whether to connect to MySQL MAWM copy or connect directly through REST API to MAWM. Providing some notes from discussions below. Meeting setup for Thursday 31/07/2025 for this.\n\n@userlet’s confirm our decision on this source early next week with @user and @user. Rachel F. mentioned Gustavo would be on leave for the whole September so we might need to schedule a call with them by end of August.\n\nStatus as of Monday (15.09.2025):\n\nWill split tasks with you @user - will share load with @user\n\nIssue split into:\n|CSCI-461|Manhattan Active (MySQL) Integration part 2|\n\nnow connecting to manhattan active - new credential requred as current user is being decomissioned.\n\n@user working on a custom proc for MYSQL to get the datatypes from the source system.\n\nIssue split into:\n|CSCI-524|Manhattan Active (MySQL) Integration - (Work on MySQL specific Procedure )|\n\nreadonlyuser deprecated and need new credentials to connect to Manhattan active.\n\nGot access to a MYSQL database and work in progress to create a procedure in snowflake to get source data types. Once finished the ticket will go back to blocked until the service account is created for Manhattan Active database.\n\nCredential Received.\n\nDiscovery exercise completed. \n\nQuestions posted to database team and continue the second part.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 440}}
{"issue_key": "CSCI-135", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Jul/25 9:35 AM", "updated": "28/Aug/25 10:39 AM", "labels": [], "summary": "Manhattan Scale Integration", "description": "DATA IN for critical identified objects from Manhattan Scale db (ILS)\n\nImplement data ingestion for critical identity objects from the on-premises Manhattan Scale (ILS) database into the cloud data platform. This includes configuring the necessary Azure Data Factory (ADF) components to support automated and reliable data flow.\n\nLatest update;\n\n* Ticket raised. REQ0156142\n* Currently with the infrastructure cloud team - Brent - Cloud services manager.\n* !image-20250729-063227 (1ed4c70f-781d-46cb-9ced-b8db1e7f0691).png|width=961,height=959,alt=\"image-20250729-063227.png\"!", "acceptance_criteria": "Definition of done:\n\n* Extract meta created (@user )\n* Linked services created\n* Pipeline run end to end\n* -Whitelisting of on-prem database server. (Done by @user )-", "comments": "ils db being added to firewall per new ticket from CK per Chloe request\n\nFollowing up with @user on this\n\n@user can you follow this up\n\n@user will do\n\nHave followed up with @user on this - have updated description - and will get further updates on this shortly.\n\nUpdate:\n- need to whiltelist this in Azure firewall.\n\n* Ticket raised. REQ0156142\n* Currently with the infrastructure cloud team - Brent.\n*\n\nHave caught up with @user CK|\nThis is tested and resolved. \nCK will update this.\n\nHave updated acceptance criteria \n\n* whitelisting on prem server done by @user - TYSM!\n\n@user @user I have split this into part 1 and part 2 so that if this doesn’t finish there will be work flowing to the next sprint.", "text": "Summary\nManhattan Scale Integration\n\n---\n\nDescription\nDATA IN for critical identified objects from Manhattan Scale db (ILS)\n\nImplement data ingestion for critical identity objects from the on-premises Manhattan Scale (ILS) database into the cloud data platform. This includes configuring the necessary Azure Data Factory (ADF) components to support automated and reliable data flow.\n\nLatest update;\n\n* Ticket raised. REQ0156142\n* Currently with the infrastructure cloud team - Brent - Cloud services manager.\n* !image-20250729-063227 (1ed4c70f-781d-46cb-9ced-b8db1e7f0691).png|width=961,height=959,alt=\"image-20250729-063227.png\"!\n\n---\n\nAcceptance Criteria\nDefinition of done:\n\n* Extract meta created (@user )\n* Linked services created\n* Pipeline run end to end\n* -Whitelisting of on-prem database server. (Done by @user )-\n\n---\n\nComments\nils db being added to firewall per new ticket from CK per Chloe request\n\nFollowing up with @user on this\n\n@user can you follow this up\n\n@user will do\n\nHave followed up with @user on this - have updated description - and will get further updates on this shortly.\n\nUpdate:\n- need to whiltelist this in Azure firewall.\n\n* Ticket raised. REQ0156142\n* Currently with the infrastructure cloud team - Brent.\n*\n\nHave caught up with @user CK|\nThis is tested and resolved. \nCK will update this.\n\nHave updated acceptance criteria \n\n* whitelisting on prem server done by @user - TYSM!\n\n@user @user I have split this into part 1 and part 2 so that if this doesn’t finish there will be work flowing to the next sprint.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 441}}
{"issue_key": "CSCI-134", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "15/Jul/25 9:33 AM", "updated": "12/Aug/25 1:27 PM", "labels": [], "summary": "SQL3 (PDB14) Integration Historical Load (Parquet)", "description": "DATA IN for critical identified objects from SQL3 MyPOS db\n\n|TDB14|TransactionStorage|BranchOrderItems|Yes|3 mins Merge from Store|467,808,855|\n|TDB14|TransactionStorage|BranchOrders|Yes|3 mins Merge from Store|22,340,006|\n|TDB14|TransactionStorage|Store_IncrementalSOHChanges|Yes|3 mins Merge from Store|8,302,390|\n|TDB14|TransactionStorage|Transactions|Yes|3 mins Merge from Store|825,948,968|\n|TDB14|TransactionStorage|Transactions_Invoice|Yes|3 mins Merge from Store|272,348,977|", "acceptance_criteria": "Definition of done:\n\n* Extract meta created\n* Linked services created\n* Pipeline run end to end", "comments": "SQL3 Server upgraded, does that require new IP Address to be provided to us, whitelist etc.\n\n@user Does this require new whitelisting etc? Need to confirm with @user\n\nWorking on bridging tasks between historical and delta(ongoing) run.\n\nDone.", "text": "Summary\nSQL3 (PDB14) Integration Historical Load (Parquet)\n\n---\n\nDescription\nDATA IN for critical identified objects from SQL3 MyPOS db\n\n|TDB14|TransactionStorage|BranchOrderItems|Yes|3 mins Merge from Store|467,808,855|\n|TDB14|TransactionStorage|BranchOrders|Yes|3 mins Merge from Store|22,340,006|\n|TDB14|TransactionStorage|Store_IncrementalSOHChanges|Yes|3 mins Merge from Store|8,302,390|\n|TDB14|TransactionStorage|Transactions|Yes|3 mins Merge from Store|825,948,968|\n|TDB14|TransactionStorage|Transactions_Invoice|Yes|3 mins Merge from Store|272,348,977|\n\n---\n\nAcceptance Criteria\nDefinition of done:\n\n* Extract meta created\n* Linked services created\n* Pipeline run end to end\n\n---\n\nComments\nSQL3 Server upgraded, does that require new IP Address to be provided to us, whitelist etc.\n\n@user Does this require new whitelisting etc? Need to confirm with @user\n\nWorking on bridging tasks between historical and delta(ongoing) run.\n\nDone.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 442}}
{"issue_key": "CSCI-133", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Jul/25 3:41 PM", "updated": "01/Aug/25 4:51 PM", "labels": [], "summary": "Analyse timing of current Supply Chain Semantic Model refresh vs SQL Server refresh", "description": "Upon completion of [https://sigmahealthcare.atlassian.net/browse/CSCI-132|https://sigmahealthcare.atlassian.net/browse/CSCI-132], need to analyse result findings with details to when each semantic model is being scheduled to refresh.\n\nNeed to understand the lag time and how long an average refresh is taking each day, per model.", "acceptance_criteria": "", "comments": "need to show Rachel Foong for review\n\nExpected to be done today", "text": "Summary\nAnalyse timing of current Supply Chain Semantic Model refresh vs SQL Server refresh\n\n---\n\nDescription\nUpon completion of [https://sigmahealthcare.atlassian.net/browse/CSCI-132|https://sigmahealthcare.atlassian.net/browse/CSCI-132], need to analyse result findings with details to when each semantic model is being scheduled to refresh.\n\nNeed to understand the lag time and how long an average refresh is taking each day, per model.\n\n---\n\nComments\nneed to show Rachel Foong for review\n\nExpected to be done today", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 443}}
{"issue_key": "CSCI-132", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Jul/25 3:40 PM", "updated": "18/Jul/25 8:51 PM", "labels": [], "summary": "Analyse timing of on-prem SQL server table refreshes", "description": "Currently, PBI05 SQL Server is at 95-100% CPU capacity. Issue stems from daily batch jobs running whilst SC team are trying to refresh their semantic models.\n\nTo help ease this issue, need to develop a report/dashboard that details the daily timings of when each table in PBI05 is being refreshed.", "acceptance_criteria": "", "comments": "Initially was able to retrieve a ‘last modified’ time for all tables and views in PBI05 using metadata from the system table *sys.tables*.\n\nHowever upon further inspection, this modified time appears to only be when a table was last altered structurally, eg: creating a new column, or performing a DROP and CREATE statement etc\n\nWas able to locate a different system table: *sys.dm_db_index_usage_stats*.\n\nThis table stores index usage stats. We can use the last modified date from this table to form as an approximation to when a table was last modified (if any data was inserted or deleted from a table, this is included within the modified date logic).\n\nCurrently need to request additional access from DB team to query this table (require VIEW SERVER STATE permissions). Have spoken to Rachel Wan and she is ok with approving this permission (at least for PBI05 initially). Awaiting access.\n\n{code:sql}SELECT TOP 1 *\nFROM sys.dm_db_index_usage_stats\nWHERE database_id = DB_ID( 'SupplyChain')\nAND OBJECT_ID in (select OBJECT_ID(name) from sys.tables)\nORDER BY last_user_update DESC;{code}\n\nhave split task to [part 2|https://sigmahealthcare.atlassian.net/browse/CSCI-154] for further work\nthis task can now be adjusted for sprint points and ready to be closed.\n\n @user", "text": "Summary\nAnalyse timing of on-prem SQL server table refreshes\n\n---\n\nDescription\nCurrently, PBI05 SQL Server is at 95-100% CPU capacity. Issue stems from daily batch jobs running whilst SC team are trying to refresh their semantic models.\n\nTo help ease this issue, need to develop a report/dashboard that details the daily timings of when each table in PBI05 is being refreshed.\n\n---\n\nComments\nInitially was able to retrieve a ‘last modified’ time for all tables and views in PBI05 using metadata from the system table *sys.tables*.\n\nHowever upon further inspection, this modified time appears to only be when a table was last altered structurally, eg: creating a new column, or performing a DROP and CREATE statement etc\n\nWas able to locate a different system table: *sys.dm_db_index_usage_stats*.\n\nThis table stores index usage stats. We can use the last modified date from this table to form as an approximation to when a table was last modified (if any data was inserted or deleted from a table, this is included within the modified date logic).\n\nCurrently need to request additional access from DB team to query this table (require VIEW SERVER STATE permissions). Have spoken to Rachel Wan and she is ok with approving this permission (at least for PBI05 initially). Awaiting access.\n\n{code:sql}SELECT TOP 1 *\nFROM sys.dm_db_index_usage_stats\nWHERE database_id = DB_ID( 'SupplyChain')\nAND OBJECT_ID in (select OBJECT_ID(name) from sys.tables)\nORDER BY last_user_update DESC;{code}\n\nhave split task to [part 2|https://sigmahealthcare.atlassian.net/browse/CSCI-154] for further work\nthis task can now be adjusted for sprint points and ready to be closed.\n\n @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 444}}
{"issue_key": "CSCI-131", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "14/Jul/25 2:49 PM", "updated": "01/Sep/25 12:29 PM", "labels": [], "summary": "MergeCo Conformed Reporting - Develop Inventory Agg Fact", "description": "* Inventory\n** SOH (Units + $)\n*** Stock on Hand\n** DOI\n*** Days of Inventory\n**** Avg SOH $ / avg Daily Sales $ at Cost (last 12 weeks)\n\n*Tasks done:*\n\n* Performed Data Discovery of SCAX2012 and PDB08 to locate and find where this data exists in CW (2 story points)\n* Developed SQL views of data (1 story point)\n* Performed validation/sense checking of data (0.5 story points)\n* Gathered all requirements of what data we need to ingest in PBI05 (0.5 story points)", "acceptance_criteria": "Develop facts from CW to create these KPIs\n\n* -SOH Units-\n* -SOH $-\n* -Days of Inventory-", "comments": "Could start this in Sprint 3 but then move to 4.\n\nBreakout into 2 tasks\n\n# Analysis of required objects to be ingested into SF from db’s\n# Building of aggregations and ingestion\n## ADF Pipeline\n\nCurrently fleshing this out and will fill out details for this + csci-205 + csci-206\n\nSOH - PBI05.[BI_Presentation].[dbo].[AX_FCT_DC_STOCK]\nDaily Sales - PBI05.[BI_Presentation].[dbo].[AX_FCT_DC_SALES]\n\nNeed to check/confirm what CW equivalent is:\n\n* Confirmed\n* Invoiced\n* Ordered\n\nSC use PBI05.[SupplyChain].[Scale].[*shipment_detail*] and PBI05.[SupplyChain].[Scale].[*shipment_header*] for “Requested Qty” and “Delivered Qty”\n\nI will convert this to a checklist in AC\n\nprioritise:\n\n* inventory\n* outbound performance\n\nSC use [SupplyChain].[SCAX].[PurchLineSimple] - purch status contains 4 PO statuses:\n\n* 1 = Open Order\n* 2 = Received\n* 3 = Invoiced\n* 4 = Cancelled\n\nNeed to discuss if 2 = “Confirmed” from Sigma’s side. If so can leverage these statuses to define the Outbound Performance metrics\n\nFor DOT% - CW have an existing metric known as DIFOT% - which is % of orders that we on-time, which is defined as within 3 business days (with public holidays included)\n\nUnderlying source of these tables is PDB19B.SCAX2012.INVENTSUM + INVENTDIM\n\nHave got across inventory\nCurrently on:\n- discovery - outbound performance\n\nudpated AC @user\n\nCurrently on outbound performance @user\n\nto go thru with @useron Friday\n\n----\n\naim:\n\n* to break out by prioritisation of reports\n\nWill split out this ticket @user today \n\nnote - outbound performance.", "text": "Summary\nMergeCo Conformed Reporting - Develop Inventory Agg Fact\n\n---\n\nDescription\n* Inventory\n** SOH (Units + $)\n*** Stock on Hand\n** DOI\n*** Days of Inventory\n**** Avg SOH $ / avg Daily Sales $ at Cost (last 12 weeks)\n\n*Tasks done:*\n\n* Performed Data Discovery of SCAX2012 and PDB08 to locate and find where this data exists in CW (2 story points)\n* Developed SQL views of data (1 story point)\n* Performed validation/sense checking of data (0.5 story points)\n* Gathered all requirements of what data we need to ingest in PBI05 (0.5 story points)\n\n---\n\nAcceptance Criteria\nDevelop facts from CW to create these KPIs\n\n* -SOH Units-\n* -SOH $-\n* -Days of Inventory-\n\n---\n\nComments\nCould start this in Sprint 3 but then move to 4.\n\nBreakout into 2 tasks\n\n# Analysis of required objects to be ingested into SF from db’s\n# Building of aggregations and ingestion\n## ADF Pipeline\n\nCurrently fleshing this out and will fill out details for this + csci-205 + csci-206\n\nSOH - PBI05.[BI_Presentation].[dbo].[AX_FCT_DC_STOCK]\nDaily Sales - PBI05.[BI_Presentation].[dbo].[AX_FCT_DC_SALES]\n\nNeed to check/confirm what CW equivalent is:\n\n* Confirmed\n* Invoiced\n* Ordered\n\nSC use PBI05.[SupplyChain].[Scale].[*shipment_detail*] and PBI05.[SupplyChain].[Scale].[*shipment_header*] for “Requested Qty” and “Delivered Qty”\n\nI will convert this to a checklist in AC\n\nprioritise:\n\n* inventory\n* outbound performance\n\nSC use [SupplyChain].[SCAX].[PurchLineSimple] - purch status contains 4 PO statuses:\n\n* 1 = Open Order\n* 2 = Received\n* 3 = Invoiced\n* 4 = Cancelled\n\nNeed to discuss if 2 = “Confirmed” from Sigma’s side. If so can leverage these statuses to define the Outbound Performance metrics\n\nFor DOT% - CW have an existing metric known as DIFOT% - which is % of orders that we on-time, which is defined as within 3 business days (with public holidays included)\n\nUnderlying source of these tables is PDB19B.SCAX2012.INVENTSUM + INVENTDIM\n\nHave got across inventory\nCurrently on:\n- discovery - outbound performance\n\nudpated AC @user\n\nCurrently on outbound performance @user\n\nto go thru with @useron Friday\n\n----\n\naim:\n\n* to break out by prioritisation of reports\n\nWill split out this ticket @user today \n\nnote - outbound performance.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 445}}
{"issue_key": "CSCI-128", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "10/Jul/25 12:20 PM", "updated": "18/Jul/25 8:45 PM", "labels": [], "summary": "Power BI - Create CI/CD POC with Git", "description": "Needed for PBI deployment\n\nCreated this ticket on the back of 10am meeting with CK and @user - thanks @user for letting me know\n\nHave placed 3 sprint points as a placeholder for now.\n\nFeel free to edit this ticket where needed, or contact me so that I can do the admin/make necessary linkages.", "acceptance_criteria": "", "comments": "@user i created a corresponding feature in DevOps [Feature 196244 Semantic Model and PowerBI Git Integration|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/196244]. \n\ncc: @user @user\n\nGit - done.\nPOC - done\n\nnext steps\nCreating report out of semantic model - in progress\n\nNOte:\n- eugene @user to work with Jess @user - \nbook a meeting Tues for demo.\n @user @user @user\n\n@user can we please split this in 2? one for CI and the other for CD? CI is completed\n\n@user \n\n@user - done\n[https://sigmahealthcare.atlassian.net/browse/CSCI-155|https://sigmahealthcare.atlassian.net/browse/CSCI-155] [https://sigmahealthcare.atlassian.net/browse/CSCI-156|https://sigmahealthcare.atlassian.net/browse/CSCI-156]\n\n@user FYI the split out tasks", "text": "Summary\nPower BI - Create CI/CD POC with Git\n\n---\n\nDescription\nNeeded for PBI deployment\n\nCreated this ticket on the back of 10am meeting with CK and @user - thanks @user for letting me know\n\nHave placed 3 sprint points as a placeholder for now.\n\nFeel free to edit this ticket where needed, or contact me so that I can do the admin/make necessary linkages.\n\n---\n\nComments\n@user i created a corresponding feature in DevOps [Feature 196244 Semantic Model and PowerBI Git Integration|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/196244]. \n\ncc: @user @user\n\nGit - done.\nPOC - done\n\nnext steps\nCreating report out of semantic model - in progress\n\nNOte:\n- eugene @user to work with Jess @user - \nbook a meeting Tues for demo.\n @user @user @user\n\n@user can we please split this in 2? one for CI and the other for CD? CI is completed\n\n@user \n\n@user - done\n[https://sigmahealthcare.atlassian.net/browse/CSCI-155|https://sigmahealthcare.atlassian.net/browse/CSCI-155] [https://sigmahealthcare.atlassian.net/browse/CSCI-156|https://sigmahealthcare.atlassian.net/browse/CSCI-156]\n\n@user FYI the split out tasks", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 446}}
{"issue_key": "CSCI-127", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "08/Jul/25 10:05 AM", "updated": "18/Jul/25 3:51 PM", "labels": [], "summary": "Snowflake User Creation", "description": "Create Snowflake users for EDP team members", "acceptance_criteria": "", "comments": "Chloe: Access granted from Chloe to Jess, Eugene, Chloe, Amit, Phil and Harrison\n\nTo create new user accounts for Bavya and Adeel and Sam in DBA team", "text": "Summary\nSnowflake User Creation\n\n---\n\nDescription\nCreate Snowflake users for EDP team members\n\n---\n\nComments\nChloe: Access granted from Chloe to Jess, Eugene, Chloe, Amit, Phil and Harrison\n\nTo create new user accounts for Bavya and Adeel and Sam in DBA team", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 447}}
{"issue_key": "CSCI-126", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "03/Jul/25 10:26 AM", "updated": "14/Jul/25 2:39 PM", "labels": [], "summary": "Snowflake - ADF service user - Key Pair authentication", "description": "ADF to SF via User Account, currently uses password however no longer available for SF sign in. I have put in a temporary solution: update user type as LEGACY SERVICE, however this method will fade out on 1st Nov. \n\n Key pair authentication method sign in required. \n\nTo do: switch all Snowflake service accounts to use Key Pair authentication.", "acceptance_criteria": "", "comments": "I have generated the key pairs and tested the connection successfully. \n\n \n\n \n\nHowever I am not able to upload to Azure Key Vault. To store a private key in Azure Key Vault, it needs to be combined with a certificate. I have to generate it as a pfx or pem files with CA/CSR signed certificate which seems very involved. My pem file doesn't include a certificate at this stage.\n\nAnjali has raised ticket *RITM0169851* created to get help from the security or network team to issue certificates we can use.\n\nSuccessfully uploaded the pem file to Key Vault using Azure CLI. Eugene to review method to add private key and passphrase via Terraform.\n\nThis is completed, please see [Task 195726 Create SSH KeyPair for Snowflake Authentication|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/195726]. Deployed in Dev, QA/SIT, Prod", "text": "Summary\nSnowflake - ADF service user - Key Pair authentication\n\n---\n\nDescription\nADF to SF via User Account, currently uses password however no longer available for SF sign in. I have put in a temporary solution: update user type as LEGACY SERVICE, however this method will fade out on 1st Nov. \n\n Key pair authentication method sign in required. \n\nTo do: switch all Snowflake service accounts to use Key Pair authentication.\n\n---\n\nComments\nI have generated the key pairs and tested the connection successfully. \n\n \n\n \n\nHowever I am not able to upload to Azure Key Vault. To store a private key in Azure Key Vault, it needs to be combined with a certificate. I have to generate it as a pfx or pem files with CA/CSR signed certificate which seems very involved. My pem file doesn't include a certificate at this stage.\n\nAnjali has raised ticket *RITM0169851* created to get help from the security or network team to issue certificates we can use.\n\nSuccessfully uploaded the pem file to Key Vault using Azure CLI. Eugene to review method to add private key and passphrase via Terraform.\n\nThis is completed, please see [Task 195726 Create SSH KeyPair for Snowflake Authentication|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/195726]. Deployed in Dev, QA/SIT, Prod", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 448}}
{"issue_key": "CSCI-125", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Jul/25 11:42 AM", "updated": "18/Jul/25 3:51 PM", "labels": [], "summary": "SCAX2012 Integration", "description": "DATA IN for critical identified objects from SCAX2012 db.\n\n|TDB19b|PDB19B|SCAX2012|CUSTINVOICEJOUR|\n|TDB19b|PDB19B|SCAX2012|CustInvoiceTrans|\n|TDB19b|PDB19B|SCAX2012|INVENTDIM|\n|TDB19b|PDB19B|SCAX2012|INVENTJOURNALNAME|\n|TDB19b|PDB19B|SCAX2012|INVENTJOURNALTABLE|\n|TDB19b|PDB19B|SCAX2012|INVENTJOURNALTRANS|\n|TDB19b|PDB19B|SCAX2012|INVENTLOCATION|\n|TDB19b|PDB19B|SCAX2012|INVENTSUM|\n|TDB19b|PDB19B|SCAX2012|PurchLine|\n|TDB19b|PDB19B|SCAX2012|VENDINVOICETRANS|", "acceptance_criteria": "Definition of done:\n\n* Extract meta created\n* Linked services created\n* Pipeline run end to end", "comments": "Chloe: 3 tables ingested in Full Load pattern, need to test a delta load.\n\nAdeel provided PK and no deletes. To review. Eugene mentioned they have seen deletes on some tables. Delete {color:#ff991f}checking{color} table. Soft deletes pattern DATA IN. Lower priority but plan in timeline post integrations\n\nJess C: CK document has BUS Matrix, analyse vs existing document. Overall CW including SC.\n\nAmit required to play bigger role in existing legacy information.\n\nThe following tables have been fully ingested to SCAX2012:\n\n# INVENTJOURNALTABLE – DONE (historical + delta)\n# INVENTJOURNALTRANS – DONE (historical + delta)\n# INVENTSUM – DONE (historical + delta)\n# INVENTDIM – DONE (direct)\n# INVENTJOURNALNAME – DONE (direct)\n# INVENTLOCATION – DONE (direct)\n\nWe need to implement a pattern for parallel load for large 3 of parquet files ~3.8k files.\n\n--\n\nThe other tables which need to be ingested via 2 patterns (historical + delta) have very large # of parquet files.\n\nWith the existing pipeline it will take days to complete a full historical load. It takes 1 ~ 1.2min to ingest 1 parquet file. \n\n|TABLE_NAME|FILE_COUNT|\n|CUSTINVOICETRANS|3857 (~77hrs or 3.2 days)|\n|PURCHLINE|1815 (~36hrs or 1.5 days)|\n|CUSTINVOICEJOUR|303 (~6hrs)|\n|VENDINVOICETRANS|1811 (~36hrs or 1.5 days)|\n\nI tested a pattern where we can bulk load all the parquet files from external stage (blob storage). \n\nNext step: I need Adeel to copy the files to the specified location. I dont have access to do it. Will also request access from Eugene to do so myself.", "text": "Summary\nSCAX2012 Integration\n\n---\n\nDescription\nDATA IN for critical identified objects from SCAX2012 db.\n\n|TDB19b|PDB19B|SCAX2012|CUSTINVOICEJOUR|\n|TDB19b|PDB19B|SCAX2012|CustInvoiceTrans|\n|TDB19b|PDB19B|SCAX2012|INVENTDIM|\n|TDB19b|PDB19B|SCAX2012|INVENTJOURNALNAME|\n|TDB19b|PDB19B|SCAX2012|INVENTJOURNALTABLE|\n|TDB19b|PDB19B|SCAX2012|INVENTJOURNALTRANS|\n|TDB19b|PDB19B|SCAX2012|INVENTLOCATION|\n|TDB19b|PDB19B|SCAX2012|INVENTSUM|\n|TDB19b|PDB19B|SCAX2012|PurchLine|\n|TDB19b|PDB19B|SCAX2012|VENDINVOICETRANS|\n\n---\n\nAcceptance Criteria\nDefinition of done:\n\n* Extract meta created\n* Linked services created\n* Pipeline run end to end\n\n---\n\nComments\nChloe: 3 tables ingested in Full Load pattern, need to test a delta load.\n\nAdeel provided PK and no deletes. To review. Eugene mentioned they have seen deletes on some tables. Delete {color:#ff991f}checking{color} table. Soft deletes pattern DATA IN. Lower priority but plan in timeline post integrations\n\nJess C: CK document has BUS Matrix, analyse vs existing document. Overall CW including SC.\n\nAmit required to play bigger role in existing legacy information.\n\nThe following tables have been fully ingested to SCAX2012:\n\n# INVENTJOURNALTABLE – DONE (historical + delta)\n# INVENTJOURNALTRANS – DONE (historical + delta)\n# INVENTSUM – DONE (historical + delta)\n# INVENTDIM – DONE (direct)\n# INVENTJOURNALNAME – DONE (direct)\n# INVENTLOCATION – DONE (direct)\n\nWe need to implement a pattern for parallel load for large 3 of parquet files ~3.8k files.\n\n--\n\nThe other tables which need to be ingested via 2 patterns (historical + delta) have very large # of parquet files.\n\nWith the existing pipeline it will take days to complete a full historical load. It takes 1 ~ 1.2min to ingest 1 parquet file. \n\n|TABLE_NAME|FILE_COUNT|\n|CUSTINVOICETRANS|3857 (~77hrs or 3.2 days)|\n|PURCHLINE|1815 (~36hrs or 1.5 days)|\n|CUSTINVOICEJOUR|303 (~6hrs)|\n|VENDINVOICETRANS|1811 (~36hrs or 1.5 days)|\n\nI tested a pattern where we can bulk load all the parquet files from external stage (blob storage). \n\nNext step: I need Adeel to copy the files to the specified location. I dont have access to do it. Will also request access from Eugene to do so myself.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 449}}
{"issue_key": "CSCI-124", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Jul/25 11:42 AM", "updated": "28/Aug/25 2:16 PM", "labels": [], "summary": "AXLink Integration", "description": "DATA IN for critical identified objects from AXLink.\n\n|Warehouse_ReservedItems|", "acceptance_criteria": "Definition of done:\n\n* -Extract meta created-\n* -Linked services created-\n* -Pipeline run end to end-", "comments": "planning to catchup with @user \n\n- what is a 'bridging db”", "text": "Summary\nAXLink Integration\n\n---\n\nDescription\nDATA IN for critical identified objects from AXLink.\n\n|Warehouse_ReservedItems|\n\n---\n\nAcceptance Criteria\nDefinition of done:\n\n* -Extract meta created-\n* -Linked services created-\n* -Pipeline run end to end-\n\n---\n\nComments\nplanning to catchup with @user \n\n- what is a 'bridging db”", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 450}}
{"issue_key": "CSCI-123", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Jul/25 11:39 AM", "updated": "25/Jul/25 12:22 PM", "labels": [], "summary": "DFIO Manhattan Source System", "description": "Follow up on understanding who can assist in obtaining source system details for DFIO data for a future state solution.\n\nWho can provide this info?\n\nWhat is system details?\n\nHow do we connect?\n\nWhat objects do we require?", "acceptance_criteria": "", "comments": "Requested Login for DFIO SCI .\n\nAccess to be provided to Amit for credentials to login and review.\n\nRequested access to setup session with manhattan team.\n\n# My SQL environment setup for Manhattan - need to confirm this is a Copy of data from Source System and frequency of timing.\n## Are we to connect to MySQL Manhattan Active Copy or is another Source System Required?\n## Is CK able to assist in this? Does he know who if not able to know about MySQL environment setup\n\n*Next Step:* Amit to share email and book call with required Infra/CK for MySQL understanding\n\nReached out to Manhattan for time slots availability for a discussion. Bhavi Sanjay Mehta <[bhmehta@manh.com|mailto:bhmehta@manh.com]> is the contact person from Manhattan\n\nManhattan has mentioned that they have shared the required details with Project Management team . Have advised to reach out to Amila. Dropped a note to him. Will update once I hear from him.\n\nHad a word with Amila. He mentioned that he has some info and will share with us soon. \nWill talk to Supply Chain team for current extract of data and source report.\n\nNo further update from Amila\n\nFollowed up again with Amila. Asked for an ETA.@user\n\nGot Below responses from Amila. I think we got our answers for now. \n\n# Details of MySQL environment\n\n \n\nAE: (from attached email)\n\nDirect from our Gateway (IP: 192.168.42.65 or 88 – both PowerBI gateway servers)\n\nTo MySQL server (172.19.232.11)\n\nThrough IP whitelisting/firewall rules\n\nStandard TCP connection on port 3306\n\n \n\n \n\n# Best way to connect mainly from data extraction purposes\n\nAE: Above, Analitics team use a MYSQL connector . not sure how they use it. Rachel Foong should be able to provide more info\n\n \n\n# Is it the live data source or a copy\n\nAE: Copy\n\n \n\n# If so , refresh frequency\n\nAE: (from attached email)\n\n“We worked with the CloudOps team and confirmed that there is no lag or issue with the Data Save replica database.”\n\n \n\n# Any Schema Documentation\n\nWe don’t have this, maybe check with  Rachel Foong\n\n(see “MAWM – Questions” mail)\n\n \n\n# Any mapping document between old Scale and New MAWM Data\n\nWe don’t have this, maybe check with  Rachel Foong\n\nNext steps: \nwhat is the best way to connect for ingestion purpose. I think we can explore Azure/Snowflake options and reconnect later. \n\nSchema Documentation: We should touch base with MAWM team . Point of action for later\n\nMapping Document: We should touch base with MAWM team . Point of action for later\n\n@user @user Any thoughts?\n\nHey @user ,\n\nNot sure what additional details you have added that weren't known 2 weeks ago from Teams and Stand-Up discussions.\n\nYou have said ask another person a lot throughout this response also. Can I confirm you are going to own DFIO portion for future analysis to understand where the data currently lives? Where will it live in future? Details etc? CK can help I would say.\n\nFor Manhattan Active, we have got those details from CK a few weeks ago. We need to know how many DC’s on it and the roll out plan, core objects to pull etc. If not same as Manhattan Scale.\n\nThanks,\n\nHarrison\n\nMOving to done - 2 new tickets for SPrint 5 - Need ticket numbers as per backlog standup @user", "text": "Summary\nDFIO Manhattan Source System\n\n---\n\nDescription\nFollow up on understanding who can assist in obtaining source system details for DFIO data for a future state solution.\n\nWho can provide this info?\n\nWhat is system details?\n\nHow do we connect?\n\nWhat objects do we require?\n\n---\n\nComments\nRequested Login for DFIO SCI .\n\nAccess to be provided to Amit for credentials to login and review.\n\nRequested access to setup session with manhattan team.\n\n# My SQL environment setup for Manhattan - need to confirm this is a Copy of data from Source System and frequency of timing.\n## Are we to connect to MySQL Manhattan Active Copy or is another Source System Required?\n## Is CK able to assist in this? Does he know who if not able to know about MySQL environment setup\n\n*Next Step:* Amit to share email and book call with required Infra/CK for MySQL understanding\n\nReached out to Manhattan for time slots availability for a discussion. Bhavi Sanjay Mehta <[bhmehta@manh.com|mailto:bhmehta@manh.com]> is the contact person from Manhattan\n\nManhattan has mentioned that they have shared the required details with Project Management team . Have advised to reach out to Amila. Dropped a note to him. Will update once I hear from him.\n\nHad a word with Amila. He mentioned that he has some info and will share with us soon. \nWill talk to Supply Chain team for current extract of data and source report.\n\nNo further update from Amila\n\nFollowed up again with Amila. Asked for an ETA.@user\n\nGot Below responses from Amila. I think we got our answers for now. \n\n# Details of MySQL environment\n\n \n\nAE: (from attached email)\n\nDirect from our Gateway (IP: 192.168.42.65 or 88 – both PowerBI gateway servers)\n\nTo MySQL server (172.19.232.11)\n\nThrough IP whitelisting/firewall rules\n\nStandard TCP connection on port 3306\n\n \n\n \n\n# Best way to connect mainly from data extraction purposes\n\nAE: Above, Analitics team use a MYSQL connector . not sure how they use it. Rachel Foong should be able to provide more info\n\n \n\n# Is it the live data source or a copy\n\nAE: Copy\n\n \n\n# If so , refresh frequency\n\nAE: (from attached email)\n\n“We worked with the CloudOps team and confirmed that there is no lag or issue with the Data Save replica database.”\n\n \n\n# Any Schema Documentation\n\nWe don’t have this, maybe check with  Rachel Foong\n\n(see “MAWM – Questions” mail)\n\n \n\n# Any mapping document between old Scale and New MAWM Data\n\nWe don’t have this, maybe check with  Rachel Foong\n\nNext steps: \nwhat is the best way to connect for ingestion purpose. I think we can explore Azure/Snowflake options and reconnect later. \n\nSchema Documentation: We should touch base with MAWM team . Point of action for later\n\nMapping Document: We should touch base with MAWM team . Point of action for later\n\n@user @user Any thoughts?\n\nHey @user ,\n\nNot sure what additional details you have added that weren't known 2 weeks ago from Teams and Stand-Up discussions.\n\nYou have said ask another person a lot throughout this response also. Can I confirm you are going to own DFIO portion for future analysis to understand where the data currently lives? Where will it live in future? Details etc? CK can help I would say.\n\nFor Manhattan Active, we have got those details from CK a few weeks ago. We need to know how many DC’s on it and the roll out plan, core objects to pull etc. If not same as Manhattan Scale.\n\nThanks,\n\nHarrison\n\nMOving to done - 2 new tickets for SPrint 5 - Need ticket numbers as per backlog standup @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 451}}
{"issue_key": "CSCI-122", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Jul/25 11:14 AM", "updated": "03/Jul/25 4:46 PM", "labels": [], "summary": "Identifying the underlying source of Supply Chain Facts and Dims - DFIO (Share Location)", "description": "Goal:\n\n* Identifying the source of Supply Chain Facts and Dims\n\nAcceptance criteria:\n\n* Location address of the source of Supply Chain Facts and Dims for the DFIO Data Domain", "acceptance_criteria": "", "comments": "Have extracted the network drive file path for each data source stored on the cwvault network drive.\n\nInfo is saved here: [Supply Chain Data Sources - Network Drive (DFIO).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Supply%20Chain%20Data%20Sources%20-%20Network%20Drive%20(DFIO).xlsx?d=w46b6938fd3884f85ba06f0c7dae7b97b&csf=1&web=1&e=QjFGGs]", "text": "Summary\nIdentifying the underlying source of Supply Chain Facts and Dims - DFIO (Share Location)\n\n---\n\nDescription\nGoal:\n\n* Identifying the source of Supply Chain Facts and Dims\n\nAcceptance criteria:\n\n* Location address of the source of Supply Chain Facts and Dims for the DFIO Data Domain\n\n---\n\nComments\nHave extracted the network drive file path for each data source stored on the cwvault network drive.\n\nInfo is saved here: [Supply Chain Data Sources - Network Drive (DFIO).xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Supply%20Chain%20Data%20Sources%20-%20Network%20Drive%20(DFIO).xlsx?d=w46b6938fd3884f85ba06f0c7dae7b97b&csf=1&web=1&e=QjFGGs]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 452}}
{"issue_key": "CSCI-121", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Jul/25 11:07 AM", "updated": "18/Jul/25 8:45 PM", "labels": [], "summary": "Snowflake - CI/CD high level design", "description": "Collaborate with Eugene to establish the approach for Snowflake deployment. \n\nTest the deployment pipeline for the 1st round of deployment.", "acceptance_criteria": "Deployment steps are well defined and documented.\n\n1st deployment pipeline runs end to end", "comments": "* Workspace created in Snowflake.\n* Folder structure is also defined and agreed between Eugene and Chloe.\n\nWaiting for Key Pair ticket ““ to be completed. To support Eugene , test and validate Eugene CI/CD pipeline build ““.\n\nEugene and Chloe discussing on settling on approach. POC to test approach.\n\nNeed to prepare deployment script \nFleshing out req\nCall booked on Tues (Mon Eugene’s time)\n\nOngoing requirements and design for CI/CD delivery.\n\nWe’ve got high level design for managing meta changes and version logging", "text": "Summary\nSnowflake - CI/CD high level design\n\n---\n\nDescription\nCollaborate with Eugene to establish the approach for Snowflake deployment. \n\nTest the deployment pipeline for the 1st round of deployment.\n\n---\n\nAcceptance Criteria\nDeployment steps are well defined and documented.\n\n1st deployment pipeline runs end to end\n\n---\n\nComments\n* Workspace created in Snowflake.\n* Folder structure is also defined and agreed between Eugene and Chloe.\n\nWaiting for Key Pair ticket ““ to be completed. To support Eugene , test and validate Eugene CI/CD pipeline build ““.\n\nEugene and Chloe discussing on settling on approach. POC to test approach.\n\nNeed to prepare deployment script \nFleshing out req\nCall booked on Tues (Mon Eugene’s time)\n\nOngoing requirements and design for CI/CD delivery.\n\nWe’ve got high level design for managing meta changes and version logging", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 453}}
{"issue_key": "CSCI-117", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Jul/25 10:36 AM", "updated": "19/Nov/25 12:33 PM", "labels": [], "summary": "Metrics Definitions for Prioritised Phase 1 Deliverables", "description": "Metrics Definitions Document created for prioritised Business Processes for Phase 1 of delivery.\n\nAs per outcome of [https://sigmahealthcare.atlassian.net/browse/CSCI-115|https://sigmahealthcare.atlassian.net/browse/CSCI-115] Phase 1 will include Critical Conformed *Master Data* and Business Processes and Dimensions relating to *Inventory Data Domain*.", "acceptance_criteria": "", "comments": "[https://cwretail.atlassian.net/wiki/spaces/ITEDP/pages/503709725/Stock+ETL+Business+calculation|https://cwretail.atlassian.net/wiki/spaces/ITEDP/pages/503709725/Stock+ETL+Business+calculation] \n\nDocumentation from EDP 1.0 relating to metric definitions for Inventory (Store and DC)\n\nWriting documentation for Metric Definitions in this file:\n\n[Business Metric Definition Glossary.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Business%20Metric%20Definition%20Glossary.xlsx?d=w8c9d0bf3b8484cb9a623c328b2e11bcd&csf=1&web=1&e=aycLBN]\n\nDiscussion on current BUS Matrix session 08/07/2025. Notes to be clarified and updating document per decisions. Share actions.\n\n*Harrison required to create new tasks for Source-To-Target.*\n\nAdeel responded back on PBI05 Lineage. Jess C has appended to BUS Matrix for sources.\n\n*Jess C to Action:* PBI03 was also a source mentioned in some lineage, can we trace lineage for these back to actual source (PDB19B, PDB08, SQL3 etc). CK email may answer this last part.\n\n*Eugene:* There were some PBI objects that could not be traced back (difficulty in tracing logic therefore decision is made) to actual source therefore decision was made in EDP project to use the PBI object rather then the source db. *Amit and Bhavya should have this background as well*.\n\n@user Just updating this comment:\n\n* All notes made from the Bus Matrix discussion on 8/7/25 are now updated and reflected in the document\n* Have emailed Adeel re: PBI03/ETLSRV - Adeel has advised (in person) he is working through this currently and will get back to us\n** Note the underlying code to populate these tables is nested and not straight forward, hence why it is taking some extended time (this is due to legacy code and processes that have been expanded on over the years)", "text": "Summary\nMetrics Definitions for Prioritised Phase 1 Deliverables\n\n---\n\nDescription\nMetrics Definitions Document created for prioritised Business Processes for Phase 1 of delivery.\n\nAs per outcome of [https://sigmahealthcare.atlassian.net/browse/CSCI-115|https://sigmahealthcare.atlassian.net/browse/CSCI-115] Phase 1 will include Critical Conformed *Master Data* and Business Processes and Dimensions relating to *Inventory Data Domain*.\n\n---\n\nComments\n[https://cwretail.atlassian.net/wiki/spaces/ITEDP/pages/503709725/Stock+ETL+Business+calculation|https://cwretail.atlassian.net/wiki/spaces/ITEDP/pages/503709725/Stock+ETL+Business+calculation] \n\nDocumentation from EDP 1.0 relating to metric definitions for Inventory (Store and DC)\n\nWriting documentation for Metric Definitions in this file:\n\n[Business Metric Definition Glossary.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Business%20Metric%20Definition%20Glossary.xlsx?d=w8c9d0bf3b8484cb9a623c328b2e11bcd&csf=1&web=1&e=aycLBN]\n\nDiscussion on current BUS Matrix session 08/07/2025. Notes to be clarified and updating document per decisions. Share actions.\n\n*Harrison required to create new tasks for Source-To-Target.*\n\nAdeel responded back on PBI05 Lineage. Jess C has appended to BUS Matrix for sources.\n\n*Jess C to Action:* PBI03 was also a source mentioned in some lineage, can we trace lineage for these back to actual source (PDB19B, PDB08, SQL3 etc). CK email may answer this last part.\n\n*Eugene:* There were some PBI objects that could not be traced back (difficulty in tracing logic therefore decision is made) to actual source therefore decision was made in EDP project to use the PBI object rather then the source db. *Amit and Bhavya should have this background as well*.\n\n@user Just updating this comment:\n\n* All notes made from the Bus Matrix discussion on 8/7/25 are now updated and reflected in the document\n* Have emailed Adeel re: PBI03/ETLSRV - Adeel has advised (in person) he is working through this currently and will get back to us\n** Note the underlying code to populate these tables is nested and not straight forward, hence why it is taking some extended time (this is due to legacy code and processes that have been expanded on over the years)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 454}}
{"issue_key": "CSCI-116", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Jul/25 10:31 AM", "updated": "09/Oct/25 8:38 AM", "labels": [], "summary": "Detailed Timeline for Phased Delivery", "description": "Detailed timeline with resource allocation for Project Phased delivery", "acceptance_criteria": "Detailed timeline created, shared and reviewed internally and with key Business Stakeholders", "comments": "Location for Project Planning\n[https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2B73F1D3-F32E-423D-84FA-C5DDB20E6040%7D&file=Project%20Planning.xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2B73F1D3-F32E-423D-84FA-C5DDB20E6040%7D&file=Project%20Planning.xlsx&action=default&mobileredirect=true]", "text": "Summary\nDetailed Timeline for Phased Delivery\n\n---\n\nDescription\nDetailed timeline with resource allocation for Project Phased delivery\n\n---\n\nAcceptance Criteria\nDetailed timeline created, shared and reviewed internally and with key Business Stakeholders\n\n---\n\nComments\nLocation for Project Planning\n[https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2B73F1D3-F32E-423D-84FA-C5DDB20E6040%7D&file=Project%20Planning.xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/doc2.aspx?sourcedoc=%7B2B73F1D3-F32E-423D-84FA-C5DDB20E6040%7D&file=Project%20Planning.xlsx&action=default&mobileredirect=true]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 455}}
{"issue_key": "CSCI-114", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "01/Jul/25 10:25 AM", "updated": "03/Jul/25 1:20 PM", "labels": [], "summary": "Identifying the underlying source of Supply Chain Facts and Dims - Manhattan (MySQL)", "description": "Goal:\n\n* Identifying the source of Supply Chain Facts and Dims at the server-database-table level\n** Including the underlying SQL code written\n\nAcceptance criteria:\n\n* Location address of the source of Supply Chain Facts and Dims from Manhattan system (MySQL database)", "acceptance_criteria": "", "comments": "Have completed the source details of all MySQL Server connections, down to the table level:\n\n* Server.Database.Table\n\nEach source detail is mapped against a table within each Semantic Model.\n\nInfo saved: [Supply Chain Data Sources - MySQL.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Supply%20Chain%20Data%20Sources%20-%20MySQL.xlsx?d=w9d74faf0c7f94fb2b66fe77c9ddc8d24&csf=1&web=1&e=tsaVfZ]", "text": "Summary\nIdentifying the underlying source of Supply Chain Facts and Dims - Manhattan (MySQL)\n\n---\n\nDescription\nGoal:\n\n* Identifying the source of Supply Chain Facts and Dims at the server-database-table level\n** Including the underlying SQL code written\n\nAcceptance criteria:\n\n* Location address of the source of Supply Chain Facts and Dims from Manhattan system (MySQL database)\n\n---\n\nComments\nHave completed the source details of all MySQL Server connections, down to the table level:\n\n* Server.Database.Table\n\nEach source detail is mapped against a table within each Semantic Model.\n\nInfo saved: [Supply Chain Data Sources - MySQL.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Supply%20Chain%20Data%20Sources%20-%20MySQL.xlsx?d=w9d74faf0c7f94fb2b66fe77c9ddc8d24&csf=1&web=1&e=tsaVfZ]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 456}}
{"issue_key": "CSCI-113", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Jun/25 2:50 PM", "updated": "04/Jul/25 2:51 PM", "labels": [], "summary": "Create link services for source databases", "description": "* To test source database credentials which are saved in the Azure Key Vault. \n* Create linked services in ADF for integration works to commence once ready\n\nDefinition of done:\n\n* New linked services created for the source databases\n* Deployment config file for dev config-dev.csv", "acceptance_criteria": "", "comments": "I've created a new PR with new linked services and a config-dev file. \n\nI was able to successfully test the credentials and created linked services for the following databases in ADF:\n\n* AXLINK\n* SCAX2012\n\nWith the other servers/ databases:\n\n* CWMgtStoreInvoices\n* General_Reference\n* SpsWhsPurchase\n* StockDb\n* ILS \n\nI got error such as \"The account is disabled\" or  \"A network-related or instance-specific error occurred while establishing a connection to SQL Server. Make sure the SQL Database firewall allows the integration runtime to access\".\n\nI will reach out to Anjali on logins validation.\n\nthe connection for 192.168.29.105 - TDB08AX2012 went through successfully. \n\n* 192.168.29.78 - TDB15 still has the same error\n\nCannot connect to SQL Database. Please contact SQL server team for further support. Server: '192.168.29.78', Database: 'ILS', User: 'ServAC_EDPADF_dSQL07'. Check the linked service configuration is correct, and make sure the SQL Database firewall allows the integration runtime to access.\n\nAnjali to raise a ticket to get TBD15 firewall fixed\n\nFill in the below information in Key Vault:\n\nuser=os.getenv('SNOWFLAKE_USER'),\npassword=os.getenv('SNOWFLAKE_PASSWORD'),\naccount=os.getenv('SNOWFLAKE_ACCOUNT'),\nwarehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),\ndatabase=os.getenv('SNOWFLAKE_DATABASE'),\nschema=os.getenv('SNOWFLAKE_SCHEMA')\n\nNew ticket for ”ADF to SF via User Account, currently uses password however no longer available for SF sign in. Key pair authentication method sign in required”\n\nnew ticket created", "text": "Summary\nCreate link services for source databases\n\n---\n\nDescription\n* To test source database credentials which are saved in the Azure Key Vault. \n* Create linked services in ADF for integration works to commence once ready\n\nDefinition of done:\n\n* New linked services created for the source databases\n* Deployment config file for dev config-dev.csv\n\n---\n\nComments\nI've created a new PR with new linked services and a config-dev file. \n\nI was able to successfully test the credentials and created linked services for the following databases in ADF:\n\n* AXLINK\n* SCAX2012\n\nWith the other servers/ databases:\n\n* CWMgtStoreInvoices\n* General_Reference\n* SpsWhsPurchase\n* StockDb\n* ILS \n\nI got error such as \"The account is disabled\" or  \"A network-related or instance-specific error occurred while establishing a connection to SQL Server. Make sure the SQL Database firewall allows the integration runtime to access\".\n\nI will reach out to Anjali on logins validation.\n\nthe connection for 192.168.29.105 - TDB08AX2012 went through successfully. \n\n* 192.168.29.78 - TDB15 still has the same error\n\nCannot connect to SQL Database. Please contact SQL server team for further support. Server: '192.168.29.78', Database: 'ILS', User: 'ServAC_EDPADF_dSQL07'. Check the linked service configuration is correct, and make sure the SQL Database firewall allows the integration runtime to access.\n\nAnjali to raise a ticket to get TBD15 firewall fixed\n\nFill in the below information in Key Vault:\n\nuser=os.getenv('SNOWFLAKE_USER'),\npassword=os.getenv('SNOWFLAKE_PASSWORD'),\naccount=os.getenv('SNOWFLAKE_ACCOUNT'),\nwarehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),\ndatabase=os.getenv('SNOWFLAKE_DATABASE'),\nschema=os.getenv('SNOWFLAKE_SCHEMA')\n\nNew ticket for ”ADF to SF via User Account, currently uses password however no longer available for SF sign in. Key pair authentication method sign in required”\n\nnew ticket created", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 457}}
{"issue_key": "CSCI-112", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "30/Jun/25 10:07 AM", "updated": "18/Jul/25 12:43 PM", "labels": [], "summary": "SC BAU Sprint 2 Work", "description": "# Gateway connection Bug fix\n## [https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/194797|https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/194797] \n# Refresh Analysis Dashboard\n## [https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/185354|https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/185354]", "acceptance_criteria": "", "comments": "Assisted SC team with a gateway connection bug for one of their Semantic Models:\n\n[https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/194797|https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/194797] \n\nTask took 1 hr to complete\n\nCompleted the development of a “Refresh Analysis” dashboard for Supply Chain.\n\nDashboard is built using PBI Service metadata, called via PBI APIs.\n\n[https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/185354|https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/185354] \n\nAs part of the development, I have created a python notebook environment setup to create and call PBI APIs - this notebook can be leveraged for data extraction for the interim project.\n\nTask has taken 2 days additional work.", "text": "Summary\nSC BAU Sprint 2 Work\n\n---\n\nDescription\n# Gateway connection Bug fix\n## [https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/194797|https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/194797] \n# Refresh Analysis Dashboard\n## [https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/185354|https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/185354]\n\n---\n\nComments\nAssisted SC team with a gateway connection bug for one of their Semantic Models:\n\n[https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/194797|https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/194797] \n\nTask took 1 hr to complete\n\nCompleted the development of a “Refresh Analysis” dashboard for Supply Chain.\n\nDashboard is built using PBI Service metadata, called via PBI APIs.\n\n[https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/185354|https://dev.azure.com/MyChemist/Operations%20Analytics/_workitems/edit/185354] \n\nAs part of the development, I have created a python notebook environment setup to create and call PBI APIs - this notebook can be leveraged for data extraction for the interim project.\n\nTask has taken 2 days additional work.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 458}}
{"issue_key": "CSCI-111", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "27/Jun/25 10:08 AM", "updated": "27/Jun/25 10:33 AM", "labels": [], "summary": "Link Eugene's tickets from Azure", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nLink Eugene's tickets from Azure", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 459}}
{"issue_key": "CSCI-110", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "27/Jun/25 10:08 AM", "updated": "27/Jun/25 10:38 AM", "labels": [], "summary": "Merge CSCI-83 and CSCI84", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nMerge CSCI-83 and CSCI84", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 460}}
{"issue_key": "CSCI-109", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "25/Jun/25 10:24 AM", "updated": "27/Jun/25 11:09 AM", "labels": [], "summary": "Phil to figure out how to visualise Jess's story points and capacity.", "description": "Problem:\n\n* Jess is currently having 2 places where her efforts are placed\n* Jess’s effort/sprint points are not constant in both \n** [Operations Analytics Devops team/board|https://dev.azure.com/MyChemist/Operations%20Analytics]\n** CW cloud data platform (this board)\n* We are trying to measure how many points she has so that we know what her capacity is from a product/project management and agile-Scrum perspective.\n\nObjective:\n\n* To Measure and visualise how much capacity Jess has on any given Sprint\n\nDefinition of Done:\n\n* To be able to discover a method of visibility so that we can effectively see the capacity Jess has on her sprints.\n* To have the above discovery implemented in JIRA/Devops.", "acceptance_criteria": "", "comments": "We’ll basically have 2 boards concurrently in standup. Both boards will be visibile during standup.", "text": "Summary\nPhil to figure out how to visualise Jess's story points and capacity.\n\n---\n\nDescription\nProblem:\n\n* Jess is currently having 2 places where her efforts are placed\n* Jess’s effort/sprint points are not constant in both \n** [Operations Analytics Devops team/board|https://dev.azure.com/MyChemist/Operations%20Analytics]\n** CW cloud data platform (this board)\n* We are trying to measure how many points she has so that we know what her capacity is from a product/project management and agile-Scrum perspective.\n\nObjective:\n\n* To Measure and visualise how much capacity Jess has on any given Sprint\n\nDefinition of Done:\n\n* To be able to discover a method of visibility so that we can effectively see the capacity Jess has on her sprints.\n* To have the above discovery implemented in JIRA/Devops.\n\n---\n\nComments\nWe’ll basically have 2 boards concurrently in standup. Both boards will be visibile during standup.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 461}}
{"issue_key": "CSCI-104", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "24/Jun/25 2:44 PM", "updated": "24/Jun/25 2:44 PM", "labels": [], "summary": "test task 2", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\ntest task 2", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 462}}
{"issue_key": "CSCI-103", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Jun/25 3:43 PM", "updated": "25/Jun/25 10:09 AM", "labels": [], "summary": "Azure tasks that need - devops linking", "description": "Phil - To link tasks that are as per listed below and re-create them in JIRA\n\n* 192465\n* 192466\n* 185363\n* 185353", "acceptance_criteria": "", "comments": "Have spoken to @user - I will need to create separate new tasks to be in backlog.", "text": "Summary\nAzure tasks that need - devops linking\n\n---\n\nDescription\nPhil - To link tasks that are as per listed below and re-create them in JIRA\n\n* 192465\n* 192466\n* 185363\n* 185353\n\n---\n\nComments\nHave spoken to @user - I will need to create separate new tasks to be in backlog.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 463}}
{"issue_key": "CSCI-102", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Jun/25 3:35 PM", "updated": "23/Jun/25 3:35 PM", "labels": [], "summary": "test task", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\ntest task", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 464}}
{"issue_key": "CSCI-101", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Jun/25 3:24 PM", "updated": "24/Jun/25 2:45 PM", "labels": [], "summary": "SC tasks not clearing in sprint", "description": "Problem:\n\n* JIRA tickets aren’t clearing, thus tickets in ‘done’ column are carrying over \n\nSolution:\n\n* Phil to check and redefine status columns where necessary\n\nDefinition of done:\n\n* upon closing of sprint, tickets in ‘done’ are closed", "acceptance_criteria": "", "comments": "", "text": "Summary\nSC tasks not clearing in sprint\n\n---\n\nDescription\nProblem:\n\n* JIRA tickets aren’t clearing, thus tickets in ‘done’ column are carrying over \n\nSolution:\n\n* Phil to check and redefine status columns where necessary\n\nDefinition of done:\n\n* upon closing of sprint, tickets in ‘done’ are closed", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 465}}
{"issue_key": "CSCI-98", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "20/Jun/25 4:04 PM", "updated": "30/Jun/25 3:11 PM", "labels": [], "summary": "Jess-Amit Bus Matrix Handover Tasks", "description": "List of action items from Amit from [https://sigmahealthcare.atlassian.net/browse/CSCI-34|https://sigmahealthcare.atlassian.net/browse/CSCI-34]:\n\n# -Document Source Systems Models and Reports- - already captured by multiple tickets\n# Are there any similar reports coming from two different source system?\n# Capture all Flat/Junk’Flag Dimensions used in existing reporting\n# Capture different kind of Date dimensions used in existing reporting. Like Order Date, Invoiced Date, Shipped Date, Delivery Date etc\n# -Grouping of Measures: group all time intelligence KPIs coming from same Measure into one bucket- - already captured in an existing ticket [https://sigmahealthcare.atlassian.net/browse/CSCI-80|https://sigmahealthcare.atlassian.net/browse/CSCI-80] \n# Look for any source system that is not captured in Bus Matrix document\n# Look for Data sets ingested from Mobile Dock. These are not captured in documents\n# Look for Data Sets captured from MAWM\n\n*Definition of done:*\n\n* questions above answered as either a list or spun off to different tasks.", "acceptance_criteria": "", "comments": "All points covered as part of tasks [https://sigmahealthcare.atlassian.net/browse/CSCI-94|https://sigmahealthcare.atlassian.net/browse/CSCI-94] and [https://sigmahealthcare.atlassian.net/browse/CSCI-81|https://sigmahealthcare.atlassian.net/browse/CSCI-81]", "text": "Summary\nJess-Amit Bus Matrix Handover Tasks\n\n---\n\nDescription\nList of action items from Amit from [https://sigmahealthcare.atlassian.net/browse/CSCI-34|https://sigmahealthcare.atlassian.net/browse/CSCI-34]:\n\n# -Document Source Systems Models and Reports- - already captured by multiple tickets\n# Are there any similar reports coming from two different source system?\n# Capture all Flat/Junk’Flag Dimensions used in existing reporting\n# Capture different kind of Date dimensions used in existing reporting. Like Order Date, Invoiced Date, Shipped Date, Delivery Date etc\n# -Grouping of Measures: group all time intelligence KPIs coming from same Measure into one bucket- - already captured in an existing ticket [https://sigmahealthcare.atlassian.net/browse/CSCI-80|https://sigmahealthcare.atlassian.net/browse/CSCI-80] \n# Look for any source system that is not captured in Bus Matrix document\n# Look for Data sets ingested from Mobile Dock. These are not captured in documents\n# Look for Data Sets captured from MAWM\n\n*Definition of done:*\n\n* questions above answered as either a list or spun off to different tasks.\n\n---\n\nComments\nAll points covered as part of tasks [https://sigmahealthcare.atlassian.net/browse/CSCI-94|https://sigmahealthcare.atlassian.net/browse/CSCI-94] and [https://sigmahealthcare.atlassian.net/browse/CSCI-81|https://sigmahealthcare.atlassian.net/browse/CSCI-81]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 466}}
{"issue_key": "CSCI-97", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "20/Jun/25 3:44 PM", "updated": "27/Jun/25 10:30 AM", "labels": [], "summary": "Review and update Bus Matrix with Analysis from Supply Chain SM's", "description": "Upon completion of [https://sigmahealthcare.atlassian.net/browse/CSCI-81|https://sigmahealthcare.atlassian.net/browse/CSCI-81] and [https://sigmahealthcare.atlassian.net/browse/CSCI-94|https://sigmahealthcare.atlassian.net/browse/CSCI-94], populate the results from this analysis to update our Bus Matrix document to include all Facts and Dims identified in the supply chain current state. \n\nIdentify also which facts and dims are not being used by the supply chain team, already created in the Bus Matrix.\n\nAcceptance Criteria:\n\n* Update Bus Matrix document with Dims and Facts identified from Supply Chain", "acceptance_criteria": "", "comments": "Have updated my supply chain discovery notes to the Bus Matrix doc for Dimensions.\nHave re-formatted the page to tidy up some descriptions being in the ‘comments’ field and some in the ‘definition’ field.\n\n*Dimension Definition:*\n\nRefers to what kind of columns/data is included in the listed Dimensions\n\n*Comments:*\n\nAny callouts relating to how Supply Chain are currently reporting/data they have requested\n\nHave also added flags to highlight if Dimensions are currently being used by SC and if will be required in future (those listed as N are tables in question - need to speak with Amit to clarify)", "text": "Summary\nReview and update Bus Matrix with Analysis from Supply Chain SM's\n\n---\n\nDescription\nUpon completion of [https://sigmahealthcare.atlassian.net/browse/CSCI-81|https://sigmahealthcare.atlassian.net/browse/CSCI-81] and [https://sigmahealthcare.atlassian.net/browse/CSCI-94|https://sigmahealthcare.atlassian.net/browse/CSCI-94], populate the results from this analysis to update our Bus Matrix document to include all Facts and Dims identified in the supply chain current state. \n\nIdentify also which facts and dims are not being used by the supply chain team, already created in the Bus Matrix.\n\nAcceptance Criteria:\n\n* Update Bus Matrix document with Dims and Facts identified from Supply Chain\n\n---\n\nComments\nHave updated my supply chain discovery notes to the Bus Matrix doc for Dimensions.\nHave re-formatted the page to tidy up some descriptions being in the ‘comments’ field and some in the ‘definition’ field.\n\n*Dimension Definition:*\n\nRefers to what kind of columns/data is included in the listed Dimensions\n\n*Comments:*\n\nAny callouts relating to how Supply Chain are currently reporting/data they have requested\n\nHave also added flags to highlight if Dimensions are currently being used by SC and if will be required in future (those listed as N are tables in question - need to speak with Amit to clarify)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 467}}
{"issue_key": "CSCI-96", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "20/Jun/25 10:07 AM", "updated": "07/Jul/25 9:48 AM", "labels": [], "summary": "ADF - Extraction pipeline template - PARQUET file", "description": "The data is too large and couldn't be loaded directly (Direct) from the source DBs. Hence CW database admins helped to extract them and upload to blob as PARQUET files. Once it is processed, the next loads are incremental and direct based on the Last Extracted Date Time column.\n\nObjective:\n\n* Create a test pipeline template for PARQUET file to do full loads on large source. \n* e.g source - TBD14\n\n*Definition of done:*\n\n* Test pipeline is up and running. Data is landed on blob storage", "acceptance_criteria": "", "comments": "Implement and test delta load\n\n@user is it okay if you entered in definition of done for this task? thanks!\n\nIncremental load is tested successfully where data is landed on the TEST blob container\n\nParquet ADF template to copy to blob successfully… Pending Storage Integration to test end to end flow.\n\nAwaiting Enda/Brent action on SNOW ticket, Harrison following up given Enda is now on leave till end of July.\n\nThanks,\n\nHarrison\n\nPipeline failed due to missing parameters. Need further investigation.\n\nPipeline able to pick up however unable to decompress Parquet format in Snowflake. I will have a look on how to resolve this\n\nthis pipeline template is successfully tested end to end", "text": "Summary\nADF - Extraction pipeline template - PARQUET file\n\n---\n\nDescription\nThe data is too large and couldn't be loaded directly (Direct) from the source DBs. Hence CW database admins helped to extract them and upload to blob as PARQUET files. Once it is processed, the next loads are incremental and direct based on the Last Extracted Date Time column.\n\nObjective:\n\n* Create a test pipeline template for PARQUET file to do full loads on large source. \n* e.g source - TBD14\n\n*Definition of done:*\n\n* Test pipeline is up and running. Data is landed on blob storage\n\n---\n\nComments\nImplement and test delta load\n\n@user is it okay if you entered in definition of done for this task? thanks!\n\nIncremental load is tested successfully where data is landed on the TEST blob container\n\nParquet ADF template to copy to blob successfully… Pending Storage Integration to test end to end flow.\n\nAwaiting Enda/Brent action on SNOW ticket, Harrison following up given Enda is now on leave till end of July.\n\nThanks,\n\nHarrison\n\nPipeline failed due to missing parameters. Need further investigation.\n\nPipeline able to pick up however unable to decompress Parquet format in Snowflake. I will have a look on how to resolve this\n\nthis pipeline template is successfully tested end to end", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 468}}
{"issue_key": "CSCI-95", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "20/Jun/25 9:43 AM", "updated": "07/Jul/25 9:48 AM", "labels": [], "summary": "Create a Data Ingestion plan", "description": "* To identify the first 3 sources (databases) to be ingested first to EDP Snowflake\n* Definition of Done:\n** Outline the database names, tables name\n** Get approval from relevant stakeholders (CK, Harrison, Rachel etc.) to ingest to Snowflake", "acceptance_criteria": "", "comments": "I’ve been working with Eugene to better understand the data ingestion challenges encountered during EDP 1.0. Based on those insights, we plan to implement *two pipelines per data source*, particularly for those with high data volumes:\n\n# *Initial Load Pipeline*\nThis pipeline will handle a one-time bulk load. The DBA team will extract data from the on-premises databases and upload it to Azure Blob Storage as *CSV or Parquet* files. We understand that using Parquet format can help mitigate data quality issues. Once the files are available in Blob Storage, we’ll trigger the pipeline to ingest the data into Snowflake.\n# *Incremental Load Pipeline*\nThis pipeline will manage ongoing data updates by loading incremental changes *directly* from the source databases.\n\nTo support this approach, I’ll be setting up an ADF template specifically for extracting data from Parquet files during the initial load phase.\n\nSo I believe we need the assistance from Rachel's / DBA team at least in the below:\n\n* Help us validate our current data source mapping is accurate and complete.\n* Extract data from the confirmed databases and upload the data to Azure Blob Storage in *Parquet* format.\n\nProvided a list of source to ingest. Harrison has sent email to Rachel to request her team’s support. [List of data sources to be extracted as parquet.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/List%20of%20data%20sources%20to%20be%20extracted%20as%20parquet.xlsx?d=w385e099996914351bc23a629ff81840d&csf=1&web=1&e=1l5y1g&xsdata=MDV8MDJ8fDdlOGY1NzhjNzQ0NTQxOTliYjkzMDhkZGI0N2I0NTQ1fDFkZWMyOWI0OWE1ZDQxZmM5Yzc1Y2RiMmNiOWM3ZmU2fDB8MHw2Mzg4NjUxNjM2OTI2NTM0NTl8VW5rbm93bnxWR1ZoYlhOVFpXTjFjbWwwZVZObGNuWnBZMlY4ZXlKRFFTSTZJbFJsWVcxelgwRlVVRk5sY25acFkyVmZVMUJQVEU5R0lpd2lWaUk2SWpBdU1DNHdNREF3SWl3aVVDSTZJbGRwYmpNeUlpd2lRVTRpT2lKUGRHaGxjaUlzSWxkVUlqb3hNWDA9fDF8TDJOb1lYUnpMekU1T2pGa01URmpZekEzT0RZelpUUTBPR05oWlRNelpUVTBObVV4WVRobE5UTTJRSFJvY21WaFpDNTJNaTl0WlhOellXZGxjeTh4TnpVd09URTVOVFkzT1RNMHw4NTk3ZDdkYmM1MmI0Y2ZmYmI5MzA4ZGRiNDdiNDU0NXw5YjJjMDc1ZGI1MGU0NjI0YTE4Yzk2YTExZWUzZDc0OQ%3D%3D&sdata=cW9oMEdyOW5Vc3FJSG85dEt2Vnc4eUNOR1U1NGk3QnFjZ1ZBSC9lUVRrOD0%3D&ovuser=fd21b7ac-6a50-45ae-99ea-568c8161f6d0%2CChloeT%40altis.com.au]\n\nNow waiting for Rachel’s response.\n\nI've sent out the email to get data ingestion approval.", "text": "Summary\nCreate a Data Ingestion plan\n\n---\n\nDescription\n* To identify the first 3 sources (databases) to be ingested first to EDP Snowflake\n* Definition of Done:\n** Outline the database names, tables name\n** Get approval from relevant stakeholders (CK, Harrison, Rachel etc.) to ingest to Snowflake\n\n---\n\nComments\nI’ve been working with Eugene to better understand the data ingestion challenges encountered during EDP 1.0. Based on those insights, we plan to implement *two pipelines per data source*, particularly for those with high data volumes:\n\n# *Initial Load Pipeline*\nThis pipeline will handle a one-time bulk load. The DBA team will extract data from the on-premises databases and upload it to Azure Blob Storage as *CSV or Parquet* files. We understand that using Parquet format can help mitigate data quality issues. Once the files are available in Blob Storage, we’ll trigger the pipeline to ingest the data into Snowflake.\n# *Incremental Load Pipeline*\nThis pipeline will manage ongoing data updates by loading incremental changes *directly* from the source databases.\n\nTo support this approach, I’ll be setting up an ADF template specifically for extracting data from Parquet files during the initial load phase.\n\nSo I believe we need the assistance from Rachel's / DBA team at least in the below:\n\n* Help us validate our current data source mapping is accurate and complete.\n* Extract data from the confirmed databases and upload the data to Azure Blob Storage in *Parquet* format.\n\nProvided a list of source to ingest. Harrison has sent email to Rachel to request her team’s support. [List of data sources to be extracted as parquet.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/List%20of%20data%20sources%20to%20be%20extracted%20as%20parquet.xlsx?d=w385e099996914351bc23a629ff81840d&csf=1&web=1&e=1l5y1g&xsdata=MDV8MDJ8fDdlOGY1NzhjNzQ0NTQxOTliYjkzMDhkZGI0N2I0NTQ1fDFkZWMyOWI0OWE1ZDQxZmM5Yzc1Y2RiMmNiOWM3ZmU2fDB8MHw2Mzg4NjUxNjM2OTI2NTM0NTl8VW5rbm93bnxWR1ZoYlhOVFpXTjFjbWwwZVZObGNuWnBZMlY4ZXlKRFFTSTZJbFJsWVcxelgwRlVVRk5sY25acFkyVmZVMUJQVEU5R0lpd2lWaUk2SWpBdU1DNHdNREF3SWl3aVVDSTZJbGRwYmpNeUlpd2lRVTRpT2lKUGRHaGxjaUlzSWxkVUlqb3hNWDA9fDF8TDJOb1lYUnpMekU1T2pGa01URmpZekEzT0RZelpUUTBPR05oWlRNelpUVTBObVV4WVRobE5UTTJRSFJvY21WaFpDNTJNaTl0WlhOellXZGxjeTh4TnpVd09URTVOVFkzT1RNMHw4NTk3ZDdkYmM1MmI0Y2ZmYmI5MzA4ZGRiNDdiNDU0NXw5YjJjMDc1ZGI1MGU0NjI0YTE4Yzk2YTExZWUzZDc0OQ%3D%3D&sdata=cW9oMEdyOW5Vc3FJSG85dEt2Vnc4eUNOR1U1NGk3QnFjZ1ZBSC9lUVRrOD0%3D&ovuser=fd21b7ac-6a50-45ae-99ea-568c8161f6d0%2CChloeT%40altis.com.au]\n\nNow waiting for Rachel’s response.\n\nI've sent out the email to get data ingestion approval.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 469}}
{"issue_key": "CSCI-94", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "19/Jun/25 12:34 PM", "updated": "24/Jun/25 3:00 PM", "labels": [], "summary": "Analysing and Grouping Facts from Supply Chain", "description": "A list of 62 Fact tables have been identified across all the Supply Chain semantic models.\n\nNeed to understand the grain and facts within each of these tables, and determine if we can group some of these facts together.\n\n*Definition of Done:* \n\n* Inspect each individual fact and identify the metrics/facts stored as well as the grain of the table\n* Group like-facts together to create a more concise list of facts", "acceptance_criteria": "", "comments": "Work has been completed in below excel doc:\n\n[Supply Chain Fact Discovery.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Supply%20Chain%20Fact%20Discovery.xlsx?d=w3ea322123bc345a4b0981cc3df1bc6f1&csf=1&web=1&e=U9cJpy]\n\nInitial Discovery complete. \n\nOf all the unique 62 fact tables, have estimated these can be grouped into 20 distinct fact tables (10 of these tables deemed not to be applicable fact tables).\n\nBelow depicts the ‘Business Areas’ that these fact tables relate to:\n\n* DFIO (3)\n* Inbound/Outbound Shipments (17)\n* Inventory (22)\n* Sales (6)\n* ‘Other’ (4)\n* Not Applicable (10)\n\nSome challenges:\n\nIt’s noted that there were a lot of similar fact tables generated with the same metric values but at a +_different aggregate_+. It appears this has been done with performance considerations in mind, due to some tables being incredibly large (eg: Table *FactDCInboundPurchaseOrdersERP* in SM ‘DC Inbound Shipments Semantic Model’ is in excess of +100M+ rows).\n\nNote: there were a couple of tables which are ‘factless facts’ - something to consider for our BI modelling", "text": "Summary\nAnalysing and Grouping Facts from Supply Chain\n\n---\n\nDescription\nA list of 62 Fact tables have been identified across all the Supply Chain semantic models.\n\nNeed to understand the grain and facts within each of these tables, and determine if we can group some of these facts together.\n\n*Definition of Done:* \n\n* Inspect each individual fact and identify the metrics/facts stored as well as the grain of the table\n* Group like-facts together to create a more concise list of facts\n\n---\n\nComments\nWork has been completed in below excel doc:\n\n[Supply Chain Fact Discovery.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Supply%20Chain%20Fact%20Discovery.xlsx?d=w3ea322123bc345a4b0981cc3df1bc6f1&csf=1&web=1&e=U9cJpy]\n\nInitial Discovery complete. \n\nOf all the unique 62 fact tables, have estimated these can be grouped into 20 distinct fact tables (10 of these tables deemed not to be applicable fact tables).\n\nBelow depicts the ‘Business Areas’ that these fact tables relate to:\n\n* DFIO (3)\n* Inbound/Outbound Shipments (17)\n* Inventory (22)\n* Sales (6)\n* ‘Other’ (4)\n* Not Applicable (10)\n\nSome challenges:\n\nIt’s noted that there were a lot of similar fact tables generated with the same metric values but at a +_different aggregate_+. It appears this has been done with performance considerations in mind, due to some tables being incredibly large (eg: Table *FactDCInboundPurchaseOrdersERP* in SM ‘DC Inbound Shipments Semantic Model’ is in excess of +100M+ rows).\n\nNote: there were a couple of tables which are ‘factless facts’ - something to consider for our BI modelling", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 470}}
{"issue_key": "CSCI-93", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "19/Jun/25 8:11 AM", "updated": "19/Nov/25 12:27 PM", "labels": [], "summary": "Designing Future State - Pairing BI application design with Business requirements definition", "description": "Objective:\n\n* Prepare designs with business requirements so that we are developing what uses will need.", "acceptance_criteria": "", "comments": "", "text": "Summary\nDesigning Future State - Pairing BI application design with Business requirements definition\n\n---\n\nDescription\nObjective:\n\n* Prepare designs with business requirements so that we are developing what uses will need.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 471}}
{"issue_key": "CSCI-92", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "19/Jun/25 8:11 AM", "updated": "19/Nov/25 12:27 PM", "labels": [], "summary": "Designing Future State - BI application Design", "description": "Objective/outcome:\n\n* To optimise design visualisation in PowerBI that will meet Supply chain’s needs.\n\nReason:\n\n* This will be part of reviewing existing reports and seeing how we can display data better to our users.\n* This would be done lieu with requirements.", "acceptance_criteria": "", "comments": "", "text": "Summary\nDesigning Future State - BI application Design\n\n---\n\nDescription\nObjective/outcome:\n\n* To optimise design visualisation in PowerBI that will meet Supply chain’s needs.\n\nReason:\n\n* This will be part of reviewing existing reports and seeing how we can display data better to our users.\n* This would be done lieu with requirements.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 472}}
{"issue_key": "CSCI-91", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "19/Jun/25 8:10 AM", "updated": "19/Nov/25 12:27 PM", "labels": [], "summary": "Designing Future state - Identifying and proposing which data source (on prem) and which data grains should be connected to ADF pipeline.", "description": "Objective:\n\n* We want to find out from each data source which particle data grains should we be uploading into ADF\n\nReason:\n\n* In order to increase performance, we should only upload what we need from data sources. These sources may be on- premises, which may take time for both initial load and bulk load/streaming.", "acceptance_criteria": "", "comments": "", "text": "Summary\nDesigning Future state - Identifying and proposing which data source (on prem) and which data grains should be connected to ADF pipeline.\n\n---\n\nDescription\nObjective:\n\n* We want to find out from each data source which particle data grains should we be uploading into ADF\n\nReason:\n\n* In order to increase performance, we should only upload what we need from data sources. These sources may be on- premises, which may take time for both initial load and bulk load/streaming.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 473}}
{"issue_key": "CSCI-90", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "19/Jun/25 8:09 AM", "updated": "19/Nov/25 12:28 PM", "labels": [], "summary": "Designing Future State - Proposing which reports should be developed through usage metrics", "description": "Outcome:\n\n* To find out what are the top 10 reports that are currently being used by supply chain (Operations Analytics)\n\nReason:\n\n* We can then use the top 10 reports to find out what data sources are being queried the most\n* We want to create the most impact for supply chain team by delivering on reports that affect them the most.", "acceptance_criteria": "", "comments": "", "text": "Summary\nDesigning Future State - Proposing which reports should be developed through usage metrics\n\n---\n\nDescription\nOutcome:\n\n* To find out what are the top 10 reports that are currently being used by supply chain (Operations Analytics)\n\nReason:\n\n* We can then use the top 10 reports to find out what data sources are being queried the most\n* We want to create the most impact for supply chain team by delivering on reports that affect them the most.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 474}}
{"issue_key": "CSCI-89", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jun/25 4:04 PM", "updated": "18/Jul/25 8:26 PM", "labels": [], "summary": "Snowflake Git Repository configuration", "description": "[Devops |https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/192817]192817\n\nObjective:\n\n* To create a separate Azure DevOps repository for Snowflake so that SQL scripts and objects can be managed independently of other resources.\n\nDefinition of Done: \n\n* Snowflake repo created with organized folder structure\n* Branch policies enforce code review and approval\n* Access granted only to authorized Snowflake contributors\n* Branch protection and restrictions are applied (e.g., no force-push, mandatory PR reviews)", "acceptance_criteria": "", "comments": "", "text": "Summary\nSnowflake Git Repository configuration\n\n---\n\nDescription\n[Devops |https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/192817]192817\n\nObjective:\n\n* To create a separate Azure DevOps repository for Snowflake so that SQL scripts and objects can be managed independently of other resources.\n\nDefinition of Done: \n\n* Snowflake repo created with organized folder structure\n* Branch policies enforce code review and approval\n* Access granted only to authorized Snowflake contributors\n* Branch protection and restrictions are applied (e.g., no force-push, mandatory PR reviews)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 475}}
{"issue_key": "CSCI-88", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jun/25 2:41 PM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "ADF - Extraction pipeline template - Rdbms/ SQL server", "description": "Objective:\n\n* Create a test pipeline template for Rdbms/ SQL server\n* e.g source - TBD14\n\n*Definition of done:*\n\n* Test pipeline is up and running. Data is landed on blob storage", "acceptance_criteria": "", "comments": "Implement and test delta load\n\n@user is it okay if you entered in definition of done for this task? thanks!\n\nIncremental load is tested successfully where data is landed on the TEST blob container", "text": "Summary\nADF - Extraction pipeline template - Rdbms/ SQL server\n\n---\n\nDescription\nObjective:\n\n* Create a test pipeline template for Rdbms/ SQL server\n* e.g source - TBD14\n\n*Definition of done:*\n\n* Test pipeline is up and running. Data is landed on blob storage\n\n---\n\nComments\nImplement and test delta load\n\n@user is it okay if you entered in definition of done for this task? thanks!\n\nIncremental load is tested successfully where data is landed on the TEST blob container", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 476}}
{"issue_key": "CSCI-87", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jun/25 12:38 PM", "updated": "19/Nov/25 12:28 PM", "labels": [], "summary": "Designing Future state - propose connection between source (snowflake) and semantic Models", "description": "Objective:\n\n* We want to come with a design where we can connnect the right Dim and Fact tables from snowflake to the models that are created by PowerBI", "acceptance_criteria": "", "comments": "", "text": "Summary\nDesigning Future state - propose connection between source (snowflake) and semantic Models\n\n---\n\nDescription\nObjective:\n\n* We want to come with a design where we can connnect the right Dim and Fact tables from snowflake to the models that are created by PowerBI", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 477}}
{"issue_key": "CSCI-86", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Duplicate", "priority": "Medium", "created": "18/Jun/25 12:24 PM", "updated": "19/Nov/25 12:29 PM", "labels": [], "summary": "Understanding the Data Lineage from BI Servers to Underlying Source System", "description": "Discussion between Jess and Chloe and Rachel (DBA Team).\n\nGoal to understand all the transformations/steps/systems between the current Power BI Semantic Model connection to the true Source System.\n\nOutcome:\n\n* Liaise with Rachel Wan - ETL steps on SSIS packages", "acceptance_criteria": "", "comments": "", "text": "Summary\nUnderstanding the Data Lineage from BI Servers to Underlying Source System\n\n---\n\nDescription\nDiscussion between Jess and Chloe and Rachel (DBA Team).\n\nGoal to understand all the transformations/steps/systems between the current Power BI Semantic Model connection to the true Source System.\n\nOutcome:\n\n* Liaise with Rachel Wan - ETL steps on SSIS packages", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 478}}
{"issue_key": "CSCI-85", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jun/25 12:23 PM", "updated": "19/Nov/25 12:30 PM", "labels": [], "summary": "Understanding Current state - Identifying Source of semantic models (from DIMs and Facts)", "description": "This task builds on the dims and facts identification task as it pulls in Multiple DIMs and Facts.\n\n[https://sigmahealthcare.atlassian.net/browse/CSCI-83|https://sigmahealthcare.atlassian.net/browse/CSCI-83] \n\nAND\n [https://sigmahealthcare.atlassian.net/browse/CSCI-84|https://sigmahealthcare.atlassian.net/browse/CSCI-84] \n\nOutcome:\n\n* Pinpoint what sources are primarily used (Which Fact and Dim that we want)", "acceptance_criteria": "", "comments": "", "text": "Summary\nUnderstanding Current state - Identifying Source of semantic models (from DIMs and Facts)\n\n---\n\nDescription\nThis task builds on the dims and facts identification task as it pulls in Multiple DIMs and Facts.\n\n[https://sigmahealthcare.atlassian.net/browse/CSCI-83|https://sigmahealthcare.atlassian.net/browse/CSCI-83] \n\nAND\n [https://sigmahealthcare.atlassian.net/browse/CSCI-84|https://sigmahealthcare.atlassian.net/browse/CSCI-84] \n\nOutcome:\n\n* Pinpoint what sources are primarily used (Which Fact and Dim that we want)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 479}}
{"issue_key": "CSCI-84", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jun/25 12:20 PM", "updated": "07/Jul/25 12:23 PM", "labels": [], "summary": "Identifying the underlying source of Supply Chain Facts", "description": "Goal:\n\n* Identifying the source of Supply Chain Facts at the server-database-table level\n** Including the underlying SQL code written\n\nAcceptance criteria:\n\n* Location address of the source of Supply Chain Facts\n\nPostulation:\n\n* Identifying the tables on which server that has most usage and prioritising it by 1,2,3 so that we can make the most client impact in our migration.\n\nWhat does this solve:\n\n* Performance issues\n* Access issues.", "acceptance_criteria": "", "comments": "merged with [https://sigmahealthcare.atlassian.net/browse/CSCI-83|https://sigmahealthcare.atlassian.net/browse/CSCI-83]", "text": "Summary\nIdentifying the underlying source of Supply Chain Facts\n\n---\n\nDescription\nGoal:\n\n* Identifying the source of Supply Chain Facts at the server-database-table level\n** Including the underlying SQL code written\n\nAcceptance criteria:\n\n* Location address of the source of Supply Chain Facts\n\nPostulation:\n\n* Identifying the tables on which server that has most usage and prioritising it by 1,2,3 so that we can make the most client impact in our migration.\n\nWhat does this solve:\n\n* Performance issues\n* Access issues.\n\n---\n\nComments\nmerged with [https://sigmahealthcare.atlassian.net/browse/CSCI-83|https://sigmahealthcare.atlassian.net/browse/CSCI-83]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 480}}
{"issue_key": "CSCI-83", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "18/Jun/25 12:15 PM", "updated": "01/Jul/25 11:14 AM", "labels": [], "summary": "Identifying the underlying source of Supply Chain Facts and Dims - SQL Server environments", "description": "Goal:\n\n* Identifying the source of Supply Chain Facts and Dims at the server-database-table level\n** Including the underlying SQL code written\n\nAcceptance criteria:\n\n* Location address of the source of Supply Chain Facts and Dims\n\nPostulation:\n\n* Identifying the tables on which server that has most usage and prioritising it by 1,2,3 so that we can make the most client impact in our migration.\n\nWhat does this solve:\n\n* Performance issues\n* Access issues.", "acceptance_criteria": "", "comments": "Merged with [https://sigmahealthcare.atlassian.net/browse/CSCI-84|https://sigmahealthcare.atlassian.net/browse/CSCI-84] @user\n\nLink provided below in chat by @user \n\n[https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B30678332-5293-4A0C-84A6-26488C56A8CD%7D&file=Supply%20Chain%20Data%20Sources%20-%20SQL.xlsx&wdOrigin=TEAMS-MAGLEV.p2p_ns.rwc&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B30678332-5293-4A0C-84A6-26488C56A8CD%7D&file=Supply%20Chain%20Data%20Sources%20-%20SQL.xlsx&wdOrigin=TEAMS-MAGLEV.p2p_ns.rwc&action=default&mobileredirect=true] \n\nThanks,\n\nHarrison\n\nHave completed the source details of all SQL Server connections, down to the table level:\n\n* Server.Database.Schema.Table(View)\n\nEach source detail is mapped against a table within each Semantic Model.\n\nFor SQL connections, there are 2 methods to connect in PBI:\n\n# Selecting a table/view from a picklist against the given database\n# Custom SQL query\n\nSource details have been mapped for each SQL connection ‘type’.", "text": "Summary\nIdentifying the underlying source of Supply Chain Facts and Dims - SQL Server environments\n\n---\n\nDescription\nGoal:\n\n* Identifying the source of Supply Chain Facts and Dims at the server-database-table level\n** Including the underlying SQL code written\n\nAcceptance criteria:\n\n* Location address of the source of Supply Chain Facts and Dims\n\nPostulation:\n\n* Identifying the tables on which server that has most usage and prioritising it by 1,2,3 so that we can make the most client impact in our migration.\n\nWhat does this solve:\n\n* Performance issues\n* Access issues.\n\n---\n\nComments\nMerged with [https://sigmahealthcare.atlassian.net/browse/CSCI-84|https://sigmahealthcare.atlassian.net/browse/CSCI-84] @user\n\nLink provided below in chat by @user \n\n[https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B30678332-5293-4A0C-84A6-26488C56A8CD%7D&file=Supply%20Chain%20Data%20Sources%20-%20SQL.xlsx&wdOrigin=TEAMS-MAGLEV.p2p_ns.rwc&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7B30678332-5293-4A0C-84A6-26488C56A8CD%7D&file=Supply%20Chain%20Data%20Sources%20-%20SQL.xlsx&wdOrigin=TEAMS-MAGLEV.p2p_ns.rwc&action=default&mobileredirect=true] \n\nThanks,\n\nHarrison\n\nHave completed the source details of all SQL Server connections, down to the table level:\n\n* Server.Database.Schema.Table(View)\n\nEach source detail is mapped against a table within each Semantic Model.\n\nFor SQL connections, there are 2 methods to connect in PBI:\n\n# Selecting a table/view from a picklist against the given database\n# Custom SQL query\n\nSource details have been mapped for each SQL connection ‘type’.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 481}}
{"issue_key": "CSCI-82", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Duplicate", "priority": "Medium", "created": "17/Jun/25 11:40 AM", "updated": "19/Nov/25 12:30 PM", "labels": [], "summary": "Understanding Current State - Identifying KPIs used in reports", "description": "Acceptance critieria:\n\n* Listing KPIs that are currently used for Current reports that are in used in CW Supply Chain.", "acceptance_criteria": "", "comments": "", "text": "Summary\nUnderstanding Current State - Identifying KPIs used in reports\n\n---\n\nDescription\nAcceptance critieria:\n\n* Listing KPIs that are currently used for Current reports that are in used in CW Supply Chain.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 482}}
{"issue_key": "CSCI-81", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "17/Jun/25 11:39 AM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "Grouping DIMs that can be merged", "description": "h3. Dimensions\n\n \n\nList of Dim’s used across models (list based on table name in semantic model containing ‘Dim’):\n\n* DimDFIOSKU\n* DimDFIOProduct\n* DimService\n* DimDate\n* DimItemCode\n* DimProduct\n* DimProductMAWM\n* DimProductUOM\n* DimProductUOM_MAWM\n* DimStore\n* DimOmniStore\n* DimStoreGroup\n* DimAutoStoreSKUs\n* DimDCCycleCountMasterPlan\n* DimDCLocation\n* DimDCNonShippingWeekdays\n* DimDistributionCentre\n* DimVendor\n* DimExemptedSuppliers\n* DimExpiryStatus\n* DimManufacturer\n* DimMobileDOCK\n\n \n\n{color:#bf2600}*Action:*{color} Group dim’s that can be merged as a single Dim (eg: store, omni-store and store group)\n\n*Definition of Done:*\n\n* DIMs identified and mapped - with business definitions.\n* Presented as a list", "acceptance_criteria": "", "comments": "Work being completed in this excel doc:\n[Supply Chain Dimension Discovery.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Supply%20Chain%20Dimension%20Discovery.xlsx?d=wd4f6edcd6a8c420d9e177ac2fc5b8d88&csf=1&web=1&e=RFejOX]\n\nInitial discovery complete.\n\nHave queried each Dim table to understand if some of the above can be grouped. Have estimated we can have 9 unique Dim’s to cover all dimension requirements for current supply chain reporting:\n\nNote: (blank) in the above refers to us dropping these 3 tables as Dims (upon inspection, 1 table is actually a fact - just labelled as a dim, the other 2 are not proper dimension tables (eg: 1 column)).\n\nAll work has been saved to the linked excel workbook.\n\nOf all Dimensions listed, DimProduct is going to be the most detailed to create. Upon inspection there are many Product Identifiers used across multiple systems/processes (noted in ‘Dim Product’ sheet of excel workbook):\n\n* MyChemID (internal product ID - key identifier)\n* SKU ID\n* SKU Ref\n* PLU\n* Item_Code (VERY similar to MyChemID but different)\n* ItemWHSKey\n* Product_SK\n* ProductUOMIDKey (ID for product-UOM grain - lowest grain we need to establish)\n* ProductBarcode", "text": "Summary\nGrouping DIMs that can be merged\n\n---\n\nDescription\nh3. Dimensions\n\n \n\nList of Dim’s used across models (list based on table name in semantic model containing ‘Dim’):\n\n* DimDFIOSKU\n* DimDFIOProduct\n* DimService\n* DimDate\n* DimItemCode\n* DimProduct\n* DimProductMAWM\n* DimProductUOM\n* DimProductUOM_MAWM\n* DimStore\n* DimOmniStore\n* DimStoreGroup\n* DimAutoStoreSKUs\n* DimDCCycleCountMasterPlan\n* DimDCLocation\n* DimDCNonShippingWeekdays\n* DimDistributionCentre\n* DimVendor\n* DimExemptedSuppliers\n* DimExpiryStatus\n* DimManufacturer\n* DimMobileDOCK\n\n \n\n{color:#bf2600}*Action:*{color} Group dim’s that can be merged as a single Dim (eg: store, omni-store and store group)\n\n*Definition of Done:*\n\n* DIMs identified and mapped - with business definitions.\n* Presented as a list\n\n---\n\nComments\nWork being completed in this excel doc:\n[Supply Chain Dimension Discovery.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Supply%20Chain%20Dimension%20Discovery.xlsx?d=wd4f6edcd6a8c420d9e177ac2fc5b8d88&csf=1&web=1&e=RFejOX]\n\nInitial discovery complete.\n\nHave queried each Dim table to understand if some of the above can be grouped. Have estimated we can have 9 unique Dim’s to cover all dimension requirements for current supply chain reporting:\n\nNote: (blank) in the above refers to us dropping these 3 tables as Dims (upon inspection, 1 table is actually a fact - just labelled as a dim, the other 2 are not proper dimension tables (eg: 1 column)).\n\nAll work has been saved to the linked excel workbook.\n\nOf all Dimensions listed, DimProduct is going to be the most detailed to create. Upon inspection there are many Product Identifiers used across multiple systems/processes (noted in ‘Dim Product’ sheet of excel workbook):\n\n* MyChemID (internal product ID - key identifier)\n* SKU ID\n* SKU Ref\n* PLU\n* Item_Code (VERY similar to MyChemID but different)\n* ItemWHSKey\n* Product_SK\n* ProductUOMIDKey (ID for product-UOM grain - lowest grain we need to establish)\n* ProductBarcode", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 483}}
{"issue_key": "CSCI-80", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Duplicate", "priority": "Medium", "created": "17/Jun/25 11:39 AM", "updated": "19/Nov/25 12:30 PM", "labels": [], "summary": "Grouping SC by KPI/Measure type", "description": "Currently across the 42 semantic models, we have *692 unique/individual measures*. Some of these can be referencing the same value but expressing this as a different output (eg: *sales* in absolute $'s, *sales* as a %, *sales* last 60 days, etc).\n\nAttached is a list of all measures as of 16.6.25.\n\n[^Supply Chain Measures.xlsx]\n\n{color:#bf2600}*Action:*{color} Work through this list to group by KPI/measure type (eg: group sales as absolute, sales as % and sales last 60 days as ‘Sales Group’ etc).\n\n*Definition of done:*\n\n* KPI and measures are identified and paired with business definitions as a table.", "acceptance_criteria": "", "comments": "@user is it okay to review this definition of done? thanks.\n\nHey @user , @user ,\n\nMy thought this should be not immediate priority.\n\nBelow are some of the analysis and deliverables I think we should include.\n\nBut @user please overlay your thoughts on this, rip what I said apart, add your thoughts on ways to step through the Dimensional Modelling delivery so we can all agree.\n\nHappy for you to come up with thoughts on top of this, remove and add what you like then maybe we have a session next week to confirm how we implement. \n\n*Business Process Review*\n\n* Business Processes captured in Tactical models vs captured in existing documentation\n** Whats included in doc that shouldnt be\n** Whats not included in doc that should be\n** Any definition or source information easily captured from this analysis?\n\n*Dimension Review*\n\n* Review Dimensions captured in Tactical models vs captured in existing documentation\n** Whats included in doc that shouldnt be\n** Whats included in doc that should be\n** Any definition or source information easily captured from this analysis? Merging of Dims etc\n\n*BUS Matrix*\n\n* Completion of HL BUS Matrix after above, mapping of Business Processes and Dims\n* Include High Level Source mapping where feasible, has already been attempted by Amit and have EDP documents to assist also\n\n*Business Metrics Definition*\n\n* Business Metrics list\n* Mapping to core Business Process\n* Metric Definitions\n* Probably worth doing iteratively based on priority Business Processes\n\n*Data Source Mapping*\n\n* Business Process mapping to\n** Source System\n** DB\n** Object and transformation\n\n*Dimensional Modelling*\n\n* Source to Target documentation\n** Business Process(Fact) - PK, Grain, column mapping, metric calculation\n** Dimension (Dim) - PK, Grain, column mapping, attribute definition\n\ncc @user \n\nThanks,\n\nHarrison\n\nHi @user,\n\nYour points below align to the convo @user and I had yesterday.\n\nAgree that this task needs to be completed after we have reviewed the business processes/facts first (task [https://sigmahealthcare.atlassian.net/browse/CSCI-94|https://sigmahealthcare.atlassian.net/browse/CSCI-94]). The dimension review has already been completed ([https://sigmahealthcare.atlassian.net/browse/CSCI-81|https://sigmahealthcare.atlassian.net/browse/CSCI-81]).\n\nChloe and I spoke that after the fact review has been complete that I will then go into our Bus Matrix document and update/add any dims and facts that are not currently included.\n\nThe data source mapping work I think will need to be its own separate task initially. From what I can see from the current Supply Chain Semantic Models, there are a lot of hops their data source goes through to go from source system to report. Majority of their connections in PBI service are to the *PBI05* server.\n\nExample of some of the lineage:\n\nPBI05 (BI Server) ← PDB08 (AX 2012 Server) ← Azure Blob Storage (for a CSV file load)\n\nChloe and I are going to look into this further next week to understand what transformations are done between each hop/source 🙂 \n\nI have details of the source connection info that supply chain team are using in PBI service, so once we unpack the lineage from the actual source system, we can join the dots to complete this task.", "text": "Summary\nGrouping SC by KPI/Measure type\n\n---\n\nDescription\nCurrently across the 42 semantic models, we have *692 unique/individual measures*. Some of these can be referencing the same value but expressing this as a different output (eg: *sales* in absolute $'s, *sales* as a %, *sales* last 60 days, etc).\n\nAttached is a list of all measures as of 16.6.25.\n\n[^Supply Chain Measures.xlsx]\n\n{color:#bf2600}*Action:*{color} Work through this list to group by KPI/measure type (eg: group sales as absolute, sales as % and sales last 60 days as ‘Sales Group’ etc).\n\n*Definition of done:*\n\n* KPI and measures are identified and paired with business definitions as a table.\n\n---\n\nComments\n@user is it okay to review this definition of done? thanks.\n\nHey @user , @user ,\n\nMy thought this should be not immediate priority.\n\nBelow are some of the analysis and deliverables I think we should include.\n\nBut @user please overlay your thoughts on this, rip what I said apart, add your thoughts on ways to step through the Dimensional Modelling delivery so we can all agree.\n\nHappy for you to come up with thoughts on top of this, remove and add what you like then maybe we have a session next week to confirm how we implement. \n\n*Business Process Review*\n\n* Business Processes captured in Tactical models vs captured in existing documentation\n** Whats included in doc that shouldnt be\n** Whats not included in doc that should be\n** Any definition or source information easily captured from this analysis?\n\n*Dimension Review*\n\n* Review Dimensions captured in Tactical models vs captured in existing documentation\n** Whats included in doc that shouldnt be\n** Whats included in doc that should be\n** Any definition or source information easily captured from this analysis? Merging of Dims etc\n\n*BUS Matrix*\n\n* Completion of HL BUS Matrix after above, mapping of Business Processes and Dims\n* Include High Level Source mapping where feasible, has already been attempted by Amit and have EDP documents to assist also\n\n*Business Metrics Definition*\n\n* Business Metrics list\n* Mapping to core Business Process\n* Metric Definitions\n* Probably worth doing iteratively based on priority Business Processes\n\n*Data Source Mapping*\n\n* Business Process mapping to\n** Source System\n** DB\n** Object and transformation\n\n*Dimensional Modelling*\n\n* Source to Target documentation\n** Business Process(Fact) - PK, Grain, column mapping, metric calculation\n** Dimension (Dim) - PK, Grain, column mapping, attribute definition\n\ncc @user \n\nThanks,\n\nHarrison\n\nHi @user,\n\nYour points below align to the convo @user and I had yesterday.\n\nAgree that this task needs to be completed after we have reviewed the business processes/facts first (task [https://sigmahealthcare.atlassian.net/browse/CSCI-94|https://sigmahealthcare.atlassian.net/browse/CSCI-94]). The dimension review has already been completed ([https://sigmahealthcare.atlassian.net/browse/CSCI-81|https://sigmahealthcare.atlassian.net/browse/CSCI-81]).\n\nChloe and I spoke that after the fact review has been complete that I will then go into our Bus Matrix document and update/add any dims and facts that are not currently included.\n\nThe data source mapping work I think will need to be its own separate task initially. From what I can see from the current Supply Chain Semantic Models, there are a lot of hops their data source goes through to go from source system to report. Majority of their connections in PBI service are to the *PBI05* server.\n\nExample of some of the lineage:\n\nPBI05 (BI Server) ← PDB08 (AX 2012 Server) ← Azure Blob Storage (for a CSV file load)\n\nChloe and I are going to look into this further next week to understand what transformations are done between each hop/source 🙂 \n\nI have details of the source connection info that supply chain team are using in PBI service, so once we unpack the lineage from the actual source system, we can join the dots to complete this task.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 484}}
{"issue_key": "CSCI-79", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Duplicate", "priority": "Medium", "created": "17/Jun/25 11:37 AM", "updated": "19/Nov/25 12:30 PM", "labels": [], "summary": "Grouping Semantic Models", "description": "Rachel and the Supply Chain team have already grouped their own semantic models into +*12 ‘master models’*+ which have already been created:\n\nh4. DC\n\n# [DC Cycle Count|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/f062aba7-efc7-4147-8cf2-7af21874a874/details?experience=power-bi]\n# [DC Employee Performance|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/2f02aa2e-3858-4015-ab7f-9f4b78495405/details?experience=power-bi]\n# [DC Inbound Shipments|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/596c4534-aae5-45ce-948c-71c836b3875d/details?experience=power-bi]\n# [DC Inventory Location|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/9deaba9d-b41b-4f5c-a3e1-19fba0e4404a/details?experience=power-bi]\n# [DC Inventory|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/d9fa86fd-cc2c-4082-bb3b-119fd7c4d3b7/details?experience=power-bi]\n# [DC MobileDOCK|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/3066b4b6-c9fb-46d9-89d2-4f6e7dbf5ff7/details?experience=power-bi]\n# [DC Outbound Shipments|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/0fd5f627-fafc-411e-aea3-77d6838ec2dc/details?experience=power-bi]\n# [DC Shipment Wave|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/a07880d1-5275-4d9a-bbc5-68b416e367d5/details?experience=power-bi]\n\nh4. Store\n\n# [DFIO|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/0234fec9-bfdc-45fe-880c-18832b409343/details?experience=power-bi]\n# [Employee Safety|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/cfb5a6a9-0ad9-4bea-baf0-4f81ce8d2993/details?experience=power-bi]\n# [Store Inventory|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/ef4f1f73-051a-4464-bf57-eac4f2d82053/details?experience=power-bi]\n# [Store Sales|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/fd95fa38-7b01-4f36-b20c-19d72bc0d7ca/details?experience=power-bi]\n\n \n\nCurrently the Supply Chain team are working to re-map all their existing semantic models to reference only from these above 12 ‘master semantic models' to become as their new source.\n\nThere are currently *42 semantic models* in the Operations Analytics workspace in total (excluding their sandbox workspace). These 42 include the above 12, meaning there are currently *30 semantic models* that are created that do not form as part of this ‘master layer’.\n\nThe team are in the work-in-progress stage of this migration/re-mapping of their semantic models. For example currently the ‘S&OP Projections’ semantic model only has inputs coming from the ‘master layer’, whereas the ‘SPS KPIs’ semantic model currently has inputs only coming from a DB source (nothing from the master layer).\n\n \n\nThe advice would be:\n\n* Continue to re-map all their current semantic models to this master layer, to reduce the number of queries against the database servers\n* Work with the SC team to establish if we can group these models further (eg: can we group ‘DC Inbound Shipments’, ‘DC Outbound Shipments’ and ‘DC Shipment Wave’ as 1 single ‘DC Shipments’ model?)\n\n*definition of done:*\n\n* Semanitc models mapped - with business definitions.", "acceptance_criteria": "", "comments": "@user is it okay to review this definition of done? Thanks.", "text": "Summary\nGrouping Semantic Models\n\n---\n\nDescription\nRachel and the Supply Chain team have already grouped their own semantic models into +*12 ‘master models’*+ which have already been created:\n\nh4. DC\n\n# [DC Cycle Count|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/f062aba7-efc7-4147-8cf2-7af21874a874/details?experience=power-bi]\n# [DC Employee Performance|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/2f02aa2e-3858-4015-ab7f-9f4b78495405/details?experience=power-bi]\n# [DC Inbound Shipments|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/596c4534-aae5-45ce-948c-71c836b3875d/details?experience=power-bi]\n# [DC Inventory Location|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/9deaba9d-b41b-4f5c-a3e1-19fba0e4404a/details?experience=power-bi]\n# [DC Inventory|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/d9fa86fd-cc2c-4082-bb3b-119fd7c4d3b7/details?experience=power-bi]\n# [DC MobileDOCK|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/3066b4b6-c9fb-46d9-89d2-4f6e7dbf5ff7/details?experience=power-bi]\n# [DC Outbound Shipments|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/0fd5f627-fafc-411e-aea3-77d6838ec2dc/details?experience=power-bi]\n# [DC Shipment Wave|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/a07880d1-5275-4d9a-bbc5-68b416e367d5/details?experience=power-bi]\n\nh4. Store\n\n# [DFIO|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/0234fec9-bfdc-45fe-880c-18832b409343/details?experience=power-bi]\n# [Employee Safety|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/cfb5a6a9-0ad9-4bea-baf0-4f81ce8d2993/details?experience=power-bi]\n# [Store Inventory|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/ef4f1f73-051a-4464-bf57-eac4f2d82053/details?experience=power-bi]\n# [Store Sales|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/fd95fa38-7b01-4f36-b20c-19d72bc0d7ca/details?experience=power-bi]\n\n \n\nCurrently the Supply Chain team are working to re-map all their existing semantic models to reference only from these above 12 ‘master semantic models' to become as their new source.\n\nThere are currently *42 semantic models* in the Operations Analytics workspace in total (excluding their sandbox workspace). These 42 include the above 12, meaning there are currently *30 semantic models* that are created that do not form as part of this ‘master layer’.\n\nThe team are in the work-in-progress stage of this migration/re-mapping of their semantic models. For example currently the ‘S&OP Projections’ semantic model only has inputs coming from the ‘master layer’, whereas the ‘SPS KPIs’ semantic model currently has inputs only coming from a DB source (nothing from the master layer).\n\n \n\nThe advice would be:\n\n* Continue to re-map all their current semantic models to this master layer, to reduce the number of queries against the database servers\n* Work with the SC team to establish if we can group these models further (eg: can we group ‘DC Inbound Shipments’, ‘DC Outbound Shipments’ and ‘DC Shipment Wave’ as 1 single ‘DC Shipments’ model?)\n\n*definition of done:*\n\n* Semanitc models mapped - with business definitions.\n\n---\n\nComments\n@user is it okay to review this definition of done? Thanks.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 485}}
{"issue_key": "CSCI-78", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "17/Jun/25 11:35 AM", "updated": "27/Jun/25 9:48 AM", "labels": ["Planning"], "summary": "Requirements Gathering questions", "description": "Document [here|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Requirements%20Gathering%20-%20Draft%201.docx?d=w82871d69c81340c7a2edd5160b87d809&csf=1&web=1&e=GPMYyh] for review.\n\nObjective:\n\n* Story on this sprint - -to update the requriements/getting definitions- to update on questions to approach to Rachel\n** If not, finding out our point of contact.\n* Highlighting what the top 3 Use cases are.\n\nDefinition of done;\n\n* -Questions listed to ask Rachel/her team-\n** questions reviewed by the team.\n* t-op 3 priorities listed-\n* Find out who our point of contact is.\n\nExpectation from stakeholders:\n\n* *they should know what they want.*\n* They will know what their top 3 priorities are when it comes to reports", "acceptance_criteria": "", "comments": "Hi @user - In preparation for tomorrow’s meeting about Supply chain Modelling Progress and Plan , I have prepared a document on what type of questions we can approach Rachel with.\n\nFeel free to have a look before the meetup tomorrow.\n\nThanks,\nPhil", "text": "Summary\nRequirements Gathering questions\n\n---\n\nDescription\nDocument [here|https://mychemist.sharepoint.com/:w:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Requirements%20Gathering%20-%20Draft%201.docx?d=w82871d69c81340c7a2edd5160b87d809&csf=1&web=1&e=GPMYyh] for review.\n\nObjective:\n\n* Story on this sprint - -to update the requriements/getting definitions- to update on questions to approach to Rachel\n** If not, finding out our point of contact.\n* Highlighting what the top 3 Use cases are.\n\nDefinition of done;\n\n* -Questions listed to ask Rachel/her team-\n** questions reviewed by the team.\n* t-op 3 priorities listed-\n* Find out who our point of contact is.\n\nExpectation from stakeholders:\n\n* *they should know what they want.*\n* They will know what their top 3 priorities are when it comes to reports\n\n---\n\nComments\nHi @user - In preparation for tomorrow’s meeting about Supply chain Modelling Progress and Plan , I have prepared a document on what type of questions we can approach Rachel with.\n\nFeel free to have a look before the meetup tomorrow.\n\nThanks,\nPhil", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 486}}
{"issue_key": "CSCI-77", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "17/Jun/25 10:08 AM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "Review of BUS Matrix Dimension Mapping and Source", "description": "Review BUS Matrix Draft completed by Chloe in [https://sigmahealthcare.atlassian.net/browse/CSCI-76|https://sigmahealthcare.atlassian.net/browse/CSCI-76] .\n\n# Include Source System mapping in column identified in BUS Matrix sheet\n# Review Dimensional Mapping in BUS Matrix", "acceptance_criteria": "", "comments": "Reviewed the Bus Matrix Detailed and added the sources for reporting and true sources. Looks good.", "text": "Summary\nReview of BUS Matrix Dimension Mapping and Source\n\n---\n\nDescription\nReview BUS Matrix Draft completed by Chloe in [https://sigmahealthcare.atlassian.net/browse/CSCI-76|https://sigmahealthcare.atlassian.net/browse/CSCI-76] .\n\n# Include Source System mapping in column identified in BUS Matrix sheet\n# Review Dimensional Mapping in BUS Matrix\n\n---\n\nComments\nReviewed the Bus Matrix Detailed and added the sources for reporting and true sources. Looks good.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 487}}
{"issue_key": "CSCI-76", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "17/Jun/25 10:04 AM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "BUS Matrix Draft 1.0", "description": "Creation of draft BUS Matrix for Supply Chain Data Modelling.\n\nSheet “BUS Matrix“ in [https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true]", "acceptance_criteria": "", "comments": "created a Bus Matrix based on Amit’s business processes and dimensions", "text": "Summary\nBUS Matrix Draft 1.0\n\n---\n\nDescription\nCreation of draft BUS Matrix for Supply Chain Data Modelling.\n\nSheet “BUS Matrix“ in [https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true]\n\n---\n\nComments\ncreated a Bus Matrix based on Amit’s business processes and dimensions", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 488}}
{"issue_key": "CSCI-75", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "17/Jun/25 10:03 AM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "Dimension Identification", "description": "Creation of working document with Dimensions Identified for Supply Chain Data Modelling. Both conformed and BP specific (junk).\n\nSheet “Dimension“ in [https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true]", "acceptance_criteria": "", "comments": "", "text": "Summary\nDimension Identification\n\n---\n\nDescription\nCreation of working document with Dimensions Identified for Supply Chain Data Modelling. Both conformed and BP specific (junk).\n\nSheet “Dimension“ in [https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 489}}
{"issue_key": "CSCI-74", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "17/Jun/25 10:03 AM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "Business Process Identification", "description": "Creation of working document with critical Business Processes Identified for Supply Chain Data Modelling.\n\nSheet “Business Process“ in [https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true]", "acceptance_criteria": "", "comments": "", "text": "Summary\nBusiness Process Identification\n\n---\n\nDescription\nCreation of working document with critical Business Processes Identified for Supply Chain Data Modelling.\n\nSheet “Business Process“ in [https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true]", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 490}}
{"issue_key": "CSCI-71", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Jun/25 4:14 PM", "updated": "18/Jul/25 3:51 PM", "labels": [], "summary": "StockDb Integration - part 1", "description": "Build data ingestion pipelines for the StockDb database.\n\nDefinition of done:\n\n* Extract meta created\n* Linked services created\n* Pipeline run end to end", "acceptance_criteria": "Tables available in Snowflake and pass validation test", "comments": "Pipeline and extract meta setup. The following tables have been ingested:\n\n* BUYERS\n* PRODUCTS @user@user\n\nPending a few more tables from StockDb to be ingested to Snowflake next sprint:\n\n|MultiBuyTriggers|\n|ProductGroup|\n|ProductNetworkCosts|\n|ProductsDrugMatch|\n|SOHAdjustmentTypes|\n|SupplierDetails|\n\nAnd the below which will require a historical load + delta pattern: \n\n|Table|RawData|Refresh Freq| Total Row Count |\n|ProductNetworkCostsHistory|Yes|Real Time|########|\n\n@user @user\n\nI’ll duplicate this task:\n- part 1 reflects work done in this sprint\n- part 2 reflects work to be done on next sprint\n\n @user", "text": "Summary\nStockDb Integration - part 1\n\n---\n\nDescription\nBuild data ingestion pipelines for the StockDb database.\n\nDefinition of done:\n\n* Extract meta created\n* Linked services created\n* Pipeline run end to end\n\n---\n\nAcceptance Criteria\nTables available in Snowflake and pass validation test\n\n---\n\nComments\nPipeline and extract meta setup. The following tables have been ingested:\n\n* BUYERS\n* PRODUCTS @user@user\n\nPending a few more tables from StockDb to be ingested to Snowflake next sprint:\n\n|MultiBuyTriggers|\n|ProductGroup|\n|ProductNetworkCosts|\n|ProductsDrugMatch|\n|SOHAdjustmentTypes|\n|SupplierDetails|\n\nAnd the below which will require a historical load + delta pattern: \n\n|Table|RawData|Refresh Freq| Total Row Count |\n|ProductNetworkCostsHistory|Yes|Real Time|########|\n\n@user @user\n\nI’ll duplicate this task:\n- part 1 reflects work done in this sprint\n- part 2 reflects work to be done on next sprint\n\n @user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 491}}
{"issue_key": "CSCI-69", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Story", "status": "Done", "priority": "Medium", "created": "16/Jun/25 4:13 PM", "updated": "20/Jun/25 10:33 AM", "labels": [], "summary": "Business Process (Current State)", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nBusiness Process (Current State)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 492}}
{"issue_key": "CSCI-68", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Story", "status": "Done", "priority": "Medium", "created": "16/Jun/25 4:13 PM", "updated": "23/Jun/25 9:43 AM", "labels": [], "summary": "BUS Matrix", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nBUS Matrix", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 493}}
{"issue_key": "CSCI-67", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Jun/25 4:13 PM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "Infrastructure", "description": "Objective:\n\n* to setup environments so that data can be loaded onto servers to be ready to ETL\n** This will entail the set up of \n*** ADF\n*** Snowflake\n**** DEV\n**** SIT\n**** UAT\n**** Prod", "acceptance_criteria": "", "comments": "", "text": "Summary\nInfrastructure\n\n---\n\nDescription\nObjective:\n\n* to setup environments so that data can be loaded onto servers to be ready to ETL\n** This will entail the set up of \n*** ADF\n*** Snowflake\n**** DEV\n**** SIT\n**** UAT\n**** Prod", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 494}}
{"issue_key": "CSCI-64", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Jun/25 4:13 PM", "updated": "24/Jun/25 2:40 PM", "labels": [], "summary": "Snowflake GIT Integration approach", "description": "Establish the approach - 3 options Flyway/ Devops V - R script, Flyway R Script or Snowflake declarative \n\n*Definition of done*\n\n* To research on Snowflake’s offering \n* Chloe/ Eugene to agree and confirm the Git approach for Snowflake", "acceptance_criteria": "", "comments": "@user is it okay if you entered in definition of done for this task? thanks!\n\nDiscussed with Eugene the options with Snowflake deployments:\n\n* Flyway/ Devops V - R script - this was implemented at Sigma. Very complex and high maintenance \n* Flyway R Script - Similar to the above but still complex/ high maintenance. \n* Snowflake declarative - new feature, simpler than the above", "text": "Summary\nSnowflake GIT Integration approach\n\n---\n\nDescription\nEstablish the approach - 3 options Flyway/ Devops V - R script, Flyway R Script or Snowflake declarative \n\n*Definition of done*\n\n* To research on Snowflake’s offering \n* Chloe/ Eugene to agree and confirm the Git approach for Snowflake\n\n---\n\nComments\n@user is it okay if you entered in definition of done for this task? thanks!\n\nDiscussed with Eugene the options with Snowflake deployments:\n\n* Flyway/ Devops V - R script - this was implemented at Sigma. Very complex and high maintenance \n* Flyway R Script - Similar to the above but still complex/ high maintenance. \n* Snowflake declarative - new feature, simpler than the above", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 495}}
{"issue_key": "CSCI-61", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/May/25 11:04 AM", "updated": "17/Jun/25 10:36 AM", "labels": [], "summary": "CW BUS matrix", "description": "expected 22/5\n\nalongside with business identification, we will need the bus Matrix so that a semantic model can be generated and thus generate business reports and other data products.\n\n[Business Process sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Business%20Matrix_Supply_Chain.xlsx?d=wcb15417700074f2ea34019d98b5c6170&csf=1&web=1&e=WCRVjt&nav=MTVfezQ0QTYyMDkzLUEwNTEtNDlCNS05ODZDLUVBREIyNkQxMkMyMH0]", "acceptance_criteria": "", "comments": "expected 22/5\n\n@user has added updates to this ticket - will add link to this ticket and align with Amit’s work\n\nUpdated Jira with link to BUS matrix\n\nUpdating:\n\n* Business Process\n* Dimensions", "text": "Summary\nCW BUS matrix\n\n---\n\nDescription\nexpected 22/5\n\nalongside with business identification, we will need the bus Matrix so that a semantic model can be generated and thus generate business reports and other data products.\n\n[Business Process sheet|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Business%20Matrix_Supply_Chain.xlsx?d=wcb15417700074f2ea34019d98b5c6170&csf=1&web=1&e=WCRVjt&nav=MTVfezQ0QTYyMDkzLUEwNTEtNDlCNS05ODZDLUVBREIyNkQxMkMyMH0]\n\n---\n\nComments\nexpected 22/5\n\n@user has added updates to this ticket - will add link to this ticket and align with Amit’s work\n\nUpdated Jira with link to BUS matrix\n\nUpdating:\n\n* Business Process\n* Dimensions", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 496}}
{"issue_key": "CSCI-60", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/Apr/25 3:32 PM", "updated": "09/Jul/25 2:57 PM", "labels": [], "summary": "Discovery - Altis to Engage with cyber / Vinay on what's we are trying to achieve", "description": "", "acceptance_criteria": "", "comments": "Hey @user can you give me some info about Vinay when you have the chance?\n\n@user I will follow up with Rhubesh. I am not across this activity.", "text": "Summary\nDiscovery - Altis to Engage with cyber / Vinay on what's we are trying to achieve\n\n---\n\nComments\nHey @user can you give me some info about Vinay when you have the chance?\n\n@user I will follow up with Rhubesh. I am not across this activity.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 497}}
{"issue_key": "CSCI-59", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "12/May/25 11:04 AM", "updated": "09/Jul/25 12:37 PM", "labels": [], "summary": "CW SC - Business Process Identification", "description": "We’ll need to identify the CW SC business processes - namely to identify what are the business transactions\n\n- warehouse, purchase, DFIO\n\ndone list:\n\n* inventory\n* SAles\n* DFIO\n* @user to update.", "acceptance_criteria": "", "comments": "expected 22/5\n\n|Section|Status|\n|Sales|Complete|\n|Supply Chain Source Systems Identification|Complete|\n|Warehouse Management|70% Complete|\n|Purchases|50% Complete|\n|Inventory Management|40% Complete|\n|DFIO|Not Started|", "text": "Summary\nCW SC - Business Process Identification\n\n---\n\nDescription\nWe’ll need to identify the CW SC business processes - namely to identify what are the business transactions\n\n- warehouse, purchase, DFIO\n\ndone list:\n\n* inventory\n* SAles\n* DFIO\n* @user to update.\n\n---\n\nComments\nexpected 22/5\n\n|Section|Status|\n|Sales|Complete|\n|Supply Chain Source Systems Identification|Complete|\n|Warehouse Management|70% Complete|\n|Purchases|50% Complete|\n|Inventory Management|40% Complete|\n|DFIO|Not Started|", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 498}}
{"issue_key": "CSCI-58", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Jun/25 3:13 PM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "ADF - Snowflake connectivity", "description": "Objective:\n\n* to Ensure secure housing and transfer of data from Source, to Snowflake, and also a secure-link.\n\nAcceptance criteria:\n\n* to have data not being able to accessed by the internet (i.e. an open port) but onliy allowed authenticated users to be able to access data.\n* Data will not be interceptable either from source/warehouse/azure servers and anywhere in between i.e. (secure layer)", "acceptance_criteria": "", "comments": "This is addressed here: [User Story 191088 Configure ADF Linked Services|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/191088]\n\nADF is connected to Snowflake via private link.", "text": "Summary\nADF - Snowflake connectivity\n\n---\n\nDescription\nObjective:\n\n* to Ensure secure housing and transfer of data from Source, to Snowflake, and also a secure-link.\n\nAcceptance criteria:\n\n* to have data not being able to accessed by the internet (i.e. an open port) but onliy allowed authenticated users to be able to access data.\n* Data will not be interceptable either from source/warehouse/azure servers and anywhere in between i.e. (secure layer)\n\n---\n\nComments\nThis is addressed here: [User Story 191088 Configure ADF Linked Services|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/191088]\n\nADF is connected to Snowflake via private link.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 499}}
{"issue_key": "CSCI-57", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Jun/25 3:13 PM", "updated": "14/Jul/25 2:39 PM", "labels": [], "summary": "Snowflake - Privatelink URL", "description": "[devops 191105|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/191105]\n\nTo connect to Snowflake over PrivateLink, so data never leaves Microsoft’s backbone.\n\nsee reference: [+Azure Private Link and Snowflake | Snowflake Documentation+|https://docs.snowflake.com/en/user-guide/privatelink-azure]\n\nCurrent status - Blocked:\n- We are waiting on DNS integration - Awaiting on cloud team for setup.", "acceptance_criteria": "", "comments": "This is mosty done, just waiting for the DNS integration, see [User Story 191105 Snowflake PrivateLink|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/191105] for reference.\n\n* Snowflake account has PrivateLink enabled\n* ADF and SHIR can resolve and reach the endpoint\n* Query latency is consistent with private path\n\nNext step: DNS integration\n\n@user - Can we know what we are waiting for on this? We’re just not clear on the ticket in regards to this. I’ll raise it on standup.", "text": "Summary\nSnowflake - Privatelink URL\n\n---\n\nDescription\n[devops 191105|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/191105]\n\nTo connect to Snowflake over PrivateLink, so data never leaves Microsoft’s backbone.\n\nsee reference: [+Azure Private Link and Snowflake | Snowflake Documentation+|https://docs.snowflake.com/en/user-guide/privatelink-azure]\n\nCurrent status - Blocked:\n- We are waiting on DNS integration - Awaiting on cloud team for setup.\n\n---\n\nComments\nThis is mosty done, just waiting for the DNS integration, see [User Story 191105 Snowflake PrivateLink|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_workitems/edit/191105] for reference.\n\n* Snowflake account has PrivateLink enabled\n* ADF and SHIR can resolve and reach the endpoint\n* Query latency is consistent with private path\n\nNext step: DNS integration\n\n@user - Can we know what we are waiting for on this? We’re just not clear on the ticket in regards to this. I’ll raise it on standup.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 500}}
{"issue_key": "CSCI-56", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Jun/25 3:12 PM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "Snowflake - DLA setup", "description": "Objective: \n\n* To create Data Ingestion Framework.\n\nDescription: \n\n* Run setup scripts for DLA framework:\n\n# DLA tables\n# DLA stored procs\n# DLA views", "acceptance_criteria": "", "comments": "", "text": "Summary\nSnowflake - DLA setup\n\n---\n\nDescription\nObjective: \n\n* To create Data Ingestion Framework.\n\nDescription: \n\n* Run setup scripts for DLA framework:\n\n# DLA tables\n# DLA stored procs\n# DLA views", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 501}}
{"issue_key": "CSCI-55", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Duplicate", "priority": "Medium", "created": "05/Jun/25 3:27 PM", "updated": "20/Jun/25 1:23 PM", "labels": [], "summary": "Review of existing SC semantic models part 2 - Connecting with BUS Matrix -", "description": "GAP analysis \n\n* Comparing documented business process and existing semantic layer\n** 2 way check.\n** \nReviewing existing Supply Chain Semanit models for CW and seeing where improvements are needed.", "acceptance_criteria": "", "comments": "", "text": "Summary\nReview of existing SC semantic models part 2 - Connecting with BUS Matrix -\n\n---\n\nDescription\nGAP analysis \n\n* Comparing documented business process and existing semantic layer\n** 2 way check.\n** \nReviewing existing Supply Chain Semanit models for CW and seeing where improvements are needed.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 502}}
{"issue_key": "CSCI-54", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Duplicate", "priority": "Medium", "created": "15/May/25 9:24 AM", "updated": "20/Jun/25 1:23 PM", "labels": [], "summary": "Review of existing CW supply chain semantic models", "description": "Reviewing existing Supply Chain Semanit models for CW and seeing where improvements are needed.", "acceptance_criteria": "", "comments": "", "text": "Summary\nReview of existing CW supply chain semantic models\n\n---\n\nDescription\nReviewing existing Supply Chain Semanit models for CW and seeing where improvements are needed.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 503}}
{"issue_key": "CSCI-53", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Jun/25 9:26 AM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "Eugene's Azure Devops task migration to JIRA", "description": "Migrating tasks from [devops|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_sprints/backlog/Enterprise%20Data%20Platform%20Implementation/Enterprise%20Data%20Platform%20Implementation/Snowflake%20Integration?workitem=192242] to JIRA\n\nWill ask for automation tooling if required", "acceptance_criteria": "", "comments": "", "text": "Summary\nEugene's Azure Devops task migration to JIRA\n\n---\n\nDescription\nMigrating tasks from [devops|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_sprints/backlog/Enterprise%20Data%20Platform%20Implementation/Enterprise%20Data%20Platform%20Implementation/Snowflake%20Integration?workitem=192242] to JIRA\n\nWill ask for automation tooling if required", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 504}}
{"issue_key": "CSCI-52", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "11/Jun/25 1:26 PM", "updated": "09/Jul/25 12:37 PM", "labels": [], "summary": "Sample business METRIC and DEFINITIONS", "description": "[Sample Business Metric and Definitions.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Sample%20Business%20Metric%20and%20Definitions.xlsx?d=w212314dd6ad0413881539bb97fb30134&csf=1&web=1&e=sL7AiX]\n\n* Parts of EWM\n* Parts of Supplier\n* Whole of KPI reporting\n* -Whole of supply chain-", "acceptance_criteria": "", "comments": "", "text": "Summary\nSample business METRIC and DEFINITIONS\n\n---\n\nDescription\n[Sample Business Metric and Definitions.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/Shared%20Documents/01.%20Discovery/Sample%20Business%20Metric%20and%20Definitions.xlsx?d=w212314dd6ad0413881539bb97fb30134&csf=1&web=1&e=sL7AiX]\n\n* Parts of EWM\n* Parts of Supplier\n* Whole of KPI reporting\n* -Whole of supply chain-", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 505}}
{"issue_key": "CSCI-50", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Story", "status": "Done", "priority": "Medium", "created": "13/Jun/25 11:27 AM", "updated": "18/Jul/25 12:42 PM", "labels": [], "summary": "BUS Matrix", "description": "* -Grouping of different Models together ( roughly belonging to similar business process/ Source Systems)-\n* -Different measures used in models and reports-\n* KPIs used in reports\n* -Dimensions used in reports- \n* -Facts used in reports-\n\nIf you can align Measures/KPIs & dimensions  to business process to the best of your knowledge , that would be awesome. Need not be 100 % accurate.\n\n*Definition of Done:*\n- List of KPI, Measures, and Dimensions identified and linked to business processes.", "acceptance_criteria": "", "comments": "h3. Grouping Semantic Models\n\nRachel and the Supply Chain team have already grouped their own semantic models into +*12 ‘master models’*+ which have already been created:\n\nh4. DC\n\n# [DC Cycle Count|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/f062aba7-efc7-4147-8cf2-7af21874a874/details?experience=power-bi]\n# [DC Employee Performance|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/2f02aa2e-3858-4015-ab7f-9f4b78495405/details?experience=power-bi]\n# [DC Inbound Shipments|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/596c4534-aae5-45ce-948c-71c836b3875d/details?experience=power-bi]\n# [DC Inventory Location|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/9deaba9d-b41b-4f5c-a3e1-19fba0e4404a/details?experience=power-bi]\n# [DC Inventory|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/d9fa86fd-cc2c-4082-bb3b-119fd7c4d3b7/details?experience=power-bi]\n# [DC MobileDOCK|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/3066b4b6-c9fb-46d9-89d2-4f6e7dbf5ff7/details?experience=power-bi]\n# [DC Outbound Shipments|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/0fd5f627-fafc-411e-aea3-77d6838ec2dc/details?experience=power-bi]\n# [DC Shipment Wave|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/a07880d1-5275-4d9a-bbc5-68b416e367d5/details?experience=power-bi]\n\nh4. Store\n\n# [DFIO|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/0234fec9-bfdc-45fe-880c-18832b409343/details?experience=power-bi]\n# [Employee Safety|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/cfb5a6a9-0ad9-4bea-baf0-4f81ce8d2993/details?experience=power-bi]\n# [Store Inventory|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/ef4f1f73-051a-4464-bf57-eac4f2d82053/details?experience=power-bi]\n# [Store Sales|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/fd95fa38-7b01-4f36-b20c-19d72bc0d7ca/details?experience=power-bi]\n\nCurrently the Supply Chain team are working to re-map all their existing semantic models to reference only from these above 12 ‘master semantic models' to become as their new source. \n\nThere are currently *42 semantic models* in the Operations Analytics workspace in total (excluding their sandbox workspace). These 42 include the above 12, meaning there are currently *30 semantic models* that are created that do not form as part of this ‘master layer’.\n\nThe team are in the work-in-progress stage of this migration/re-mapping of their semantic models. For example currently the ‘S&OP Projections’ semantic model only has inputs coming from the ‘master layer’, whereas the ‘SPS KPIs’ semantic model currently has inputs only coming from a DB source (nothing from the master layer).\n\nThe advice would be:\n\n* Continue to re-map all their current semantic models to this master layer, to reduce the number of queries against the database servers\n* Work with the SC team to establish if we can group these models further (eg: can we group ‘DC Inbound Shipments’, ‘DC Outbound Shipments’ and ‘DC Shipment Wave’ as 1 single ‘DC Shipments’ model?)\n\nh3. Measures\n\nCurrently across the 42 semantic models, we have *692 unique/individual measures*. Some of these can be referencing the same value but expressing this as a different output (eg: *sales* in absolute $'s, *sales* as a %, *sales* last 60 days, etc).\n\nAttached is a list of all measures as of 16.6.25.\n\n[^Supply Chain Measures.xlsx]\n\n{color:#bf2600}*Action:*{color} Work through this list to group by KPI/measure type (eg: group sales as absolute, sales as % and sales last 60 days as ‘Sales Group’ etc).\n\nh3. Dimensions\n\nList of Dim’s used across models (list based on table name in semantic model containing ‘Dim’):\n\n* DimDFIOSKU\n* DimDFIOProduct\n* DimService\n* DimDate\n* DimItemCode\n* DimProduct\n* DimProductMAWM\n* DimProductUOM\n* DimProductUOM_MAWM\n* DimStore\n* DimOmniStore\n* DimStoreGroup\n* DimAutoStoreSKUs\n* DimDCCycleCountMasterPlan\n* DimDCLocation\n* DimDCNonShippingWeekdays\n* DimDistributionCentre\n* DimVendor\n* DimExemptedSuppliers\n* DimExpiryStatus\n* DimManufacturer\n* DimMobileDOCK\n\n{color:#bf2600}*Action:*{color} Group dim’s that can be merged as a single Dim (eg: store, omni-store and store group)\n\nh3. Facts\n\nList of Facts used across models (list based on table name in semantic model containing ‘Fact’):\n\n|Fact12MonthsDCSales|\n|Fact90DaysStoreItemTransactionSummary|\n|FactCostToServeSummary|\n|FactDCCycleCount|\n|FactDCEmployeeHoursWorked|\n|FactDCInboundASN|\n|FactDCInboundPurchaseOrdersERP|\n|FactDCInboundPurchaseOrdersERP_detail|\n|FactDCInboundPurchaseOrdersPOResponse|\n|FactDCInboundPutaway|\n|FactDCInboundPutawaySummarybyPO|\n|FactDCInboundReceipts|\n|FactDCInventoryAdjustmentsERP|\n|FactDCInventoryERP|\n|FactDCInventoryERPLatest|\n|FactDCInventoryERPSummaryByBucket|\n|FactDCInventoryLocation_MAWM|\n|FactDCInventoryLocationCombined|\n|FactDCInventoryLocationWMS|\n|FactDCInventoryLocationWMS_MAWM|\n|FactDCInventoryMovementERP|\n|FactDCInventoryPPFvsReserve|\n|FactDCInventoryRMACredits|\n|FactDCInventoryUnitCost|\n|FactDCInventoryWMSLatest|\n|FactDCLocationSummary|\n|FactDCOpenPOLinesERP|\n|FactDCOutboundSales|\n|FactDCOutboundSalesSummary|\n|FactDCOutboundShipmentContainer|\n|FactDCOutboundShipmentInvoices|\n|FactDCOutboundShipmentOrders|\n|FactDCOutboundShipmentsSummaryByERPID|\n|FactDCOutboundShipmentsSummaryByERPIDOnTime|\n|FactDCPicks|\n|FactDCPicksSummary|\n|FactDCRangingReview|\n|FactDCShipmentFailure_MAWM|\n|FactDCShipmentWave_MAWM|\n|FactDCTransferOrderReceipts|\n|FactDCTransferOrderSummary|\n|FactDCWaveVolumeSummary_MAWM|\n|FactDFIOHistoricalProjections|\n|FactDFIOLCPTracker|\n|FactDFIOProjectionsRejected|\n|FactDFIOStockOnOrder|\n|FactDFIOStoreDCBuyerNameLatest|\n|FactDockToStockSummarybyPO|\n|FactInboundOutboundSummary|\n|FactNonConformancePOList|\n|FactProductCAS|\n|FactProductSupplier|\n|FactStoreInventoryERPEOMSummary|\n|FactStoreInventoryERPLatest|\n|FactStoreInventoryERPLatestSummary|\n|FactStoreInventoryERPWeeklySummary|\n|FactStoreSalesERP|\n|FactStoreSalesERP_Last 60 Days|\n|FactStoreSalesWeeklySummary|\n|FactStoreStockSales|\n|vw_Store_IncrementalSOHChanges_Fact_CurrentDay|\n|ZZ - Fact Base Tables|\n\n{color:#bf2600}*Action:*{color} Understand the grain of these facts to see if some of these can be grouped (also to understand what fact ‘ZZ - Fact Base Tables’ refers to)\n\nNeed to append work from [https://sigmahealthcare.atlassian.net/browse/CSCI-84|https://sigmahealthcare.atlassian.net/browse/CSCI-84] to the Bus Matrix document - outlining which source tables map to which business process.\n\nWill be using tasks for now @user\n\nMoved to done with no impact to sprint points.", "text": "Summary\nBUS Matrix\n\n---\n\nDescription\n* -Grouping of different Models together ( roughly belonging to similar business process/ Source Systems)-\n* -Different measures used in models and reports-\n* KPIs used in reports\n* -Dimensions used in reports- \n* -Facts used in reports-\n\nIf you can align Measures/KPIs & dimensions  to business process to the best of your knowledge , that would be awesome. Need not be 100 % accurate.\n\n*Definition of Done:*\n- List of KPI, Measures, and Dimensions identified and linked to business processes.\n\n---\n\nComments\nh3. Grouping Semantic Models\n\nRachel and the Supply Chain team have already grouped their own semantic models into +*12 ‘master models’*+ which have already been created:\n\nh4. DC\n\n# [DC Cycle Count|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/f062aba7-efc7-4147-8cf2-7af21874a874/details?experience=power-bi]\n# [DC Employee Performance|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/2f02aa2e-3858-4015-ab7f-9f4b78495405/details?experience=power-bi]\n# [DC Inbound Shipments|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/596c4534-aae5-45ce-948c-71c836b3875d/details?experience=power-bi]\n# [DC Inventory Location|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/9deaba9d-b41b-4f5c-a3e1-19fba0e4404a/details?experience=power-bi]\n# [DC Inventory|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/d9fa86fd-cc2c-4082-bb3b-119fd7c4d3b7/details?experience=power-bi]\n# [DC MobileDOCK|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/3066b4b6-c9fb-46d9-89d2-4f6e7dbf5ff7/details?experience=power-bi]\n# [DC Outbound Shipments|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/0fd5f627-fafc-411e-aea3-77d6838ec2dc/details?experience=power-bi]\n# [DC Shipment Wave|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/a07880d1-5275-4d9a-bbc5-68b416e367d5/details?experience=power-bi]\n\nh4. Store\n\n# [DFIO|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/0234fec9-bfdc-45fe-880c-18832b409343/details?experience=power-bi]\n# [Employee Safety|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/cfb5a6a9-0ad9-4bea-baf0-4f81ce8d2993/details?experience=power-bi]\n# [Store Inventory|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/ef4f1f73-051a-4464-bf57-eac4f2d82053/details?experience=power-bi]\n# [Store Sales|https://app.powerbi.com/groups/e6f93822-e2ba-44ce-92f0-1449dfb35db1/datasets/fd95fa38-7b01-4f36-b20c-19d72bc0d7ca/details?experience=power-bi]\n\nCurrently the Supply Chain team are working to re-map all their existing semantic models to reference only from these above 12 ‘master semantic models' to become as their new source. \n\nThere are currently *42 semantic models* in the Operations Analytics workspace in total (excluding their sandbox workspace). These 42 include the above 12, meaning there are currently *30 semantic models* that are created that do not form as part of this ‘master layer’.\n\nThe team are in the work-in-progress stage of this migration/re-mapping of their semantic models. For example currently the ‘S&OP Projections’ semantic model only has inputs coming from the ‘master layer’, whereas the ‘SPS KPIs’ semantic model currently has inputs only coming from a DB source (nothing from the master layer).\n\nThe advice would be:\n\n* Continue to re-map all their current semantic models to this master layer, to reduce the number of queries against the database servers\n* Work with the SC team to establish if we can group these models further (eg: can we group ‘DC Inbound Shipments’, ‘DC Outbound Shipments’ and ‘DC Shipment Wave’ as 1 single ‘DC Shipments’ model?)\n\nh3. Measures\n\nCurrently across the 42 semantic models, we have *692 unique/individual measures*. Some of these can be referencing the same value but expressing this as a different output (eg: *sales* in absolute $'s, *sales* as a %, *sales* last 60 days, etc).\n\nAttached is a list of all measures as of 16.6.25.\n\n[^Supply Chain Measures.xlsx]\n\n{color:#bf2600}*Action:*{color} Work through this list to group by KPI/measure type (eg: group sales as absolute, sales as % and sales last 60 days as ‘Sales Group’ etc).\n\nh3. Dimensions\n\nList of Dim’s used across models (list based on table name in semantic model containing ‘Dim’):\n\n* DimDFIOSKU\n* DimDFIOProduct\n* DimService\n* DimDate\n* DimItemCode\n* DimProduct\n* DimProductMAWM\n* DimProductUOM\n* DimProductUOM_MAWM\n* DimStore\n* DimOmniStore\n* DimStoreGroup\n* DimAutoStoreSKUs\n* DimDCCycleCountMasterPlan\n* DimDCLocation\n* DimDCNonShippingWeekdays\n* DimDistributionCentre\n* DimVendor\n* DimExemptedSuppliers\n* DimExpiryStatus\n* DimManufacturer\n* DimMobileDOCK\n\n{color:#bf2600}*Action:*{color} Group dim’s that can be merged as a single Dim (eg: store, omni-store and store group)\n\nh3. Facts\n\nList of Facts used across models (list based on table name in semantic model containing ‘Fact’):\n\n|Fact12MonthsDCSales|\n|Fact90DaysStoreItemTransactionSummary|\n|FactCostToServeSummary|\n|FactDCCycleCount|\n|FactDCEmployeeHoursWorked|\n|FactDCInboundASN|\n|FactDCInboundPurchaseOrdersERP|\n|FactDCInboundPurchaseOrdersERP_detail|\n|FactDCInboundPurchaseOrdersPOResponse|\n|FactDCInboundPutaway|\n|FactDCInboundPutawaySummarybyPO|\n|FactDCInboundReceipts|\n|FactDCInventoryAdjustmentsERP|\n|FactDCInventoryERP|\n|FactDCInventoryERPLatest|\n|FactDCInventoryERPSummaryByBucket|\n|FactDCInventoryLocation_MAWM|\n|FactDCInventoryLocationCombined|\n|FactDCInventoryLocationWMS|\n|FactDCInventoryLocationWMS_MAWM|\n|FactDCInventoryMovementERP|\n|FactDCInventoryPPFvsReserve|\n|FactDCInventoryRMACredits|\n|FactDCInventoryUnitCost|\n|FactDCInventoryWMSLatest|\n|FactDCLocationSummary|\n|FactDCOpenPOLinesERP|\n|FactDCOutboundSales|\n|FactDCOutboundSalesSummary|\n|FactDCOutboundShipmentContainer|\n|FactDCOutboundShipmentInvoices|\n|FactDCOutboundShipmentOrders|\n|FactDCOutboundShipmentsSummaryByERPID|\n|FactDCOutboundShipmentsSummaryByERPIDOnTime|\n|FactDCPicks|\n|FactDCPicksSummary|\n|FactDCRangingReview|\n|FactDCShipmentFailure_MAWM|\n|FactDCShipmentWave_MAWM|\n|FactDCTransferOrderReceipts|\n|FactDCTransferOrderSummary|\n|FactDCWaveVolumeSummary_MAWM|\n|FactDFIOHistoricalProjections|\n|FactDFIOLCPTracker|\n|FactDFIOProjectionsRejected|\n|FactDFIOStockOnOrder|\n|FactDFIOStoreDCBuyerNameLatest|\n|FactDockToStockSummarybyPO|\n|FactInboundOutboundSummary|\n|FactNonConformancePOList|\n|FactProductCAS|\n|FactProductSupplier|\n|FactStoreInventoryERPEOMSummary|\n|FactStoreInventoryERPLatest|\n|FactStoreInventoryERPLatestSummary|\n|FactStoreInventoryERPWeeklySummary|\n|FactStoreSalesERP|\n|FactStoreSalesERP_Last 60 Days|\n|FactStoreSalesWeeklySummary|\n|FactStoreStockSales|\n|vw_Store_IncrementalSOHChanges_Fact_CurrentDay|\n|ZZ - Fact Base Tables|\n\n{color:#bf2600}*Action:*{color} Understand the grain of these facts to see if some of these can be grouped (also to understand what fact ‘ZZ - Fact Base Tables’ refers to)\n\nNeed to append work from [https://sigmahealthcare.atlassian.net/browse/CSCI-84|https://sigmahealthcare.atlassian.net/browse/CSCI-84] to the Bus Matrix document - outlining which source tables map to which business process.\n\nWill be using tasks for now @user\n\nMoved to done with no impact to sprint points.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 506}}
{"issue_key": "CSCI-49", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Story", "status": "Done", "priority": "Medium", "created": "23/Apr/25 3:32 PM", "updated": "09/Jul/25 12:37 PM", "labels": [], "summary": "Discovery - Engage infra set up for Azure tenancy", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nDiscovery - Engage infra set up for Azure tenancy", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 507}}
{"issue_key": "CSCI-48", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Story", "status": "Done", "priority": "Medium", "created": "23/Apr/25 3:32 PM", "updated": "09/Jul/25 2:57 PM", "labels": [], "summary": "Story - Source Snowflake Azure Tenancy", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nStory - Source Snowflake Azure Tenancy", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 508}}
{"issue_key": "CSCI-47", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Story", "status": "Done", "priority": "Medium", "created": "12/May/25 11:04 AM", "updated": "09/Jul/25 12:37 PM", "labels": ["Planning"], "summary": "Data Source Identification", "description": "Needing to identify and document where data sources are (and where unknowns are) so that we can model data effectively", "acceptance_criteria": "", "comments": "expected 22/5\n\nSource Systems are documented", "text": "Summary\nData Source Identification\n\n---\n\nDescription\nNeeding to identify and document where data sources are (and where unknowns are) so that we can model data effectively\n\n---\n\nComments\nexpected 22/5\n\nSource Systems are documented", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 509}}
{"issue_key": "CSCI-46", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Jun/25 3:18 PM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "Snowflake - Environment Setup", "description": "To setup high level Snowflake structure for Chemist Warehouse which follows similar design and naming conventions already established for Sigma Healthcare.", "acceptance_criteria": "", "comments": "[^high level org CW.xlsx]\n\n \n\n{adf:display=block}\n{\"type\":\"table\",\"attrs\":{\"isNumberColumnEnabled\":false,\"layout\":\"center\",\"localId\":\"d298ba13-74bf-4ac0-8517-1f606bf29616\"},\"content\":[{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"EDP_PROD\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":11},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"A single database will separate data layers by schema.\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"CONTROL\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":11},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Control Schema -  metadata, configuration settings, control tables, and job orchestration information\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"LDN\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":11},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Landing Schema – untransformed copies of source system data held for a short period\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"STG\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":11},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Staging Schema – historical data (untransformed) and transformed data joining relevant data sources and applying stipulated business rules\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"STG_TRNS\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":11},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Staging Transformed Schema – transformed data joining relevant data sources and applying stipulated business rules\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"PRES\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":11},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Presentation Schema – information presented to users in a clear structure to support pre-defined and self-service reporting\"}]}]}]}]}\n{adf}", "text": "Summary\nSnowflake - Environment Setup\n\n---\n\nDescription\nTo setup high level Snowflake structure for Chemist Warehouse which follows similar design and naming conventions already established for Sigma Healthcare.\n\n---\n\nComments\n[^high level org CW.xlsx]\n\n \n\n{adf:display=block}\n{\"type\":\"table\",\"attrs\":{\"isNumberColumnEnabled\":false,\"layout\":\"center\",\"localId\":\"d298ba13-74bf-4ac0-8517-1f606bf29616\"},\"content\":[{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"EDP_PROD\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":11},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"A single database will separate data layers by schema.\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"CONTROL\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":11},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Control Schema -  metadata, configuration settings, control tables, and job orchestration information\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"LDN\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":11},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Landing Schema – untransformed copies of source system data held for a short period\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"STG\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":11},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Staging Schema – historical data (untransformed) and transformed data joining relevant data sources and applying stipulated business rules\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"STG_TRNS\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":11},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Staging Transformed Schema – transformed data joining relevant data sources and applying stipulated business rules\"}]}]}]},{\"type\":\"tableRow\",\"content\":[{\"type\":\"tableCell\",\"attrs\":{},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"PRES\"}]}]},{\"type\":\"tableCell\",\"attrs\":{\"colspan\":11},\"content\":[{\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":\"Presentation Schema – information presented to users in a clear structure to support pre-defined and self-service reporting\"}]}]}]}]}\n{adf}", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 510}}
{"issue_key": "CSCI-45", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Jun/25 3:15 PM", "updated": "03/Jul/25 10:04 AM", "labels": [], "summary": "End to end data flow - TBD14 - BrandOrders", "description": "End to end data flow testing from on-prem source Transaction Storage (TBD14 - BrandOrders) to Snowflake. \n\nBeing blocked by Storage Integration. \n\n*Definition of done*", "acceptance_criteria": "", "comments": "Create a test pipeline based on source Transaction Storage (TBD14 - BrandOrders)\n\n@user is it okay if you entered in definition of done for this task? thanks!\n\nStorage Integration was enabled yesterday however missing firewall rules to allow reversed traffic. Error: \n\nAnjali to create a SNOW ticket and Enda/ cloud team to action next steps.\n\nAwaiting Enda/Brent action on SNOW ticket, Harrison following up given Enda is now on leave till end of July.\n\nThanks,\n\nHarrison\n\nStorage Integration completed. Pipeline ran end to end.", "text": "Summary\nEnd to end data flow - TBD14 - BrandOrders\n\n---\n\nDescription\nEnd to end data flow testing from on-prem source Transaction Storage (TBD14 - BrandOrders) to Snowflake. \n\nBeing blocked by Storage Integration. \n\n*Definition of done*\n\n---\n\nComments\nCreate a test pipeline based on source Transaction Storage (TBD14 - BrandOrders)\n\n@user is it okay if you entered in definition of done for this task? thanks!\n\nStorage Integration was enabled yesterday however missing firewall rules to allow reversed traffic. Error: \n\nAnjali to create a SNOW ticket and Enda/ cloud team to action next steps.\n\nAwaiting Enda/Brent action on SNOW ticket, Harrison following up given Enda is now on leave till end of July.\n\nThanks,\n\nHarrison\n\nStorage Integration completed. Pipeline ran end to end.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 511}}
{"issue_key": "CSCI-44", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Jun/25 3:14 PM", "updated": "16/Jul/25 2:17 PM", "labels": [], "summary": "Azure Devops setup - CICD pipeline", "description": "", "acceptance_criteria": "", "comments": "Eugene created the following repo for Data Factory: [edp-data-factory - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory?version=GBmaster]\n\nPipeline is setup and ready for code deployment.\n\nChloe successfully tested with feature branch.\n\nPR for CI created at: [Pull request 21856: Implemented #192947 - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/a9d4deeb-ad55-465e-9425-438e29d991d5/pullrequest/21856]\n\nSample pipeline run can be viewed at: [Pipelines - Run 20250619.4|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_build/results?buildId=32990&view=results]\n\nUpdated branch policies also to include the CI check on PRs:", "text": "Summary\nAzure Devops setup - CICD pipeline\n\n---\n\nComments\nEugene created the following repo for Data Factory: [edp-data-factory - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/edp-data-factory?version=GBmaster]\n\nPipeline is setup and ready for code deployment.\n\nChloe successfully tested with feature branch.\n\nPR for CI created at: [Pull request 21856: Implemented #192947 - Repos|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_git/a9d4deeb-ad55-465e-9425-438e29d991d5/pullrequest/21856]\n\nSample pipeline run can be viewed at: [Pipelines - Run 20250619.4|https://dev.azure.com/MyChemist/Enterprise%20Data%20Platform%20Implementation/_build/results?buildId=32990&view=results]\n\nUpdated branch policies also to include the CI check on PRs:", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 512}}
{"issue_key": "CSCI-43", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "23/May/25 1:06 PM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "Azure network and infrastructure", "description": "Effort and ETA required for Network and Infrastructure teams in IT to allow CW Snowflake setup before Chloe can begin work.\n\nCK and Xavier to provide details post conversation with Rod on resource availability and effort involved", "acceptance_criteria": "", "comments": "outcome - ETA of the work - as this is the dependency Chloe has for setup of CW snowflake\n\nHavent' got an eta yet but getting breakdown of work @user\n\n@user FYI\n\nADF instances created\n\nSelf-hosted integration runtime (SHIR) created", "text": "Summary\nAzure network and infrastructure\n\n---\n\nDescription\nEffort and ETA required for Network and Infrastructure teams in IT to allow CW Snowflake setup before Chloe can begin work.\n\nCK and Xavier to provide details post conversation with Rod on resource availability and effort involved\n\n---\n\nComments\noutcome - ETA of the work - as this is the dependency Chloe has for setup of CW snowflake\n\nHavent' got an eta yet but getting breakdown of work @user\n\n@user FYI\n\nADF instances created\n\nSelf-hosted integration runtime (SHIR) created", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 513}}
{"issue_key": "CSCI-37", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Jun/25 1:25 PM", "updated": "17/Jul/25 3:12 PM", "labels": [], "summary": "Business Matrix Review", "description": "To be done after Jess does her BI modelling task\n\nObjective:\n- To review Fact - Dim table concept so that it will be able to be used to create data visualisations/Data products.\n\n[Business Matrix_Supply_Chain.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true]", "acceptance_criteria": "", "comments": "reviewed the dim tables and shared my comments in the spread sheet\n\nNeed few inputs from @user on newly added dimensions to review and add comments\n\n|Product UOM|\n|Manufacturer|\n|Service Region|\n|DC Master Plan|\n|Sales Purchase Type|\n\nSession to be booked for Bus Matrix Review for making decisions. @user to book\n\nUploaded EDP project Data Source Details document\n\nTo review previous BUS matrix from EDP 1.0\n\n @user \n\nto be followed up on tuesday\n\nReview in Progress . Have reviewed couple of docs  . Added my findings in a separate Tab.  Will add more tomorrow\n\n \n\nHi @user & @user , will connect you tomorrow  on how to accommodate them in our main doc\n\nCreated DIM_Currency Table Structure. Team to provide comments@user @user\n\nReview complete. Observations are added to Matrix spread sheet.", "text": "Summary\nBusiness Matrix Review\n\n---\n\nDescription\nTo be done after Jess does her BI modelling task\n\nObjective:\n- To review Fact - Dim table concept so that it will be able to be used to create data visualisations/Data products.\n\n[Business Matrix_Supply_Chain.xlsx|https://mychemist.sharepoint.com/:x:/r/sites/TheLanding/MergeCoData/_layouts/15/Doc.aspx?sourcedoc=%7BCB154177-0007-4F2E-A340-19D98B5C6170%7D&file=Business%20Matrix_Supply_Chain.xlsx&action=default&mobileredirect=true]\n\n---\n\nComments\nreviewed the dim tables and shared my comments in the spread sheet\n\nNeed few inputs from @user on newly added dimensions to review and add comments\n\n|Product UOM|\n|Manufacturer|\n|Service Region|\n|DC Master Plan|\n|Sales Purchase Type|\n\nSession to be booked for Bus Matrix Review for making decisions. @user to book\n\nUploaded EDP project Data Source Details document\n\nTo review previous BUS matrix from EDP 1.0\n\n @user \n\nto be followed up on tuesday\n\nReview in Progress . Have reviewed couple of docs  . Added my findings in a separate Tab.  Will add more tomorrow\n\n \n\nHi @user & @user , will connect you tomorrow  on how to accommodate them in our main doc\n\nCreated DIM_Currency Table Structure. Team to provide comments@user @user\n\nReview complete. Observations are added to Matrix spread sheet.", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 514}}
{"issue_key": "CSCI-36", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "05/Jun/25 3:13 PM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "ADF - DLA setup", "description": "Objective: To create Data Ingestion Framework in ADF\n\nDescription:\n\n* Import DLA extraction templates \n* Create Linked Services (snowflake private link, key vault and blob storage, service user etc.)\n* Create Global environments (DB, Warehouse, Service Users etc.)", "acceptance_criteria": "", "comments": "High level setup has been complete:\n\n* Linked Services created (snowflake private link, key vault and blob storage, service user etc.)\n* Global environments created (DB, Warehouse, Service Users etc.)", "text": "Summary\nADF - DLA setup\n\n---\n\nDescription\nObjective: To create Data Ingestion Framework in ADF\n\nDescription:\n\n* Import DLA extraction templates \n* Create Linked Services (snowflake private link, key vault and blob storage, service user etc.)\n* Create Global environments (DB, Warehouse, Service Users etc.)\n\n---\n\nComments\nHigh level setup has been complete:\n\n* Linked Services created (snowflake private link, key vault and blob storage, service user etc.)\n* Global environments created (DB, Warehouse, Service Users etc.)", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 515}}
{"issue_key": "CSCI-34", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Done", "priority": "Medium", "created": "16/Jun/25 11:04 AM", "updated": "24/Jun/25 2:39 PM", "labels": [], "summary": "Amit > Jess - Semantic model handover", "description": "Actionable handover points:\n\n* What models are identified\n* What is unsure\n* What needs to be modelled\n* Which reports (that we know of ) are these models contributing to.", "acceptance_criteria": "", "comments": "Hi All,\n\nWe had the handover meeting and below are the tasks needs to be done \n\n1. Document Source Systems Models and Reports. Jess is already doing this\n2. Is there any similar reports coming from two different source system\n\n# Capture all Flat/Junk Dimensions used in existing reporting\n# Capture different kind of Date dimensions used in existing reporting. Like Order Date, Invoiced Date, Shipped Date, Delivery Date etc.\n# Grouping of Measures: group all time intelligence KPIs coming from same Measure into one bucket\n# Looks for any source system that is not captured in Bus Matrix document\n# Look for Data sets ingested from Mobile Dock . These are not captured in documents\n# Look for Data Sets captured from MAWM\n\n @user @user@user", "text": "Summary\nAmit > Jess - Semantic model handover\n\n---\n\nDescription\nActionable handover points:\n\n* What models are identified\n* What is unsure\n* What needs to be modelled\n* Which reports (that we know of ) are these models contributing to.\n\n---\n\nComments\nHi All,\n\nWe had the handover meeting and below are the tasks needs to be done \n\n1. Document Source Systems Models and Reports. Jess is already doing this\n2. Is there any similar reports coming from two different source system\n\n# Capture all Flat/Junk Dimensions used in existing reporting\n# Capture different kind of Date dimensions used in existing reporting. Like Order Date, Invoiced Date, Shipped Date, Delivery Date etc.\n# Grouping of Measures: group all time intelligence KPIs coming from same Measure into one bucket\n# Looks for any source system that is not captured in Bus Matrix document\n# Look for Data sets ingested from Mobile Dock . These are not captured in documents\n# Look for Data Sets captured from MAWM\n\n @user @user@user", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 516}}
{"issue_key": "CSCI-33", "project_key": "CSCI", "project_name": "CW Cloud Data Platform Interim Solution", "issue_type": "Task", "status": "Duplicate", "priority": "Medium", "created": "16/Jun/25 11:04 AM", "updated": "20/Jun/25 1:23 PM", "labels": [], "summary": "Documentation of existing CW supply chain - Semantic models", "description": "", "acceptance_criteria": "", "comments": "", "text": "Summary\nDocumentation of existing CW supply chain - Semantic models", "source": {"type": "csv", "name": "CSCI.csv", "row_index": 517}}
